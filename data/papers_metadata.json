[
  {
    "arxiv_id": "1707.06347v2",
    "title": "Proximal Policy Optimization Algorithms",
    "authors": [
      "John Schulman",
      "Filip Wolski",
      "Prafulla Dhariwal",
      "Alec Radford",
      "Oleg Klimov"
    ],
    "published": "2017-07-20T02:32:33+00:00",
    "summary": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
    "pdf_path": "data/papers/1707.06347v2.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1707.06347v2"
  },
  {
    "arxiv_id": "1602.01783v2",
    "title": "Asynchronous Methods for Deep Reinforcement Learning",
    "authors": [
      "Volodymyr Mnih",
      "Adri\u00e0 Puigdom\u00e8nech Badia",
      "Mehdi Mirza",
      "Alex Graves",
      "Timothy P. Lillicrap",
      "Tim Harley",
      "David Silver",
      "Koray Kavukcuoglu"
    ],
    "published": "2016-02-04T18:38:41+00:00",
    "summary": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
    "pdf_path": "data/papers/1602.01783v2.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1602.01783v2"
  },
  {
    "arxiv_id": "1801.01290v2",
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "authors": [
      "Tuomas Haarnoja",
      "Aurick Zhou",
      "Pieter Abbeel",
      "Sergey Levine"
    ],
    "published": "2018-01-04T09:50:50+00:00",
    "summary": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "pdf_path": "data/papers/1801.01290v2.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1801.01290v2"
  },
  {
    "arxiv_id": "1706.03762v7",
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "published": "2017-06-12T17:57:34+00:00",
    "summary": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "pdf_path": "data/papers/1706.03762v7.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1706.03762v7"
  },
  {
    "arxiv_id": "1703.03864v2",
    "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
    "authors": [
      "Tim Salimans",
      "Jonathan Ho",
      "Xi Chen",
      "Szymon Sidor",
      "Ilya Sutskever"
    ],
    "published": "2017-03-10T23:02:19+00:00",
    "summary": "We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.",
    "pdf_path": "data/papers/1703.03864v2.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1703.03864v2"
  },
  {
    "arxiv_id": "1801.01290v2",
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "authors": [
      "Tuomas Haarnoja",
      "Aurick Zhou",
      "Pieter Abbeel",
      "Sergey Levine"
    ],
    "published": "2018-01-04T09:50:50+00:00",
    "summary": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "pdf_path": "data/papers/1801.01290v2.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1801.01290v2"
  },
  {
    "arxiv_id": "1906.10667v1",
    "title": "Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives",
    "authors": [
      "Anirudh Goyal",
      "Shagun Sodhani",
      "Jonathan Binas",
      "Xue Bin Peng",
      "Sergey Levine",
      "Yoshua Bengio"
    ],
    "published": "2019-06-25T17:04:48+00:00",
    "summary": "Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.",
    "pdf_path": "data/papers/1906.10667v1.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1906.10667v1"
  },
  {
    "arxiv_id": "1911.08265v2",
    "title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
    "authors": [
      "Julian Schrittwieser",
      "Ioannis Antonoglou",
      "Thomas Hubert",
      "Karen Simonyan",
      "Laurent Sifre",
      "Simon Schmitt",
      "Arthur Guez",
      "Edward Lockhart",
      "Demis Hassabis",
      "Thore Graepel",
      "Timothy Lillicrap",
      "David Silver"
    ],
    "published": "2019-11-19T13:58:52+00:00",
    "summary": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",
    "pdf_path": "data/papers/1911.08265v2.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/1911.08265v2"
  },
  {
    "arxiv_id": "2009.01325v3",
    "title": "Learning to summarize from human feedback",
    "authors": [
      "Nisan Stiennon",
      "Long Ouyang",
      "Jeff Wu",
      "Daniel M. Ziegler",
      "Ryan Lowe",
      "Chelsea Voss",
      "Alec Radford",
      "Dario Amodei",
      "Paul Christiano"
    ],
    "published": "2020-09-02T19:54:41+00:00",
    "summary": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.",
    "pdf_path": "data/papers/2009.01325v3.pdf",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2009.01325v3"
  }
]