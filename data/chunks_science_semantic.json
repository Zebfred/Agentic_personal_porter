[
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Abstract\n\nWe propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\u201csurrogate\u201d objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\nproximal policy optimization (PPO), have some of the bene\ufb01ts of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 951,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\nlarge models and parallel implementations), data e\ufb03cient, and robust (i.e., successful on a variety\nof problems without hyperparameter tuning). Q-learning (with function approximation) fails on\nmany simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\ne\ufb03ency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\n(between the policy and value function, or with auxiliary tasks).",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 924,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nThis paper seeks to improve the current state of a\ufb00airs by introducing an algorithm that attains\nthe data e\ufb03ciency and reliable performance of TRPO, while using only \ufb01rst-order optimization. We propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\n(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between\nsampling data from the policy and performing several epochs of optimization on the sampled data. Our experiments compare the performance of various di\ufb00erent versions of the surrogate objec-\ntive, and \ufb01nd that the version with the clipped probability ratios performs best. We also compare\nPPO to several previous algorithms from the literature. On continuous control tasks, it performs\nbetter than the algorithms we compare against. On Atari, it performs signi\ufb01cantly better (in terms\nof sample complexity) than A2C and similarly to ACER though it is much simpler.",
    "chunk_index": 1,
    "num_sentences": 7,
    "chunk_size": 948,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\n1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete\naction spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in\nOpenAI Gym [Bro+16] and described by Duan et al. 1\narXiv:1707.06347v2  [cs.LG]  28 Aug 2017\n\n\n2\nBackground: Policy Optimization\n2.1",
    "chunk_index": 2,
    "num_sentences": 2,
    "chunk_size": 344,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Policy gradient methods work by computing an estimator of the policy gradient and plugging it\n\ninto a stochastic gradient ascent algorithm. The most commonly used gradient estimator has the\nform\n\u02c6g = \u02c6Et\nh\n\u2207\u03b8 log \u03c0\u03b8(at | st) \u02c6At\ni\n(1)\nwhere \u03c0\u03b8 is a stochastic policy and \u02c6At is an estimator of the advantage function at timestep t. Here, the expectation \u02c6Et[. .] indicates the empirical average over a \ufb01nite batch of samples, in an\nalgorithm that alternates between sampling and optimization. Implementations that use automatic\ndi\ufb00erentiation software work by constructing an objective function whose gradient is the policy\ngradient estimator; the estimator \u02c6g is obtained by di\ufb00erentiating the objective\nLPG(\u03b8) = \u02c6Et\nh\nlog \u03c0\u03b8(at | st) \u02c6At\ni\n.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 648,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Policy gradient methods work by computing an estimator of the policy gradient and plugging it",
      "section_index": 2,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: While it is appealing to perform multiple steps of optimization on this loss LPG using the same\n\ntrajectory, doing so is not well-justi\ufb01ed, and empirically it often leads to destructively large policy\nupdates (see Section 6.1; results are not shown but were similar or worse than the \u201cno clipping or\npenalty\u201d setting).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 221,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "While it is appealing to perform multiple steps of optimization on this loss LPG using the same",
      "section_index": 3,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n(3)\nsubject to\n\u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]] \u2264\u03b4. (4)\nHere, \u03b8old is the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint. The theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 972,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nTRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2 that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, experiments show\nthat it is not su\ufb03cient to simply choose a \ufb01xed penalty coe\ufb03cient \u03b2 and optimize the penalized\nobjective Equation (5) with SGD; additional modi\ufb01cations are required.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 526,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\n\u201csurrogate\u201d objective\nLCPI(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n= \u02c6Et\nh\nrt(\u03b8) \u02c6At\ni\n. (6)\nThe superscript CPI refers to conservative policy iteration [KL02], where this objective was pro-\nposed. Without a constraint, maximization of LCPI would lead to an excessively large policy\nupdate; hence, we now consider how to modify the objective, to penalize changes to the policy that\nmove rt(\u03b8) away from 1. The main objective we propose is the following:\nLCLIP (\u03b8) = \u02c6Et\nh\nmin(rt(\u03b8) \u02c6At, clip(rt(\u03b8), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At)\ni\n(7)\nwhere epsilon is a hyperparameter, say, \u03f5 = 0.2. The motivation for this objective is as follows. The\n\ufb01rst term inside the min is LCPI. The second term, clip(rt(\u03b8), 1\u2212\u03f5, 1+\u03f5) \u02c6At, modi\ufb01es the surrogate\nobjective by clipping the probability ratio, which removes the incentive for moving rt outside of the\ninterval [1 \u2212\u03f5, 1 + \u03f5].",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 854,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nFinally, we take the minimum of the clipped and unclipped objective, so the\n\ufb01nal objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective. With this\nscheme, we only ignore the change in probability ratio when it would make the objective improve,\nand we include it when it makes the objective worse. Note that LCLIP (\u03b8) = LCPI(\u03b8) to \ufb01rst order\naround \u03b8old (i.e., where r = 1), however, they become di\ufb00erent as \u03b8 moves away from \u03b8old. Figure 1\nplots a single term (i.e., a single t) in LCLIP ; note that the probability ratio r is clipped at 1 \u2212\u03f5\nor 1 + \u03f5 depending on whether the advantage is positive or negative.",
    "chunk_index": 1,
    "num_sentences": 4,
    "chunk_size": 638,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\n0\n1\n1 \u2212\u03f5\nA < 0\nFigure 1: Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of\nthe probability ratio r, for positive advantages (left) and negative advantages (right). The red circle on each\nplot shows the starting point for the optimization, i.e., r = 1. Note that LCLIP sums many of these terms. Figure 2 provides another source of intuition about the surrogate objective LCLIP . It shows how\nseveral objectives vary as we interpolate along the policy update direction, obtained by proximal\npolicy optimization (the algorithm we will introduce shortly) on a continuous control problem. We\ncan see that LCLIP is a lower bound on LCPI, with a penalty for having too large of a policy\nupdate.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 735,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 6,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nEt[KLt]\nLCPI = Et[rtAt]\nEt[clip(rt, 1\n, 1 + )At]\nLCLIP = Et[min(rtAt, clip(rt, 1\n, 1 + )At)]\nFigure 2: Surrogate objectives, as we interpolate between the initial policy parameter \u03b8old, and the updated\npolicy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of\nabout 0.02 from the initial policy, and this is the point at which LCLIP is maximal. This plot corresponds\nto the \ufb01rst policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1. 4\nAdaptive KL Penalty Coe\ufb03cient\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in\naddition to it, is to use a penalty on KL divergence, and to adapt the penalty coe\ufb03cient so that we\nachieve some target value of the KL divergence dtarg each policy update.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 853,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\nIn our experiments, we\nfound that the KL penalty performed worse than the clipped surrogate objective, however, we\u2019ve\nincluded it here because it\u2019s an important baseline. In the simplest instantiation of this algorithm, we perform the following steps in each policy\nupdate:\n\u2022 Using several epochs of minibatch SGD, optimize the KL-penalized objective\nLKLPEN(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(8)\n\u2022 Compute d = \u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]]\n\u2013 If d < dtarg/1.5, \u03b2 \u2190\u03b2/2\n\u2013 If d > dtarg \u00d7 1.5, \u03b2 \u2190\u03b2 \u00d7 2\nThe updated \u03b2 is used for the next policy update. With this scheme, we occasionally see policy\nupdates where the KL divergence is signi\ufb01cantly di\ufb00erent from dtarg, however, these are rare, and\n\u03b2 quickly adjusts. The parameters 1.5 and 2 above are chosen heuristically, but the algorithm is\nnot very sensitive to them. The initial value of \u03b2 is a another hyperparameter but is not important\nin practice because the algorithm quickly adjusts it.",
    "chunk_index": 1,
    "num_sentences": 5,
    "chunk_size": 989,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nThe surrogate losses from the previous sections can be computed and di\ufb00erentiated with a minor\nchange to a typical policy gradient implementation. For implementations that use automatic dif-\nferentation, one simply constructs the loss LCLIP or LKLPEN instead of LPG, and one performs\nmultiple steps of stochastic gradient ascent on this objective. Most techniques for computing variance-reduced advantage-function estimators make use a\nlearned state-value function V (s); for example, generalized advantage estimation [Sch+15a], or the\n4\n\n\n\ufb01nite-horizon estimators in [Mni+16]. If using a neural network architecture that shares parameters\nbetween the policy and value function, we must use a loss function that combines the policy\nsurrogate and a value function error term. This objective can further be augmented by adding\nan entropy bonus to ensure su\ufb03cient exploration, as suggested in past work [Wil92; Mni+16].",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 916,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nCombining these terms, we obtain the following objective, which is (approximately) maximized\neach iteration:\nLCLIP+V F+S\nt\n(\u03b8) = \u02c6Et\n\u0002",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 134,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nt\n(\u03b8) \u2212c1LV F\nt\n(\u03b8) + c2S[\u03c0\u03b8](st)\n\u0003\n,\n(9)\nwhere c1, c2 are coe\ufb03cients, and S denotes an entropy bonus, and LV F\nt\nis a squared-error loss\n(V\u03b8(st) \u2212V targ\nt\n)2. One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\nwith recurrent neural networks, runs the policy for T timesteps (where T is much less than the\nepisode length), and uses the collected samples for an update. This style requires an advantage\nestimator that does not look beyond timestep T. The estimator used by [Mni+16] is\n\u02c6At = \u2212V (st) + rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3T\u2212t+1rT\u22121 + \u03b3T\u2212tV (sT )\n(10)\nwhere t speci\ufb01es the time index in [0, T], within a given length-T trajectory segment.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 679,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nGeneralizing\nthis choice, we can use a truncated version of generalized advantage estimation, which reduces to\nEquation (10) when \u03bb = 1:\n\u02c6At = \u03b4t + (\u03b3\u03bb)\u03b4t+1 + \u00b7 \u00b7 \u00b7 + \u00b7 \u00b7 \u00b7 + (\u03b3\u03bb)T\u2212t+1\u03b4T\u22121,\n(11)\nwhere\n\u03b4t = rt + \u03b3V (st+1) \u2212V (st)\n(12)\nA proximal policy optimization (PPO) algorithm that uses \ufb01xed-length trajectory segments is\nshown below. Each iteration, each of N (parallel) actors collect T timesteps of data. Then we\nconstruct the surrogate loss on these NT timesteps of data, and optimize it with minibatch SGD\n(or usually for better performance, Adam [KB14]), for K epochs. Algorithm 1 PPO, Actor-Critic Style\nfor iteration=1, 2, . do\nfor actor=1, 2, . , N do\nRun policy \u03c0\u03b8old in environment for T timesteps\nCompute advantage estimates \u02c6A1, . , \u02c6AT\nend for\nOptimize surrogate L wrt \u03b8, with K epochs and minibatch size M \u2264NT\n\u03b8old \u2190\u03b8\nend for",
    "chunk_index": 1,
    "num_sentences": 7,
    "chunk_size": 844,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIP to several natural variations and ablated versions. No clipping or penalty:\nLt(\u03b8) = rt(\u03b8) \u02c6At\nClipping:\nLt(\u03b8) = min(rt(\u03b8) \u02c6At, clip(rt(\u03b8)), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At\nKL penalty (\ufb01xed or adaptive)\nLt(\u03b8) = rt(\u03b8) \u02c6At \u2212\u03b2 KL[\u03c0\u03b8old, \u03c0\u03b8]\n5\n\n\nFor the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2 or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better. Because we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 920,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nBesides the hyperparameters used for clipping (\u03f5)\nand the KL penalty (\u03b2, dtarg), which we search over, the other hyperparameters are provided in in\nTable 3. To represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1 is irrelevant), and we don\u2019t use an entropy bonus. Each algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting. The results are shown in Table 1.",
    "chunk_index": 1,
    "num_sentences": 7,
    "chunk_size": 938,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nNote that the score is negative for the setting without clipping\nor penalties, because for one environment (half cheetah) it leads to a very negative score, which is\nworse than the initial random policy. algorithm\navg. normalized score",
    "chunk_index": 2,
    "num_sentences": 3,
    "chunk_size": 235,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: No clipping or penalty\n\n-0.39\nClipping, \u03f5 = 0.1\n0.76\nClipping, \u03f5 = 0.2\n0.82\nClipping, \u03f5 = 0.3\n0.70\nAdaptive KL dtarg = 0.003\n0.68\nAdaptive KL dtarg = 0.01\n0.74\nAdaptive KL dtarg = 0.03\n0.71\nFixed KL, \u03b2 = 0.3\n0.62\nFixed KL, \u03b2 = 1. 0.71\nFixed KL, \u03b2 = 3. 0.72\nFixed KL, \u03b2 = 10. 0.69\nTable 1: Results from continuous control benchmark. Average normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2 was initialized at 1.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 454,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "No clipping or penalty",
      "section_index": 11,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nNext, we compare PPO (with the \u201cclipped\u201d surrogate objective from Section 3) to several other\nmethods from the literature, which are considered to be e\ufb00ective for continuous problems. We com-\npared against tuned implementations of the following algorithms: trust region policy optimization\n[Sch+15b], cross-entropy method (CEM) [SL06], vanilla policy gradient with adaptive stepsize3,\n2HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, all \u201c-v1\u201d\n3After each batch of data, the Adam stepsize is adjusted based on the KL divergence of the original and updated\npolicy, using a rule similar to the one shown in Section 4. An implementation is available at https://github.com/\nberkeleydeeprlcourse/homework/tree/master/hw4. 6\n\n\nA2C [Mni+16], A2C with trust region [Wan+16]. A2C stands for advantage actor critic, and is\na synchronous version of A3C, which we found to have the same or better performance than the\nasynchronous version.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 974,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nFor PPO, we used the hyperparameters from the previous section, with\n\u03f5 = 0.2. We see that PPO outperforms the previous methods on almost all the continuous control\nenvironments. 0\n1000000\n500\n0\n500\n1000\n1500\n2000\nHalfCheetah-v1\n0\n1000000\n0\n500\n1000\n1500\n2000\n2500\nHopper-v1\n0\n1000000\n0\n2000\n4000\n6000\n8000\nInvertedDoublePendulum-v1\n0\n1000000\n0\n200\n400\n600\n800\n1000\nInvertedPendulum-v1\n0\n1000000\n120\n100\n80\n60\n40\n20\nReacher-v1\n0\n1000000\n0\n20\n40\n60\n80\n100\n120\nSwimmer-v1\n0\n1000000\n0\n1000\n2000\n3000\nWalker2d-v1\nA2C\nA2C + Trust Region",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 530,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\nFigure 3: Comparison of several algorithms on several MuJoCo environments, training for one million\ntimesteps. 6.3\nShowcase in the Continuous Domain: Humanoid Running and Steering\nTo showcase the performance of PPO on high-dimensional continuous control problems, we train\non a set of problems involving a 3D humanoid, where the robot must run, steer, and get up\no\ufb00the ground, possibly while being pelted by cubes. The three tasks we test on are (1) Ro-\nboschoolHumanoid: forward locomotion only, (2) RoboschoolHumanoidFlagrun: position of target\nis randomly varied every 200 timesteps or whenever the goal is reached, (3) RoboschoolHumanoid-\nFlagrunHarder, where the robot is pelted by cubes and needs to get up o\ufb00the ground. See Figure 5\nfor still frames of a learned policy, and Figure 4 for learning curves on the three tasks. Hyperpa-\nrameters are provided in Table 4. In concurrent work, Heess et al.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 906,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\n[Hee+17] used the adaptive KL\nvariant of PPO (Section 4) to learn locomotion policies for 3D robots.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 100,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: M\nTimestep\n\n0\n1000\n2000\n3000\nRoboschoolHumanoidFlagrunHarder-v0\nFigure 4: Learning curves from PPO on 3D humanoid control tasks, using Roboschool. 7\n\n\nFigure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun. In the \ufb01rst six frames, the\nrobot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward\nthe new target.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 363,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "M\nTimestep",
      "section_index": 14,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nWe also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against\nwell-tuned implementations of A2C [Mni+16] and ACER [Wan+16]. For all three algorithms, we\nused the same policy network architechture as used in [Mni+16]. The hyperparameters for PPO\nare provided in Table 5. For the other two algorithms, we used hyperparameters that were tuned\nto maximize performance on this benchmark. A table of results and learning curves for all 49 games is provided in Appendix B. We consider\nthe following two scoring metrics: (1) average reward per episode over entire training period (which\nfavors fast learning), and (2) average reward per episode over last 100 episodes of training (which\nfavors \ufb01nal performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we\ncompute the victor by averaging the scoring metric across three trials.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 873,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Tie\n\nepisode reward over all of training\n1\n18\n30\n0\n(2) avg. episode reward over last 100 episodes\n1\n28\n19\n1\nTable 2: Number of games \u201cwon\u201d by each algorithm, where the scoring metric is averaged across three trials.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 210,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Tie",
      "section_index": 16,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Conclusion\n\nWe have introduced proximal policy optimization, a family of policy optimization methods that use\nmultiple epochs of stochastic gradient ascent to perform each policy update. These methods have\nthe stability and reliability of trust-region methods but are much simpler to implement, requiring\nonly few lines of code change to a vanilla policy gradient implementation, applicable in more general\nsettings (for example, when using a joint architecture for the policy and value function), and have\nbetter overall performance.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 522,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Conclusion",
      "section_index": 17,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Bel+15]\nM. Bellemare, Y. Veness, and M. \u201cThe arcade learning environ-\nment: An evaluation platform for general agents\u201d. In: Twenty-Fourth International\nJoint Conference on Arti\ufb01cial Intelligence. [Bro+16]\nG. Brockman, V. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. \u201cOpenAI Gym\u201d. In: arXiv preprint arXiv:1606.01540 (2016). [Dua+16]\nY. Houthooft, J. Schulman, and P. \u201cBenchmarking Deep\nReinforcement Learning for Continuous Control\u201d. In: arXiv preprint arXiv:1604.06778\n(2016). [Hee+17]\nN. Riedmiller, et al. \u201cEmergence of Locomotion Behaviours in Rich Envi-\nronments\u201d. In: arXiv preprint arXiv:1707.02286 (2017). Kakade and J. \u201cApproximately optimal approximate reinforcement learn-\ning\u201d. Kingma and J. \u201cAdam: A method for stochastic optimization\u201d. In: arXiv\npreprint arXiv:1412.6980 (2014). [Mni+15]\nV. Kavukcuoglu, D. Bellemare, A. Riedmiller, A. Fidjeland, G. Ostrovski, et al. \u201cHuman-level control through deep\nreinforcement learning\u201d. In: Nature 518.7540 (2015), pp. [Mni+16]\nV.",
    "chunk_index": 0,
    "num_sentences": 36,
    "chunk_size": 995,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nLillicrap, T. Silver, and\nK. Kavukcuoglu. \u201cAsynchronous methods for deep reinforcement learning\u201d. In: arXiv\npreprint arXiv:1602.01783 (2016). [Sch+15a]\nJ. Schulman, P. Jordan, and P. \u201cHigh-dimensional contin-\nuous control using generalized advantage estimation\u201d. In: arXiv preprint arXiv:1506.02438\n(2015). [Sch+15b]\nJ. Schulman, S. Jordan, and P. \u201cTrust region policy\noptimization\u201d. In: CoRR, abs/1502.05477 (2015). Szita and A. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In:\nNeural computation 18.12 (2006), pp. Todorov, T. Erez, and Y. \u201cMuJoCo: A physics engine for model-based con-\ntrol\u201d. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Con-\nference on. [Wan+16]\nZ. Kavukcuoglu, and N. de Freitas. \u201cSample E\ufb03cient Actor-Critic with Experience Replay\u201d. In: arXiv preprint arXiv:1611.01224\n(2016). \u201cSimple statistical gradient-following algorithms for connectionist re-\ninforcement learning\u201d. In: Machine learning 8.3-4 (1992), pp.",
    "chunk_index": 1,
    "num_sentences": 29,
    "chunk_size": 969,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Minibatch size\n\n64\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nTable 3: PPO hyperparameters used for the Mujoco 1 million timestep benchmark.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 122,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Minibatch size",
      "section_index": 19,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\n32 (locomotion), 128 (\ufb02agrun)\nLog stdev. of action distribution\nLinearAnneal(\u22120.7, \u22121.6)\nTable 4: PPO hyperparameters used for the Roboschool experiments. Adam stepsize was adjusted based on\nthe target value of the KL divergence.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 229,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 20,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\n8\nClipping parameter \u03f5\n0.1 \u00d7 \u03b1\nVF coe\ufb00. c1 (9)\n1\nEntropy coe\ufb00. c2 (9)\n0.01\nTable 5: PPO hyperparameters used in Atari experiments. \u03b1 is linearly annealed from 1 to 0 over the course\nof learning.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 194,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 21,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: B\nPerformance on More Atari Games\n\nHere we include a comparison of PPO against A2C on a larger collection of 49 Atari games. Figure 6\nshows the learning curves of each of three random seeds, while Table 6 shows the mean performance.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 197,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "B\nPerformance on More Atari Games",
      "section_index": 22,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: PPO\n\nFigure 6: Comparison of PPO and A2C on all 49 ATARI games included in OpenAI Gym at the time of\npublication.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 108,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "PPO",
      "section_index": 23,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Zaxxon\n\n16.3\n29.0\n5008.7\nTable 6: Mean \ufb01nal scores (last 100 episodes) of PPO and A2C on Atari games after 40M game frames (10M\ntimesteps).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Zaxxon",
      "section_index": 24,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nVolodymyr Mnih1\nVMNIH@GOOGLE.COM\nAdri\u00e0 Puigdom\u00e8nech Badia1\nADRIAP@GOOGLE.COM\nMehdi Mirza1,2\nMIRZAMOM@IRO.UMONTREAL.CA\nAlex Graves1\nGRAVESA@GOOGLE.COM\nTim Harley1\nTHARLEY@GOOGLE.COM\nTimothy P. Lillicrap1\nCOUNTZERO@GOOGLE.COM\nDavid Silver1\nDAVIDSILVER@GOOGLE.COM\nKoray Kavukcuoglu 1\nKORAYK@GOOGLE.COM",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 298,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 0,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We\n\npropose\na\nconceptually\nsimple\nand\nlightweight\nframework\nfor\ndeep\nreinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers. The best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 817,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We",
      "section_index": 1,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nenable reinforcement learning (RL) algorithms to perform\neffectively. However, it was previously thought that the\ncombination of simple online RL algorithms with deep\nneural networks was fundamentally unstable. Instead, a va-\nriety of solutions have been proposed to stabilize the algo-\nrithm (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Has-\nselt et al., 2015; Schulman et al., 2015a). These approaches\nshare a common idea: the sequence of observed data en-\ncountered by an online RL agent is non-stationary, and on-\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s). line RL updates are strongly correlated.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 712,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: By storing the\n\nagent\u2019s data in an experience replay memory, the data can\nbe batched (Riedmiller, 2005; Schulman et al., 2015a) or\nrandomly sampled (Mnih et al., 2013; 2015; Van Hasselt\net al., 2015) from different time-steps. Aggregating over\nmemory in this way reduces non-stationarity and decorre-\nlates updates, but at the same time limits the methods to\noff-policy reinforcement learning algorithms.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 388,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "By storing the",
      "section_index": 3,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep RL algorithms based on experience replay have\n\nachieved unprecedented success in challenging domains\nsuch as Atari 2600. However, experience replay has several\ndrawbacks: it uses more memory and computation per real\ninteraction; and it requires off-policy learning algorithms\nthat can update from data generated by an older policy.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 284,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep RL algorithms based on experience replay have",
      "section_index": 4,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In this paper we provide a very different paradigm for deep\n\nreinforcement learning. Instead of experience replay, we\nasynchronously execute multiple agents in parallel, on mul-\ntiple instances of the environment. This parallelism also\ndecorrelates the agents\u2019 data into a more stationary process,\nsince at any given time-step the parallel agents will be ex-\nperiencing a variety of different states. This simple idea\nenables a much larger spectrum of fundamental on-policy\nRL algorithms, such as Sarsa, n-step methods, and actor-\ncritic methods, as well as off-policy RL algorithms such\nas Q-learning, to be applied robustly and effectively using\ndeep neural networks.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 608,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In this paper we provide a very different paradigm for deep",
      "section_index": 5,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Our parallel reinforcement learning paradigm also offers\n\npractical bene\ufb01ts. Whereas previous approaches to deep re-\ninforcement learning rely heavily on specialized hardware\nsuch as GPUs (Mnih et al., 2015; Van Hasselt et al., 2015;\nSchaul et al., 2015) or massively distributed architectures\n(Nair et al., 2015), our experiments run on a single machine\nwith a standard multi-core CPU. When applied to a vari-\nety of Atari 2600 domains, on many games asynchronous\nreinforcement learning achieves better results, in far less\narXiv:1602.01783v2  [cs.LG]  16 Jun 2016",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 507,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Our parallel reinforcement learning paradigm also offers",
      "section_index": 6,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\ntime than previous GPU-based algorithms, using far less\nresource than massively distributed approaches. The best\nof the proposed methods, asynchronous advantage actor-\ncritic (A3C), also mastered a variety of continuous motor\ncontrol tasks as well as learned general strategies for ex-\nploring 3D mazes purely from visual inputs. We believe\nthat the success of A3C on both 2D and 3D games, discrete\nand continuous action spaces, as well as its ability to train\nfeedforward and recurrent agents makes it the most general\nand successful reinforcement learning agent to date.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 572,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 7,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nThe General Reinforcement Learning Architecture (Gorila)\nof (Nair et al., 2015) performs asynchronous training of re-\ninforcement learning agents in a distributed setting. In Go-\nrila, each process contains an actor that acts in its own copy\nof the environment, a separate replay memory, and a learner\nthat samples data from the replay memory and computes\ngradients of the DQN loss (Mnih et al., 2015) with respect\nto the policy parameters. The gradients are asynchronously\nsent to a central parameter server which updates a central\ncopy of the model. The updated policy parameters are sent\nto the actor-learners at \ufb01xed intervals. By using 100 sep-\narate actor-learner processes and 30 parameter server in-\nstances, a total of 130 machines, Gorila was able to signif-\nicantly outperform DQN over 49 Atari games. On many\ngames Gorila reached the score achieved by DQN over 20\ntimes faster than DQN. We also note that a similar way of\nparallelizing DQN was proposed by (Chavez et al., 2015).",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 990,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nIn earlier work, (Li & Schuurmans, 2011) applied the\nMap Reduce framework to parallelizing batch reinforce-\nment learning methods with linear function approximation.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 165,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nbut not to parallelize the collection of experience or sta-\nbilize learning. (Grounds & Kudenko, 2008) proposed a\nparallel version of the Sarsa algorithm that uses multiple\nseparate actor-learners to accelerate training. Each actor-\nlearner learns separately and periodically sends updates to\nweights that have changed signi\ufb01cantly to the other learn-\ners using peer-to-peer communication. (Tsitsiklis, 1994) studied convergence properties of Q-\nlearning in the asynchronous optimization setting. These\nresults show that Q-learning is still guaranteed to converge\nwhen some of the information is outdated as long as out-\ndated information is always eventually discarded and sev-\neral other technical assumptions are satis\ufb01ed. Even earlier,\n(Bertsekas, 1982) studied the related problem of distributed\ndynamic programming.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 821,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nAnother related area of work is in evolutionary meth-\nods, which are often straightforward to parallelize by dis-\ntributing \ufb01tness evaluations over multiple machines or\nthreads (Tomassini, 1999). Such parallel evolutionary ap-\nproaches have recently been applied to some visual rein-\nforcement learning tasks. In one example, (Koutn\u00edk et al.,\n2014) evolved convolutional neural network controllers for\nthe TORCS driving simulator by performing \ufb01tness evalu-\nations on 8 CPU cores in parallel.",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 492,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nwhere an agent interacts with an environment E over a\nnumber of discrete time steps. At each time step t, the\nagent receives a state st and selects an action at from some\nset of possible actions A according to its policy \u03c0, where\n\u03c0 is a mapping from states st to actions at. In return, the\nagent receives the next state st+1 and receives a scalar re-\nward rt. The process continues until the agent reaches a\nterminal state after which the process restarts. The return\nRt = P\u221e\nk=0 \u03b3krt+k is the total accumulated return from\ntime step t with discount factor \u03b3 \u2208(0, 1]. The goal of the\nagent is to maximize the expected return from each state st. The action value Q\u03c0(s, a) = E [Rt|st = s, a] is the ex-\npected return for selecting action a in state s and follow-\ning policy \u03c0. The optimal value function Q\u2217(s, a) =\nmax\u03c0 Q\u03c0(s, a) gives the maximum action value for state\ns and action a achievable by any policy.",
    "chunk_index": 0,
    "num_sentences": 8,
    "chunk_size": 908,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nSimilarly, the\nvalue of state s under policy \u03c0 is de\ufb01ned as V \u03c0(s) =\nE [Rt|st = s] and is simply the expected return for follow-\ning policy \u03c0 from state s. In value-based model-free reinforcement learning methods,\nthe action value function is represented using a function ap-\nproximator, such as a neural network. Let Q(s, a; \u03b8) be an\napproximate action-value function with parameters \u03b8. The\nupdates to \u03b8 can be derived from a variety of reinforcement\nlearning algorithms. One example of such an algorithm is\nQ-learning, which aims to directly approximate the optimal\naction value function: Q\u2217(s, a) \u2248Q(s, a; \u03b8). In one-step\nQ-learning, the parameters \u03b8 of the action value function\nQ(s, a; \u03b8) are learned by iteratively minimizing a sequence\nof loss functions, where the ith loss function de\ufb01ned as\nLi(\u03b8i) = E\n\u0010\nr + \u03b3 max\na\u2032 Q(s\u2032, a\u2032; \u03b8i\u22121) \u2212Q(s, a; \u03b8i)\n\u00112\nwhere s\u2032 is the state encountered after state s.",
    "chunk_index": 1,
    "num_sentences": 6,
    "chunk_size": 906,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nWe refer to the above method as one-step Q-learning be-\ncause it updates the action value Q(s, a) toward the one-\nstep return r + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8). One drawback of us-\ning one-step methods is that obtaining a reward r only di-\nrectly affects the value of the state action pair s, a that led\nto the reward. The values of other state action pairs are\naffected only indirectly through the updated value Q(s, a). This can make the learning process slow since many up-\ndates are required the propagate a reward to the relevant\npreceding states and actions.",
    "chunk_index": 2,
    "num_sentences": 4,
    "chunk_size": 555,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nOne way of propagating rewards faster is by using n-\nstep returns (Watkins, 1989; Peng & Williams, 1996). In n-step Q-learning, Q(s, a) is updated toward the n-\nstep return de\ufb01ned as rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3n\u22121rt+n\u22121 +\nmaxa \u03b3nQ(st+n, a). This results in a single reward r di-\nrectly affecting the values of n preceding state action pairs.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 336,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 11,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: This makes the process of propagating rewards to relevant\n\nstate-action pairs potentially much more ef\ufb01cient. In contrast to value-based methods, policy-based model-\nfree methods directly parameterize the policy \u03c0(a|s; \u03b8) and\nupdate the parameters \u03b8 by performing, typically approx-\nimate, gradient ascent on E[Rt].",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 256,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "This makes the process of propagating rewards to relevant",
      "section_index": 12,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: One example of such\n\na method is the REINFORCE family of algorithms due\nto Williams (1992). Standard REINFORCE updates the\npolicy parameters \u03b8 in the direction \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt,\nwhich is an unbiased estimate of \u2207\u03b8E[Rt]. It is possible to\nreduce the variance of this estimate while keeping it unbi-\nased by subtracting a learned function of the state bt(st),\nknown as a baseline (Williams, 1992), from the return. The\nresulting gradient is \u2207\u03b8 log \u03c0(at|st; \u03b8) (Rt \u2212bt(st)).",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 454,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "One example of such",
      "section_index": 13,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: A learned estimate of the value function is commonly used\n\nas the baseline bt(st) \u2248V \u03c0(st) leading to a much lower\nvariance estimate of the policy gradient. When an approx-\nimate value function is used as the baseline, the quantity\nRt \u2212bt used to scale the policy gradient can be seen as\nan estimate of the advantage of action at in state st, or\nA(at, st) = Q(at, st)\u2212V (st), because Rt is an estimate of\nQ\u03c0(at, st) and bt is an estimate of V \u03c0(st). This approach\ncan be viewed as an actor-critic architecture where the pol-\nicy \u03c0 is the actor and the baseline bt is the critic (Sutton &\nBarto, 1998; Degris et al., 2012).",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 563,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "A learned estimate of the value function is commonly used",
      "section_index": 14,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nWe now present multi-threaded asynchronous variants of\none-step Sarsa, one-step Q-learning, n-step Q-learning, and\nadvantage actor-critic. The aim in designing these methods\nwas to \ufb01nd RL algorithms that can train deep neural net-\nwork policies reliably and without large resource require-\nments. While the underlying RL methods are quite dif-\nferent, with actor-critic being an on-policy policy search\nmethod and Q-learning being an off-policy value-based\nmethod, we use two main ideas to make all four algorithms\npractical given our design goal. First, we use asynchronous actor-learners, similarly to the\nGorila framework (Nair et al., 2015), but instead of using\nseparate machines and a parameter server, we use multi-\nple CPU threads on a single machine. Keeping the learn-\ners on a single machine removes the communication costs\nof sending gradients and parameters and enables us to use\nHogwild! (Recht et al., 2011) style updates for training.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 950,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nSecond, we make the observation that multiple actors-\nAlgorithm 1 Asynchronous one-step Q-learning - pseu-\ndocode for each actor-learner thread. // Assume global shared \u03b8, \u03b8\u2212, and counter T = 0. Initialize thread step counter t \u21900\nInitialize target network weights \u03b8\u2212\u2190\u03b8\nInitialize network gradients d\u03b8 \u21900",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 304,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nrepeat\nTake action a with \u03f5-greedy policy based on Q(s, a; \u03b8)\nReceive new state s\u2032 and reward r\ny =\n\u001a r\nfor terminal s\u2032\nr + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8\u2212)\nfor non-terminal s\u2032\nAccumulate gradients wrt \u03b8: d\u03b8 \u2190d\u03b8 + \u2202(y\u2212Q(s,a;\u03b8))2\n\u2202\u03b8\ns = s\u2032\nT \u2190T + 1 and t \u2190t + 1\nif T\nmod Itarget == 0 then\nUpdate the target network \u03b8\u2212\u2190\u03b8\nend if\nif t mod IAsyncUpdate == 0 or s is terminal then\nPerform asynchronous update of \u03b8 using d\u03b8. Clear gradients d\u03b8 \u21900. end if\nuntil T > Tmax\nlearners running in parallel are likely to be exploring dif-\nferent parts of the environment. Moreover, one can explic-\nitly use different exploration policies in each actor-learner\nto maximize this diversity. By running different explo-\nration policies in different threads, the overall changes be-\ning made to the parameters by multiple actor-learners ap-\nplying online updates in parallel are likely to be less corre-\nlated in time than a single agent applying online updates.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 931,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nHence, we do not use a replay memory and rely on parallel\nactors employing different exploration policies to perform\nthe stabilizing role undertaken by experience replay in the\nDQN training algorithm. In addition to stabilizing learning, using multiple parallel\nactor-learners has multiple practical bene\ufb01ts. First, we ob-\ntain a reduction in training time that is roughly linear in\nthe number of parallel actor-learners. Second, since we no\nlonger rely on experience replay for stabilizing learning we\nare able to use on-policy reinforcement learning methods\nsuch as Sarsa and actor-critic to train neural networks in a\nstable way. We now describe our variants of one-step Q-\nlearning, one-step Sarsa, n-step Q-learning and advantage\nactor-critic. Asynchronous one-step Q-learning: Pseudocode for our\nvariant of Q-learning, which we call Asynchronous one-\nstep Q-learning, is shown in Algorithm 1.",
    "chunk_index": 1,
    "num_sentences": 6,
    "chunk_size": 898,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nEach thread in-\nteracts with its own copy of the environment and at each\nstep computes a gradient of the Q-learning loss. We use\na shared and slowly changing target network in comput-\ning the Q-learning loss, as was proposed in the DQN train-\ning method. We also accumulate gradients over multiple\ntimesteps before they are applied, which is similar to us-",
    "chunk_index": 2,
    "num_sentences": 3,
    "chunk_size": 356,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\ning minibatches. This reduces the chances of multiple ac-\ntor learners overwriting each other\u2019s updates. Accumulat-\ning updates over several steps also provides some ability to\ntrade off computational ef\ufb01ciency for data ef\ufb01ciency. Finally, we found that giving each thread a different explo-\nration policy helps improve robustness. Adding diversity\nto exploration in this manner also generally improves per-\nformance through better exploration. While there are many\npossible ways of making the exploration policies differ we\nexperiment with using \u03f5-greedy exploration with \u03f5 periodi-\ncally sampled from some distribution by each thread. Asynchronous one-step Sarsa: The asynchronous one-\nstep Sarsa algorithm is the same as asynchronous one-step\nQ-learning as given in Algorithm 1 except that it uses a dif-\nferent target value for Q(s, a). The target value used by\none-step Sarsa is r + \u03b3Q(s\u2032, a\u2032; \u03b8\u2212) where a\u2032 is the action\ntaken in state s\u2032 (Rummery & Niranjan, 1994; Sutton &\nBarto, 1998).",
    "chunk_index": 0,
    "num_sentences": 8,
    "chunk_size": 993,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe again use a target network and updates\naccumulated over multiple timesteps to stabilize learning. Asynchronous n-step Q-learning: Pseudocode for our\nvariant of multi-step Q-learning is shown in Supplementary\nAlgorithm S2. The algorithm is somewhat unusual because\nit operates in the forward view by explicitly computing n-\nstep returns, as opposed to the more common backward\nview used by techniques like eligibility traces (Sutton &\nBarto, 1998). We found that using the forward view is eas-\nier when training neural networks with momentum-based\nmethods and backpropagation through time. In order to\ncompute a single update, the algorithm \ufb01rst selects actions\nusing its exploration policy for up to tmax steps or until a\nterminal state is reached. This process results in the agent\nreceiving up to tmax rewards from the environment since\nits last update. The algorithm then computes gradients for\nn-step Q-learning updates for each of the state-action pairs\nencountered since the last update.",
    "chunk_index": 1,
    "num_sentences": 7,
    "chunk_size": 996,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nEach n-step update uses\nthe longest possible n-step return resulting in a one-step\nupdate for the last state, a two-step update for the second\nlast state, and so on for a total of up to tmax updates. The\naccumulated updates are applied in a single gradient step. Asynchronous advantage actor-critic: The algorithm,\nwhich we call asynchronous advantage actor-critic (A3C),\nmaintains a policy \u03c0(at|st; \u03b8) and an estimate of the value\nfunction V (st; \u03b8v). Like our variant of n-step Q-learning,\nour variant of actor-critic also operates in the forward view\nand uses the same mix of n-step returns to update both the\npolicy and the value-function. The policy and the value\nfunction are updated after every tmax actions or when a\nterminal state is reached.",
    "chunk_index": 2,
    "num_sentences": 5,
    "chunk_size": 751,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe update performed by the al-\ngorithm can be seen as \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)A(st, at; \u03b8, \u03b8v)\nwhere A(st, at; \u03b8, \u03b8v) is an estimate of the advantage func-\ntion given by Pk\u22121\ni=0 \u03b3irt+i + \u03b3kV (st+k; \u03b8v) \u2212V (st; \u03b8v),\nwhere k can vary from state to state and is upper-bounded\nby tmax. The pseudocode for the algorithm is presented in\nSupplementary Algorithm S3. As with the value-based methods we rely on parallel actor-\nlearners and accumulated updates for improving training\nstability. Note that while the parameters \u03b8 of the policy\nand \u03b8v of the value function are shown as being separate\nfor generality, we always share some of the parameters in\npractice. We typically use a convolutional neural network\nthat has one softmax output for the policy \u03c0(at|st; \u03b8) and\none linear output for the value function V (st; \u03b8v), with all\nnon-output layers shared.",
    "chunk_index": 3,
    "num_sentences": 5,
    "chunk_size": 849,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe also found that adding the entropy of the policy \u03c0 to the\nobjective function improved exploration by discouraging\npremature convergence to suboptimal deterministic poli-\ncies. This technique was originally proposed by (Williams\n& Peng, 1991), who found that it was particularly help-\nful on tasks requiring hierarchical behavior. The gradi-\nent of the full objective function including the entropy\nregularization term with respect to the policy parame-\nters takes the form \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)(Rt \u2212V (st; \u03b8v)) +\n\u03b2\u2207\u03b8\u2032H(\u03c0(st; \u03b8\u2032)), where H is the entropy. The hyperpa-\nrameter \u03b2 controls the strength of the entropy regulariza-\ntion term. Optimization: We investigated three different optimiza-\ntion algorithms in our asynchronous framework \u2013 SGD\nwith momentum, RMSProp (Tieleman & Hinton, 2012)\nwithout shared statistics, and RMSProp with shared statis-\ntics.",
    "chunk_index": 4,
    "num_sentences": 5,
    "chunk_size": 861,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe used the standard non-centered RMSProp update\ngiven by\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82 and \u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(1)\nwhere all operations are performed elementwise. A com-\nparison on a subset of Atari 2600 games showed that a vari-\nant of RMSProp where statistics g are shared across threads\nis considerably more robust than the other two methods.",
    "chunk_index": 5,
    "num_sentences": 2,
    "chunk_size": 334,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We use four different platforms for assessing the properties\n\nof the proposed framework. We perform most of our exper-\niments using the Arcade Learning Environment (Bellemare\net al., 2012), which provides a simulator for Atari 2600\ngames. This is one of the most commonly used benchmark\nenvironments for RL algorithms. We use the Atari domain\nto compare against state of the art results (Van Hasselt et al.,\n2015; Wang et al., 2015; Schaul et al., 2015; Nair et al.,\n2015; Mnih et al., 2015), as well as to carry out a detailed\nstability and scalability analysis of the proposed methods. We performed further comparisons using the TORCS 3D\ncar racing simulator (Wymann et al., 2013). We also use",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 633,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We use four different platforms for assessing the properties",
      "section_index": 18,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 103,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 19,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 108,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 20,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\n1-step Q\n1-step SARSA\nn-step Q\nA3C\nFigure 1. Learning speed comparison for DQN and the new asynchronous algorithms on \ufb01ve Atari 2600 games. DQN was trained on\na single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores. The plots are averaged over 5 runs. In\nthe case of DQN the runs were for different seeds with \ufb01xed hyperparameters. For asynchronous methods we average over the best 5\nmodels from 50 experiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed. two additional domains to evaluate only the A3C algorithm\n\u2013 Mujoco and Labyrinth. MuJoCo (Todorov, 2015) is a\nphysics simulator for evaluating agents on continuous mo-\ntor control tasks with contact dynamics. Labyrinth is a new\n3D environment where the agent must learn to \ufb01nd rewards\nin randomly generated mazes from a visual input. The pre-\ncise details of our experimental setup can be found in Sup-\nplementary Section 8.",
    "chunk_index": 0,
    "num_sentences": 10,
    "chunk_size": 958,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nAtari 2600 Games\nWe \ufb01rst present results on a subset of Atari 2600 games to\ndemonstrate the training speed of the new methods. Fig-\nure 1 compares the learning speed of the DQN algorithm\ntrained on an Nvidia K40 GPU with the asynchronous\nmethods trained using 16 CPU cores on \ufb01ve Atari 2600\ngames. The results show that all four asynchronous meth-\nods we presented can successfully train neural network\ncontrollers on the Atari domain. The asynchronous meth-\nods tend to learn faster than DQN, with signi\ufb01cantly faster\nlearning on some games, while training on only 16 CPU\ncores. Additionally, the results suggest that n-step methods\nlearn faster than one-step methods on some games. Over-\nall, the policy-based advantage actor-critic method signi\ufb01-\ncantly outperforms all three value-based methods. We then evaluated asynchronous advantage actor-critic on\n57 Atari games.",
    "chunk_index": 1,
    "num_sentences": 7,
    "chunk_size": 872,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nIn order to compare with the state of the\nart in Atari game playing, we largely followed the train-\ning and evaluation protocol of (Van Hasselt et al., 2015). Speci\ufb01cally, we tuned hyperparameters (learning rate and\namount of gradient norm clipping) using a search on six\nAtari games (Beamrider, Breakout, Pong, Q*bert, Seaquest\nand Space Invaders) and then \ufb01xed all hyperparameters for\nall 57 games. We trained both a feedforward agent with the\nsame architecture as (Mnih et al., 2015; Nair et al., 2015;\nVan Hasselt et al., 2015) as well as a recurrent agent with an\nadditional 256 LSTM cells after the \ufb01nal hidden layer. We\nadditionally used the \ufb01nal network weights for evaluation\nto make the results more comparable to the original results",
    "chunk_index": 2,
    "num_sentences": 4,
    "chunk_size": 744,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Gorila\n\n4 days, 100 machines\n215.2%\n71.3%\nD-DQN\n8 days on GPU\n332.9%\n110.9%\nDueling D-DQN\n8 days on GPU\n343.8%\n117.1%",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 109,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Gorila",
      "section_index": 22,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\n8 days on GPU\n463.6%\n127.6%\nA3C, FF\n1 day on CPU\n344.1%\n68.2%\nA3C, FF\n4 days on CPU\n496.8%\n116.6%\nA3C, LSTM\n4 days on CPU\n623.0%\n112.6%\nTable 1. Mean and median human-normalized scores on 57 Atari\ngames using the human starts evaluation metric. Supplementary\nTable SS3 shows the raw scores for all games. from (Bellemare et al., 2012). We trained our agents for\nfour days using 16 CPU cores, while the other agents were\ntrained for 8 to 10 days on Nvidia K40 GPUs. Table 1\nshows the average and median human-normalized scores\nobtained by our agents trained by asynchronous advantage\nactor-critic (A3C) as well as the current state-of-the art. Supplementary Table S3 shows the scores on all games. A3C signi\ufb01cantly improves on state-of-the-art the average\nscore over 57 games in half the training time of the other\nmethods while using only 16 CPU cores and no GPU.",
    "chunk_index": 0,
    "num_sentences": 8,
    "chunk_size": 863,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nFur-\nthermore, after just one day of training, A3C matches the\naverage human normalized score of Dueling Double DQN\nand almost reaches the median human normalized score of\nGorila. We note that many of the improvements that are\npresented in Double DQN (Van Hasselt et al., 2015) and\nDueling Double DQN (Wang et al., 2015) can be incorpo-\nrated to 1-step Q and n-step Q methods presented in this\nwork with similar potential improvements. TORCS Car Racing Simulator",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 462,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: TORCS not only has more realistic graphics than Atari\n\n2600 games, but also requires the agent to learn the dy-\nnamics of the car it is controlling. At each step, an agent\nreceived only a visual input in the form of an RGB image",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 173,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "TORCS not only has more realistic graphics than Atari",
      "section_index": 24,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nof the current frame as well as a reward proportional to the\nagent\u2019s velocity along the center of the track at the agent\u2019s\ncurrent position. We used the same neural network archi-\ntecture as the one used in the Atari experiments speci\ufb01ed in\nSupplementary Section 8. We performed experiments us-\ning four different settings \u2013 the agent controlling a slow car\nwith and without opponent bots, and the agent controlling a\nfast car with and without opponent bots. Full results can be\nfound in Supplementary Figure S6. A3C was the best per-\nforming agent, reaching between roughly 75% and 90% of\nthe score obtained by a human tester on all four game con-\n\ufb01gurations in about 12 hours of training. A video showing\nthe learned driving behavior of the A3C agent can be found\nat https://youtu.be/0xo1Ldx3L5Q. Continuous Action Control Using the MuJoCo",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 841,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We also examined a set of tasks where the action space\n\nis continuous. In particular, we looked at a set of rigid\nbody physics domains with contact dynamics where the\ntasks include many examples of manipulation and loco-\nmotion.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 172,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We also examined a set of tasks where the action space",
      "section_index": 26,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nphysics engine. We evaluated only the asynchronous ad-\nvantage actor-critic algorithm since, unlike the value-based\nmethods, it is easily extended to continuous actions. In all\nproblems, using either the physical state or pixels as in-\nput, Asynchronous Advantage-Critic found good solutions\nin less than 24 hours of training and typically in under a few\nhours. Some successful policies learned by our agent can\nbe seen in the following video https://youtu.be/\nAjjc08-iPx8. Further details about this experiment can\nbe found in Supplementary Section 9. Labyrinth\nWe performed an additional set of experiments with A3C\non a new 3D environment called Labyrinth. The speci\ufb01c\ntask we considered involved the agent learning to \ufb01nd re-\nwards in randomly generated mazes. At the beginning of\neach episode the agent was placed in a new randomly gen-\nerated maze consisting of rooms and corridors. Each maze\ncontained two types of objects that the agent was rewarded\nfor \ufb01nding \u2013 apples and portals.",
    "chunk_index": 0,
    "num_sentences": 9,
    "chunk_size": 990,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nPicking up an apple led to\na reward of 1. Entering a portal led to a reward of 10 after\nwhich the agent was respawned in a new random location in\nthe maze and all previously collected apples were regener-\nated. An episode terminated after 60 seconds after which a\nnew episode would begin. The aim of the agent is to collect\nas many points as possible in the time limit and the optimal\nstrategy involves \ufb01rst \ufb01nding the portal and then repeatedly\ngoing back to it after each respawn. This task is much more\nchallenging than the TORCS driving domain because the\nagent is faced with a new maze in each episode and must\nlearn a general strategy for exploring random mazes.",
    "chunk_index": 1,
    "num_sentences": 5,
    "chunk_size": 668,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\n1\n2\n4\n8\n16\n1-step Q\n1.0\n3.0\n6.3\n13.3\n24.1\n1-step SARSA\n1.0\n2.8\n5.9\n13.1\n22.1\nn-step Q\n1.0\n2.7\n5.9\n10.7\n17.2\nA3C\n1.0\n2.1\n3.7\n6.9\n12.5\nTable 2.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 141,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\nThe average training speedup for each method and num-\nber of threads averaged over seven Atari games. To compute the\ntraining speed-up on a single game we measured the time to re-\nquired reach a \ufb01xed reference score using each method and num-\nber of threads. The speedup from using n threads on a game was\nde\ufb01ned as the time required to reach a \ufb01xed reference score using\none thread divided the time required to reach the reference score\nusing n threads. The table shows the speedups averaged over\nseven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert,\nSeaquest, and Space Invaders). We trained an A3C LSTM agent on this task using only\n84 \u00d7 84 RGB images as input. The \ufb01nal average score\nof around 50 indicates that the agent learned a reason-\nable strategy for exploring random 3D maxes using only\na visual input. A video showing one of the agents ex-\nploring previously unseen mazes is included at https:\n//youtu.be/nMR5mjCFZCw. Scalability and Data Ef\ufb01ciency",
    "chunk_index": 1,
    "num_sentences": 8,
    "chunk_size": 969,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nby looking at how the training time and data ef\ufb01ciency\nchanges with the number of parallel actor-learners. When\nusing multiple workers in parallel and updating a shared\nmodel, one would expect that in an ideal case, for a given\ntask and algorithm, the number of training steps to achieve\na certain score would remain the same with varying num-\nbers of workers. Therefore, the advantage would be solely\ndue to the ability of the system to consume more data in\nthe same amount of wall clock time and possibly improved\nexploration. Table 2 shows the training speed-up achieved\nby using increasing numbers of parallel actor-learners av-\neraged over seven Atari games. These results show that all\nfour methods achieve substantial speedups from using mul-\ntiple worker threads, with 16 threads leading to at least an\norder of magnitude speedup. This con\ufb01rms that our pro-\nposed framework scales well with the number of parallel\nworkers, making ef\ufb01cient use of resources.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 964,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nSomewhat surprisingly, asynchronous one-step Q-learning\nand Sarsa algorithms exhibit superlinear speedups that\ncannot be explained by purely computational gains. We\nobserve that one-step methods (one-step Q and one-step\nSarsa) often require less data to achieve a particular score\nwhen using more parallel actor-learners. We believe this\nis due to positive effect of multiple threads to reduce the\nbias in one-step methods. These effects are shown more\nclearly in Figure 3, which shows plots of the average score\nagainst the total number of training frames for different",
    "chunk_index": 1,
    "num_sentences": 4,
    "chunk_size": 570,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nA3C, Space Invaders\nFigure 2. Scatter plots of scores obtained by asynchronous advantage actor-critic on \ufb01ve games (Beamrider, Breakout, Pong, Q*bert,\nSpace Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for\nwhich all random initializations acheive good scores. This shows that A3C is quite robust to learning rates and initial random weights. numbers of actor-learners and training methods on \ufb01ve\nAtari games, and Figure 4, which shows plots of the av-\nerage score against wall-clock time. Robustness and Stability\nFinally, we analyzed the stability and robustness of the\nfour proposed asynchronous algorithms. For each of the\nfour algorithms we trained models on \ufb01ve games (Break-\nout, Beamrider, Pong, Q*bert, Space Invaders) using 50\ndifferent learning rates and random initializations. Figure 2\nshows scatter plots of the resulting scores for A3C, while\nSupplementary Figure S11 shows plots for the other three\nmethods.",
    "chunk_index": 0,
    "num_sentences": 8,
    "chunk_size": 1001,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nThere is usually a range of learning rates for each\nmethod and game combination that leads to good scores,\nindicating that all methods are quite robust to the choice of\nlearning rate and random initialization. The fact that there\nare virtually no points with scores of 0 in regions with good\nlearning rates indicates that the methods are stable and do\nnot collapse or diverge once they are learning.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 399,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We have presented asynchronous versions of four standard\n\nreinforcement learning algorithms and showed that they\nare able to train neural network controllers on a variety\nof domains in a stable manner. Our results show that in\nour proposed framework stable training of neural networks\nthrough reinforcement learning is possible with both value-\nbased and policy-based methods, off-policy as well as on-\npolicy methods, and in discrete as well as continuous do-\nmains. When trained on the Atari domain using 16 CPU\ncores, the proposed asynchronous algorithms train faster\nthan DQN trained on an Nvidia K40 GPU, with A3C sur-\npassing the current state-of-the-art in half the training time. One of our main \ufb01ndings is that using parallel actor-\nlearners to update a shared model had a stabilizing effect on\nthe learning process of the three value-based methods we\nconsidered. While this shows that stable online Q-learning\nis possible without experience replay, which was used for\nthis purpose in DQN, it does not mean that experience re-\nplay is not useful.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 997,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We have presented asynchronous versions of four standard",
      "section_index": 31,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nthe asynchronous reinforcement learning framework could\nsubstantially improve the data ef\ufb01ciency of these methods\nby reusing old data. This could in turn lead to much faster\ntraining times in domains like TORCS where interacting\nwith the environment is more expensive than updating the\nmodel for the architecture we used. Combining other existing reinforcement learning meth-\nods or recent advances in deep reinforcement learning\nwith our asynchronous framework presents many possibil-\nities for immediate improvements to the methods we pre-\nsented. While our n-step methods operate in the forward\nview (Sutton & Barto, 1998) by using corrected n-step re-\nturns directly as targets, it has been more common to use\nthe backward view to implicitly combine different returns\nthrough eligibility traces (Watkins, 1989; Sutton & Barto,\n1998; Peng & Williams, 1996).",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 860,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nThe asynchronous ad-\nvantage actor-critic method could be potentially improved\nby using other ways of estimating the advantage function,\nsuch as generalized advantage estimation of (Schulman\net al., 2015b). All of the value-based methods we inves-\ntigated could bene\ufb01t from different ways of reducing over-\nestimation bias of Q-values (Van Hasselt et al., 2015; Belle-\nmare et al., 2016). Yet another, more speculative, direction\nis to try and combine the recent work on true online tempo-\nral difference methods (van Seijen et al., 2015) with non-\nlinear function approximation. In addition to these algorithmic improvements, a number\nof complementary improvements to the neural network ar-\nchitecture are possible. The dueling architecture of (Wang\net al., 2015) has been shown to produce more accurate es-\ntimates of Q-values by including separate streams for the\nstate value and advantage in the network.",
    "chunk_index": 1,
    "num_sentences": 5,
    "chunk_size": 908,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nThe spatial soft-\nmax proposed by (Levine et al., 2015) could improve both\nvalue-based and policy-based methods by making it easier\nfor the network to represent feature coordinates.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 181,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Sasha Vezhnevets and Joseph Modayil for many helpful\n\ndiscussions, suggestions and comments on the paper. We\nalso thank the DeepMind evaluation team for setting up the\nenvironments used to evaluate the agents in the paper.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 168,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Sasha Vezhnevets and Joseph Modayil for many helpful",
      "section_index": 33,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 34,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 35,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 36,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 118,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 37,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 38,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 39,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 40,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 41,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 118,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 42,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 43,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 3. Data ef\ufb01ciency comparison of different numbers of actor-learners for three asynchronous methods on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis\nshows the average score. Each curve shows the average over the three best learning rates. Single step methods show increased data\nef\ufb01ciency from more parallel workers. Results for Sarsa are shown in Supplementary Figure S9. 0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000",
    "chunk_index": 0,
    "num_sentences": 8,
    "chunk_size": 644,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 162,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 45,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 165,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 46,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 177,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 47,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 176,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 48,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 169,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 49,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 166,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 50,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 165,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 51,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 182,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 52,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 176,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 53,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 181,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 54,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n100\n200\n300\n400\n500",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 138,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 55,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n30\n20\n10\n0\n10\n20",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 133,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 56,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 144,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 57,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 156,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 58,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 4. Training speed comparison of different numbers of actor-learners on \ufb01ve Atari games. The x-axis shows training time in\nhours while the y-axis shows the average score. Each curve shows the average over the three best learning rates. All asynchronous\nmethods show signi\ufb01cant speedups from using greater numbers of parallel actor-learners. Results for Sarsa are shown in Supplementary\nFigure S10.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 479,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 59,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nBellemare, Marc G, Naddaf, Yavar, Veness, Joel, and\nBowling, Michael. The arcade learning environment:\nAn evaluation platform for general agents. Journal of\nArti\ufb01cial Intelligence Research, 2012. Bellemare, Marc G., Ostrovski, Georg, Guez, Arthur,\nThomas, Philip S., and Munos, R\u00e9mi. Increasing the ac-\ntion gap: New operators for reinforcement learning. In\nProceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence, 2016. Bertsekas, Dimitri P. Distributed dynamic programming. Automatic Control, IEEE Transactions on, 27(3):610\u2013\n616, 1982. Chavez, Kevin, Ong, Hao Yi, and Hong, Augustus. Dis-\ntributed deep q-learning. Technical report, Stanford Uni-\nversity, June 2015. Degris, Thomas, Pilarski, Patrick M, and Sutton, Richard S. Model-free reinforcement learning with continuous ac-\ntion in practice. In American Control Conference (ACC),\n2012, pp. IEEE, 2012. Grounds, Matthew and Kudenko, Daniel. Parallel rein-\nforcement learning with linear function approximation.",
    "chunk_index": 0,
    "num_sentences": 18,
    "chunk_size": 976,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nIn Proceedings of the 5th, 6th and 7th European Confer-\nence on Adaptive and Learning Agents and Multi-agent\nSystems: Adaptation and Multi-agent Learning, pp. Springer-Verlag, 2008. Koutn\u00edk, Jan, Schmidhuber, J\u00fcrgen, and Gomez, Faustino.",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 237,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nvision-based reinforcement learning. In Proceedings of\nthe 2014 conference on Genetic and evolutionary com-\nputation, pp. Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,\nPieter. End-to-end training of deep visuomotor policies. arXiv preprint arXiv:1504.00702, 2015. Li, Yuxi and Schuurmans, Dale. Mapreduce for parallel re-\ninforcement learning. In Recent Advances in Reinforce-\nment Learning - 9th European Workshop, EWRL 2011,\nAthens, Greece, September 9-11, 2011, Revised Selected\nPapers, pp. 309\u2013320, 2011. Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,\nHeess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,\nand Wierstra, Daan. Continuous control with deep re-\ninforcement learning. arXiv preprint arXiv:1509.02971,\n2015. Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nGraves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and\nRiedmiller, Martin. Playing atari with deep reinforce-\nment learning. In NIPS Deep Learning Workshop.",
    "chunk_index": 0,
    "num_sentences": 15,
    "chunk_size": 961,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nRusu, Andrei A., Veness, Joel, Bellemare, Marc G.,\nGraves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,\nOstrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,\nAmir, Antonoglou, Ioannis, King, Helen, Kumaran,\nDharshan, Wierstra, Daan, Legg, Shane, and Hassabis,\nDemis. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529\u2013533, 02 2015. URL\nhttp://dx.doi.org/10.1038/nature14236. Nair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alci-\ncek, Cagdas, Fearon, Rory, Maria, Alessandro De, Pan-\nneershelvam, Vedavyas, Suleyman, Mustafa, Beattie,\nCharles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr,\nKavukcuoglu, Koray, and Silver, David. Massively par-\nallel methods for deep reinforcement learning. In ICML\nDeep Learning Workshop. Peng, Jing and Williams, Ronald J. Incremental multi-step\nq-learning. Machine Learning, 22(1-3):283\u2013290, 1996. Recht, Benjamin, Re, Christopher, Wright, Stephen, and\nNiu, Feng.",
    "chunk_index": 1,
    "num_sentences": 11,
    "chunk_size": 988,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nHogwild: A lock-free approach to paralleliz-\ning stochastic gradient descent. In Advances in Neural\nInformation Processing Systems, pp. 693\u2013701, 2011. Riedmiller, Martin. Neural \ufb01tted q iteration\u2013\ufb01rst experi-\nences with a data ef\ufb01cient neural reinforcement learning\nmethod. In Machine Learning: ECML 2005, pp. Springer Berlin Heidelberg, 2005. Rummery, Gavin A and Niranjan, Mahesan. On-line q-\nlearning using connectionist systems. Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Sil-\nver, David. Prioritized experience replay. arXiv preprint\narXiv:1511.05952, 2015. Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan,\nMichael I, and Abbeel, Pieter. Trust region policy op-\ntimization.",
    "chunk_index": 2,
    "num_sentences": 14,
    "chunk_size": 693,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nLearning (ICML), 2015a. Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan,\nMichael, and Abbeel, Pieter. High-dimensional con-\ntinuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b. and Barto, A. Reinforcement Learning: an In-\ntroduction. MIT Press, 1998. Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-\nrmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural Networks for\nMachine Learning, 4, 2012. Todorov, E. MuJoCo: Modeling, Simulation and Visual-\nization of Multi-Joint Dynamics with Contact (ed 1.0). Roboti Publishing, 2015.",
    "chunk_index": 0,
    "num_sentences": 13,
    "chunk_size": 618,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nTomassini, Marco. Parallel and distributed evolutionary al-\ngorithms: A review. Technical report, 1999. Tsitsiklis, John N. Asynchronous stochastic approxima-\ntion and q-learning. Machine Learning, 16(3):185\u2013202,\n1994. Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep\nreinforcement learning with double q-learning. arXiv\npreprint arXiv:1509.06461, 2015. van Seijen, H., Rupam Mahmood, A., Pilarski, P. M.,\nMachado, M. C., and Sutton, R.",
    "chunk_index": 0,
    "num_sentences": 12,
    "chunk_size": 444,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nTemporal-Difference Learning. ArXiv e-prints, Decem-\nber 2015. Wang, Z., de Freitas, N., and Lanctot, M. Dueling Network\nArchitectures for Deep Reinforcement Learning. ArXiv\ne-prints, November 2015. Watkins, Christopher John Cornish Hellaby. Learning from\ndelayed rewards. PhD thesis, University of Cambridge\nEngland, 1989. Williams, R.J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Ma-\nchine Learning, 8(3):229\u2013256, 1992. Williams, Ronald J and Peng, Jing. Function optimization\nusing connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991. Wymann, B., Espi\u00c3l\u2019, E., Guionneau, C., Dimitrakakis, C.,\nCoulom, R., and Sumner, A. Torcs: The open racing car\nsimulator, v1.3.5, 2013. Supplementary Material for \"Asynchronous Methods for Deep\nReinforcement Learning\"\nJune 17, 2016",
    "chunk_index": 0,
    "num_sentences": 17,
    "chunk_size": 856,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nWe investigated two different optimization algorithms with our asynchronous framework \u2013 stochastic gradient\ndescent and RMSProp. Our implementations of these algorithms do not use any locking in order to maximize\nthroughput when using a large number of threads. Momentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and\nwell studied (Recht et al., 2011). Let \u03b8 be the parameter vector that is shared across all threads and let \u2206\u03b8i\nbe the accumulated gradients of the loss with respect to parameters \u03b8 computed by thread number i. Each\nthread i independently applies the standard momentum SGD update mi = \u03b1mi + (1 \u2212\u03b1)\u2206\u03b8i followed by\n\u03b8 \u2190\u03b8 \u2212\u03b7mi with learning rate \u03b7, momentum \u03b1 and without any locks. Note that in this setting, each thread\nmaintains its own separate gradient and momentum vector.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 836,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nRMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature,\nit has not been extensively studied in the asynchronous optimization setting. The standard non-centered",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 206,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82\n(S2)\n\u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(S3)\nwhere all operations are performed elementwise. In order to apply RMSProp in the asynchronous optimiza-\ntion setting one must decide whether the moving average of elementwise squared gradients g is shared or\nper-thread. We experimented with two versions of the algorithm. In one version, which we refer to as RM-\nSProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared\nRMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing\nstatistics among threads also reduces memory requirements by using one fewer copy of the parameter vector\nper thread. We compared these three asynchronous optimization algorithms in terms of their sensitivity to different learn-\ning rates and random network initializations.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 844,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nFigure S5 shows a comparison of the methods for two different\nreinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games\n(Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the scores for 50 experiments that\ncorrespond to 50 different random learning rates and initializations. The x-axis shows the rank of the model\nafter sorting in descending order by \ufb01nal average score and the y-axis shows the \ufb01nal average score achieved\nby the corresponding model. In this representation, the algorithm that performs better would achieve higher\nmaximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizon-\ntal, thus maximizing the area under the curve. RMSProp with shared statistics tends to be more robust than\nRMSProp with per-thread statistics, which is in turn more robust than Momentum SGD.",
    "chunk_index": 1,
    "num_sentences": 5,
    "chunk_size": 896,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS\nexperiments (Figure S6) used the following setup. Each experiment used 16 actor-learner threads running\non a single machine and no GPUs. All methods performed updates after every 5 actions (tmax = 5 and\nIUpdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods\nused a shared target network that was updated every 40000 frames. The Atari experiments used the same\ninput preprocessing as (Mnih et al., 2015) and an action repeat of 4. The agents used the network architecture\nfrom (Mnih et al., 2013). The network used a convolutional layer with 16 \ufb01lters of size 8 \u00d7 8 with stride\n4, followed by a convolutional layer with with 32 \ufb01lters of size 4 \u00d7 4 with stride 2, followed by a fully\nconnected layer with 256 hidden units. All three hidden layers were followed by a recti\ufb01er nonlinearity.",
    "chunk_index": 0,
    "num_sentences": 8,
    "chunk_size": 938,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe\nvalue-based methods had a single linear output unit for each action representing the action-value. The model\nused by actor-critic agents had two set of outputs \u2013 a softmax output with one entry per action representing the\nprobability of selecting the action, and a single linear output representing the value function. All experiments\nused a discount of \u03b3 = 0.99 and an RMSProp decay factor of \u03b1 = 0.99. The value based methods sampled the exploration rate \u03f5 from a distribution taking three values \u03f51, \u03f52, \u03f53 with\nprobabilities 0.4, 0.3, 0.3. The values of \u03f51, \u03f52, \u03f53 were annealed from 1 to 0.1, 0.01, 0.5 respectively over\nthe \ufb01rst four million frames. Advantage actor-critic used entropy regularization with a weight \u03b2 = 0.01 for\nall Atari and TORCS experiments. We performed a set of 50 experiments for \ufb01ve Atari games and every\nTORCS level, each using a different random initialization and initial learning rate.",
    "chunk_index": 1,
    "num_sentences": 7,
    "chunk_size": 922,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe initial learning rate\nwas sampled from a LogUniform(10\u22124, 10\u22122) distribution and annealed to 0 over the course of training. Note that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used\n\ufb01xed hyperparameters.",
    "chunk_index": 2,
    "num_sentences": 2,
    "chunk_size": 256,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nTo apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly\nidentical to that used in the discrete action domains, so here we enumerate only the differences required for\nthe continuous action domains. The essential elements for many of the tasks (i.e. the physics models and\ntask objectives) are near identical to the tasks examined in (Lillicrap et al., 2015). However, the rewards and\nthus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco\nwhich altered the contact model. For all the domains we attempted to learn the task using the physical state as input. The physical state\nconsisted of the joint positions and velocities as well as the target position if the task required a target. In\naddition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from\nRGB pixel inputs.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 920,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn the low dimensional physical state case, the inputs are mapped to a hidden state using\none hidden layer with 200 ReLU units. In the cases where we used pixels, the input was passed through two\nlayers of spatial convolutions without any non-linearity or pooling. In either case, the output of the encoder\nlayers were fed to a single layer of 128 LSTM cells. The most important difference in the architecture is in the\nthe output layer of the policy network. Unlike the discrete action domain where the action output is a Softmax,\nhere the two outputs of the policy network are two real number vectors which we treat as the mean vector \u00b5\nand scalar variance \u03c32 of a multidimensional normal distribution with a spherical covariance. To act, the input\nis passed through the model to the output layer where we sample from the normal distribution determined by\n\u00b5 and \u03c32.",
    "chunk_index": 1,
    "num_sentences": 6,
    "chunk_size": 867,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn practice, \u00b5 is modeled by a linear layer and \u03c32 by a SoftPlus operation, log(1 + exp(x)), as the\nactivation computed as a function of the output of a linear layer. In our experiments with continuous control\nproblems the networks for policy network and value network do not share any parameters, though this detail\nis unlikely to be crucial. Finally, since the episodes were typically at most several hundred time steps long,\nwe did not use any bootstrapping in the policy or value function updates and batched each episode into a\nsingle update. As in the discrete action case, we included an entropy cost which encouraged exploration.",
    "chunk_index": 2,
    "num_sentences": 4,
    "chunk_size": 637,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn the continuous",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\ncase the we used a cost on the differential entropy of the normal distribution de\ufb01ned by the output of the\nactor network, \u22121\n2(log(2\u03c0\u03c32) + 1), we used a constant multiplier of 10\u22124 for this cost across all of the tasks\nexamined. The asynchronous advantage actor-critic algorithm \ufb01nds solutions for all the domains. Figure S8\nshows learning curves against wall-clock time, and demonstrates that most of the domains from states can be\nsolved within a few hours. All of the experiments, including those done from pixel based observations, were\nrun on CPU. Even in the case of solving the domains directly from pixel inputs we found that it was possible\nto reliably discover solutions within 24 hours. Figure S7 shows scatter plots of the top scores against the\nsampled learning rates. In most of the domains there is large range of learning rates that consistently achieve\ngood performance on the task. Algorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread.",
    "chunk_index": 0,
    "num_sentences": 8,
    "chunk_size": 987,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\n// Assume global shared parameter vector \u03b8. // Assume global shared target parameter vector \u03b8\u2212. // Assume global shared counter T = 0. Initialize thread step counter t \u21901\nInitialize target network parameters \u03b8\u2212\u2190\u03b8\nInitialize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\nInitialize network gradients d\u03b8 \u21900\nrepeat\nClear gradients d\u03b8 \u21900\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\ntstart = t",
    "chunk_index": 1,
    "num_sentences": 4,
    "chunk_size": 376,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\nrepeat\nTake action at according to the \u03f5-greedy policy based on Q(st, a; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a 0\nfor terminal st\nmaxa Q(st, a; \u03b8\u2212)\nfor non-terminal st\nfor i \u2208{t \u22121, . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 +\n\u2202(R\u2212Q(si,ai;\u03b8\u2032))\n2\n\u2202\u03b8\u2032\nend for\nPerform asynchronous update of \u03b8 using d\u03b8. if T\nmod Itarget == 0 then\n\u03b8\u2212\u2190\u03b8\nend if\nuntil T > Tmax",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 438,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 70,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAlgorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread. // Assume global shared parameter vectors \u03b8 and \u03b8v and global shared counter T = 0\n// Assume thread-speci\ufb01c parameter vectors \u03b8\u2032 and \u03b8\u2032\nv\nInitialize thread step counter t \u21901\nrepeat\nReset gradients: d\u03b8 \u21900 and d\u03b8v \u21900. Synchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8 and \u03b8\u2032\nv = \u03b8v\ntstart = t",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 378,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 71,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\nrepeat\nPerform at according to policy \u03c0(at|st; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a\n0\nfor terminal st\nV (st, \u03b8\u2032\nv)\nfor non-terminal st// Bootstrap from last state\nfor i \u2208{t \u22121, . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 + \u2207\u03b8\u2032 log \u03c0(ai|si; \u03b8\u2032)(R \u2212V (si; \u03b8\u2032\nv))\nAccumulate gradients wrt \u03b8\u2032\nv: d\u03b8v \u2190d\u03b8v + \u2202(R \u2212V (si; \u03b8\u2032\nv))2/\u2202\u03b8\u2032\nv\nend for\nPerform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 484,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 72,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\nuntil T > Tmax",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 72,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\nFigure S5. Comparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested\nusing two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Break-\nout, Beamrider, Seaquest and Space Invaders). Each curve shows the \ufb01nal scores for 50 experiments sorted in descending\norder that covers a search over 50 random initializations and learning rates. The top row shows results using Async n-step\nQ algorithm and bottom row shows results with Async Advantage Actor-Critic. Each individual graph shows results for\none of the four games and three different optimization methods. Shared RMSProp tends to be more robust to different\nlearning rates and random initializations than Momentum SGD and RMSProp without sharing. 0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 895,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Human tester\n\nComparison of algorithms on the TORCS car racing simulator. Four different con\ufb01gurations of car speed and\nopponent presence or absence are shown. In each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and\nAdvantage Actor-Critic) are compared on score vs training time in wall clock hours. Multi-step algorithms achieve better\npolicies much faster than one-step algorithms on all four levels. The curves show averages over the 5 best runs from 50\nexperiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 566,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Human tester",
      "section_index": 74,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nPerformance for the Mujoco continuous action domains. Scatter plot of the best score obtained against\nlearning rates sampled from LogUniform(10\u22125, 10\u22121). For nearly all of the tasks there is a wide range of learning\nrates that lead to good performance on the task.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 264,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 75,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nScore per episode vs wall-clock time plots for the Mujoco domains. Each plot shows error bars for the top 5\nexperiments.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 120,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 76,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 77,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 78,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 79,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 138,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 80,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S9. Data ef\ufb01ciency comparison of different numbers of actor-learners one-step Sarsa on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over\n50 random learning rates. Sarsa shows increased data ef\ufb01ciency with increased numbers of parallel workers.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 583,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 81,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 186,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 82,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 185,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 83,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 192,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 84,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 196,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 85,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S10. Training speed comparison of different numbers of actor-learners for all one-step Sarsa on \ufb01ve Atari games. The x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average of the\nthree best performing agents from a search over 50 random learning rates. Sarsa shows signi\ufb01cant speedups from using\ngreater numbers of parallel actor-learners.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 519,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n10-4\n10-3\n10-2",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nn-step Q, Space Invaders\nFigure S11. Scatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on \ufb01ve games (Beamrider,\nBreakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. All algorithms exhibit\nsome level of robustness to the choice of learning rate.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 318,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 87,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Gravitar\n\n216.5\n538.4\n200.5\n297.0\n218.0\n269.5\n303.5\n320.0\nH.E.R.O. 12952.5\n8963.4\n14892.5\n15207.9\n20506.4\n28765.8\n32464.1\n28889.5",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 119,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Gravitar",
      "section_index": 88,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Krull\n\n3864.0\n6363.1\n6796.1\n8051.6\n7406.5\n8066.6\n5560.0\n5911.4\nKung-Fu Master\n11875.0\n20620.0\n30207.0\n24288.0\n31244.0\n3046.0\n28819.0\n40835.0\nMontezuma\u2019s Revenge\n50.0\n84.0\n42.0\n22.0\n13.0\n53.0\n67.0\n41.0\nMs. Pacman\n763.5\n1263.0\n1241.3\n2250.6\n1824.6\n594.4\n653.7\n850.7",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 256,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Krull",
      "section_index": 89,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Private Eye\n\n298.2\n2598.6\n-575.5\n292.6\n179.0\n194.4\n206.9\n421.1\nQ*Bert\n4589.8\n7089.8\n11020.8\n14175.8\n11277.0\n13752.3\n15148.8\n21307.5",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 118,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Private Eye",
      "section_index": 90,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Zaxxon\n\n831.0\n6159.4\n8593.0\n10164.0\n9501.0\n2659.0\n24622.0\n23519.0\nTable S3. Raw scores for the human start condition (30 minutes emulator time). DQN scores taken from (Nair et al.,\n2015). Double DQN scores taken from (Van Hasselt et al., 2015), Dueling scores from (Wang et al., 2015) and Prioritized\nscores taken from (Schaul et al., 2015)",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 332,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Zaxxon",
      "section_index": 91,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nModel-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks. However, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 844,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nBy combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 414,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nModel-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>. of these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 894,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks. One cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step. This quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015).",
    "chunk_index": 1,
    "num_sentences": 6,
    "chunk_size": 991,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nUnfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017). We explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces.",
    "chunk_index": 2,
    "num_sentences": 4,
    "chunk_size": 796,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nTo that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n\n\nSoft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors.",
    "chunk_index": 3,
    "num_sentences": 3,
    "chunk_size": 834,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nPrior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces. In this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016).",
    "chunk_index": 4,
    "num_sentences": 4,
    "chunk_size": 956,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nSAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods. We also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG.",
    "chunk_index": 5,
    "num_sentences": 4,
    "chunk_size": 857,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Related Work\n\nOur soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 299,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Related Work",
      "section_index": 2,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nthis section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly. In this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 997,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function. As such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).",
    "chunk_index": 1,
    "num_sentences": 5,
    "chunk_size": 946,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.",
    "chunk_index": 2,
    "num_sentences": 5,
    "chunk_size": 876,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n\n\nSoft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al.",
    "chunk_index": 3,
    "num_sentences": 6,
    "chunk_size": 931,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\n(2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization.",
    "chunk_index": 4,
    "num_sentences": 7,
    "chunk_size": 767,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nFur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin.",
    "chunk_index": 5,
    "num_sentences": 2,
    "chunk_size": 445,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Preliminaries\n\nWe \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks. Notation\nWe address policy learning in continuous action spaces. We consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A. The environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition. We will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st). Maximum Entropy Reinforcement Learning",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 774,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Preliminaries",
      "section_index": 4,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Standard RL maximizes the expected sum of rewards\n\nP\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) =",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 236,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Standard RL maximizes the expected sum of rewards",
      "section_index": 5,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: T\nX\n\nt=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] . (1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 569,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "T\nX",
      "section_index": 6,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective has a number of conceptual and practical\n\nadvantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues. Second, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 916,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective has a number of conceptual and practical",
      "section_index": 7,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective has a number of conceptual and practical\n\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework.",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 640,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective has a number of conceptual and practical",
      "section_index": 7,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nActor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory. Soft Actor-Critic\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 832,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nWe\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities. In the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below. Lemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e. See Appendix B.1.",
    "chunk_index": 1,
    "num_sentences": 8,
    "chunk_size": 992,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies. While in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n.",
    "chunk_index": 2,
    "num_sentences": 6,
    "chunk_size": 888,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\n(4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2. Lemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e. See Appendix B.2.",
    "chunk_index": 3,
    "num_sentences": 7,
    "chunk_size": 634,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nthe soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1). Although this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic. Theorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e. See Appendix B.3.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 872,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nSoft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st). The parameters of these networks are \u03c8, \u03b8, and \u03c6. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors. The state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3.",
    "chunk_index": 1,
    "num_sentences": 8,
    "chunk_size": 988,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThis quantity can be\n\n\nSoft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer.",
    "chunk_index": 2,
    "num_sentences": 3,
    "chunk_size": 770,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThe soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001. (9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014",
    "chunk_index": 3,
    "num_sentences": 4,
    "chunk_size": 779,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\n\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n. (10)\nThere are several options for minimizing J\u03c0. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 730,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.",
    "chunk_index": 1,
    "num_sentences": 4,
    "chunk_size": 928,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al. Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer.",
    "chunk_index": 2,
    "num_sentences": 6,
    "chunk_size": 943,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nIn practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n\n\nSoft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 706,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SQL\n\nTD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks. for all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple.",
    "chunk_index": 0,
    "num_sentences": 6,
    "chunk_size": 555,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SQL",
      "section_index": 11,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nhow the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 967,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements.",
    "chunk_index": 1,
    "num_sentences": 4,
    "chunk_size": 889,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC). The source code of our SAC implementation1 and videos2\nare available online. 1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n\n\nSoft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.",
    "chunk_index": 2,
    "num_sentences": 8,
    "chunk_size": 938,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance.",
    "chunk_index": 3,
    "num_sentences": 4,
    "chunk_size": 683,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The quantitative results attained by SAC in our experiments\n\nalso compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D. Ablation Study",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 358,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The quantitative results attained by SAC in our experiments",
      "section_index": 13,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nbased on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant. Stochastic vs. deterministic policy. Soft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5).",
    "chunk_index": 0,
    "num_sentences": 9,
    "chunk_size": 796,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nmaximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise. Figure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 839,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nSoft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks). Policy evaluation.",
    "chunk_index": 1,
    "num_sentences": 5,
    "chunk_size": 663,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\npolicies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy. Reward scale. Soft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 896,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nFigure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 255,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nFor\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n\n\nSoft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 492,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nSensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately. (c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue). but the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.",
    "chunk_index": 3,
    "num_sentences": 8,
    "chunk_size": 863,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter. Target network update.",
    "chunk_index": 4,
    "num_sentences": 3,
    "chunk_size": 409,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\ntarget value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies. Large \u03c4 can lead to instabilities while small\n\u03c4 can make training slower. However, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps.",
    "chunk_index": 0,
    "num_sentences": 7,
    "chunk_size": 920,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\nWe found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 175,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Conclusion\n\nWe present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 648,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Conclusion",
      "section_index": 18,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Conclusion\n\nOur results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 392,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Conclusion",
      "section_index": 18,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Acknowledgments\n\nWe would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 227,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Acknowledgments",
      "section_index": 19,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nG., Sutton, R. S., and Anderson, C. Neuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834\u2013846, 1983. Bhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv\u00b4ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation.",
    "chunk_index": 0,
    "num_sentences": 9,
    "chunk_size": 361,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n(NIPS), pp. 1204\u20131212, 2009. Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016. Duan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016. Fox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016. Fujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018. Gruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R. The reactor: A sample-ef\ufb01cient actor-critic architecture. arXiv preprint arXiv:1704.04651, 2017. Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic.",
    "chunk_index": 0,
    "num_sentences": 23,
    "chunk_size": 972,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint arXiv:1611.02247,\n2016. Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352\u20131361, 2017. Hasselt, H. Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613\u20132621,\n2010. Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y. Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944\u20132952, 2015. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015. and Koltun, V. Guided policy search. In Interna-\ntional Conference on Machine Learning (ICML), pp. Levine, S., Finn, C., Darrell, T., and Abbeel, P.",
    "chunk_index": 1,
    "num_sentences": 22,
    "chunk_size": 982,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nEnd-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1\u201340, 2016. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, 2015. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.",
    "chunk_index": 2,
    "num_sentences": 21,
    "chunk_size": 958,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Bridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772\u20132782, 2017a. Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Trust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b. O\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. PGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682\u2013\n697, 2008. Rawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS),\n2012. Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp. 1889\u20131897, 2015.",
    "chunk_index": 3,
    "num_sentences": 21,
    "chunk_size": 988,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSoft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning. arXiv preprint\narXiv:1704.06440, 2017a. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484\u2013489, Jan 2016. ISSN 0028-0836. and Barto, A. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.",
    "chunk_index": 4,
    "num_sentences": 17,
    "chunk_size": 996,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nBias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp. 441\u2013448, 2014. Todorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. IEEE, 2008. Toussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. Williams, R. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229\u2013256, 1992. Ziebart, B. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010. Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. Maximum entropy inverse reinforcement learning. In\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp. 1433\u20131438, 2008. Soft Actor-Critic\nA.",
    "chunk_index": 5,
    "num_sentences": 24,
    "chunk_size": 864,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMaximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014). However, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.",
    "chunk_index": 6,
    "num_sentences": 4,
    "chunk_size": 640,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nevery state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy. Proofs\nB.1. Lemma 1\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e. De\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded. Lemma 2\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.",
    "chunk_index": 0,
    "num_sentences": 10,
    "chunk_size": 909,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nLet \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)). (16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st). (18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n... \u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18. Convergence to Q\u03c0new follows from Lemma 1.",
    "chunk_index": 1,
    "num_sentences": 6,
    "chunk_size": 997,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nSoft Actor-Critic\nB.3. Theorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e. Let \u03c0i be the policy at iteration i. By Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217. We will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217. Using the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0. Enforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution.",
    "chunk_index": 2,
    "num_sentences": 11,
    "chunk_size": 936,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nHowever, in practice, the actions needs to be bounded to a \ufb01nite\ninterval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n. (20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212",
    "chunk_index": 3,
    "num_sentences": 5,
    "chunk_size": 656,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: D\nX\n\ni=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u. Soft Actor-Critic\nD. Hyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the\nreward scale parameter that was tuned for each environment. SAC Hyperparameters",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 300,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "D\nX",
      "section_index": 23,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Shared\n\noptimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 225,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Shared",
      "section_index": 24,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SAC\n\ntarget smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2. SAC Environment Speci\ufb01c Parameters",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 275,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SAC",
      "section_index": 25,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\nHopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n\n\nSoft Actor-Critic\nE. Additional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 873,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\nBoth of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 146,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 581,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SAC\n\nSAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.",
    "chunk_index": 0,
    "num_sentences": 5,
    "chunk_size": 763,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SAC",
      "section_index": 27,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  }
]