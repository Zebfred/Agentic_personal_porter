[
  {
    "text": "Proximal Policy Optimization Algorithms\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\nOpenAI\n{joschu, filip, prafulla, alec, oleg}@openai.com\nAbstract\nWe propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\u201csurrogate\u201d objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\nproximal policy optimization (PPO), have some of the bene\ufb01ts of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time.\n1\nIntroduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\nlarge models and parallel implementations), data e\ufb03cient, and robust (i.e., successful on a variety\nof problems without hyperparameter tuning). Q-learning (with function approximation) fails on\nmany simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\ne\ufb03ency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\n(between the policy and value function, or with auxiliary tasks).\nThis paper seeks to improve the current state of a\ufb00airs by introducing an algorithm that attains\nthe data e\ufb03ciency and reliable performance of TRPO, while using only \ufb01rst-order optimization.\nWe propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\n(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between\nsampling data from the policy and performing several epochs of optimization on the sampled data.\nOur experiments compare the performance of various di\ufb00erent versions of the surrogate objec-\ntive, and \ufb01nd that the version with the clipped probability ratios performs best. We also compare\nPPO to several previous algorithms from the literature. On continuous control tasks, it performs\nbetter than the algorithms we compare against. On Atari, it performs signi\ufb01cantly better (in terms\nof sample complexity) than A2C and similarly to ACER though it is much simpler.\n1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete\naction spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in\nOpenAI Gym [Bro+16] and described by Duan et al. [Dua+16].\n1\narXiv:1707.06347v2  [cs.LG]  28 Aug 2017\n\n\n2\nBackground: Policy Optimization\n2.1\nPolicy Gradient Methods\nPolicy gradient methods work by computing an estimator of the policy gradient and plugging it\ninto a stochastic gradient ascent algorithm. The most commonly used gradient estimator has the\nform\n\u02c6g = \u02c6Et\nh\n\u2207\u03b8 log \u03c0\u03b8(at | st) \u02c6At\ni\n(1)\nwhere \u03c0\u03b8 is a stochastic policy and \u02c6At is an estimator of the advantage function at timestep t.\nHere, the expectation \u02c6Et[. . .] indicates the empirical average over a \ufb01nite batch of samples, in an\nalgorithm that alternates between sampling and optimization. Implementations that use automatic\ndi\ufb00erentiation software work by constructing an objective function whose gradient is the policy\ngradient estimator; the estimator \u02c6g is obtained by di\ufb00erentiating the objective\nLPG(\u03b8) = \u02c6Et\nh\nlog \u03c0\u03b8(at | st) \u02c6At\ni\n.\n(2)\nWhile it is appealing to perform multiple steps of optimization on this loss LPG using the same\ntrajectory, doing so is not well-justi\ufb01ed, and empirically it often leads to destructively large policy\nupdates (see Section 6.1; results are not shown but were similar or worse than the \u201cno clipping or\npenalty\u201d setting).\n2.2\nTrust Region Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n(3)\nsubject to\n\u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]] \u2264\u03b4.\n(4)\nHere, \u03b8old is the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2 that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, experiments show\nthat it is not su\ufb03cient to simply choose a \ufb01xed penalty coe\ufb03cient \u03b2 and optimize the penalized\nobjective Equation (5) with SGD; additional modi\ufb01cations are required.\n2\n\n\n3\nClipped Surrogate Objective\nLet rt(\u03b8) denote the probability ratio rt(\u03b8) =\n\u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st), so r(\u03b8old) = 1.\nTRPO maximizes a\n\u201csurrogate\u201d objective\nLCPI(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n= \u02c6Et\nh\nrt(\u03b8) \u02c6At\ni\n.\n(6)\nThe superscript CPI refers to conservative policy iteration [KL02], where this objective was pro-\nposed.\nWithout a constraint, maximization of LCPI would lead to an excessively large policy\nupdate; hence, we now consider how to modify the objective, to penalize changes to the policy that\nmove rt(\u03b8) away from 1.\nThe main objective we propose is the following:\nLCLIP (\u03b8) = \u02c6Et\nh\nmin(rt(\u03b8) \u02c6At, clip(rt(\u03b8), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At)\ni\n(7)\nwhere epsilon is a hyperparameter, say, \u03f5 = 0.2. The motivation for this objective is as follows. The\n\ufb01rst term inside the min is LCPI. The second term, clip(rt(\u03b8), 1\u2212\u03f5, 1+\u03f5) \u02c6At, modi\ufb01es the surrogate\nobjective by clipping the probability ratio, which removes the incentive for moving rt outside of the\ninterval [1 \u2212\u03f5, 1 + \u03f5]. Finally, we take the minimum of the clipped and unclipped objective, so the\n\ufb01nal objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective. With this\nscheme, we only ignore the change in probability ratio when it would make the objective improve,\nand we include it when it makes the objective worse. Note that LCLIP (\u03b8) = LCPI(\u03b8) to \ufb01rst order\naround \u03b8old (i.e., where r = 1), however, they become di\ufb00erent as \u03b8 moves away from \u03b8old. Figure 1\nplots a single term (i.e., a single t) in LCLIP ; note that the probability ratio r is clipped at 1 \u2212\u03f5\nor 1 + \u03f5 depending on whether the advantage is positive or negative.\nr\nLCLIP\n0\n1 1 + \u03f5\nA > 0\nr\nLCLIP\n0\n1\n1 \u2212\u03f5\nA < 0\nFigure 1: Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of\nthe probability ratio r, for positive advantages (left) and negative advantages (right). The red circle on each\nplot shows the starting point for the optimization, i.e., r = 1. Note that LCLIP sums many of these terms.\nFigure 2 provides another source of intuition about the surrogate objective LCLIP . It shows how\nseveral objectives vary as we interpolate along the policy update direction, obtained by proximal\npolicy optimization (the algorithm we will introduce shortly) on a continuous control problem. We\ncan see that LCLIP is a lower bound on LCPI, with a penalty for having too large of a policy\nupdate.\n3\n\n\n0\n1\nLinear interpolation factor\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nEt[KLt]\nLCPI = Et[rtAt]\nEt[clip(rt, 1\n, 1 + )At]\nLCLIP = Et[min(rtAt, clip(rt, 1\n, 1 + )At)]\nFigure 2: Surrogate objectives, as we interpolate between the initial policy parameter \u03b8old, and the updated\npolicy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of\nabout 0.02 from the initial policy, and this is the point at which LCLIP is maximal. This plot corresponds\nto the \ufb01rst policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1.\n4\nAdaptive KL Penalty Coe\ufb03cient\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in\naddition to it, is to use a penalty on KL divergence, and to adapt the penalty coe\ufb03cient so that we\nachieve some target value of the KL divergence dtarg each policy update. In our experiments, we\nfound that the KL penalty performed worse than the clipped surrogate objective, however, we\u2019ve\nincluded it here because it\u2019s an important baseline.\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy\nupdate:\n\u2022 Using several epochs of minibatch SGD, optimize the KL-penalized objective\nLKLPEN(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(8)\n\u2022 Compute d = \u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]]\n\u2013 If d < dtarg/1.5, \u03b2 \u2190\u03b2/2\n\u2013 If d > dtarg \u00d7 1.5, \u03b2 \u2190\u03b2 \u00d7 2\nThe updated \u03b2 is used for the next policy update. With this scheme, we occasionally see policy\nupdates where the KL divergence is signi\ufb01cantly di\ufb00erent from dtarg, however, these are rare, and\n\u03b2 quickly adjusts. The parameters 1.5 and 2 above are chosen heuristically, but the algorithm is\nnot very sensitive to them. The initial value of \u03b2 is a another hyperparameter but is not important\nin practice because the algorithm quickly adjusts it.\n5\nAlgorithm\nThe surrogate losses from the previous sections can be computed and di\ufb00erentiated with a minor\nchange to a typical policy gradient implementation. For implementations that use automatic dif-\nferentation, one simply constructs the loss LCLIP or LKLPEN instead of LPG, and one performs\nmultiple steps of stochastic gradient ascent on this objective.\nMost techniques for computing variance-reduced advantage-function estimators make use a\nlearned state-value function V (s); for example, generalized advantage estimation [Sch+15a], or the\n4\n\n\n\ufb01nite-horizon estimators in [Mni+16]. If using a neural network architecture that shares parameters\nbetween the policy and value function, we must use a loss function that combines the policy\nsurrogate and a value function error term. This objective can further be augmented by adding\nan entropy bonus to ensure su\ufb03cient exploration, as suggested in past work [Wil92; Mni+16].\nCombining these terms, we obtain the following objective, which is (approximately) maximized\neach iteration:\nLCLIP+V F+S\nt\n(\u03b8) = \u02c6Et\n\u0002\nLCLIP\nt\n(\u03b8) \u2212c1LV F\nt\n(\u03b8) + c2S[\u03c0\u03b8](st)\n\u0003\n,\n(9)\nwhere c1, c2 are coe\ufb03cients, and S denotes an entropy bonus, and LV F\nt\nis a squared-error loss\n(V\u03b8(st) \u2212V targ\nt\n)2.\nOne style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\nwith recurrent neural networks, runs the policy for T timesteps (where T is much less than the\nepisode length), and uses the collected samples for an update. This style requires an advantage\nestimator that does not look beyond timestep T. The estimator used by [Mni+16] is\n\u02c6At = \u2212V (st) + rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3T\u2212t+1rT\u22121 + \u03b3T\u2212tV (sT )\n(10)\nwhere t speci\ufb01es the time index in [0, T], within a given length-T trajectory segment. Generalizing\nthis choice, we can use a truncated version of generalized advantage estimation, which reduces to\nEquation (10) when \u03bb = 1:\n\u02c6At = \u03b4t + (\u03b3\u03bb)\u03b4t+1 + \u00b7 \u00b7 \u00b7 + \u00b7 \u00b7 \u00b7 + (\u03b3\u03bb)T\u2212t+1\u03b4T\u22121,\n(11)\nwhere\n\u03b4t = rt + \u03b3V (st+1) \u2212V (st)\n(12)\nA proximal policy optimization (PPO) algorithm that uses \ufb01xed-length trajectory segments is\nshown below. Each iteration, each of N (parallel) actors collect T timesteps of data. Then we\nconstruct the surrogate loss on these NT timesteps of data, and optimize it with minibatch SGD\n(or usually for better performance, Adam [KB14]), for K epochs.\nAlgorithm 1 PPO, Actor-Critic Style\nfor iteration=1, 2, . . . do\nfor actor=1, 2, . . . , N do\nRun policy \u03c0\u03b8old in environment for T timesteps\nCompute advantage estimates \u02c6A1, . . . , \u02c6AT\nend for\nOptimize surrogate L wrt \u03b8, with K epochs and minibatch size M \u2264NT\n\u03b8old \u2190\u03b8\nend for\n6\nExperiments\n6.1\nComparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIP to several natural variations and ablated versions.\nNo clipping or penalty:\nLt(\u03b8) = rt(\u03b8) \u02c6At\nClipping:\nLt(\u03b8) = min(rt(\u03b8) \u02c6At, clip(rt(\u03b8)), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At\nKL penalty (\ufb01xed or adaptive)\nLt(\u03b8) = rt(\u03b8) \u02c6At \u2212\u03b2 KL[\u03c0\u03b8old, \u03c0\u03b8]\n5\n\n\nFor the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2 or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping (\u03f5)\nand the KL penalty (\u03b2, dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1 is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe results are shown in Table 1. Note that the score is negative for the setting without clipping\nor penalties, because for one environment (half cheetah) it leads to a very negative score, which is\nworse than the initial random policy.\nalgorithm\navg. normalized score\nNo clipping or penalty\n-0.39\nClipping, \u03f5 = 0.1\n0.76\nClipping, \u03f5 = 0.2\n0.82\nClipping, \u03f5 = 0.3\n0.70\nAdaptive KL dtarg = 0.003\n0.68\nAdaptive KL dtarg = 0.01\n0.74\nAdaptive KL dtarg = 0.03\n0.71\nFixed KL, \u03b2 = 0.3\n0.62\nFixed KL, \u03b2 = 1.\n0.71\nFixed KL, \u03b2 = 3.\n0.72\nFixed KL, \u03b2 = 10.\n0.69\nTable 1: Results from continuous control benchmark.\nAverage normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2 was initialized at 1.\n6.2\nComparison to Other Algorithms in the Continuous Domain\nNext, we compare PPO (with the \u201cclipped\u201d surrogate objective from Section 3) to several other\nmethods from the literature, which are considered to be e\ufb00ective for continuous problems. We com-\npared against tuned implementations of the following algorithms: trust region policy optimization\n[Sch+15b], cross-entropy method (CEM) [SL06], vanilla policy gradient with adaptive stepsize3,\n2HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, all \u201c-v1\u201d\n3After each batch of data, the Adam stepsize is adjusted based on the KL divergence of the original and updated\npolicy, using a rule similar to the one shown in Section 4. An implementation is available at https://github.com/\nberkeleydeeprlcourse/homework/tree/master/hw4.\n6\n\n\nA2C [Mni+16], A2C with trust region [Wan+16]. A2C stands for advantage actor critic, and is\na synchronous version of A3C, which we found to have the same or better performance than the\nasynchronous version. For PPO, we used the hyperparameters from the previous section, with\n\u03f5 = 0.2. We see that PPO outperforms the previous methods on almost all the continuous control\nenvironments.\n0\n1000000\n500\n0\n500\n1000\n1500\n2000\nHalfCheetah-v1\n0\n1000000\n0\n500\n1000\n1500\n2000\n2500\nHopper-v1\n0\n1000000\n0\n2000\n4000\n6000\n8000\nInvertedDoublePendulum-v1\n0\n1000000\n0\n200\n400\n600\n800\n1000\nInvertedPendulum-v1\n0\n1000000\n120\n100\n80\n60\n40\n20\nReacher-v1\n0\n1000000\n0\n20\n40\n60\n80\n100\n120\nSwimmer-v1\n0\n1000000\n0\n1000\n2000\n3000\nWalker2d-v1\nA2C\nA2C + Trust Region\nCEM\nPPO (Clip)\nVanilla PG, Adaptive\nTRPO\nFigure 3: Comparison of several algorithms on several MuJoCo environments, training for one million\ntimesteps.\n6.3\nShowcase in the Continuous Domain: Humanoid Running and Steering\nTo showcase the performance of PPO on high-dimensional continuous control problems, we train\non a set of problems involving a 3D humanoid, where the robot must run, steer, and get up\no\ufb00the ground, possibly while being pelted by cubes.\nThe three tasks we test on are (1) Ro-\nboschoolHumanoid: forward locomotion only, (2) RoboschoolHumanoidFlagrun: position of target\nis randomly varied every 200 timesteps or whenever the goal is reached, (3) RoboschoolHumanoid-\nFlagrunHarder, where the robot is pelted by cubes and needs to get up o\ufb00the ground. See Figure 5\nfor still frames of a learned policy, and Figure 4 for learning curves on the three tasks. Hyperpa-\nrameters are provided in Table 4. In concurrent work, Heess et al. [Hee+17] used the adaptive KL\nvariant of PPO (Section 4) to learn locomotion policies for 3D robots.\n0\n50M\nTimestep\n0\n1000\n2000\n3000\n4000\nRoboschoolHumanoid-v0\n0\n100M\nTimestep\n0\n500\n1000\n1500\n2000\n2500\nRoboschoolHumanoidFlagrun-v0\n0\n100M\nTimestep\n0\n1000\n2000\n3000\nRoboschoolHumanoidFlagrunHarder-v0\nFigure 4: Learning curves from PPO on 3D humanoid control tasks, using Roboschool.\n7\n\n\nFigure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun. In the \ufb01rst six frames, the\nrobot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward\nthe new target.\n6.4\nComparison to Other Algorithms on the Atari Domain\nWe also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against\nwell-tuned implementations of A2C [Mni+16] and ACER [Wan+16]. For all three algorithms, we\nused the same policy network architechture as used in [Mni+16]. The hyperparameters for PPO\nare provided in Table 5. For the other two algorithms, we used hyperparameters that were tuned\nto maximize performance on this benchmark.\nA table of results and learning curves for all 49 games is provided in Appendix B. We consider\nthe following two scoring metrics: (1) average reward per episode over entire training period (which\nfavors fast learning), and (2) average reward per episode over last 100 episodes of training (which\nfavors \ufb01nal performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we\ncompute the victor by averaging the scoring metric across three trials.\nA2C\nACER\nPPO\nTie\n(1) avg. episode reward over all of training\n1\n18\n30\n0\n(2) avg. episode reward over last 100 episodes\n1\n28\n19\n1\nTable 2: Number of games \u201cwon\u201d by each algorithm, where the scoring metric is averaged across three trials.\n7\nConclusion\nWe have introduced proximal policy optimization, a family of policy optimization methods that use\nmultiple epochs of stochastic gradient ascent to perform each policy update. These methods have\nthe stability and reliability of trust-region methods but are much simpler to implement, requiring\nonly few lines of code change to a vanilla policy gradient implementation, applicable in more general\nsettings (for example, when using a joint architecture for the policy and value function), and have\nbetter overall performance.\n8\nAcknowledgements\nThanks to Rocky Duan, Peter Chen, and others at OpenAI for insightful comments.\n8\n\n\nReferences\n[Bel+15]\nM. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. \u201cThe arcade learning environ-\nment: An evaluation platform for general agents\u201d. In: Twenty-Fourth International\nJoint Conference on Arti\ufb01cial Intelligence. 2015.\n[Bro+16]\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W.\nZaremba. \u201cOpenAI Gym\u201d. In: arXiv preprint arXiv:1606.01540 (2016).\n[Dua+16]\nY. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. \u201cBenchmarking Deep\nReinforcement Learning for Continuous Control\u201d. In: arXiv preprint arXiv:1604.06778\n(2016).\n[Hee+17]\nN. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang,\nA. Eslami, M. Riedmiller, et al. \u201cEmergence of Locomotion Behaviours in Rich Envi-\nronments\u201d. In: arXiv preprint arXiv:1707.02286 (2017).\n[KL02]\nS. Kakade and J. Langford. \u201cApproximately optimal approximate reinforcement learn-\ning\u201d. In: ICML. Vol. 2. 2002, pp. 267\u2013274.\n[KB14]\nD. Kingma and J. Ba. \u201cAdam: A method for stochastic optimization\u201d. In: arXiv\npreprint arXiv:1412.6980 (2014).\n[Mni+15]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. \u201cHuman-level control through deep\nreinforcement learning\u201d. In: Nature 518.7540 (2015), pp. 529\u2013533.\n[Mni+16]\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. \u201cAsynchronous methods for deep reinforcement learning\u201d. In: arXiv\npreprint arXiv:1602.01783 (2016).\n[Sch+15a]\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. \u201cHigh-dimensional contin-\nuous control using generalized advantage estimation\u201d. In: arXiv preprint arXiv:1506.02438\n(2015).\n[Sch+15b]\nJ. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. \u201cTrust region policy\noptimization\u201d. In: CoRR, abs/1502.05477 (2015).\n[SL06]\nI. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In:\nNeural computation 18.12 (2006), pp. 2936\u20132941.\n[TET12]\nE. Todorov, T. Erez, and Y. Tassa. \u201cMuJoCo: A physics engine for model-based con-\ntrol\u201d. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Con-\nference on. IEEE. 2012, pp. 5026\u20135033.\n[Wan+16]\nZ. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas.\n\u201cSample E\ufb03cient Actor-Critic with Experience Replay\u201d. In: arXiv preprint arXiv:1611.01224\n(2016).\n[Wil92]\nR. J. Williams. \u201cSimple statistical gradient-following algorithms for connectionist re-\ninforcement learning\u201d. In: Machine learning 8.3-4 (1992), pp. 229\u2013256.\n9\n\n\nA\nHyperparameters\nHyperparameter\nValue\nHorizon (T)\n2048\nAdam stepsize\n3 \u00d7 10\u22124\nNum. epochs\n10\nMinibatch size\n64\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nTable 3: PPO hyperparameters used for the Mujoco 1 million timestep benchmark.\nHyperparameter\nValue\nHorizon (T)\n512\nAdam stepsize\n\u2217\nNum. epochs\n15\nMinibatch size\n4096\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nNumber of actors\n32 (locomotion), 128 (\ufb02agrun)\nLog stdev. of action distribution\nLinearAnneal(\u22120.7, \u22121.6)\nTable 4: PPO hyperparameters used for the Roboschool experiments. Adam stepsize was adjusted based on\nthe target value of the KL divergence.\nHyperparameter\nValue\nHorizon (T)\n128\nAdam stepsize\n2.5 \u00d7 10\u22124 \u00d7 \u03b1\nNum. epochs\n3\nMinibatch size\n32 \u00d7 8\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nNumber of actors\n8\nClipping parameter \u03f5\n0.1 \u00d7 \u03b1\nVF coe\ufb00. c1 (9)\n1\nEntropy coe\ufb00. c2 (9)\n0.01\nTable 5: PPO hyperparameters used in Atari experiments. \u03b1 is linearly annealed from 1 to 0 over the course\nof learning.\nB\nPerformance on More Atari Games\nHere we include a comparison of PPO against A2C on a larger collection of 49 Atari games. Figure 6\nshows the learning curves of each of three random seeds, while Table 6 shows the mean performance.\n10\n\n\n1000\n2000\nAlien\n0\n250\n500\n750\nAmidar\n0\n2000\n4000\nAssault\n0\n2500\n5000\n7500\nAsterix\n1500\n2000\n2500\nAsteroids\n0\n1000000\n2000000\n3000000\nAtlantis\n0\n500\n1000\nBankHeist\n5000\n10000\n15000\n20000\nBattleZone\n1000\n2000\n3000\n4000\nBeamRider\n30\n40\n50\nBowling\n0\n50\n100\nBoxing\n0\n200\n400\nBreakout\n5000\n10000\nCentipede\n2000\n4000\n6000\nChopperCommand\n50000\n100000\nCrazyClimber\n0\n20000\n40000\nDemonAttack\n17.5\n15.0\n12.5\n10.0\nDoubleDunk\n0\n250\n500\n750\nEnduro\n100\n50\n0\nFishingDerby\n0\n10\n20\n30\nFreeway\n100\n200\n300\nFrostbite\n0\n20000\n40000\nGopher\n250\n500\n750\nGravitar\n10\n8\n6\n4\nIceHockey\n0\n200\n400\n600\nJamesbond\n0\n5000\n10000\nKangaroo\n2000\n4000\n6000\n8000\nKrull\n0\n20000\n40000\nKungFuMaster\n0\n50\n100\nMontezumaRevenge\n1000\n2000\n3000\nMsPacman\n2500\n5000\n7500\n10000\nNameThisGame\n100\n0\nPitfall\n20\n0\n20\nPong\n0\n500\nPrivateEye\n0\n5000\n10000\n15000\nQbert\n2500\n5000\n7500\n10000\nRiverraid\n0\n20000\n40000\nRoadRunner\n2\n4\n6\nRobotank\n0\n500\n1000\n1500\nSeaquest\n500\n1000\nSpaceInvaders\n0\n20000\n40000\nStarGunner\n20\n15\n10\nTennis\n3000\n4000\nTimePilot\n0\n100\n200\n300\nTutankham\n0\n100000\n200000\nUpNDown\n0\n40M\nFrames\n0\n5\n10\nVenture\n0\n40M\nFrames\n50000\n100000\n150000\nVideoPinball\n0\n40M\nFrames\n2000\n4000\nWizardOfWor\n0\n40M\nFrames\n0\n2000\n4000\n6000\nZaxxon\nA2C\nACER\nPPO\nFigure 6: Comparison of PPO and A2C on all 49 ATARI games included in OpenAI Gym at the time of\npublication.\n11\n\n\nA2C\nACER\nPPO\nAlien\n1141.7\n1655.4\n1850.3\nAmidar\n380.8\n827.6\n674.6\nAssault\n1562.9\n4653.8\n4971.9\nAsterix\n3176.3\n6801.2\n4532.5\nAsteroids\n1653.3\n2389.3\n2097.5\nAtlantis\n729265.3\n1841376.0\n2311815.0\nBankHeist\n1095.3\n1177.5\n1280.6\nBattleZone\n3080.0\n8983.3\n17366.7\nBeamRider\n3031.7\n3863.3\n1590.0\nBowling\n30.1\n33.3\n40.1\nBoxing\n17.7\n98.9\n94.6\nBreakout\n303.0\n456.4\n274.8\nCentipede\n3496.5\n8904.8\n4386.4\nChopperCommand\n1171.7\n5287.7\n3516.3\nCrazyClimber\n107770.0\n132461.0\n110202.0\nDemonAttack\n6639.1\n38808.3\n11378.4\nDoubleDunk\n-16.2\n-13.2\n-14.9\nEnduro\n0.0\n0.0\n758.3\nFishingDerby\n20.6\n34.7\n17.8\nFreeway\n0.0\n0.0\n32.5\nFrostbite\n261.8\n285.6\n314.2\nGopher\n1500.9\n37802.3\n2932.9\nGravitar\n194.0\n225.3\n737.2\nIceHockey\n-6.4\n-5.9\n-4.2\nJamesbond\n52.3\n261.8\n560.7\nKangaroo\n45.3\n50.0\n9928.7\nKrull\n8367.4\n7268.4\n7942.3\nKungFuMaster\n24900.3\n27599.3\n23310.3\nMontezumaRevenge\n0.0\n0.3\n42.0\nMsPacman\n1626.9\n2718.5\n2096.5\nNameThisGame\n5961.2\n8488.0\n6254.9\nPitfall\n-55.0\n-16.9\n-32.9\nPong\n19.7\n20.7\n20.7\nPrivateEye\n91.3\n182.0\n69.5\nQbert\n10065.7\n15316.6\n14293.3\nRiverraid\n7653.5\n9125.1\n8393.6\nRoadRunner\n32810.0\n35466.0\n25076.0\nRobotank\n2.2\n2.5\n5.5\nSeaquest\n1714.3\n1739.5\n1204.5\nSpaceInvaders\n744.5\n1213.9\n942.5\nStarGunner\n26204.0\n49817.7\n32689.0\nTennis\n-22.2\n-17.6\n-14.8\nTimePilot\n2898.0\n4175.7\n4342.0\nTutankham\n206.8\n280.8\n254.4\nUpNDown\n17369.8\n145051.4\n95445.0\nVenture\n0.0\n0.0\n0.0\nVideoPinball\n19735.9\n156225.6\n37389.0\nWizardOfWor\n859.0\n2308.3\n4185.3\nZaxxon\n16.3\n29.0\n5008.7\nTable 6: Mean \ufb01nal scores (last 100 episodes) of PPO and A2C on Atari games after 40M game frames (10M\ntimesteps).\n12\n\n\n",
    "title": "Proximal Policy Optimization Algorithms",
    "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which al- ternate between sampling data through interaction with the environment, and optimizing a \u201csurrogate\u201d objective function using stochastic gradient ascent. Whereas standard policy gra- dient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the bene\ufb01ts of trust region policy optimiza- tion (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ- ing simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. 1",
    "sections": [
      {
        "header": "Abstract",
        "content": "We propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\u201csurrogate\u201d objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\nproximal policy optimization (PPO), have some of the bene\ufb01ts of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time."
      },
      {
        "header": "Introduction",
        "content": "In recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\nlarge models and parallel implementations), data e\ufb03cient, and robust (i.e., successful on a variety\nof problems without hyperparameter tuning). Q-learning (with function approximation) fails on\nmany simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\ne\ufb03ency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\n(between the policy and value function, or with auxiliary tasks).\nThis paper seeks to improve the current state of a\ufb00airs by introducing an algorithm that attains\nthe data e\ufb03ciency and reliable performance of TRPO, while using only \ufb01rst-order optimization.\nWe propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\n(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between\nsampling data from the policy and performing several epochs of optimization on the sampled data.\nOur experiments compare the performance of various di\ufb00erent versions of the surrogate objec-\ntive, and \ufb01nd that the version with the clipped probability ratios performs best. We also compare\nPPO to several previous algorithms from the literature. On continuous control tasks, it performs\nbetter than the algorithms we compare against. On Atari, it performs signi\ufb01cantly better (in terms\nof sample complexity) than A2C and similarly to ACER though it is much simpler.\n1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete\naction spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in\nOpenAI Gym [Bro+16] and described by Duan et al. [Dua+16].\n1\narXiv:1707.06347v2  [cs.LG]  28 Aug 2017\n\n\n2\nBackground: Policy Optimization\n2.1"
      },
      {
        "header": "Policy gradient methods work by computing an estimator of the policy gradient and plugging it",
        "content": "into a stochastic gradient ascent algorithm. The most commonly used gradient estimator has the\nform\n\u02c6g = \u02c6Et\nh\n\u2207\u03b8 log \u03c0\u03b8(at | st) \u02c6At\ni\n(1)\nwhere \u03c0\u03b8 is a stochastic policy and \u02c6At is an estimator of the advantage function at timestep t.\nHere, the expectation \u02c6Et[. . .] indicates the empirical average over a \ufb01nite batch of samples, in an\nalgorithm that alternates between sampling and optimization. Implementations that use automatic\ndi\ufb00erentiation software work by constructing an objective function whose gradient is the policy\ngradient estimator; the estimator \u02c6g is obtained by di\ufb00erentiating the objective\nLPG(\u03b8) = \u02c6Et\nh\nlog \u03c0\u03b8(at | st) \u02c6At\ni\n.\n(2)"
      },
      {
        "header": "While it is appealing to perform multiple steps of optimization on this loss LPG using the same",
        "content": "trajectory, doing so is not well-justi\ufb01ed, and empirically it often leads to destructively large policy\nupdates (see Section 6.1; results are not shown but were similar or worse than the \u201cno clipping or\npenalty\u201d setting).\n2.2"
      },
      {
        "header": "Trust Region Methods",
        "content": "In TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n(3)\nsubject to\n\u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]] \u2264\u03b4.\n(4)\nHere, \u03b8old is the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2 that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, experiments show\nthat it is not su\ufb03cient to simply choose a \ufb01xed penalty coe\ufb03cient \u03b2 and optimize the penalized\nobjective Equation (5) with SGD; additional modi\ufb01cations are required.\n2"
      },
      {
        "header": "TRPO maximizes a",
        "content": "\u201csurrogate\u201d objective\nLCPI(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n= \u02c6Et\nh\nrt(\u03b8) \u02c6At\ni\n.\n(6)\nThe superscript CPI refers to conservative policy iteration [KL02], where this objective was pro-\nposed.\nWithout a constraint, maximization of LCPI would lead to an excessively large policy\nupdate; hence, we now consider how to modify the objective, to penalize changes to the policy that\nmove rt(\u03b8) away from 1.\nThe main objective we propose is the following:\nLCLIP (\u03b8) = \u02c6Et\nh\nmin(rt(\u03b8) \u02c6At, clip(rt(\u03b8), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At)\ni\n(7)\nwhere epsilon is a hyperparameter, say, \u03f5 = 0.2. The motivation for this objective is as follows. The\n\ufb01rst term inside the min is LCPI. The second term, clip(rt(\u03b8), 1\u2212\u03f5, 1+\u03f5) \u02c6At, modi\ufb01es the surrogate\nobjective by clipping the probability ratio, which removes the incentive for moving rt outside of the\ninterval [1 \u2212\u03f5, 1 + \u03f5]. Finally, we take the minimum of the clipped and unclipped objective, so the\n\ufb01nal objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective. With this\nscheme, we only ignore the change in probability ratio when it would make the objective improve,\nand we include it when it makes the objective worse. Note that LCLIP (\u03b8) = LCPI(\u03b8) to \ufb01rst order\naround \u03b8old (i.e., where r = 1), however, they become di\ufb00erent as \u03b8 moves away from \u03b8old. Figure 1\nplots a single term (i.e., a single t) in LCLIP ; note that the probability ratio r is clipped at 1 \u2212\u03f5\nor 1 + \u03f5 depending on whether the advantage is positive or negative.\nr"
      },
      {
        "header": "LCLIP",
        "content": "0\n1\n1 \u2212\u03f5\nA < 0\nFigure 1: Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of\nthe probability ratio r, for positive advantages (left) and negative advantages (right). The red circle on each\nplot shows the starting point for the optimization, i.e., r = 1. Note that LCLIP sums many of these terms.\nFigure 2 provides another source of intuition about the surrogate objective LCLIP . It shows how\nseveral objectives vary as we interpolate along the policy update direction, obtained by proximal\npolicy optimization (the algorithm we will introduce shortly) on a continuous control problem. We\ncan see that LCLIP is a lower bound on LCPI, with a penalty for having too large of a policy\nupdate.\n3\n\n\n0"
      },
      {
        "header": "Linear interpolation factor",
        "content": "0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nEt[KLt]\nLCPI = Et[rtAt]\nEt[clip(rt, 1\n, 1 + )At]\nLCLIP = Et[min(rtAt, clip(rt, 1\n, 1 + )At)]\nFigure 2: Surrogate objectives, as we interpolate between the initial policy parameter \u03b8old, and the updated\npolicy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of\nabout 0.02 from the initial policy, and this is the point at which LCLIP is maximal. This plot corresponds\nto the \ufb01rst policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1.\n4\nAdaptive KL Penalty Coe\ufb03cient\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in\naddition to it, is to use a penalty on KL divergence, and to adapt the penalty coe\ufb03cient so that we\nachieve some target value of the KL divergence dtarg each policy update. In our experiments, we\nfound that the KL penalty performed worse than the clipped surrogate objective, however, we\u2019ve\nincluded it here because it\u2019s an important baseline.\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy\nupdate:\n\u2022 Using several epochs of minibatch SGD, optimize the KL-penalized objective\nLKLPEN(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(8)\n\u2022 Compute d = \u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]]\n\u2013 If d < dtarg/1.5, \u03b2 \u2190\u03b2/2\n\u2013 If d > dtarg \u00d7 1.5, \u03b2 \u2190\u03b2 \u00d7 2\nThe updated \u03b2 is used for the next policy update. With this scheme, we occasionally see policy\nupdates where the KL divergence is signi\ufb01cantly di\ufb00erent from dtarg, however, these are rare, and\n\u03b2 quickly adjusts. The parameters 1.5 and 2 above are chosen heuristically, but the algorithm is\nnot very sensitive to them. The initial value of \u03b2 is a another hyperparameter but is not important\nin practice because the algorithm quickly adjusts it."
      },
      {
        "header": "Algorithm",
        "content": "The surrogate losses from the previous sections can be computed and di\ufb00erentiated with a minor\nchange to a typical policy gradient implementation. For implementations that use automatic dif-\nferentation, one simply constructs the loss LCLIP or LKLPEN instead of LPG, and one performs\nmultiple steps of stochastic gradient ascent on this objective.\nMost techniques for computing variance-reduced advantage-function estimators make use a\nlearned state-value function V (s); for example, generalized advantage estimation [Sch+15a], or the\n4\n\n\n\ufb01nite-horizon estimators in [Mni+16]. If using a neural network architecture that shares parameters\nbetween the policy and value function, we must use a loss function that combines the policy\nsurrogate and a value function error term. This objective can further be augmented by adding\nan entropy bonus to ensure su\ufb03cient exploration, as suggested in past work [Wil92; Mni+16].\nCombining these terms, we obtain the following objective, which is (approximately) maximized\neach iteration:\nLCLIP+V F+S\nt\n(\u03b8) = \u02c6Et\n\u0002"
      },
      {
        "header": "LCLIP",
        "content": "t\n(\u03b8) \u2212c1LV F\nt\n(\u03b8) + c2S[\u03c0\u03b8](st)\n\u0003\n,\n(9)\nwhere c1, c2 are coe\ufb03cients, and S denotes an entropy bonus, and LV F\nt\nis a squared-error loss\n(V\u03b8(st) \u2212V targ\nt\n)2.\nOne style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\nwith recurrent neural networks, runs the policy for T timesteps (where T is much less than the\nepisode length), and uses the collected samples for an update. This style requires an advantage\nestimator that does not look beyond timestep T. The estimator used by [Mni+16] is\n\u02c6At = \u2212V (st) + rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3T\u2212t+1rT\u22121 + \u03b3T\u2212tV (sT )\n(10)\nwhere t speci\ufb01es the time index in [0, T], within a given length-T trajectory segment. Generalizing\nthis choice, we can use a truncated version of generalized advantage estimation, which reduces to\nEquation (10) when \u03bb = 1:\n\u02c6At = \u03b4t + (\u03b3\u03bb)\u03b4t+1 + \u00b7 \u00b7 \u00b7 + \u00b7 \u00b7 \u00b7 + (\u03b3\u03bb)T\u2212t+1\u03b4T\u22121,\n(11)\nwhere\n\u03b4t = rt + \u03b3V (st+1) \u2212V (st)\n(12)\nA proximal policy optimization (PPO) algorithm that uses \ufb01xed-length trajectory segments is\nshown below. Each iteration, each of N (parallel) actors collect T timesteps of data. Then we\nconstruct the surrogate loss on these NT timesteps of data, and optimize it with minibatch SGD\n(or usually for better performance, Adam [KB14]), for K epochs.\nAlgorithm 1 PPO, Actor-Critic Style\nfor iteration=1, 2, . . . do\nfor actor=1, 2, . . . , N do\nRun policy \u03c0\u03b8old in environment for T timesteps\nCompute advantage estimates \u02c6A1, . . . , \u02c6AT\nend for\nOptimize surrogate L wrt \u03b8, with K epochs and minibatch size M \u2264NT\n\u03b8old \u2190\u03b8\nend for"
      },
      {
        "header": "Comparison of Surrogate Objectives",
        "content": "First, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIP to several natural variations and ablated versions.\nNo clipping or penalty:\nLt(\u03b8) = rt(\u03b8) \u02c6At\nClipping:\nLt(\u03b8) = min(rt(\u03b8) \u02c6At, clip(rt(\u03b8)), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At\nKL penalty (\ufb01xed or adaptive)\nLt(\u03b8) = rt(\u03b8) \u02c6At \u2212\u03b2 KL[\u03c0\u03b8old, \u03c0\u03b8]\n5\n\n\nFor the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2 or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping (\u03f5)\nand the KL penalty (\u03b2, dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1 is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe results are shown in Table 1. Note that the score is negative for the setting without clipping\nor penalties, because for one environment (half cheetah) it leads to a very negative score, which is\nworse than the initial random policy.\nalgorithm\navg. normalized score"
      },
      {
        "header": "No clipping or penalty",
        "content": "-0.39\nClipping, \u03f5 = 0.1\n0.76\nClipping, \u03f5 = 0.2\n0.82\nClipping, \u03f5 = 0.3\n0.70\nAdaptive KL dtarg = 0.003\n0.68\nAdaptive KL dtarg = 0.01\n0.74\nAdaptive KL dtarg = 0.03\n0.71\nFixed KL, \u03b2 = 0.3\n0.62\nFixed KL, \u03b2 = 1.\n0.71\nFixed KL, \u03b2 = 3.\n0.72\nFixed KL, \u03b2 = 10.\n0.69\nTable 1: Results from continuous control benchmark.\nAverage normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2 was initialized at 1.\n6.2"
      },
      {
        "header": "Comparison to Other Algorithms in the Continuous Domain",
        "content": "Next, we compare PPO (with the \u201cclipped\u201d surrogate objective from Section 3) to several other\nmethods from the literature, which are considered to be e\ufb00ective for continuous problems. We com-\npared against tuned implementations of the following algorithms: trust region policy optimization\n[Sch+15b], cross-entropy method (CEM) [SL06], vanilla policy gradient with adaptive stepsize3,\n2HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, all \u201c-v1\u201d\n3After each batch of data, the Adam stepsize is adjusted based on the KL divergence of the original and updated\npolicy, using a rule similar to the one shown in Section 4. An implementation is available at https://github.com/\nberkeleydeeprlcourse/homework/tree/master/hw4.\n6\n\n\nA2C [Mni+16], A2C with trust region [Wan+16]. A2C stands for advantage actor critic, and is\na synchronous version of A3C, which we found to have the same or better performance than the\nasynchronous version. For PPO, we used the hyperparameters from the previous section, with\n\u03f5 = 0.2. We see that PPO outperforms the previous methods on almost all the continuous control\nenvironments.\n0\n1000000\n500\n0\n500\n1000\n1500\n2000\nHalfCheetah-v1\n0\n1000000\n0\n500\n1000\n1500\n2000\n2500\nHopper-v1\n0\n1000000\n0\n2000\n4000\n6000\n8000\nInvertedDoublePendulum-v1\n0\n1000000\n0\n200\n400\n600\n800\n1000\nInvertedPendulum-v1\n0\n1000000\n120\n100\n80\n60\n40\n20\nReacher-v1\n0\n1000000\n0\n20\n40\n60\n80\n100\n120\nSwimmer-v1\n0\n1000000\n0\n1000\n2000\n3000\nWalker2d-v1\nA2C\nA2C + Trust Region"
      },
      {
        "header": "TRPO",
        "content": "Figure 3: Comparison of several algorithms on several MuJoCo environments, training for one million\ntimesteps.\n6.3\nShowcase in the Continuous Domain: Humanoid Running and Steering\nTo showcase the performance of PPO on high-dimensional continuous control problems, we train\non a set of problems involving a 3D humanoid, where the robot must run, steer, and get up\no\ufb00the ground, possibly while being pelted by cubes.\nThe three tasks we test on are (1) Ro-\nboschoolHumanoid: forward locomotion only, (2) RoboschoolHumanoidFlagrun: position of target\nis randomly varied every 200 timesteps or whenever the goal is reached, (3) RoboschoolHumanoid-\nFlagrunHarder, where the robot is pelted by cubes and needs to get up o\ufb00the ground. See Figure 5\nfor still frames of a learned policy, and Figure 4 for learning curves on the three tasks. Hyperpa-\nrameters are provided in Table 4. In concurrent work, Heess et al. [Hee+17] used the adaptive KL\nvariant of PPO (Section 4) to learn locomotion policies for 3D robots.\n0"
      },
      {
        "header": "M\nTimestep",
        "content": "0\n1000\n2000\n3000\nRoboschoolHumanoidFlagrunHarder-v0\nFigure 4: Learning curves from PPO on 3D humanoid control tasks, using Roboschool.\n7\n\n\nFigure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun. In the \ufb01rst six frames, the\nrobot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward\nthe new target.\n6.4"
      },
      {
        "header": "Comparison to Other Algorithms on the Atari Domain",
        "content": "We also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against\nwell-tuned implementations of A2C [Mni+16] and ACER [Wan+16]. For all three algorithms, we\nused the same policy network architechture as used in [Mni+16]. The hyperparameters for PPO\nare provided in Table 5. For the other two algorithms, we used hyperparameters that were tuned\nto maximize performance on this benchmark.\nA table of results and learning curves for all 49 games is provided in Appendix B. We consider\nthe following two scoring metrics: (1) average reward per episode over entire training period (which\nfavors fast learning), and (2) average reward per episode over last 100 episodes of training (which\nfavors \ufb01nal performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we\ncompute the victor by averaging the scoring metric across three trials.\nA2C"
      },
      {
        "header": "Tie",
        "content": "(1) avg. episode reward over all of training\n1\n18\n30\n0\n(2) avg. episode reward over last 100 episodes\n1\n28\n19\n1\nTable 2: Number of games \u201cwon\u201d by each algorithm, where the scoring metric is averaged across three trials."
      },
      {
        "header": "Conclusion",
        "content": "We have introduced proximal policy optimization, a family of policy optimization methods that use\nmultiple epochs of stochastic gradient ascent to perform each policy update. These methods have\nthe stability and reliability of trust-region methods but are much simpler to implement, requiring\nonly few lines of code change to a vanilla policy gradient implementation, applicable in more general\nsettings (for example, when using a joint architecture for the policy and value function), and have\nbetter overall performance."
      },
      {
        "header": "References",
        "content": "[Bel+15]\nM. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. \u201cThe arcade learning environ-\nment: An evaluation platform for general agents\u201d. In: Twenty-Fourth International\nJoint Conference on Arti\ufb01cial Intelligence. 2015.\n[Bro+16]\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W.\nZaremba. \u201cOpenAI Gym\u201d. In: arXiv preprint arXiv:1606.01540 (2016).\n[Dua+16]\nY. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. \u201cBenchmarking Deep\nReinforcement Learning for Continuous Control\u201d. In: arXiv preprint arXiv:1604.06778\n(2016).\n[Hee+17]\nN. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang,\nA. Eslami, M. Riedmiller, et al. \u201cEmergence of Locomotion Behaviours in Rich Envi-\nronments\u201d. In: arXiv preprint arXiv:1707.02286 (2017).\n[KL02]\nS. Kakade and J. Langford. \u201cApproximately optimal approximate reinforcement learn-\ning\u201d. In: ICML. Vol. 2. 2002, pp. 267\u2013274.\n[KB14]\nD. Kingma and J. Ba. \u201cAdam: A method for stochastic optimization\u201d. In: arXiv\npreprint arXiv:1412.6980 (2014).\n[Mni+15]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. \u201cHuman-level control through deep\nreinforcement learning\u201d. In: Nature 518.7540 (2015), pp. 529\u2013533.\n[Mni+16]\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. \u201cAsynchronous methods for deep reinforcement learning\u201d. In: arXiv\npreprint arXiv:1602.01783 (2016).\n[Sch+15a]\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. \u201cHigh-dimensional contin-\nuous control using generalized advantage estimation\u201d. In: arXiv preprint arXiv:1506.02438\n(2015).\n[Sch+15b]\nJ. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. \u201cTrust region policy\noptimization\u201d. In: CoRR, abs/1502.05477 (2015).\n[SL06]\nI. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In:\nNeural computation 18.12 (2006), pp. 2936\u20132941.\n[TET12]\nE. Todorov, T. Erez, and Y. Tassa. \u201cMuJoCo: A physics engine for model-based con-\ntrol\u201d. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Con-\nference on. IEEE. 2012, pp. 5026\u20135033.\n[Wan+16]\nZ. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas.\n\u201cSample E\ufb03cient Actor-Critic with Experience Replay\u201d. In: arXiv preprint arXiv:1611.01224\n(2016).\n[Wil92]\nR. J. Williams. \u201cSimple statistical gradient-following algorithms for connectionist re-\ninforcement learning\u201d. In: Machine learning 8.3-4 (1992), pp. 229\u2013256."
      },
      {
        "header": "Minibatch size",
        "content": "64\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nTable 3: PPO hyperparameters used for the Mujoco 1 million timestep benchmark."
      },
      {
        "header": "Number of actors",
        "content": "32 (locomotion), 128 (\ufb02agrun)\nLog stdev. of action distribution\nLinearAnneal(\u22120.7, \u22121.6)\nTable 4: PPO hyperparameters used for the Roboschool experiments. Adam stepsize was adjusted based on\nthe target value of the KL divergence."
      },
      {
        "header": "Number of actors",
        "content": "8\nClipping parameter \u03f5\n0.1 \u00d7 \u03b1\nVF coe\ufb00. c1 (9)\n1\nEntropy coe\ufb00. c2 (9)\n0.01\nTable 5: PPO hyperparameters used in Atari experiments. \u03b1 is linearly annealed from 1 to 0 over the course\nof learning."
      },
      {
        "header": "B\nPerformance on More Atari Games",
        "content": "Here we include a comparison of PPO against A2C on a larger collection of 49 Atari games. Figure 6\nshows the learning curves of each of three random seeds, while Table 6 shows the mean performance.\n10\n\n\n1000"
      },
      {
        "header": "PPO",
        "content": "Figure 6: Comparison of PPO and A2C on all 49 ATARI games included in OpenAI Gym at the time of\npublication.\n11\n\n\nA2C"
      },
      {
        "header": "Zaxxon",
        "content": "16.3\n29.0\n5008.7\nTable 6: Mean \ufb01nal scores (last 100 episodes) of PPO and A2C on Atari games after 40M game frames (10M\ntimesteps).\n12"
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "",
      "author": "",
      "subject": "",
      "keywords": "",
      "creator": "LaTeX with hyperref package",
      "producer": "pdfTeX-1.40.17",
      "creationDate": "D:20170829001149Z",
      "modDate": "D:20170829001149Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 12,
    "pages": [
      "Proximal Policy Optimization Algorithms\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\nOpenAI\n{joschu, filip, prafulla, alec, oleg}@openai.com\nAbstract\nWe propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\u201csurrogate\u201d objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates. The new methods, which we call\nproximal policy optimization (PPO), have some of the bene\ufb01ts of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time.\n1\nIntroduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\nlarge models and parallel implementations), data e\ufb03cient, and robust (i.e., successful on a variety\nof problems without hyperparameter tuning). Q-learning (with function approximation) fails on\nmany simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\ne\ufb03ency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\n(between the policy and value function, or with auxiliary tasks).\nThis paper seeks to improve the current state of a\ufb00airs by introducing an algorithm that attains\nthe data e\ufb03ciency and reliable performance of TRPO, while using only \ufb01rst-order optimization.\nWe propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\n(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between\nsampling data from the policy and performing several epochs of optimization on the sampled data.\nOur experiments compare the performance of various di\ufb00erent versions of the surrogate objec-\ntive, and \ufb01nd that the version with the clipped probability ratios performs best. We also compare\nPPO to several previous algorithms from the literature. On continuous control tasks, it performs\nbetter than the algorithms we compare against. On Atari, it performs signi\ufb01cantly better (in terms\nof sample complexity) than A2C and similarly to ACER though it is much simpler.\n1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete\naction spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in\nOpenAI Gym [Bro+16] and described by Duan et al. [Dua+16].\n1\narXiv:1707.06347v2  [cs.LG]  28 Aug 2017\n",
      "2\nBackground: Policy Optimization\n2.1\nPolicy Gradient Methods\nPolicy gradient methods work by computing an estimator of the policy gradient and plugging it\ninto a stochastic gradient ascent algorithm. The most commonly used gradient estimator has the\nform\n\u02c6g = \u02c6Et\nh\n\u2207\u03b8 log \u03c0\u03b8(at | st) \u02c6At\ni\n(1)\nwhere \u03c0\u03b8 is a stochastic policy and \u02c6At is an estimator of the advantage function at timestep t.\nHere, the expectation \u02c6Et[. . .] indicates the empirical average over a \ufb01nite batch of samples, in an\nalgorithm that alternates between sampling and optimization. Implementations that use automatic\ndi\ufb00erentiation software work by constructing an objective function whose gradient is the policy\ngradient estimator; the estimator \u02c6g is obtained by di\ufb00erentiating the objective\nLPG(\u03b8) = \u02c6Et\nh\nlog \u03c0\u03b8(at | st) \u02c6At\ni\n.\n(2)\nWhile it is appealing to perform multiple steps of optimization on this loss LPG using the same\ntrajectory, doing so is not well-justi\ufb01ed, and empirically it often leads to destructively large policy\nupdates (see Section 6.1; results are not shown but were similar or worse than the \u201cno clipping or\npenalty\u201d setting).\n2.2\nTrust Region Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n(3)\nsubject to\n\u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]] \u2264\u03b4.\n(4)\nHere, \u03b8old is the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2 that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, experiments show\nthat it is not su\ufb03cient to simply choose a \ufb01xed penalty coe\ufb03cient \u03b2 and optimize the penalized\nobjective Equation (5) with SGD; additional modi\ufb01cations are required.\n2\n",
      "3\nClipped Surrogate Objective\nLet rt(\u03b8) denote the probability ratio rt(\u03b8) =\n\u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st), so r(\u03b8old) = 1.\nTRPO maximizes a\n\u201csurrogate\u201d objective\nLCPI(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n= \u02c6Et\nh\nrt(\u03b8) \u02c6At\ni\n.\n(6)\nThe superscript CPI refers to conservative policy iteration [KL02], where this objective was pro-\nposed.\nWithout a constraint, maximization of LCPI would lead to an excessively large policy\nupdate; hence, we now consider how to modify the objective, to penalize changes to the policy that\nmove rt(\u03b8) away from 1.\nThe main objective we propose is the following:\nLCLIP (\u03b8) = \u02c6Et\nh\nmin(rt(\u03b8) \u02c6At, clip(rt(\u03b8), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At)\ni\n(7)\nwhere epsilon is a hyperparameter, say, \u03f5 = 0.2. The motivation for this objective is as follows. The\n\ufb01rst term inside the min is LCPI. The second term, clip(rt(\u03b8), 1\u2212\u03f5, 1+\u03f5) \u02c6At, modi\ufb01es the surrogate\nobjective by clipping the probability ratio, which removes the incentive for moving rt outside of the\ninterval [1 \u2212\u03f5, 1 + \u03f5]. Finally, we take the minimum of the clipped and unclipped objective, so the\n\ufb01nal objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective. With this\nscheme, we only ignore the change in probability ratio when it would make the objective improve,\nand we include it when it makes the objective worse. Note that LCLIP (\u03b8) = LCPI(\u03b8) to \ufb01rst order\naround \u03b8old (i.e., where r = 1), however, they become di\ufb00erent as \u03b8 moves away from \u03b8old. Figure 1\nplots a single term (i.e., a single t) in LCLIP ; note that the probability ratio r is clipped at 1 \u2212\u03f5\nor 1 + \u03f5 depending on whether the advantage is positive or negative.\nr\nLCLIP\n0\n1 1 + \u03f5\nA > 0\nr\nLCLIP\n0\n1\n1 \u2212\u03f5\nA < 0\nFigure 1: Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of\nthe probability ratio r, for positive advantages (left) and negative advantages (right). The red circle on each\nplot shows the starting point for the optimization, i.e., r = 1. Note that LCLIP sums many of these terms.\nFigure 2 provides another source of intuition about the surrogate objective LCLIP . It shows how\nseveral objectives vary as we interpolate along the policy update direction, obtained by proximal\npolicy optimization (the algorithm we will introduce shortly) on a continuous control problem. We\ncan see that LCLIP is a lower bound on LCPI, with a penalty for having too large of a policy\nupdate.\n3\n",
      "0\n1\nLinear interpolation factor\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nEt[KLt]\nLCPI = Et[rtAt]\nEt[clip(rt, 1\n, 1 + )At]\nLCLIP = Et[min(rtAt, clip(rt, 1\n, 1 + )At)]\nFigure 2: Surrogate objectives, as we interpolate between the initial policy parameter \u03b8old, and the updated\npolicy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of\nabout 0.02 from the initial policy, and this is the point at which LCLIP is maximal. This plot corresponds\nto the \ufb01rst policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1.\n4\nAdaptive KL Penalty Coe\ufb03cient\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in\naddition to it, is to use a penalty on KL divergence, and to adapt the penalty coe\ufb03cient so that we\nachieve some target value of the KL divergence dtarg each policy update. In our experiments, we\nfound that the KL penalty performed worse than the clipped surrogate objective, however, we\u2019ve\nincluded it here because it\u2019s an important baseline.\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy\nupdate:\n\u2022 Using several epochs of minibatch SGD, optimize the KL-penalized objective\nLKLPEN(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(8)\n\u2022 Compute d = \u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]]\n\u2013 If d < dtarg/1.5, \u03b2 \u2190\u03b2/2\n\u2013 If d > dtarg \u00d7 1.5, \u03b2 \u2190\u03b2 \u00d7 2\nThe updated \u03b2 is used for the next policy update. With this scheme, we occasionally see policy\nupdates where the KL divergence is signi\ufb01cantly di\ufb00erent from dtarg, however, these are rare, and\n\u03b2 quickly adjusts. The parameters 1.5 and 2 above are chosen heuristically, but the algorithm is\nnot very sensitive to them. The initial value of \u03b2 is a another hyperparameter but is not important\nin practice because the algorithm quickly adjusts it.\n5\nAlgorithm\nThe surrogate losses from the previous sections can be computed and di\ufb00erentiated with a minor\nchange to a typical policy gradient implementation. For implementations that use automatic dif-\nferentation, one simply constructs the loss LCLIP or LKLPEN instead of LPG, and one performs\nmultiple steps of stochastic gradient ascent on this objective.\nMost techniques for computing variance-reduced advantage-function estimators make use a\nlearned state-value function V (s); for example, generalized advantage estimation [Sch+15a], or the\n4\n",
      "\ufb01nite-horizon estimators in [Mni+16]. If using a neural network architecture that shares parameters\nbetween the policy and value function, we must use a loss function that combines the policy\nsurrogate and a value function error term. This objective can further be augmented by adding\nan entropy bonus to ensure su\ufb03cient exploration, as suggested in past work [Wil92; Mni+16].\nCombining these terms, we obtain the following objective, which is (approximately) maximized\neach iteration:\nLCLIP+V F+S\nt\n(\u03b8) = \u02c6Et\n\u0002\nLCLIP\nt\n(\u03b8) \u2212c1LV F\nt\n(\u03b8) + c2S[\u03c0\u03b8](st)\n\u0003\n,\n(9)\nwhere c1, c2 are coe\ufb03cients, and S denotes an entropy bonus, and LV F\nt\nis a squared-error loss\n(V\u03b8(st) \u2212V targ\nt\n)2.\nOne style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\nwith recurrent neural networks, runs the policy for T timesteps (where T is much less than the\nepisode length), and uses the collected samples for an update. This style requires an advantage\nestimator that does not look beyond timestep T. The estimator used by [Mni+16] is\n\u02c6At = \u2212V (st) + rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3T\u2212t+1rT\u22121 + \u03b3T\u2212tV (sT )\n(10)\nwhere t speci\ufb01es the time index in [0, T], within a given length-T trajectory segment. Generalizing\nthis choice, we can use a truncated version of generalized advantage estimation, which reduces to\nEquation (10) when \u03bb = 1:\n\u02c6At = \u03b4t + (\u03b3\u03bb)\u03b4t+1 + \u00b7 \u00b7 \u00b7 + \u00b7 \u00b7 \u00b7 + (\u03b3\u03bb)T\u2212t+1\u03b4T\u22121,\n(11)\nwhere\n\u03b4t = rt + \u03b3V (st+1) \u2212V (st)\n(12)\nA proximal policy optimization (PPO) algorithm that uses \ufb01xed-length trajectory segments is\nshown below. Each iteration, each of N (parallel) actors collect T timesteps of data. Then we\nconstruct the surrogate loss on these NT timesteps of data, and optimize it with minibatch SGD\n(or usually for better performance, Adam [KB14]), for K epochs.\nAlgorithm 1 PPO, Actor-Critic Style\nfor iteration=1, 2, . . . do\nfor actor=1, 2, . . . , N do\nRun policy \u03c0\u03b8old in environment for T timesteps\nCompute advantage estimates \u02c6A1, . . . , \u02c6AT\nend for\nOptimize surrogate L wrt \u03b8, with K epochs and minibatch size M \u2264NT\n\u03b8old \u2190\u03b8\nend for\n6\nExperiments\n6.1\nComparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIP to several natural variations and ablated versions.\nNo clipping or penalty:\nLt(\u03b8) = rt(\u03b8) \u02c6At\nClipping:\nLt(\u03b8) = min(rt(\u03b8) \u02c6At, clip(rt(\u03b8)), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At\nKL penalty (\ufb01xed or adaptive)\nLt(\u03b8) = rt(\u03b8) \u02c6At \u2212\u03b2 KL[\u03c0\u03b8old, \u03c0\u03b8]\n5\n",
      "For the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2 or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping (\u03f5)\nand the KL penalty (\u03b2, dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1 is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe results are shown in Table 1. Note that the score is negative for the setting without clipping\nor penalties, because for one environment (half cheetah) it leads to a very negative score, which is\nworse than the initial random policy.\nalgorithm\navg. normalized score\nNo clipping or penalty\n-0.39\nClipping, \u03f5 = 0.1\n0.76\nClipping, \u03f5 = 0.2\n0.82\nClipping, \u03f5 = 0.3\n0.70\nAdaptive KL dtarg = 0.003\n0.68\nAdaptive KL dtarg = 0.01\n0.74\nAdaptive KL dtarg = 0.03\n0.71\nFixed KL, \u03b2 = 0.3\n0.62\nFixed KL, \u03b2 = 1.\n0.71\nFixed KL, \u03b2 = 3.\n0.72\nFixed KL, \u03b2 = 10.\n0.69\nTable 1: Results from continuous control benchmark.\nAverage normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2 was initialized at 1.\n6.2\nComparison to Other Algorithms in the Continuous Domain\nNext, we compare PPO (with the \u201cclipped\u201d surrogate objective from Section 3) to several other\nmethods from the literature, which are considered to be e\ufb00ective for continuous problems. We com-\npared against tuned implementations of the following algorithms: trust region policy optimization\n[Sch+15b], cross-entropy method (CEM) [SL06], vanilla policy gradient with adaptive stepsize3,\n2HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, all \u201c-v1\u201d\n3After each batch of data, the Adam stepsize is adjusted based on the KL divergence of the original and updated\npolicy, using a rule similar to the one shown in Section 4. An implementation is available at https://github.com/\nberkeleydeeprlcourse/homework/tree/master/hw4.\n6\n",
      "A2C [Mni+16], A2C with trust region [Wan+16]. A2C stands for advantage actor critic, and is\na synchronous version of A3C, which we found to have the same or better performance than the\nasynchronous version. For PPO, we used the hyperparameters from the previous section, with\n\u03f5 = 0.2. We see that PPO outperforms the previous methods on almost all the continuous control\nenvironments.\n0\n1000000\n500\n0\n500\n1000\n1500\n2000\nHalfCheetah-v1\n0\n1000000\n0\n500\n1000\n1500\n2000\n2500\nHopper-v1\n0\n1000000\n0\n2000\n4000\n6000\n8000\nInvertedDoublePendulum-v1\n0\n1000000\n0\n200\n400\n600\n800\n1000\nInvertedPendulum-v1\n0\n1000000\n120\n100\n80\n60\n40\n20\nReacher-v1\n0\n1000000\n0\n20\n40\n60\n80\n100\n120\nSwimmer-v1\n0\n1000000\n0\n1000\n2000\n3000\nWalker2d-v1\nA2C\nA2C + Trust Region\nCEM\nPPO (Clip)\nVanilla PG, Adaptive\nTRPO\nFigure 3: Comparison of several algorithms on several MuJoCo environments, training for one million\ntimesteps.\n6.3\nShowcase in the Continuous Domain: Humanoid Running and Steering\nTo showcase the performance of PPO on high-dimensional continuous control problems, we train\non a set of problems involving a 3D humanoid, where the robot must run, steer, and get up\no\ufb00the ground, possibly while being pelted by cubes.\nThe three tasks we test on are (1) Ro-\nboschoolHumanoid: forward locomotion only, (2) RoboschoolHumanoidFlagrun: position of target\nis randomly varied every 200 timesteps or whenever the goal is reached, (3) RoboschoolHumanoid-\nFlagrunHarder, where the robot is pelted by cubes and needs to get up o\ufb00the ground. See Figure 5\nfor still frames of a learned policy, and Figure 4 for learning curves on the three tasks. Hyperpa-\nrameters are provided in Table 4. In concurrent work, Heess et al. [Hee+17] used the adaptive KL\nvariant of PPO (Section 4) to learn locomotion policies for 3D robots.\n0\n50M\nTimestep\n0\n1000\n2000\n3000\n4000\nRoboschoolHumanoid-v0\n0\n100M\nTimestep\n0\n500\n1000\n1500\n2000\n2500\nRoboschoolHumanoidFlagrun-v0\n0\n100M\nTimestep\n0\n1000\n2000\n3000\nRoboschoolHumanoidFlagrunHarder-v0\nFigure 4: Learning curves from PPO on 3D humanoid control tasks, using Roboschool.\n7\n",
      "Figure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun. In the \ufb01rst six frames, the\nrobot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward\nthe new target.\n6.4\nComparison to Other Algorithms on the Atari Domain\nWe also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against\nwell-tuned implementations of A2C [Mni+16] and ACER [Wan+16]. For all three algorithms, we\nused the same policy network architechture as used in [Mni+16]. The hyperparameters for PPO\nare provided in Table 5. For the other two algorithms, we used hyperparameters that were tuned\nto maximize performance on this benchmark.\nA table of results and learning curves for all 49 games is provided in Appendix B. We consider\nthe following two scoring metrics: (1) average reward per episode over entire training period (which\nfavors fast learning), and (2) average reward per episode over last 100 episodes of training (which\nfavors \ufb01nal performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we\ncompute the victor by averaging the scoring metric across three trials.\nA2C\nACER\nPPO\nTie\n(1) avg. episode reward over all of training\n1\n18\n30\n0\n(2) avg. episode reward over last 100 episodes\n1\n28\n19\n1\nTable 2: Number of games \u201cwon\u201d by each algorithm, where the scoring metric is averaged across three trials.\n7\nConclusion\nWe have introduced proximal policy optimization, a family of policy optimization methods that use\nmultiple epochs of stochastic gradient ascent to perform each policy update. These methods have\nthe stability and reliability of trust-region methods but are much simpler to implement, requiring\nonly few lines of code change to a vanilla policy gradient implementation, applicable in more general\nsettings (for example, when using a joint architecture for the policy and value function), and have\nbetter overall performance.\n8\nAcknowledgements\nThanks to Rocky Duan, Peter Chen, and others at OpenAI for insightful comments.\n8\n",
      "References\n[Bel+15]\nM. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. \u201cThe arcade learning environ-\nment: An evaluation platform for general agents\u201d. In: Twenty-Fourth International\nJoint Conference on Arti\ufb01cial Intelligence. 2015.\n[Bro+16]\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W.\nZaremba. \u201cOpenAI Gym\u201d. In: arXiv preprint arXiv:1606.01540 (2016).\n[Dua+16]\nY. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. \u201cBenchmarking Deep\nReinforcement Learning for Continuous Control\u201d. In: arXiv preprint arXiv:1604.06778\n(2016).\n[Hee+17]\nN. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang,\nA. Eslami, M. Riedmiller, et al. \u201cEmergence of Locomotion Behaviours in Rich Envi-\nronments\u201d. In: arXiv preprint arXiv:1707.02286 (2017).\n[KL02]\nS. Kakade and J. Langford. \u201cApproximately optimal approximate reinforcement learn-\ning\u201d. In: ICML. Vol. 2. 2002, pp. 267\u2013274.\n[KB14]\nD. Kingma and J. Ba. \u201cAdam: A method for stochastic optimization\u201d. In: arXiv\npreprint arXiv:1412.6980 (2014).\n[Mni+15]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. \u201cHuman-level control through deep\nreinforcement learning\u201d. In: Nature 518.7540 (2015), pp. 529\u2013533.\n[Mni+16]\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. \u201cAsynchronous methods for deep reinforcement learning\u201d. In: arXiv\npreprint arXiv:1602.01783 (2016).\n[Sch+15a]\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. \u201cHigh-dimensional contin-\nuous control using generalized advantage estimation\u201d. In: arXiv preprint arXiv:1506.02438\n(2015).\n[Sch+15b]\nJ. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. \u201cTrust region policy\noptimization\u201d. In: CoRR, abs/1502.05477 (2015).\n[SL06]\nI. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In:\nNeural computation 18.12 (2006), pp. 2936\u20132941.\n[TET12]\nE. Todorov, T. Erez, and Y. Tassa. \u201cMuJoCo: A physics engine for model-based con-\ntrol\u201d. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Con-\nference on. IEEE. 2012, pp. 5026\u20135033.\n[Wan+16]\nZ. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas.\n\u201cSample E\ufb03cient Actor-Critic with Experience Replay\u201d. In: arXiv preprint arXiv:1611.01224\n(2016).\n[Wil92]\nR. J. Williams. \u201cSimple statistical gradient-following algorithms for connectionist re-\ninforcement learning\u201d. In: Machine learning 8.3-4 (1992), pp. 229\u2013256.\n9\n",
      "A\nHyperparameters\nHyperparameter\nValue\nHorizon (T)\n2048\nAdam stepsize\n3 \u00d7 10\u22124\nNum. epochs\n10\nMinibatch size\n64\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nTable 3: PPO hyperparameters used for the Mujoco 1 million timestep benchmark.\nHyperparameter\nValue\nHorizon (T)\n512\nAdam stepsize\n\u2217\nNum. epochs\n15\nMinibatch size\n4096\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nNumber of actors\n32 (locomotion), 128 (\ufb02agrun)\nLog stdev. of action distribution\nLinearAnneal(\u22120.7, \u22121.6)\nTable 4: PPO hyperparameters used for the Roboschool experiments. Adam stepsize was adjusted based on\nthe target value of the KL divergence.\nHyperparameter\nValue\nHorizon (T)\n128\nAdam stepsize\n2.5 \u00d7 10\u22124 \u00d7 \u03b1\nNum. epochs\n3\nMinibatch size\n32 \u00d7 8\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nNumber of actors\n8\nClipping parameter \u03f5\n0.1 \u00d7 \u03b1\nVF coe\ufb00. c1 (9)\n1\nEntropy coe\ufb00. c2 (9)\n0.01\nTable 5: PPO hyperparameters used in Atari experiments. \u03b1 is linearly annealed from 1 to 0 over the course\nof learning.\nB\nPerformance on More Atari Games\nHere we include a comparison of PPO against A2C on a larger collection of 49 Atari games. Figure 6\nshows the learning curves of each of three random seeds, while Table 6 shows the mean performance.\n10\n",
      "1000\n2000\nAlien\n0\n250\n500\n750\nAmidar\n0\n2000\n4000\nAssault\n0\n2500\n5000\n7500\nAsterix\n1500\n2000\n2500\nAsteroids\n0\n1000000\n2000000\n3000000\nAtlantis\n0\n500\n1000\nBankHeist\n5000\n10000\n15000\n20000\nBattleZone\n1000\n2000\n3000\n4000\nBeamRider\n30\n40\n50\nBowling\n0\n50\n100\nBoxing\n0\n200\n400\nBreakout\n5000\n10000\nCentipede\n2000\n4000\n6000\nChopperCommand\n50000\n100000\nCrazyClimber\n0\n20000\n40000\nDemonAttack\n17.5\n15.0\n12.5\n10.0\nDoubleDunk\n0\n250\n500\n750\nEnduro\n100\n50\n0\nFishingDerby\n0\n10\n20\n30\nFreeway\n100\n200\n300\nFrostbite\n0\n20000\n40000\nGopher\n250\n500\n750\nGravitar\n10\n8\n6\n4\nIceHockey\n0\n200\n400\n600\nJamesbond\n0\n5000\n10000\nKangaroo\n2000\n4000\n6000\n8000\nKrull\n0\n20000\n40000\nKungFuMaster\n0\n50\n100\nMontezumaRevenge\n1000\n2000\n3000\nMsPacman\n2500\n5000\n7500\n10000\nNameThisGame\n100\n0\nPitfall\n20\n0\n20\nPong\n0\n500\nPrivateEye\n0\n5000\n10000\n15000\nQbert\n2500\n5000\n7500\n10000\nRiverraid\n0\n20000\n40000\nRoadRunner\n2\n4\n6\nRobotank\n0\n500\n1000\n1500\nSeaquest\n500\n1000\nSpaceInvaders\n0\n20000\n40000\nStarGunner\n20\n15\n10\nTennis\n3000\n4000\nTimePilot\n0\n100\n200\n300\nTutankham\n0\n100000\n200000\nUpNDown\n0\n40M\nFrames\n0\n5\n10\nVenture\n0\n40M\nFrames\n50000\n100000\n150000\nVideoPinball\n0\n40M\nFrames\n2000\n4000\nWizardOfWor\n0\n40M\nFrames\n0\n2000\n4000\n6000\nZaxxon\nA2C\nACER\nPPO\nFigure 6: Comparison of PPO and A2C on all 49 ATARI games included in OpenAI Gym at the time of\npublication.\n11\n",
      "A2C\nACER\nPPO\nAlien\n1141.7\n1655.4\n1850.3\nAmidar\n380.8\n827.6\n674.6\nAssault\n1562.9\n4653.8\n4971.9\nAsterix\n3176.3\n6801.2\n4532.5\nAsteroids\n1653.3\n2389.3\n2097.5\nAtlantis\n729265.3\n1841376.0\n2311815.0\nBankHeist\n1095.3\n1177.5\n1280.6\nBattleZone\n3080.0\n8983.3\n17366.7\nBeamRider\n3031.7\n3863.3\n1590.0\nBowling\n30.1\n33.3\n40.1\nBoxing\n17.7\n98.9\n94.6\nBreakout\n303.0\n456.4\n274.8\nCentipede\n3496.5\n8904.8\n4386.4\nChopperCommand\n1171.7\n5287.7\n3516.3\nCrazyClimber\n107770.0\n132461.0\n110202.0\nDemonAttack\n6639.1\n38808.3\n11378.4\nDoubleDunk\n-16.2\n-13.2\n-14.9\nEnduro\n0.0\n0.0\n758.3\nFishingDerby\n20.6\n34.7\n17.8\nFreeway\n0.0\n0.0\n32.5\nFrostbite\n261.8\n285.6\n314.2\nGopher\n1500.9\n37802.3\n2932.9\nGravitar\n194.0\n225.3\n737.2\nIceHockey\n-6.4\n-5.9\n-4.2\nJamesbond\n52.3\n261.8\n560.7\nKangaroo\n45.3\n50.0\n9928.7\nKrull\n8367.4\n7268.4\n7942.3\nKungFuMaster\n24900.3\n27599.3\n23310.3\nMontezumaRevenge\n0.0\n0.3\n42.0\nMsPacman\n1626.9\n2718.5\n2096.5\nNameThisGame\n5961.2\n8488.0\n6254.9\nPitfall\n-55.0\n-16.9\n-32.9\nPong\n19.7\n20.7\n20.7\nPrivateEye\n91.3\n182.0\n69.5\nQbert\n10065.7\n15316.6\n14293.3\nRiverraid\n7653.5\n9125.1\n8393.6\nRoadRunner\n32810.0\n35466.0\n25076.0\nRobotank\n2.2\n2.5\n5.5\nSeaquest\n1714.3\n1739.5\n1204.5\nSpaceInvaders\n744.5\n1213.9\n942.5\nStarGunner\n26204.0\n49817.7\n32689.0\nTennis\n-22.2\n-17.6\n-14.8\nTimePilot\n2898.0\n4175.7\n4342.0\nTutankham\n206.8\n280.8\n254.4\nUpNDown\n17369.8\n145051.4\n95445.0\nVenture\n0.0\n0.0\n0.0\nVideoPinball\n19735.9\n156225.6\n37389.0\nWizardOfWor\n859.0\n2308.3\n4185.3\nZaxxon\n16.3\n29.0\n5008.7\nTable 6: Mean \ufb01nal scores (last 100 episodes) of PPO and A2C on Atari games after 40M game frames (10M\ntimesteps).\n12\n"
    ],
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Asynchronous Methods for Deep Reinforcement Learning\nVolodymyr Mnih1\nVMNIH@GOOGLE.COM\nAdri\u00e0 Puigdom\u00e8nech Badia1\nADRIAP@GOOGLE.COM\nMehdi Mirza1,2\nMIRZAMOM@IRO.UMONTREAL.CA\nAlex Graves1\nGRAVESA@GOOGLE.COM\nTim Harley1\nTHARLEY@GOOGLE.COM\nTimothy P. Lillicrap1\nCOUNTZERO@GOOGLE.COM\nDavid Silver1\nDAVIDSILVER@GOOGLE.COM\nKoray Kavukcuoglu 1\nKORAYK@GOOGLE.COM\n1 Google DeepMind\n2 Montreal Institute for Learning Algorithms (MILA), University of Montreal\nAbstract\nWe\npropose\na\nconceptually\nsimple\nand\nlightweight\nframework\nfor\ndeep\nreinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers.\nThe best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input.\n1. Introduction\nDeep neural networks provide rich representations that can\nenable reinforcement learning (RL) algorithms to perform\neffectively. However, it was previously thought that the\ncombination of simple online RL algorithms with deep\nneural networks was fundamentally unstable. Instead, a va-\nriety of solutions have been proposed to stabilize the algo-\nrithm (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Has-\nselt et al., 2015; Schulman et al., 2015a). These approaches\nshare a common idea: the sequence of observed data en-\ncountered by an online RL agent is non-stationary, and on-\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s).\nline RL updates are strongly correlated.\nBy storing the\nagent\u2019s data in an experience replay memory, the data can\nbe batched (Riedmiller, 2005; Schulman et al., 2015a) or\nrandomly sampled (Mnih et al., 2013; 2015; Van Hasselt\net al., 2015) from different time-steps. Aggregating over\nmemory in this way reduces non-stationarity and decorre-\nlates updates, but at the same time limits the methods to\noff-policy reinforcement learning algorithms.\nDeep RL algorithms based on experience replay have\nachieved unprecedented success in challenging domains\nsuch as Atari 2600. However, experience replay has several\ndrawbacks: it uses more memory and computation per real\ninteraction; and it requires off-policy learning algorithms\nthat can update from data generated by an older policy.\nIn this paper we provide a very different paradigm for deep\nreinforcement learning. Instead of experience replay, we\nasynchronously execute multiple agents in parallel, on mul-\ntiple instances of the environment. This parallelism also\ndecorrelates the agents\u2019 data into a more stationary process,\nsince at any given time-step the parallel agents will be ex-\nperiencing a variety of different states. This simple idea\nenables a much larger spectrum of fundamental on-policy\nRL algorithms, such as Sarsa, n-step methods, and actor-\ncritic methods, as well as off-policy RL algorithms such\nas Q-learning, to be applied robustly and effectively using\ndeep neural networks.\nOur parallel reinforcement learning paradigm also offers\npractical bene\ufb01ts. Whereas previous approaches to deep re-\ninforcement learning rely heavily on specialized hardware\nsuch as GPUs (Mnih et al., 2015; Van Hasselt et al., 2015;\nSchaul et al., 2015) or massively distributed architectures\n(Nair et al., 2015), our experiments run on a single machine\nwith a standard multi-core CPU. When applied to a vari-\nety of Atari 2600 domains, on many games asynchronous\nreinforcement learning achieves better results, in far less\narXiv:1602.01783v2  [cs.LG]  16 Jun 2016\n\n\nAsynchronous Methods for Deep Reinforcement Learning\ntime than previous GPU-based algorithms, using far less\nresource than massively distributed approaches. The best\nof the proposed methods, asynchronous advantage actor-\ncritic (A3C), also mastered a variety of continuous motor\ncontrol tasks as well as learned general strategies for ex-\nploring 3D mazes purely from visual inputs. We believe\nthat the success of A3C on both 2D and 3D games, discrete\nand continuous action spaces, as well as its ability to train\nfeedforward and recurrent agents makes it the most general\nand successful reinforcement learning agent to date.\n2. Related Work\nThe General Reinforcement Learning Architecture (Gorila)\nof (Nair et al., 2015) performs asynchronous training of re-\ninforcement learning agents in a distributed setting. In Go-\nrila, each process contains an actor that acts in its own copy\nof the environment, a separate replay memory, and a learner\nthat samples data from the replay memory and computes\ngradients of the DQN loss (Mnih et al., 2015) with respect\nto the policy parameters. The gradients are asynchronously\nsent to a central parameter server which updates a central\ncopy of the model. The updated policy parameters are sent\nto the actor-learners at \ufb01xed intervals. By using 100 sep-\narate actor-learner processes and 30 parameter server in-\nstances, a total of 130 machines, Gorila was able to signif-\nicantly outperform DQN over 49 Atari games. On many\ngames Gorila reached the score achieved by DQN over 20\ntimes faster than DQN. We also note that a similar way of\nparallelizing DQN was proposed by (Chavez et al., 2015).\nIn earlier work, (Li & Schuurmans, 2011) applied the\nMap Reduce framework to parallelizing batch reinforce-\nment learning methods with linear function approximation.\nParallelism was used to speed up large matrix operations\nbut not to parallelize the collection of experience or sta-\nbilize learning. (Grounds & Kudenko, 2008) proposed a\nparallel version of the Sarsa algorithm that uses multiple\nseparate actor-learners to accelerate training. Each actor-\nlearner learns separately and periodically sends updates to\nweights that have changed signi\ufb01cantly to the other learn-\ners using peer-to-peer communication.\n(Tsitsiklis, 1994) studied convergence properties of Q-\nlearning in the asynchronous optimization setting. These\nresults show that Q-learning is still guaranteed to converge\nwhen some of the information is outdated as long as out-\ndated information is always eventually discarded and sev-\neral other technical assumptions are satis\ufb01ed. Even earlier,\n(Bertsekas, 1982) studied the related problem of distributed\ndynamic programming.\nAnother related area of work is in evolutionary meth-\nods, which are often straightforward to parallelize by dis-\ntributing \ufb01tness evaluations over multiple machines or\nthreads (Tomassini, 1999). Such parallel evolutionary ap-\nproaches have recently been applied to some visual rein-\nforcement learning tasks. In one example, (Koutn\u00edk et al.,\n2014) evolved convolutional neural network controllers for\nthe TORCS driving simulator by performing \ufb01tness evalu-\nations on 8 CPU cores in parallel.\n3. Reinforcement Learning Background\nWe consider the standard reinforcement learning setting\nwhere an agent interacts with an environment E over a\nnumber of discrete time steps. At each time step t, the\nagent receives a state st and selects an action at from some\nset of possible actions A according to its policy \u03c0, where\n\u03c0 is a mapping from states st to actions at. In return, the\nagent receives the next state st+1 and receives a scalar re-\nward rt. The process continues until the agent reaches a\nterminal state after which the process restarts. The return\nRt = P\u221e\nk=0 \u03b3krt+k is the total accumulated return from\ntime step t with discount factor \u03b3 \u2208(0, 1]. The goal of the\nagent is to maximize the expected return from each state st.\nThe action value Q\u03c0(s, a) = E [Rt|st = s, a] is the ex-\npected return for selecting action a in state s and follow-\ning policy \u03c0.\nThe optimal value function Q\u2217(s, a) =\nmax\u03c0 Q\u03c0(s, a) gives the maximum action value for state\ns and action a achievable by any policy.\nSimilarly, the\nvalue of state s under policy \u03c0 is de\ufb01ned as V \u03c0(s) =\nE [Rt|st = s] and is simply the expected return for follow-\ning policy \u03c0 from state s.\nIn value-based model-free reinforcement learning methods,\nthe action value function is represented using a function ap-\nproximator, such as a neural network. Let Q(s, a; \u03b8) be an\napproximate action-value function with parameters \u03b8. The\nupdates to \u03b8 can be derived from a variety of reinforcement\nlearning algorithms. One example of such an algorithm is\nQ-learning, which aims to directly approximate the optimal\naction value function: Q\u2217(s, a) \u2248Q(s, a; \u03b8). In one-step\nQ-learning, the parameters \u03b8 of the action value function\nQ(s, a; \u03b8) are learned by iteratively minimizing a sequence\nof loss functions, where the ith loss function de\ufb01ned as\nLi(\u03b8i) = E\n\u0010\nr + \u03b3 max\na\u2032 Q(s\u2032, a\u2032; \u03b8i\u22121) \u2212Q(s, a; \u03b8i)\n\u00112\nwhere s\u2032 is the state encountered after state s.\nWe refer to the above method as one-step Q-learning be-\ncause it updates the action value Q(s, a) toward the one-\nstep return r + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8). One drawback of us-\ning one-step methods is that obtaining a reward r only di-\nrectly affects the value of the state action pair s, a that led\nto the reward. The values of other state action pairs are\naffected only indirectly through the updated value Q(s, a).\nThis can make the learning process slow since many up-\ndates are required the propagate a reward to the relevant\npreceding states and actions.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nOne way of propagating rewards faster is by using n-\nstep returns (Watkins, 1989; Peng & Williams, 1996).\nIn n-step Q-learning, Q(s, a) is updated toward the n-\nstep return de\ufb01ned as rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3n\u22121rt+n\u22121 +\nmaxa \u03b3nQ(st+n, a). This results in a single reward r di-\nrectly affecting the values of n preceding state action pairs.\nThis makes the process of propagating rewards to relevant\nstate-action pairs potentially much more ef\ufb01cient.\nIn contrast to value-based methods, policy-based model-\nfree methods directly parameterize the policy \u03c0(a|s; \u03b8) and\nupdate the parameters \u03b8 by performing, typically approx-\nimate, gradient ascent on E[Rt].\nOne example of such\na method is the REINFORCE family of algorithms due\nto Williams (1992). Standard REINFORCE updates the\npolicy parameters \u03b8 in the direction \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt,\nwhich is an unbiased estimate of \u2207\u03b8E[Rt]. It is possible to\nreduce the variance of this estimate while keeping it unbi-\nased by subtracting a learned function of the state bt(st),\nknown as a baseline (Williams, 1992), from the return. The\nresulting gradient is \u2207\u03b8 log \u03c0(at|st; \u03b8) (Rt \u2212bt(st)).\nA learned estimate of the value function is commonly used\nas the baseline bt(st) \u2248V \u03c0(st) leading to a much lower\nvariance estimate of the policy gradient. When an approx-\nimate value function is used as the baseline, the quantity\nRt \u2212bt used to scale the policy gradient can be seen as\nan estimate of the advantage of action at in state st, or\nA(at, st) = Q(at, st)\u2212V (st), because Rt is an estimate of\nQ\u03c0(at, st) and bt is an estimate of V \u03c0(st). This approach\ncan be viewed as an actor-critic architecture where the pol-\nicy \u03c0 is the actor and the baseline bt is the critic (Sutton &\nBarto, 1998; Degris et al., 2012).\n4. Asynchronous RL Framework\nWe now present multi-threaded asynchronous variants of\none-step Sarsa, one-step Q-learning, n-step Q-learning, and\nadvantage actor-critic. The aim in designing these methods\nwas to \ufb01nd RL algorithms that can train deep neural net-\nwork policies reliably and without large resource require-\nments. While the underlying RL methods are quite dif-\nferent, with actor-critic being an on-policy policy search\nmethod and Q-learning being an off-policy value-based\nmethod, we use two main ideas to make all four algorithms\npractical given our design goal.\nFirst, we use asynchronous actor-learners, similarly to the\nGorila framework (Nair et al., 2015), but instead of using\nseparate machines and a parameter server, we use multi-\nple CPU threads on a single machine. Keeping the learn-\ners on a single machine removes the communication costs\nof sending gradients and parameters and enables us to use\nHogwild! (Recht et al., 2011) style updates for training.\nSecond, we make the observation that multiple actors-\nAlgorithm 1 Asynchronous one-step Q-learning - pseu-\ndocode for each actor-learner thread.\n// Assume global shared \u03b8, \u03b8\u2212, and counter T = 0.\nInitialize thread step counter t \u21900\nInitialize target network weights \u03b8\u2212\u2190\u03b8\nInitialize network gradients d\u03b8 \u21900\nGet initial state s\nrepeat\nTake action a with \u03f5-greedy policy based on Q(s, a; \u03b8)\nReceive new state s\u2032 and reward r\ny =\n\u001a r\nfor terminal s\u2032\nr + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8\u2212)\nfor non-terminal s\u2032\nAccumulate gradients wrt \u03b8: d\u03b8 \u2190d\u03b8 + \u2202(y\u2212Q(s,a;\u03b8))2\n\u2202\u03b8\ns = s\u2032\nT \u2190T + 1 and t \u2190t + 1\nif T\nmod Itarget == 0 then\nUpdate the target network \u03b8\u2212\u2190\u03b8\nend if\nif t mod IAsyncUpdate == 0 or s is terminal then\nPerform asynchronous update of \u03b8 using d\u03b8.\nClear gradients d\u03b8 \u21900.\nend if\nuntil T > Tmax\nlearners running in parallel are likely to be exploring dif-\nferent parts of the environment. Moreover, one can explic-\nitly use different exploration policies in each actor-learner\nto maximize this diversity.\nBy running different explo-\nration policies in different threads, the overall changes be-\ning made to the parameters by multiple actor-learners ap-\nplying online updates in parallel are likely to be less corre-\nlated in time than a single agent applying online updates.\nHence, we do not use a replay memory and rely on parallel\nactors employing different exploration policies to perform\nthe stabilizing role undertaken by experience replay in the\nDQN training algorithm.\nIn addition to stabilizing learning, using multiple parallel\nactor-learners has multiple practical bene\ufb01ts. First, we ob-\ntain a reduction in training time that is roughly linear in\nthe number of parallel actor-learners. Second, since we no\nlonger rely on experience replay for stabilizing learning we\nare able to use on-policy reinforcement learning methods\nsuch as Sarsa and actor-critic to train neural networks in a\nstable way. We now describe our variants of one-step Q-\nlearning, one-step Sarsa, n-step Q-learning and advantage\nactor-critic.\nAsynchronous one-step Q-learning: Pseudocode for our\nvariant of Q-learning, which we call Asynchronous one-\nstep Q-learning, is shown in Algorithm 1. Each thread in-\nteracts with its own copy of the environment and at each\nstep computes a gradient of the Q-learning loss. We use\na shared and slowly changing target network in comput-\ning the Q-learning loss, as was proposed in the DQN train-\ning method. We also accumulate gradients over multiple\ntimesteps before they are applied, which is similar to us-\n\n\nAsynchronous Methods for Deep Reinforcement Learning\ning minibatches. This reduces the chances of multiple ac-\ntor learners overwriting each other\u2019s updates. Accumulat-\ning updates over several steps also provides some ability to\ntrade off computational ef\ufb01ciency for data ef\ufb01ciency.\nFinally, we found that giving each thread a different explo-\nration policy helps improve robustness. Adding diversity\nto exploration in this manner also generally improves per-\nformance through better exploration. While there are many\npossible ways of making the exploration policies differ we\nexperiment with using \u03f5-greedy exploration with \u03f5 periodi-\ncally sampled from some distribution by each thread.\nAsynchronous one-step Sarsa: The asynchronous one-\nstep Sarsa algorithm is the same as asynchronous one-step\nQ-learning as given in Algorithm 1 except that it uses a dif-\nferent target value for Q(s, a). The target value used by\none-step Sarsa is r + \u03b3Q(s\u2032, a\u2032; \u03b8\u2212) where a\u2032 is the action\ntaken in state s\u2032 (Rummery & Niranjan, 1994; Sutton &\nBarto, 1998). We again use a target network and updates\naccumulated over multiple timesteps to stabilize learning.\nAsynchronous n-step Q-learning: Pseudocode for our\nvariant of multi-step Q-learning is shown in Supplementary\nAlgorithm S2. The algorithm is somewhat unusual because\nit operates in the forward view by explicitly computing n-\nstep returns, as opposed to the more common backward\nview used by techniques like eligibility traces (Sutton &\nBarto, 1998). We found that using the forward view is eas-\nier when training neural networks with momentum-based\nmethods and backpropagation through time. In order to\ncompute a single update, the algorithm \ufb01rst selects actions\nusing its exploration policy for up to tmax steps or until a\nterminal state is reached. This process results in the agent\nreceiving up to tmax rewards from the environment since\nits last update. The algorithm then computes gradients for\nn-step Q-learning updates for each of the state-action pairs\nencountered since the last update. Each n-step update uses\nthe longest possible n-step return resulting in a one-step\nupdate for the last state, a two-step update for the second\nlast state, and so on for a total of up to tmax updates. The\naccumulated updates are applied in a single gradient step.\nAsynchronous advantage actor-critic: The algorithm,\nwhich we call asynchronous advantage actor-critic (A3C),\nmaintains a policy \u03c0(at|st; \u03b8) and an estimate of the value\nfunction V (st; \u03b8v). Like our variant of n-step Q-learning,\nour variant of actor-critic also operates in the forward view\nand uses the same mix of n-step returns to update both the\npolicy and the value-function. The policy and the value\nfunction are updated after every tmax actions or when a\nterminal state is reached. The update performed by the al-\ngorithm can be seen as \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)A(st, at; \u03b8, \u03b8v)\nwhere A(st, at; \u03b8, \u03b8v) is an estimate of the advantage func-\ntion given by Pk\u22121\ni=0 \u03b3irt+i + \u03b3kV (st+k; \u03b8v) \u2212V (st; \u03b8v),\nwhere k can vary from state to state and is upper-bounded\nby tmax. The pseudocode for the algorithm is presented in\nSupplementary Algorithm S3.\nAs with the value-based methods we rely on parallel actor-\nlearners and accumulated updates for improving training\nstability. Note that while the parameters \u03b8 of the policy\nand \u03b8v of the value function are shown as being separate\nfor generality, we always share some of the parameters in\npractice. We typically use a convolutional neural network\nthat has one softmax output for the policy \u03c0(at|st; \u03b8) and\none linear output for the value function V (st; \u03b8v), with all\nnon-output layers shared.\nWe also found that adding the entropy of the policy \u03c0 to the\nobjective function improved exploration by discouraging\npremature convergence to suboptimal deterministic poli-\ncies. This technique was originally proposed by (Williams\n& Peng, 1991), who found that it was particularly help-\nful on tasks requiring hierarchical behavior.\nThe gradi-\nent of the full objective function including the entropy\nregularization term with respect to the policy parame-\nters takes the form \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)(Rt \u2212V (st; \u03b8v)) +\n\u03b2\u2207\u03b8\u2032H(\u03c0(st; \u03b8\u2032)), where H is the entropy. The hyperpa-\nrameter \u03b2 controls the strength of the entropy regulariza-\ntion term.\nOptimization: We investigated three different optimiza-\ntion algorithms in our asynchronous framework \u2013 SGD\nwith momentum, RMSProp (Tieleman & Hinton, 2012)\nwithout shared statistics, and RMSProp with shared statis-\ntics. We used the standard non-centered RMSProp update\ngiven by\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82 and \u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(1)\nwhere all operations are performed elementwise. A com-\nparison on a subset of Atari 2600 games showed that a vari-\nant of RMSProp where statistics g are shared across threads\nis considerably more robust than the other two methods.\nFull details of the methods and comparisons are included\nin Supplementary Section 7.\n5. Experiments\nWe use four different platforms for assessing the properties\nof the proposed framework. We perform most of our exper-\niments using the Arcade Learning Environment (Bellemare\net al., 2012), which provides a simulator for Atari 2600\ngames. This is one of the most commonly used benchmark\nenvironments for RL algorithms. We use the Atari domain\nto compare against state of the art results (Van Hasselt et al.,\n2015; Wang et al., 2015; Schaul et al., 2015; Nair et al.,\n2015; Mnih et al., 2015), as well as to carry out a detailed\nstability and scalability analysis of the proposed methods.\nWe performed further comparisons using the TORCS 3D\ncar racing simulator (Wymann et al., 2013). We also use\n\n\nAsynchronous Methods for Deep Reinforcement Learning\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nBeamrider\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n100\n200\n300\n400\n500\n600\nScore\nBreakout\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n30\n20\n10\n0\n10\n20\n30\nScore\nPong\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nQ*bert\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nScore\nSpace Invaders\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\nFigure 1. Learning speed comparison for DQN and the new asynchronous algorithms on \ufb01ve Atari 2600 games. DQN was trained on\na single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores. The plots are averaged over 5 runs. In\nthe case of DQN the runs were for different seeds with \ufb01xed hyperparameters. For asynchronous methods we average over the best 5\nmodels from 50 experiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.\ntwo additional domains to evaluate only the A3C algorithm\n\u2013 Mujoco and Labyrinth. MuJoCo (Todorov, 2015) is a\nphysics simulator for evaluating agents on continuous mo-\ntor control tasks with contact dynamics. Labyrinth is a new\n3D environment where the agent must learn to \ufb01nd rewards\nin randomly generated mazes from a visual input. The pre-\ncise details of our experimental setup can be found in Sup-\nplementary Section 8.\n5.1. Atari 2600 Games\nWe \ufb01rst present results on a subset of Atari 2600 games to\ndemonstrate the training speed of the new methods. Fig-\nure 1 compares the learning speed of the DQN algorithm\ntrained on an Nvidia K40 GPU with the asynchronous\nmethods trained using 16 CPU cores on \ufb01ve Atari 2600\ngames. The results show that all four asynchronous meth-\nods we presented can successfully train neural network\ncontrollers on the Atari domain. The asynchronous meth-\nods tend to learn faster than DQN, with signi\ufb01cantly faster\nlearning on some games, while training on only 16 CPU\ncores. Additionally, the results suggest that n-step methods\nlearn faster than one-step methods on some games. Over-\nall, the policy-based advantage actor-critic method signi\ufb01-\ncantly outperforms all three value-based methods.\nWe then evaluated asynchronous advantage actor-critic on\n57 Atari games. In order to compare with the state of the\nart in Atari game playing, we largely followed the train-\ning and evaluation protocol of (Van Hasselt et al., 2015).\nSpeci\ufb01cally, we tuned hyperparameters (learning rate and\namount of gradient norm clipping) using a search on six\nAtari games (Beamrider, Breakout, Pong, Q*bert, Seaquest\nand Space Invaders) and then \ufb01xed all hyperparameters for\nall 57 games. We trained both a feedforward agent with the\nsame architecture as (Mnih et al., 2015; Nair et al., 2015;\nVan Hasselt et al., 2015) as well as a recurrent agent with an\nadditional 256 LSTM cells after the \ufb01nal hidden layer. We\nadditionally used the \ufb01nal network weights for evaluation\nto make the results more comparable to the original results\nMethod\nTraining Time\nMean\nMedian\nDQN\n8 days on GPU\n121.9%\n47.5%\nGorila\n4 days, 100 machines\n215.2%\n71.3%\nD-DQN\n8 days on GPU\n332.9%\n110.9%\nDueling D-DQN\n8 days on GPU\n343.8%\n117.1%\nPrioritized DQN\n8 days on GPU\n463.6%\n127.6%\nA3C, FF\n1 day on CPU\n344.1%\n68.2%\nA3C, FF\n4 days on CPU\n496.8%\n116.6%\nA3C, LSTM\n4 days on CPU\n623.0%\n112.6%\nTable 1. Mean and median human-normalized scores on 57 Atari\ngames using the human starts evaluation metric. Supplementary\nTable SS3 shows the raw scores for all games.\nfrom (Bellemare et al., 2012). We trained our agents for\nfour days using 16 CPU cores, while the other agents were\ntrained for 8 to 10 days on Nvidia K40 GPUs. Table 1\nshows the average and median human-normalized scores\nobtained by our agents trained by asynchronous advantage\nactor-critic (A3C) as well as the current state-of-the art.\nSupplementary Table S3 shows the scores on all games.\nA3C signi\ufb01cantly improves on state-of-the-art the average\nscore over 57 games in half the training time of the other\nmethods while using only 16 CPU cores and no GPU. Fur-\nthermore, after just one day of training, A3C matches the\naverage human normalized score of Dueling Double DQN\nand almost reaches the median human normalized score of\nGorila. We note that many of the improvements that are\npresented in Double DQN (Van Hasselt et al., 2015) and\nDueling Double DQN (Wang et al., 2015) can be incorpo-\nrated to 1-step Q and n-step Q methods presented in this\nwork with similar potential improvements.\n5.2. TORCS Car Racing Simulator\nWe also compared the four asynchronous methods on\nthe TORCS 3D car racing game (Wymann et al., 2013).\nTORCS not only has more realistic graphics than Atari\n2600 games, but also requires the agent to learn the dy-\nnamics of the car it is controlling. At each step, an agent\nreceived only a visual input in the form of an RGB image\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nof the current frame as well as a reward proportional to the\nagent\u2019s velocity along the center of the track at the agent\u2019s\ncurrent position. We used the same neural network archi-\ntecture as the one used in the Atari experiments speci\ufb01ed in\nSupplementary Section 8. We performed experiments us-\ning four different settings \u2013 the agent controlling a slow car\nwith and without opponent bots, and the agent controlling a\nfast car with and without opponent bots. Full results can be\nfound in Supplementary Figure S6. A3C was the best per-\nforming agent, reaching between roughly 75% and 90% of\nthe score obtained by a human tester on all four game con-\n\ufb01gurations in about 12 hours of training. A video showing\nthe learned driving behavior of the A3C agent can be found\nat https://youtu.be/0xo1Ldx3L5Q.\n5.3. Continuous Action Control Using the MuJoCo\nPhysics Simulator\nWe also examined a set of tasks where the action space\nis continuous. In particular, we looked at a set of rigid\nbody physics domains with contact dynamics where the\ntasks include many examples of manipulation and loco-\nmotion.\nThese tasks were simulated using the Mujoco\nphysics engine. We evaluated only the asynchronous ad-\nvantage actor-critic algorithm since, unlike the value-based\nmethods, it is easily extended to continuous actions. In all\nproblems, using either the physical state or pixels as in-\nput, Asynchronous Advantage-Critic found good solutions\nin less than 24 hours of training and typically in under a few\nhours. Some successful policies learned by our agent can\nbe seen in the following video https://youtu.be/\nAjjc08-iPx8. Further details about this experiment can\nbe found in Supplementary Section 9.\n5.4. Labyrinth\nWe performed an additional set of experiments with A3C\non a new 3D environment called Labyrinth. The speci\ufb01c\ntask we considered involved the agent learning to \ufb01nd re-\nwards in randomly generated mazes. At the beginning of\neach episode the agent was placed in a new randomly gen-\nerated maze consisting of rooms and corridors. Each maze\ncontained two types of objects that the agent was rewarded\nfor \ufb01nding \u2013 apples and portals. Picking up an apple led to\na reward of 1. Entering a portal led to a reward of 10 after\nwhich the agent was respawned in a new random location in\nthe maze and all previously collected apples were regener-\nated. An episode terminated after 60 seconds after which a\nnew episode would begin. The aim of the agent is to collect\nas many points as possible in the time limit and the optimal\nstrategy involves \ufb01rst \ufb01nding the portal and then repeatedly\ngoing back to it after each respawn. This task is much more\nchallenging than the TORCS driving domain because the\nagent is faced with a new maze in each episode and must\nlearn a general strategy for exploring random mazes.\nNumber of threads\nMethod\n1\n2\n4\n8\n16\n1-step Q\n1.0\n3.0\n6.3\n13.3\n24.1\n1-step SARSA\n1.0\n2.8\n5.9\n13.1\n22.1\nn-step Q\n1.0\n2.7\n5.9\n10.7\n17.2\nA3C\n1.0\n2.1\n3.7\n6.9\n12.5\nTable 2. The average training speedup for each method and num-\nber of threads averaged over seven Atari games. To compute the\ntraining speed-up on a single game we measured the time to re-\nquired reach a \ufb01xed reference score using each method and num-\nber of threads. The speedup from using n threads on a game was\nde\ufb01ned as the time required to reach a \ufb01xed reference score using\none thread divided the time required to reach the reference score\nusing n threads. The table shows the speedups averaged over\nseven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert,\nSeaquest, and Space Invaders).\nWe trained an A3C LSTM agent on this task using only\n84 \u00d7 84 RGB images as input. The \ufb01nal average score\nof around 50 indicates that the agent learned a reason-\nable strategy for exploring random 3D maxes using only\na visual input.\nA video showing one of the agents ex-\nploring previously unseen mazes is included at https:\n//youtu.be/nMR5mjCFZCw.\n5.5. Scalability and Data Ef\ufb01ciency\nWe analyzed the effectiveness of our proposed framework\nby looking at how the training time and data ef\ufb01ciency\nchanges with the number of parallel actor-learners. When\nusing multiple workers in parallel and updating a shared\nmodel, one would expect that in an ideal case, for a given\ntask and algorithm, the number of training steps to achieve\na certain score would remain the same with varying num-\nbers of workers. Therefore, the advantage would be solely\ndue to the ability of the system to consume more data in\nthe same amount of wall clock time and possibly improved\nexploration. Table 2 shows the training speed-up achieved\nby using increasing numbers of parallel actor-learners av-\neraged over seven Atari games. These results show that all\nfour methods achieve substantial speedups from using mul-\ntiple worker threads, with 16 threads leading to at least an\norder of magnitude speedup. This con\ufb01rms that our pro-\nposed framework scales well with the number of parallel\nworkers, making ef\ufb01cient use of resources.\nSomewhat surprisingly, asynchronous one-step Q-learning\nand Sarsa algorithms exhibit superlinear speedups that\ncannot be explained by purely computational gains. We\nobserve that one-step methods (one-step Q and one-step\nSarsa) often require less data to achieve a particular score\nwhen using more parallel actor-learners. We believe this\nis due to positive effect of multiple threads to reduce the\nbias in one-step methods. These effects are shown more\nclearly in Figure 3, which shows plots of the average score\nagainst the total number of training frames for different\n\n\nAsynchronous Methods for Deep Reinforcement Learning\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nA3C, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n200\n0\n200\n400\n600\n800\n1000\nScore\nA3C, Breakout\n10-4\n10-3\n10-2\nLearning rate\n30\n20\n10\n0\n10\n20\n30\nScore\nA3C, Pong\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nA3C, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n0\n200\n400\n600\n800\n1000\n1200\n1400\nScore\nA3C, Space Invaders\nFigure 2. Scatter plots of scores obtained by asynchronous advantage actor-critic on \ufb01ve games (Beamrider, Breakout, Pong, Q*bert,\nSpace Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for\nwhich all random initializations acheive good scores. This shows that A3C is quite robust to learning rates and initial random weights.\nnumbers of actor-learners and training methods on \ufb01ve\nAtari games, and Figure 4, which shows plots of the av-\nerage score against wall-clock time.\n5.6. Robustness and Stability\nFinally, we analyzed the stability and robustness of the\nfour proposed asynchronous algorithms. For each of the\nfour algorithms we trained models on \ufb01ve games (Break-\nout, Beamrider, Pong, Q*bert, Space Invaders) using 50\ndifferent learning rates and random initializations. Figure 2\nshows scatter plots of the resulting scores for A3C, while\nSupplementary Figure S11 shows plots for the other three\nmethods. There is usually a range of learning rates for each\nmethod and game combination that leads to good scores,\nindicating that all methods are quite robust to the choice of\nlearning rate and random initialization. The fact that there\nare virtually no points with scores of 0 in regions with good\nlearning rates indicates that the methods are stable and do\nnot collapse or diverge once they are learning.\n6. Conclusions and Discussion\nWe have presented asynchronous versions of four standard\nreinforcement learning algorithms and showed that they\nare able to train neural network controllers on a variety\nof domains in a stable manner. Our results show that in\nour proposed framework stable training of neural networks\nthrough reinforcement learning is possible with both value-\nbased and policy-based methods, off-policy as well as on-\npolicy methods, and in discrete as well as continuous do-\nmains. When trained on the Atari domain using 16 CPU\ncores, the proposed asynchronous algorithms train faster\nthan DQN trained on an Nvidia K40 GPU, with A3C sur-\npassing the current state-of-the-art in half the training time.\nOne of our main \ufb01ndings is that using parallel actor-\nlearners to update a shared model had a stabilizing effect on\nthe learning process of the three value-based methods we\nconsidered. While this shows that stable online Q-learning\nis possible without experience replay, which was used for\nthis purpose in DQN, it does not mean that experience re-\nplay is not useful.\nIncorporating experience replay into\nthe asynchronous reinforcement learning framework could\nsubstantially improve the data ef\ufb01ciency of these methods\nby reusing old data. This could in turn lead to much faster\ntraining times in domains like TORCS where interacting\nwith the environment is more expensive than updating the\nmodel for the architecture we used.\nCombining other existing reinforcement learning meth-\nods or recent advances in deep reinforcement learning\nwith our asynchronous framework presents many possibil-\nities for immediate improvements to the methods we pre-\nsented. While our n-step methods operate in the forward\nview (Sutton & Barto, 1998) by using corrected n-step re-\nturns directly as targets, it has been more common to use\nthe backward view to implicitly combine different returns\nthrough eligibility traces (Watkins, 1989; Sutton & Barto,\n1998; Peng & Williams, 1996).\nThe asynchronous ad-\nvantage actor-critic method could be potentially improved\nby using other ways of estimating the advantage function,\nsuch as generalized advantage estimation of (Schulman\net al., 2015b). All of the value-based methods we inves-\ntigated could bene\ufb01t from different ways of reducing over-\nestimation bias of Q-values (Van Hasselt et al., 2015; Belle-\nmare et al., 2016). Yet another, more speculative, direction\nis to try and combine the recent work on true online tempo-\nral difference methods (van Seijen et al., 2015) with non-\nlinear function approximation.\nIn addition to these algorithmic improvements, a number\nof complementary improvements to the neural network ar-\nchitecture are possible. The dueling architecture of (Wang\net al., 2015) has been shown to produce more accurate es-\ntimates of Q-values by including separate streams for the\nstate value and advantage in the network. The spatial soft-\nmax proposed by (Levine et al., 2015) could improve both\nvalue-based and policy-based methods by making it easier\nfor the network to represent feature coordinates.\nACKNOWLEDGMENTS\nWe thank Thomas Degris, Remi Munos, Marc Lanctot,\nSasha Vezhnevets and Joseph Modayil for many helpful\ndiscussions, suggestions and comments on the paper. We\nalso thank the DeepMind evaluation team for setting up the\nenvironments used to evaluate the agents in the paper.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\nScore\nBeamrider\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nScore\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nBeamrider\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nBreakout\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n30\n20\n10\n0\n10\n20\n30\nScore\nPong\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nQ*bert\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n200\n400\n600\n800\n1000\n1200\n1400\nScore\nSpace Invaders\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 3. Data ef\ufb01ciency comparison of different numbers of actor-learners for three asynchronous methods on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis\nshows the average score. Each curve shows the average over the three best learning rates. Single step methods show increased data\nef\ufb01ciency from more parallel workers. Results for Sarsa are shown in Supplementary Figure S9.\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\nScore\nBeamrider\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300\nScore\nBreakout\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nScore\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nScore\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nBeamrider\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n100\n200\n300\n400\n500\n600\nScore\nBreakout\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n30\n20\n10\n0\n10\n20\n30\nScore\nPong\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nQ*bert\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nScore\nSpace Invaders\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 4. Training speed comparison of different numbers of actor-learners on \ufb01ve Atari games. The x-axis shows training time in\nhours while the y-axis shows the average score. Each curve shows the average over the three best learning rates. All asynchronous\nmethods show signi\ufb01cant speedups from using greater numbers of parallel actor-learners. Results for Sarsa are shown in Supplementary\nFigure S10.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nReferences\nBellemare, Marc G, Naddaf, Yavar, Veness, Joel, and\nBowling, Michael.\nThe arcade learning environment:\nAn evaluation platform for general agents. Journal of\nArti\ufb01cial Intelligence Research, 2012.\nBellemare, Marc G., Ostrovski, Georg, Guez, Arthur,\nThomas, Philip S., and Munos, R\u00e9mi. Increasing the ac-\ntion gap: New operators for reinforcement learning. In\nProceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence, 2016.\nBertsekas, Dimitri P. Distributed dynamic programming.\nAutomatic Control, IEEE Transactions on, 27(3):610\u2013\n616, 1982.\nChavez, Kevin, Ong, Hao Yi, and Hong, Augustus. Dis-\ntributed deep q-learning. Technical report, Stanford Uni-\nversity, June 2015.\nDegris, Thomas, Pilarski, Patrick M, and Sutton, Richard S.\nModel-free reinforcement learning with continuous ac-\ntion in practice. In American Control Conference (ACC),\n2012, pp. 2177\u20132182. IEEE, 2012.\nGrounds, Matthew and Kudenko, Daniel.\nParallel rein-\nforcement learning with linear function approximation.\nIn Proceedings of the 5th, 6th and 7th European Confer-\nence on Adaptive and Learning Agents and Multi-agent\nSystems: Adaptation and Multi-agent Learning, pp. 60\u2013\n74. Springer-Verlag, 2008.\nKoutn\u00edk, Jan, Schmidhuber, J\u00fcrgen, and Gomez, Faustino.\nEvolving deep unsupervised convolutional networks for\nvision-based reinforcement learning. In Proceedings of\nthe 2014 conference on Genetic and evolutionary com-\nputation, pp. 541\u2013548. ACM, 2014.\nLevine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,\nPieter. End-to-end training of deep visuomotor policies.\narXiv preprint arXiv:1504.00702, 2015.\nLi, Yuxi and Schuurmans, Dale. Mapreduce for parallel re-\ninforcement learning. In Recent Advances in Reinforce-\nment Learning - 9th European Workshop, EWRL 2011,\nAthens, Greece, September 9-11, 2011, Revised Selected\nPapers, pp. 309\u2013320, 2011.\nLillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,\nHeess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,\nand Wierstra, Daan. Continuous control with deep re-\ninforcement learning. arXiv preprint arXiv:1509.02971,\n2015.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nGraves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and\nRiedmiller, Martin. Playing atari with deep reinforce-\nment learning. In NIPS Deep Learning Workshop. 2013.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nRusu, Andrei A., Veness, Joel, Bellemare, Marc G.,\nGraves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,\nOstrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,\nAmir, Antonoglou, Ioannis, King, Helen, Kumaran,\nDharshan, Wierstra, Daan, Legg, Shane, and Hassabis,\nDemis. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529\u2013533, 02 2015. URL\nhttp://dx.doi.org/10.1038/nature14236.\nNair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alci-\ncek, Cagdas, Fearon, Rory, Maria, Alessandro De, Pan-\nneershelvam, Vedavyas, Suleyman, Mustafa, Beattie,\nCharles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr,\nKavukcuoglu, Koray, and Silver, David. Massively par-\nallel methods for deep reinforcement learning. In ICML\nDeep Learning Workshop. 2015.\nPeng, Jing and Williams, Ronald J. Incremental multi-step\nq-learning. Machine Learning, 22(1-3):283\u2013290, 1996.\nRecht, Benjamin, Re, Christopher, Wright, Stephen, and\nNiu, Feng. Hogwild: A lock-free approach to paralleliz-\ning stochastic gradient descent. In Advances in Neural\nInformation Processing Systems, pp. 693\u2013701, 2011.\nRiedmiller, Martin. Neural \ufb01tted q iteration\u2013\ufb01rst experi-\nences with a data ef\ufb01cient neural reinforcement learning\nmethod. In Machine Learning: ECML 2005, pp. 317\u2013\n328. Springer Berlin Heidelberg, 2005.\nRummery, Gavin A and Niranjan, Mahesan.\nOn-line q-\nlearning using connectionist systems. 1994.\nSchaul, Tom, Quan, John, Antonoglou, Ioannis, and Sil-\nver, David. Prioritized experience replay. arXiv preprint\narXiv:1511.05952, 2015.\nSchulman, John, Levine, Sergey, Moritz, Philipp, Jordan,\nMichael I, and Abbeel, Pieter. Trust region policy op-\ntimization.\nIn International Conference on Machine\nLearning (ICML), 2015a.\nSchulman, John, Moritz, Philipp, Levine, Sergey, Jordan,\nMichael, and Abbeel, Pieter.\nHigh-dimensional con-\ntinuous control using generalized advantage estimation.\narXiv preprint arXiv:1506.02438, 2015b.\nSutton, R. and Barto, A. Reinforcement Learning: an In-\ntroduction. MIT Press, 1998.\nTieleman, Tijmen and Hinton, Geoffrey.\nLecture 6.5-\nrmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural Networks for\nMachine Learning, 4, 2012.\nTodorov, E. MuJoCo: Modeling, Simulation and Visual-\nization of Multi-Joint Dynamics with Contact (ed 1.0).\nRoboti Publishing, 2015.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nTomassini, Marco. Parallel and distributed evolutionary al-\ngorithms: A review. Technical report, 1999.\nTsitsiklis, John N. Asynchronous stochastic approxima-\ntion and q-learning. Machine Learning, 16(3):185\u2013202,\n1994.\nVan Hasselt, Hado, Guez, Arthur, and Silver, David. Deep\nreinforcement learning with double q-learning.\narXiv\npreprint arXiv:1509.06461, 2015.\nvan Seijen, H., Rupam Mahmood, A., Pilarski, P. M.,\nMachado, M. C., and Sutton, R. S.\nTrue Online\nTemporal-Difference Learning. ArXiv e-prints, Decem-\nber 2015.\nWang, Z., de Freitas, N., and Lanctot, M. Dueling Network\nArchitectures for Deep Reinforcement Learning. ArXiv\ne-prints, November 2015.\nWatkins, Christopher John Cornish Hellaby. Learning from\ndelayed rewards. PhD thesis, University of Cambridge\nEngland, 1989.\nWilliams, R.J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning.\nMa-\nchine Learning, 8(3):229\u2013256, 1992.\nWilliams, Ronald J and Peng, Jing. Function optimization\nusing connectionist reinforcement learning algorithms.\nConnection Science, 3(3):241\u2013268, 1991.\nWymann, B., Espi\u00c3l\u2019, E., Guionneau, C., Dimitrakakis, C.,\nCoulom, R., and Sumner, A. Torcs: The open racing car\nsimulator, v1.3.5, 2013.\n\n\nSupplementary Material for \"Asynchronous Methods for Deep\nReinforcement Learning\"\nJune 17, 2016\n7. Optimization Details\nWe investigated two different optimization algorithms with our asynchronous framework \u2013 stochastic gradient\ndescent and RMSProp. Our implementations of these algorithms do not use any locking in order to maximize\nthroughput when using a large number of threads.\nMomentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and\nwell studied (Recht et al., 2011). Let \u03b8 be the parameter vector that is shared across all threads and let \u2206\u03b8i\nbe the accumulated gradients of the loss with respect to parameters \u03b8 computed by thread number i. Each\nthread i independently applies the standard momentum SGD update mi = \u03b1mi + (1 \u2212\u03b1)\u2206\u03b8i followed by\n\u03b8 \u2190\u03b8 \u2212\u03b7mi with learning rate \u03b7, momentum \u03b1 and without any locks. Note that in this setting, each thread\nmaintains its own separate gradient and momentum vector.\nRMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature,\nit has not been extensively studied in the asynchronous optimization setting. The standard non-centered\nRMSProp update is given by\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82\n(S2)\n\u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(S3)\nwhere all operations are performed elementwise. In order to apply RMSProp in the asynchronous optimiza-\ntion setting one must decide whether the moving average of elementwise squared gradients g is shared or\nper-thread. We experimented with two versions of the algorithm. In one version, which we refer to as RM-\nSProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared\nRMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing\nstatistics among threads also reduces memory requirements by using one fewer copy of the parameter vector\nper thread.\nWe compared these three asynchronous optimization algorithms in terms of their sensitivity to different learn-\ning rates and random network initializations. Figure S5 shows a comparison of the methods for two different\nreinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games\n(Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the scores for 50 experiments that\ncorrespond to 50 different random learning rates and initializations. The x-axis shows the rank of the model\nafter sorting in descending order by \ufb01nal average score and the y-axis shows the \ufb01nal average score achieved\nby the corresponding model. In this representation, the algorithm that performs better would achieve higher\nmaximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizon-\ntal, thus maximizing the area under the curve. RMSProp with shared statistics tends to be more robust than\nRMSProp with per-thread statistics, which is in turn more robust than Momentum SGD.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\n8. Experimental Setup\nThe experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS\nexperiments (Figure S6) used the following setup. Each experiment used 16 actor-learner threads running\non a single machine and no GPUs. All methods performed updates after every 5 actions (tmax = 5 and\nIUpdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods\nused a shared target network that was updated every 40000 frames. The Atari experiments used the same\ninput preprocessing as (Mnih et al., 2015) and an action repeat of 4. The agents used the network architecture\nfrom (Mnih et al., 2013). The network used a convolutional layer with 16 \ufb01lters of size 8 \u00d7 8 with stride\n4, followed by a convolutional layer with with 32 \ufb01lters of size 4 \u00d7 4 with stride 2, followed by a fully\nconnected layer with 256 hidden units. All three hidden layers were followed by a recti\ufb01er nonlinearity. The\nvalue-based methods had a single linear output unit for each action representing the action-value. The model\nused by actor-critic agents had two set of outputs \u2013 a softmax output with one entry per action representing the\nprobability of selecting the action, and a single linear output representing the value function. All experiments\nused a discount of \u03b3 = 0.99 and an RMSProp decay factor of \u03b1 = 0.99.\nThe value based methods sampled the exploration rate \u03f5 from a distribution taking three values \u03f51, \u03f52, \u03f53 with\nprobabilities 0.4, 0.3, 0.3. The values of \u03f51, \u03f52, \u03f53 were annealed from 1 to 0.1, 0.01, 0.5 respectively over\nthe \ufb01rst four million frames. Advantage actor-critic used entropy regularization with a weight \u03b2 = 0.01 for\nall Atari and TORCS experiments. We performed a set of 50 experiments for \ufb01ve Atari games and every\nTORCS level, each using a different random initialization and initial learning rate. The initial learning rate\nwas sampled from a LogUniform(10\u22124, 10\u22122) distribution and annealed to 0 over the course of training.\nNote that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used\n\ufb01xed hyperparameters.\n9. Continuous Action Control Using the MuJoCo Physics Simulator\nTo apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly\nidentical to that used in the discrete action domains, so here we enumerate only the differences required for\nthe continuous action domains. The essential elements for many of the tasks (i.e. the physics models and\ntask objectives) are near identical to the tasks examined in (Lillicrap et al., 2015). However, the rewards and\nthus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco\nwhich altered the contact model.\nFor all the domains we attempted to learn the task using the physical state as input. The physical state\nconsisted of the joint positions and velocities as well as the target position if the task required a target. In\naddition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from\nRGB pixel inputs. In the low dimensional physical state case, the inputs are mapped to a hidden state using\none hidden layer with 200 ReLU units. In the cases where we used pixels, the input was passed through two\nlayers of spatial convolutions without any non-linearity or pooling. In either case, the output of the encoder\nlayers were fed to a single layer of 128 LSTM cells. The most important difference in the architecture is in the\nthe output layer of the policy network. Unlike the discrete action domain where the action output is a Softmax,\nhere the two outputs of the policy network are two real number vectors which we treat as the mean vector \u00b5\nand scalar variance \u03c32 of a multidimensional normal distribution with a spherical covariance. To act, the input\nis passed through the model to the output layer where we sample from the normal distribution determined by\n\u00b5 and \u03c32. In practice, \u00b5 is modeled by a linear layer and \u03c32 by a SoftPlus operation, log(1 + exp(x)), as the\nactivation computed as a function of the output of a linear layer. In our experiments with continuous control\nproblems the networks for policy network and value network do not share any parameters, though this detail\nis unlikely to be crucial. Finally, since the episodes were typically at most several hundred time steps long,\nwe did not use any bootstrapping in the policy or value function updates and batched each episode into a\nsingle update.\nAs in the discrete action case, we included an entropy cost which encouraged exploration. In the continuous\n\n\nAsynchronous Methods for Deep Reinforcement Learning\ncase the we used a cost on the differential entropy of the normal distribution de\ufb01ned by the output of the\nactor network, \u22121\n2(log(2\u03c0\u03c32) + 1), we used a constant multiplier of 10\u22124 for this cost across all of the tasks\nexamined. The asynchronous advantage actor-critic algorithm \ufb01nds solutions for all the domains. Figure S8\nshows learning curves against wall-clock time, and demonstrates that most of the domains from states can be\nsolved within a few hours. All of the experiments, including those done from pixel based observations, were\nrun on CPU. Even in the case of solving the domains directly from pixel inputs we found that it was possible\nto reliably discover solutions within 24 hours. Figure S7 shows scatter plots of the top scores against the\nsampled learning rates. In most of the domains there is large range of learning rates that consistently achieve\ngood performance on the task.\nAlgorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread.\n// Assume global shared parameter vector \u03b8.\n// Assume global shared target parameter vector \u03b8\u2212.\n// Assume global shared counter T = 0.\nInitialize thread step counter t \u21901\nInitialize target network parameters \u03b8\u2212\u2190\u03b8\nInitialize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\nInitialize network gradients d\u03b8 \u21900\nrepeat\nClear gradients d\u03b8 \u21900\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\ntstart = t\nGet state st\nrepeat\nTake action at according to the \u03f5-greedy policy based on Q(st, a; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a 0\nfor terminal st\nmaxa Q(st, a; \u03b8\u2212)\nfor non-terminal st\nfor i \u2208{t \u22121, . . . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 +\n\u2202(R\u2212Q(si,ai;\u03b8\u2032))\n2\n\u2202\u03b8\u2032\nend for\nPerform asynchronous update of \u03b8 using d\u03b8.\nif T\nmod Itarget == 0 then\n\u03b8\u2212\u2190\u03b8\nend if\nuntil T > Tmax\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nAlgorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.\n// Assume global shared parameter vectors \u03b8 and \u03b8v and global shared counter T = 0\n// Assume thread-speci\ufb01c parameter vectors \u03b8\u2032 and \u03b8\u2032\nv\nInitialize thread step counter t \u21901\nrepeat\nReset gradients: d\u03b8 \u21900 and d\u03b8v \u21900.\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8 and \u03b8\u2032\nv = \u03b8v\ntstart = t\nGet state st\nrepeat\nPerform at according to policy \u03c0(at|st; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a\n0\nfor terminal st\nV (st, \u03b8\u2032\nv)\nfor non-terminal st// Bootstrap from last state\nfor i \u2208{t \u22121, . . . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 + \u2207\u03b8\u2032 log \u03c0(ai|si; \u03b8\u2032)(R \u2212V (si; \u03b8\u2032\nv))\nAccumulate gradients wrt \u03b8\u2032\nv: d\u03b8v \u2190d\u03b8v + \u2202(R \u2212V (si; \u03b8\u2032\nv))2/\u2202\u03b8\u2032\nv\nend for\nPerform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v.\nuntil T > Tmax\n\n\nAsynchronous Methods for Deep Reinforcement Learning\n10\n20\n30\n40\n50\nModel Rank\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\nBreakout\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n5000\n10000\n15000\n20000\n25000\nScore\nBeamrider\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nSeaquest\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\nScore\nSpace Invaders\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\nScore\nBreakout\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n5000\n10000\n15000\n20000\n25000\nScore\nBeamrider\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\nScore\nSeaquest\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nScore\nSpace Invaders\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\nFigure S5. Comparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested\nusing two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Break-\nout, Beamrider, Seaquest and Space Invaders). Each curve shows the \ufb01nal scores for 50 experiments sorted in descending\norder that covers a search over 50 random initializations and learning rates. The top row shows results using Async n-step\nQ algorithm and bottom row shows results with Async Advantage Actor-Critic. Each individual graph shows results for\none of the four games and three different optimization methods. Shared RMSProp tends to be more robust to different\nlearning rates and random initializations than Momentum SGD and RMSProp without sharing.\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\nSlow car, no bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\nSlow car, bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nFast car, no bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nFast car, bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\nFigure S6. Comparison of algorithms on the TORCS car racing simulator. Four different con\ufb01gurations of car speed and\nopponent presence or absence are shown. In each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and\nAdvantage Actor-Critic) are compared on score vs training time in wall clock hours. Multi-step algorithms achieve better\npolicies much faster than one-step algorithms on all four levels. The curves show averages over the 5 best runs from 50\nexperiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nFigure S7. Performance for the Mujoco continuous action domains. Scatter plot of the best score obtained against\nlearning rates sampled from LogUniform(10\u22125, 10\u22121). For nearly all of the tasks there is a wide range of learning\nrates that lead to good performance on the task.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nFigure S8. Score per episode vs wall-clock time plots for the Mujoco domains. Each plot shows error bars for the top 5\nexperiments.\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nScore\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n100\n200\n300\n400\n500\n600\n700\n800\n900\nScore\nSpace Invaders\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S9. Data ef\ufb01ciency comparison of different numbers of actor-learners one-step Sarsa on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads).\nThe y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over\n50 random learning rates. Sarsa shows increased data ef\ufb01ciency with increased numbers of parallel workers.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nScore\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S10. Training speed comparison of different numbers of actor-learners for all one-step Sarsa on \ufb01ve Atari games.\nThe x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average of the\nthree best performing agents from a search over 50 random learning rates. Sarsa shows signi\ufb01cant speedups from using\ngreater numbers of parallel actor-learners.\n10-4\n10-3\n10-2\nLearning rate\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\n1-step Q, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n50\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\n1-step Q, Breakout\n10-4\n10-3\n10-2\nLearning rate\n30\n20\n10\n0\n10\n20\n30\nScore\n1-step Q, Pong\n10-4\n10-3\n10-2\nLearning rate\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\n1-step Q, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n100\n200\n300\n400\n500\n600\n700\n800\nScore\n1-step Q, Space Invaders\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nScore\n1-step SARSA, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n50\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\n1-step SARSA, Breakout\n10-4\n10-3\n10-2\nLearning rate\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\n1-step SARSA, Pong\n10-4\n10-3\n10-2\nLearning rate\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\n1-step SARSA, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n100\n200\n300\n400\n500\n600\n700\n800\n900\nScore\n1-step SARSA, Space Invaders\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nn-step Q, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n50\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\nn-step Q, Breakout\n10-4\n10-3\n10-2\nLearning rate\n30\n20\n10\n0\n10\n20\n30\nScore\nn-step Q, Pong\n10-4\n10-3\n10-2\nLearning rate\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\nn-step Q, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n300\n400\n500\n600\n700\n800\n900\n1000\nScore\nn-step Q, Space Invaders\nFigure S11. Scatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on \ufb01ve games (Beamrider,\nBreakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. All algorithms exhibit\nsome level of robustness to the choice of learning rate.\n\n\nAsynchronous Methods for Deep Reinforcement Learning\nGame\nDQN\nGorila\nDouble\nDueling\nPrioritized\nA3C FF, 1 day\nA3C FF\nA3C LSTM\nAlien\n570.2\n813.5\n1033.4\n1486.5\n900.5\n182.1\n518.4\n945.3\nAmidar\n133.4\n189.2\n169.1\n172.7\n218.4\n283.9\n263.9\n173.0\nAssault\n3332.3\n1195.8\n6060.8\n3994.8\n7748.5\n3746.1\n5474.9\n14497.9\nAsterix\n124.5\n3324.7\n16837.0\n15840.0\n31907.5\n6723.0\n22140.5\n17244.5\nAsteroids\n697.1\n933.6\n1193.2\n2035.4\n1654.0\n3009.4\n4474.5\n5093.1\nAtlantis\n76108.0\n629166.5\n319688.0\n445360.0\n593642.0\n772392.0\n911091.0\n875822.0\nBank Heist\n176.3\n399.4\n886.0\n1129.3\n816.8\n946.0\n970.1\n932.8\nBattle Zone\n17560.0\n19938.0\n24740.0\n31320.0\n29100.0\n11340.0\n12950.0\n20760.0\nBeam Rider\n8672.4\n3822.1\n17417.2\n14591.3\n26172.7\n13235.9\n22707.9\n24622.2\nBerzerk\n1011.1\n910.6\n1165.6\n1433.4\n817.9\n862.2\nBowling\n41.2\n54.0\n69.6\n65.7\n65.8\n36.2\n35.1\n41.8\nBoxing\n25.8\n74.2\n73.5\n77.3\n68.6\n33.7\n59.8\n37.3\nBreakout\n303.9\n313.0\n368.9\n411.6\n371.6\n551.6\n681.9\n766.8\nCentipede\n3773.1\n6296.9\n3853.5\n4881.0\n3421.9\n3306.5\n3755.8\n1997.0\nChopper Comman\n3046.0\n3191.8\n3495.0\n3784.0\n6604.0\n4669.0\n7021.0\n10150.0\nCrazy Climber\n50992.0\n65451.0\n113782.0\n124566.0\n131086.0\n101624.0\n112646.0\n138518.0\nDefender\n27510.0\n33996.0\n21093.5\n36242.5\n56533.0\n233021.5\nDemon Attack\n12835.2\n14880.1\n69803.4\n56322.8\n73185.8\n84997.5\n113308.4\n115201.9\nDouble Dunk\n-21.6\n-11.3\n-0.3\n-0.8\n2.7\n0.1\n-0.1\n0.1\nEnduro\n475.6\n71.0\n1216.6\n2077.4\n1884.4\n-82.2\n-82.5\n-82.5\nFishing Derby\n-2.3\n4.6\n3.2\n-4.1\n9.2\n13.6\n18.8\n22.6\nFreeway\n25.8\n10.2\n28.8\n0.2\n27.9\n0.1\n0.1\n0.1\nFrostbite\n157.4\n426.6\n1448.1\n2332.4\n2930.2\n180.1\n190.5\n197.6\nGopher\n2731.8\n4373.0\n15253.0\n20051.4\n57783.8\n8442.8\n10022.8\n17106.8\nGravitar\n216.5\n538.4\n200.5\n297.0\n218.0\n269.5\n303.5\n320.0\nH.E.R.O.\n12952.5\n8963.4\n14892.5\n15207.9\n20506.4\n28765.8\n32464.1\n28889.5\nIce Hockey\n-3.8\n-1.7\n-2.5\n-1.3\n-1.0\n-4.7\n-2.8\n-1.7\nJames Bond\n348.5\n444.0\n573.0\n835.5\n3511.5\n351.5\n541.0\n613.0\nKangaroo\n2696.0\n1431.0\n11204.0\n10334.0\n10241.0\n106.0\n94.0\n125.0\nKrull\n3864.0\n6363.1\n6796.1\n8051.6\n7406.5\n8066.6\n5560.0\n5911.4\nKung-Fu Master\n11875.0\n20620.0\n30207.0\n24288.0\n31244.0\n3046.0\n28819.0\n40835.0\nMontezuma\u2019s Revenge\n50.0\n84.0\n42.0\n22.0\n13.0\n53.0\n67.0\n41.0\nMs. Pacman\n763.5\n1263.0\n1241.3\n2250.6\n1824.6\n594.4\n653.7\n850.7\nName This Game\n5439.9\n9238.5\n8960.3\n11185.1\n11836.1\n5614.0\n10476.1\n12093.7\nPhoenix\n12366.5\n20410.5\n27430.1\n28181.8\n52894.1\n74786.7\nPit Fall\n-186.7\n-46.9\n-14.8\n-123.0\n-78.5\n-135.7\nPong\n16.2\n16.7\n19.1\n18.8\n18.9\n11.4\n5.6\n10.7\nPrivate Eye\n298.2\n2598.6\n-575.5\n292.6\n179.0\n194.4\n206.9\n421.1\nQ*Bert\n4589.8\n7089.8\n11020.8\n14175.8\n11277.0\n13752.3\n15148.8\n21307.5\nRiver Raid\n4065.3\n5310.3\n10838.4\n16569.4\n18184.4\n10001.2\n12201.8\n6591.9\nRoad Runner\n9264.0\n43079.8\n43156.0\n58549.0\n56990.0\n31769.0\n34216.0\n73949.0\nRobotank\n58.5\n61.8\n59.1\n62.0\n55.4\n2.3\n32.8\n2.6\nSeaquest\n2793.9\n10145.9\n14498.0\n37361.6\n39096.7\n2300.2\n2355.4\n1326.1\nSkiing\n-11490.4\n-11928.0\n-10852.8\n-13700.0\n-10911.1\n-14863.8\nSolaris\n810.0\n1768.4\n2238.2\n1884.8\n1956.0\n1936.4\nSpace Invaders\n1449.7\n1183.3\n2628.7\n5993.1\n9063.0\n2214.7\n15730.5\n23846.0\nStar Gunner\n34081.0\n14919.2\n58365.0\n90804.0\n51959.0\n64393.0\n138218.0\n164766.0\nSurround\n1.9\n4.0\n-0.9\n-9.6\n-9.7\n-8.3\nTennis\n-2.3\n-0.7\n-7.8\n4.4\n-2.0\n-10.2\n-6.3\n-6.4\nTime Pilot\n5640.0\n8267.8\n6608.0\n6601.0\n7448.0\n5825.0\n12679.0\n27202.0\nTutankham\n32.4\n118.5\n92.2\n48.0\n33.6\n26.1\n156.3\n144.2\nUp and Down\n3311.3\n8747.7\n19086.9\n24759.2\n29443.7\n54525.4\n74705.7\n105728.7\nVenture\n54.0\n523.4\n21.0\n200.0\n244.0\n19.0\n23.0\n25.0\nVideo Pinball\n20228.1\n112093.4\n367823.7\n110976.2\n374886.9\n185852.6\n331628.1\n470310.5\nWizard of Wor\n246.0\n10431.0\n6201.0\n7054.0\n7451.0\n5278.0\n17244.0\n18082.0\nYars Revenge\n6270.6\n25976.5\n5965.1\n7270.8\n7157.5\n5615.5\nZaxxon\n831.0\n6159.4\n8593.0\n10164.0\n9501.0\n2659.0\n24622.0\n23519.0\nTable S3. Raw scores for the human start condition (30 minutes emulator time). DQN scores taken from (Nair et al.,\n2015). Double DQN scores taken from (Van Hasselt et al., 2015), Dueling scores from (Wang et al., 2015) and Prioritized\nscores taken from (Schaul et al., 2015)\n\n\n",
    "title": "Asynchronous Methods for Deep Reinforcement Learning",
    "abstract": "We propose a conceptually simple and lightweight framework for deep reinforce- ment learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
    "sections": [
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "Volodymyr Mnih1\nVMNIH@GOOGLE.COM\nAdri\u00e0 Puigdom\u00e8nech Badia1\nADRIAP@GOOGLE.COM\nMehdi Mirza1,2\nMIRZAMOM@IRO.UMONTREAL.CA\nAlex Graves1\nGRAVESA@GOOGLE.COM\nTim Harley1\nTHARLEY@GOOGLE.COM\nTimothy P. Lillicrap1\nCOUNTZERO@GOOGLE.COM\nDavid Silver1\nDAVIDSILVER@GOOGLE.COM\nKoray Kavukcuoglu 1\nKORAYK@GOOGLE.COM"
      },
      {
        "header": "We",
        "content": "propose\na\nconceptually\nsimple\nand\nlightweight\nframework\nfor\ndeep\nreinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers.\nThe best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input."
      },
      {
        "header": "Deep neural networks provide rich representations that can",
        "content": "enable reinforcement learning (RL) algorithms to perform\neffectively. However, it was previously thought that the\ncombination of simple online RL algorithms with deep\nneural networks was fundamentally unstable. Instead, a va-\nriety of solutions have been proposed to stabilize the algo-\nrithm (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Has-\nselt et al., 2015; Schulman et al., 2015a). These approaches\nshare a common idea: the sequence of observed data en-\ncountered by an online RL agent is non-stationary, and on-\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s).\nline RL updates are strongly correlated."
      },
      {
        "header": "By storing the",
        "content": "agent\u2019s data in an experience replay memory, the data can\nbe batched (Riedmiller, 2005; Schulman et al., 2015a) or\nrandomly sampled (Mnih et al., 2013; 2015; Van Hasselt\net al., 2015) from different time-steps. Aggregating over\nmemory in this way reduces non-stationarity and decorre-\nlates updates, but at the same time limits the methods to\noff-policy reinforcement learning algorithms."
      },
      {
        "header": "Deep RL algorithms based on experience replay have",
        "content": "achieved unprecedented success in challenging domains\nsuch as Atari 2600. However, experience replay has several\ndrawbacks: it uses more memory and computation per real\ninteraction; and it requires off-policy learning algorithms\nthat can update from data generated by an older policy."
      },
      {
        "header": "In this paper we provide a very different paradigm for deep",
        "content": "reinforcement learning. Instead of experience replay, we\nasynchronously execute multiple agents in parallel, on mul-\ntiple instances of the environment. This parallelism also\ndecorrelates the agents\u2019 data into a more stationary process,\nsince at any given time-step the parallel agents will be ex-\nperiencing a variety of different states. This simple idea\nenables a much larger spectrum of fundamental on-policy\nRL algorithms, such as Sarsa, n-step methods, and actor-\ncritic methods, as well as off-policy RL algorithms such\nas Q-learning, to be applied robustly and effectively using\ndeep neural networks."
      },
      {
        "header": "Our parallel reinforcement learning paradigm also offers",
        "content": "practical bene\ufb01ts. Whereas previous approaches to deep re-\ninforcement learning rely heavily on specialized hardware\nsuch as GPUs (Mnih et al., 2015; Van Hasselt et al., 2015;\nSchaul et al., 2015) or massively distributed architectures\n(Nair et al., 2015), our experiments run on a single machine\nwith a standard multi-core CPU. When applied to a vari-\nety of Atari 2600 domains, on many games asynchronous\nreinforcement learning achieves better results, in far less\narXiv:1602.01783v2  [cs.LG]  16 Jun 2016"
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "time than previous GPU-based algorithms, using far less\nresource than massively distributed approaches. The best\nof the proposed methods, asynchronous advantage actor-\ncritic (A3C), also mastered a variety of continuous motor\ncontrol tasks as well as learned general strategies for ex-\nploring 3D mazes purely from visual inputs. We believe\nthat the success of A3C on both 2D and 3D games, discrete\nand continuous action spaces, as well as its ability to train\nfeedforward and recurrent agents makes it the most general\nand successful reinforcement learning agent to date."
      },
      {
        "header": "Related Work",
        "content": "The General Reinforcement Learning Architecture (Gorila)\nof (Nair et al., 2015) performs asynchronous training of re-\ninforcement learning agents in a distributed setting. In Go-\nrila, each process contains an actor that acts in its own copy\nof the environment, a separate replay memory, and a learner\nthat samples data from the replay memory and computes\ngradients of the DQN loss (Mnih et al., 2015) with respect\nto the policy parameters. The gradients are asynchronously\nsent to a central parameter server which updates a central\ncopy of the model. The updated policy parameters are sent\nto the actor-learners at \ufb01xed intervals. By using 100 sep-\narate actor-learner processes and 30 parameter server in-\nstances, a total of 130 machines, Gorila was able to signif-\nicantly outperform DQN over 49 Atari games. On many\ngames Gorila reached the score achieved by DQN over 20\ntimes faster than DQN. We also note that a similar way of\nparallelizing DQN was proposed by (Chavez et al., 2015).\nIn earlier work, (Li & Schuurmans, 2011) applied the\nMap Reduce framework to parallelizing batch reinforce-\nment learning methods with linear function approximation."
      },
      {
        "header": "Parallelism was used to speed up large matrix operations",
        "content": "but not to parallelize the collection of experience or sta-\nbilize learning. (Grounds & Kudenko, 2008) proposed a\nparallel version of the Sarsa algorithm that uses multiple\nseparate actor-learners to accelerate training. Each actor-\nlearner learns separately and periodically sends updates to\nweights that have changed signi\ufb01cantly to the other learn-\ners using peer-to-peer communication.\n(Tsitsiklis, 1994) studied convergence properties of Q-\nlearning in the asynchronous optimization setting. These\nresults show that Q-learning is still guaranteed to converge\nwhen some of the information is outdated as long as out-\ndated information is always eventually discarded and sev-\neral other technical assumptions are satis\ufb01ed. Even earlier,\n(Bertsekas, 1982) studied the related problem of distributed\ndynamic programming.\nAnother related area of work is in evolutionary meth-\nods, which are often straightforward to parallelize by dis-\ntributing \ufb01tness evaluations over multiple machines or\nthreads (Tomassini, 1999). Such parallel evolutionary ap-\nproaches have recently been applied to some visual rein-\nforcement learning tasks. In one example, (Koutn\u00edk et al.,\n2014) evolved convolutional neural network controllers for\nthe TORCS driving simulator by performing \ufb01tness evalu-\nations on 8 CPU cores in parallel."
      },
      {
        "header": "We consider the standard reinforcement learning setting",
        "content": "where an agent interacts with an environment E over a\nnumber of discrete time steps. At each time step t, the\nagent receives a state st and selects an action at from some\nset of possible actions A according to its policy \u03c0, where\n\u03c0 is a mapping from states st to actions at. In return, the\nagent receives the next state st+1 and receives a scalar re-\nward rt. The process continues until the agent reaches a\nterminal state after which the process restarts. The return\nRt = P\u221e\nk=0 \u03b3krt+k is the total accumulated return from\ntime step t with discount factor \u03b3 \u2208(0, 1]. The goal of the\nagent is to maximize the expected return from each state st.\nThe action value Q\u03c0(s, a) = E [Rt|st = s, a] is the ex-\npected return for selecting action a in state s and follow-\ning policy \u03c0.\nThe optimal value function Q\u2217(s, a) =\nmax\u03c0 Q\u03c0(s, a) gives the maximum action value for state\ns and action a achievable by any policy.\nSimilarly, the\nvalue of state s under policy \u03c0 is de\ufb01ned as V \u03c0(s) =\nE [Rt|st = s] and is simply the expected return for follow-\ning policy \u03c0 from state s.\nIn value-based model-free reinforcement learning methods,\nthe action value function is represented using a function ap-\nproximator, such as a neural network. Let Q(s, a; \u03b8) be an\napproximate action-value function with parameters \u03b8. The\nupdates to \u03b8 can be derived from a variety of reinforcement\nlearning algorithms. One example of such an algorithm is\nQ-learning, which aims to directly approximate the optimal\naction value function: Q\u2217(s, a) \u2248Q(s, a; \u03b8). In one-step\nQ-learning, the parameters \u03b8 of the action value function\nQ(s, a; \u03b8) are learned by iteratively minimizing a sequence\nof loss functions, where the ith loss function de\ufb01ned as\nLi(\u03b8i) = E\n\u0010\nr + \u03b3 max\na\u2032 Q(s\u2032, a\u2032; \u03b8i\u22121) \u2212Q(s, a; \u03b8i)\n\u00112\nwhere s\u2032 is the state encountered after state s.\nWe refer to the above method as one-step Q-learning be-\ncause it updates the action value Q(s, a) toward the one-\nstep return r + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8). One drawback of us-\ning one-step methods is that obtaining a reward r only di-\nrectly affects the value of the state action pair s, a that led\nto the reward. The values of other state action pairs are\naffected only indirectly through the updated value Q(s, a).\nThis can make the learning process slow since many up-\ndates are required the propagate a reward to the relevant\npreceding states and actions."
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "One way of propagating rewards faster is by using n-\nstep returns (Watkins, 1989; Peng & Williams, 1996).\nIn n-step Q-learning, Q(s, a) is updated toward the n-\nstep return de\ufb01ned as rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3n\u22121rt+n\u22121 +\nmaxa \u03b3nQ(st+n, a). This results in a single reward r di-\nrectly affecting the values of n preceding state action pairs."
      },
      {
        "header": "This makes the process of propagating rewards to relevant",
        "content": "state-action pairs potentially much more ef\ufb01cient.\nIn contrast to value-based methods, policy-based model-\nfree methods directly parameterize the policy \u03c0(a|s; \u03b8) and\nupdate the parameters \u03b8 by performing, typically approx-\nimate, gradient ascent on E[Rt]."
      },
      {
        "header": "One example of such",
        "content": "a method is the REINFORCE family of algorithms due\nto Williams (1992). Standard REINFORCE updates the\npolicy parameters \u03b8 in the direction \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt,\nwhich is an unbiased estimate of \u2207\u03b8E[Rt]. It is possible to\nreduce the variance of this estimate while keeping it unbi-\nased by subtracting a learned function of the state bt(st),\nknown as a baseline (Williams, 1992), from the return. The\nresulting gradient is \u2207\u03b8 log \u03c0(at|st; \u03b8) (Rt \u2212bt(st))."
      },
      {
        "header": "A learned estimate of the value function is commonly used",
        "content": "as the baseline bt(st) \u2248V \u03c0(st) leading to a much lower\nvariance estimate of the policy gradient. When an approx-\nimate value function is used as the baseline, the quantity\nRt \u2212bt used to scale the policy gradient can be seen as\nan estimate of the advantage of action at in state st, or\nA(at, st) = Q(at, st)\u2212V (st), because Rt is an estimate of\nQ\u03c0(at, st) and bt is an estimate of V \u03c0(st). This approach\ncan be viewed as an actor-critic architecture where the pol-\nicy \u03c0 is the actor and the baseline bt is the critic (Sutton &\nBarto, 1998; Degris et al., 2012)."
      },
      {
        "header": "Asynchronous RL Framework",
        "content": "We now present multi-threaded asynchronous variants of\none-step Sarsa, one-step Q-learning, n-step Q-learning, and\nadvantage actor-critic. The aim in designing these methods\nwas to \ufb01nd RL algorithms that can train deep neural net-\nwork policies reliably and without large resource require-\nments. While the underlying RL methods are quite dif-\nferent, with actor-critic being an on-policy policy search\nmethod and Q-learning being an off-policy value-based\nmethod, we use two main ideas to make all four algorithms\npractical given our design goal.\nFirst, we use asynchronous actor-learners, similarly to the\nGorila framework (Nair et al., 2015), but instead of using\nseparate machines and a parameter server, we use multi-\nple CPU threads on a single machine. Keeping the learn-\ners on a single machine removes the communication costs\nof sending gradients and parameters and enables us to use\nHogwild! (Recht et al., 2011) style updates for training.\nSecond, we make the observation that multiple actors-\nAlgorithm 1 Asynchronous one-step Q-learning - pseu-\ndocode for each actor-learner thread.\n// Assume global shared \u03b8, \u03b8\u2212, and counter T = 0.\nInitialize thread step counter t \u21900\nInitialize target network weights \u03b8\u2212\u2190\u03b8\nInitialize network gradients d\u03b8 \u21900"
      },
      {
        "header": "Get initial state s",
        "content": "repeat\nTake action a with \u03f5-greedy policy based on Q(s, a; \u03b8)\nReceive new state s\u2032 and reward r\ny =\n\u001a r\nfor terminal s\u2032\nr + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8\u2212)\nfor non-terminal s\u2032\nAccumulate gradients wrt \u03b8: d\u03b8 \u2190d\u03b8 + \u2202(y\u2212Q(s,a;\u03b8))2\n\u2202\u03b8\ns = s\u2032\nT \u2190T + 1 and t \u2190t + 1\nif T\nmod Itarget == 0 then\nUpdate the target network \u03b8\u2212\u2190\u03b8\nend if\nif t mod IAsyncUpdate == 0 or s is terminal then\nPerform asynchronous update of \u03b8 using d\u03b8.\nClear gradients d\u03b8 \u21900.\nend if\nuntil T > Tmax\nlearners running in parallel are likely to be exploring dif-\nferent parts of the environment. Moreover, one can explic-\nitly use different exploration policies in each actor-learner\nto maximize this diversity.\nBy running different explo-\nration policies in different threads, the overall changes be-\ning made to the parameters by multiple actor-learners ap-\nplying online updates in parallel are likely to be less corre-\nlated in time than a single agent applying online updates.\nHence, we do not use a replay memory and rely on parallel\nactors employing different exploration policies to perform\nthe stabilizing role undertaken by experience replay in the\nDQN training algorithm.\nIn addition to stabilizing learning, using multiple parallel\nactor-learners has multiple practical bene\ufb01ts. First, we ob-\ntain a reduction in training time that is roughly linear in\nthe number of parallel actor-learners. Second, since we no\nlonger rely on experience replay for stabilizing learning we\nare able to use on-policy reinforcement learning methods\nsuch as Sarsa and actor-critic to train neural networks in a\nstable way. We now describe our variants of one-step Q-\nlearning, one-step Sarsa, n-step Q-learning and advantage\nactor-critic.\nAsynchronous one-step Q-learning: Pseudocode for our\nvariant of Q-learning, which we call Asynchronous one-\nstep Q-learning, is shown in Algorithm 1. Each thread in-\nteracts with its own copy of the environment and at each\nstep computes a gradient of the Q-learning loss. We use\na shared and slowly changing target network in comput-\ning the Q-learning loss, as was proposed in the DQN train-\ning method. We also accumulate gradients over multiple\ntimesteps before they are applied, which is similar to us-"
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "ing minibatches. This reduces the chances of multiple ac-\ntor learners overwriting each other\u2019s updates. Accumulat-\ning updates over several steps also provides some ability to\ntrade off computational ef\ufb01ciency for data ef\ufb01ciency.\nFinally, we found that giving each thread a different explo-\nration policy helps improve robustness. Adding diversity\nto exploration in this manner also generally improves per-\nformance through better exploration. While there are many\npossible ways of making the exploration policies differ we\nexperiment with using \u03f5-greedy exploration with \u03f5 periodi-\ncally sampled from some distribution by each thread.\nAsynchronous one-step Sarsa: The asynchronous one-\nstep Sarsa algorithm is the same as asynchronous one-step\nQ-learning as given in Algorithm 1 except that it uses a dif-\nferent target value for Q(s, a). The target value used by\none-step Sarsa is r + \u03b3Q(s\u2032, a\u2032; \u03b8\u2212) where a\u2032 is the action\ntaken in state s\u2032 (Rummery & Niranjan, 1994; Sutton &\nBarto, 1998). We again use a target network and updates\naccumulated over multiple timesteps to stabilize learning.\nAsynchronous n-step Q-learning: Pseudocode for our\nvariant of multi-step Q-learning is shown in Supplementary\nAlgorithm S2. The algorithm is somewhat unusual because\nit operates in the forward view by explicitly computing n-\nstep returns, as opposed to the more common backward\nview used by techniques like eligibility traces (Sutton &\nBarto, 1998). We found that using the forward view is eas-\nier when training neural networks with momentum-based\nmethods and backpropagation through time. In order to\ncompute a single update, the algorithm \ufb01rst selects actions\nusing its exploration policy for up to tmax steps or until a\nterminal state is reached. This process results in the agent\nreceiving up to tmax rewards from the environment since\nits last update. The algorithm then computes gradients for\nn-step Q-learning updates for each of the state-action pairs\nencountered since the last update. Each n-step update uses\nthe longest possible n-step return resulting in a one-step\nupdate for the last state, a two-step update for the second\nlast state, and so on for a total of up to tmax updates. The\naccumulated updates are applied in a single gradient step.\nAsynchronous advantage actor-critic: The algorithm,\nwhich we call asynchronous advantage actor-critic (A3C),\nmaintains a policy \u03c0(at|st; \u03b8) and an estimate of the value\nfunction V (st; \u03b8v). Like our variant of n-step Q-learning,\nour variant of actor-critic also operates in the forward view\nand uses the same mix of n-step returns to update both the\npolicy and the value-function. The policy and the value\nfunction are updated after every tmax actions or when a\nterminal state is reached. The update performed by the al-\ngorithm can be seen as \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)A(st, at; \u03b8, \u03b8v)\nwhere A(st, at; \u03b8, \u03b8v) is an estimate of the advantage func-\ntion given by Pk\u22121\ni=0 \u03b3irt+i + \u03b3kV (st+k; \u03b8v) \u2212V (st; \u03b8v),\nwhere k can vary from state to state and is upper-bounded\nby tmax. The pseudocode for the algorithm is presented in\nSupplementary Algorithm S3.\nAs with the value-based methods we rely on parallel actor-\nlearners and accumulated updates for improving training\nstability. Note that while the parameters \u03b8 of the policy\nand \u03b8v of the value function are shown as being separate\nfor generality, we always share some of the parameters in\npractice. We typically use a convolutional neural network\nthat has one softmax output for the policy \u03c0(at|st; \u03b8) and\none linear output for the value function V (st; \u03b8v), with all\nnon-output layers shared.\nWe also found that adding the entropy of the policy \u03c0 to the\nobjective function improved exploration by discouraging\npremature convergence to suboptimal deterministic poli-\ncies. This technique was originally proposed by (Williams\n& Peng, 1991), who found that it was particularly help-\nful on tasks requiring hierarchical behavior.\nThe gradi-\nent of the full objective function including the entropy\nregularization term with respect to the policy parame-\nters takes the form \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)(Rt \u2212V (st; \u03b8v)) +\n\u03b2\u2207\u03b8\u2032H(\u03c0(st; \u03b8\u2032)), where H is the entropy. The hyperpa-\nrameter \u03b2 controls the strength of the entropy regulariza-\ntion term.\nOptimization: We investigated three different optimiza-\ntion algorithms in our asynchronous framework \u2013 SGD\nwith momentum, RMSProp (Tieleman & Hinton, 2012)\nwithout shared statistics, and RMSProp with shared statis-\ntics. We used the standard non-centered RMSProp update\ngiven by\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82 and \u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(1)\nwhere all operations are performed elementwise. A com-\nparison on a subset of Atari 2600 games showed that a vari-\nant of RMSProp where statistics g are shared across threads\nis considerably more robust than the other two methods."
      },
      {
        "header": "We use four different platforms for assessing the properties",
        "content": "of the proposed framework. We perform most of our exper-\niments using the Arcade Learning Environment (Bellemare\net al., 2012), which provides a simulator for Atari 2600\ngames. This is one of the most commonly used benchmark\nenvironments for RL algorithms. We use the Atari domain\nto compare against state of the art results (Van Hasselt et al.,\n2015; Wang et al., 2015; Schaul et al., 2015; Nair et al.,\n2015; Mnih et al., 2015), as well as to carry out a detailed\nstability and scalability analysis of the proposed methods.\nWe performed further comparisons using the TORCS 3D\ncar racing simulator (Wymann et al., 2013). We also use"
      },
      {
        "header": "DQN",
        "content": "1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000"
      },
      {
        "header": "DQN",
        "content": "1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400"
      },
      {
        "header": "DQN",
        "content": "1-step Q\n1-step SARSA\nn-step Q\nA3C\nFigure 1. Learning speed comparison for DQN and the new asynchronous algorithms on \ufb01ve Atari 2600 games. DQN was trained on\na single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores. The plots are averaged over 5 runs. In\nthe case of DQN the runs were for different seeds with \ufb01xed hyperparameters. For asynchronous methods we average over the best 5\nmodels from 50 experiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.\ntwo additional domains to evaluate only the A3C algorithm\n\u2013 Mujoco and Labyrinth. MuJoCo (Todorov, 2015) is a\nphysics simulator for evaluating agents on continuous mo-\ntor control tasks with contact dynamics. Labyrinth is a new\n3D environment where the agent must learn to \ufb01nd rewards\nin randomly generated mazes from a visual input. The pre-\ncise details of our experimental setup can be found in Sup-\nplementary Section 8.\n5.1. Atari 2600 Games\nWe \ufb01rst present results on a subset of Atari 2600 games to\ndemonstrate the training speed of the new methods. Fig-\nure 1 compares the learning speed of the DQN algorithm\ntrained on an Nvidia K40 GPU with the asynchronous\nmethods trained using 16 CPU cores on \ufb01ve Atari 2600\ngames. The results show that all four asynchronous meth-\nods we presented can successfully train neural network\ncontrollers on the Atari domain. The asynchronous meth-\nods tend to learn faster than DQN, with signi\ufb01cantly faster\nlearning on some games, while training on only 16 CPU\ncores. Additionally, the results suggest that n-step methods\nlearn faster than one-step methods on some games. Over-\nall, the policy-based advantage actor-critic method signi\ufb01-\ncantly outperforms all three value-based methods.\nWe then evaluated asynchronous advantage actor-critic on\n57 Atari games. In order to compare with the state of the\nart in Atari game playing, we largely followed the train-\ning and evaluation protocol of (Van Hasselt et al., 2015).\nSpeci\ufb01cally, we tuned hyperparameters (learning rate and\namount of gradient norm clipping) using a search on six\nAtari games (Beamrider, Breakout, Pong, Q*bert, Seaquest\nand Space Invaders) and then \ufb01xed all hyperparameters for\nall 57 games. We trained both a feedforward agent with the\nsame architecture as (Mnih et al., 2015; Nair et al., 2015;\nVan Hasselt et al., 2015) as well as a recurrent agent with an\nadditional 256 LSTM cells after the \ufb01nal hidden layer. We\nadditionally used the \ufb01nal network weights for evaluation\nto make the results more comparable to the original results"
      },
      {
        "header": "Gorila",
        "content": "4 days, 100 machines\n215.2%\n71.3%\nD-DQN\n8 days on GPU\n332.9%\n110.9%\nDueling D-DQN\n8 days on GPU\n343.8%\n117.1%"
      },
      {
        "header": "Prioritized DQN",
        "content": "8 days on GPU\n463.6%\n127.6%\nA3C, FF\n1 day on CPU\n344.1%\n68.2%\nA3C, FF\n4 days on CPU\n496.8%\n116.6%\nA3C, LSTM\n4 days on CPU\n623.0%\n112.6%\nTable 1. Mean and median human-normalized scores on 57 Atari\ngames using the human starts evaluation metric. Supplementary\nTable SS3 shows the raw scores for all games.\nfrom (Bellemare et al., 2012). We trained our agents for\nfour days using 16 CPU cores, while the other agents were\ntrained for 8 to 10 days on Nvidia K40 GPUs. Table 1\nshows the average and median human-normalized scores\nobtained by our agents trained by asynchronous advantage\nactor-critic (A3C) as well as the current state-of-the art.\nSupplementary Table S3 shows the scores on all games.\nA3C signi\ufb01cantly improves on state-of-the-art the average\nscore over 57 games in half the training time of the other\nmethods while using only 16 CPU cores and no GPU. Fur-\nthermore, after just one day of training, A3C matches the\naverage human normalized score of Dueling Double DQN\nand almost reaches the median human normalized score of\nGorila. We note that many of the improvements that are\npresented in Double DQN (Van Hasselt et al., 2015) and\nDueling Double DQN (Wang et al., 2015) can be incorpo-\nrated to 1-step Q and n-step Q methods presented in this\nwork with similar potential improvements.\n5.2. TORCS Car Racing Simulator"
      },
      {
        "header": "TORCS not only has more realistic graphics than Atari",
        "content": "2600 games, but also requires the agent to learn the dy-\nnamics of the car it is controlling. At each step, an agent\nreceived only a visual input in the form of an RGB image"
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "of the current frame as well as a reward proportional to the\nagent\u2019s velocity along the center of the track at the agent\u2019s\ncurrent position. We used the same neural network archi-\ntecture as the one used in the Atari experiments speci\ufb01ed in\nSupplementary Section 8. We performed experiments us-\ning four different settings \u2013 the agent controlling a slow car\nwith and without opponent bots, and the agent controlling a\nfast car with and without opponent bots. Full results can be\nfound in Supplementary Figure S6. A3C was the best per-\nforming agent, reaching between roughly 75% and 90% of\nthe score obtained by a human tester on all four game con-\n\ufb01gurations in about 12 hours of training. A video showing\nthe learned driving behavior of the A3C agent can be found\nat https://youtu.be/0xo1Ldx3L5Q.\n5.3. Continuous Action Control Using the MuJoCo"
      },
      {
        "header": "We also examined a set of tasks where the action space",
        "content": "is continuous. In particular, we looked at a set of rigid\nbody physics domains with contact dynamics where the\ntasks include many examples of manipulation and loco-\nmotion."
      },
      {
        "header": "These tasks were simulated using the Mujoco",
        "content": "physics engine. We evaluated only the asynchronous ad-\nvantage actor-critic algorithm since, unlike the value-based\nmethods, it is easily extended to continuous actions. In all\nproblems, using either the physical state or pixels as in-\nput, Asynchronous Advantage-Critic found good solutions\nin less than 24 hours of training and typically in under a few\nhours. Some successful policies learned by our agent can\nbe seen in the following video https://youtu.be/\nAjjc08-iPx8. Further details about this experiment can\nbe found in Supplementary Section 9.\n5.4. Labyrinth\nWe performed an additional set of experiments with A3C\non a new 3D environment called Labyrinth. The speci\ufb01c\ntask we considered involved the agent learning to \ufb01nd re-\nwards in randomly generated mazes. At the beginning of\neach episode the agent was placed in a new randomly gen-\nerated maze consisting of rooms and corridors. Each maze\ncontained two types of objects that the agent was rewarded\nfor \ufb01nding \u2013 apples and portals. Picking up an apple led to\na reward of 1. Entering a portal led to a reward of 10 after\nwhich the agent was respawned in a new random location in\nthe maze and all previously collected apples were regener-\nated. An episode terminated after 60 seconds after which a\nnew episode would begin. The aim of the agent is to collect\nas many points as possible in the time limit and the optimal\nstrategy involves \ufb01rst \ufb01nding the portal and then repeatedly\ngoing back to it after each respawn. This task is much more\nchallenging than the TORCS driving domain because the\nagent is faced with a new maze in each episode and must\nlearn a general strategy for exploring random mazes."
      },
      {
        "header": "Method",
        "content": "1\n2\n4\n8\n16\n1-step Q\n1.0\n3.0\n6.3\n13.3\n24.1\n1-step SARSA\n1.0\n2.8\n5.9\n13.1\n22.1\nn-step Q\n1.0\n2.7\n5.9\n10.7\n17.2\nA3C\n1.0\n2.1\n3.7\n6.9\n12.5\nTable 2. The average training speedup for each method and num-\nber of threads averaged over seven Atari games. To compute the\ntraining speed-up on a single game we measured the time to re-\nquired reach a \ufb01xed reference score using each method and num-\nber of threads. The speedup from using n threads on a game was\nde\ufb01ned as the time required to reach a \ufb01xed reference score using\none thread divided the time required to reach the reference score\nusing n threads. The table shows the speedups averaged over\nseven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert,\nSeaquest, and Space Invaders).\nWe trained an A3C LSTM agent on this task using only\n84 \u00d7 84 RGB images as input. The \ufb01nal average score\nof around 50 indicates that the agent learned a reason-\nable strategy for exploring random 3D maxes using only\na visual input.\nA video showing one of the agents ex-\nploring previously unseen mazes is included at https:\n//youtu.be/nMR5mjCFZCw.\n5.5. Scalability and Data Ef\ufb01ciency"
      },
      {
        "header": "We analyzed the effectiveness of our proposed framework",
        "content": "by looking at how the training time and data ef\ufb01ciency\nchanges with the number of parallel actor-learners. When\nusing multiple workers in parallel and updating a shared\nmodel, one would expect that in an ideal case, for a given\ntask and algorithm, the number of training steps to achieve\na certain score would remain the same with varying num-\nbers of workers. Therefore, the advantage would be solely\ndue to the ability of the system to consume more data in\nthe same amount of wall clock time and possibly improved\nexploration. Table 2 shows the training speed-up achieved\nby using increasing numbers of parallel actor-learners av-\neraged over seven Atari games. These results show that all\nfour methods achieve substantial speedups from using mul-\ntiple worker threads, with 16 threads leading to at least an\norder of magnitude speedup. This con\ufb01rms that our pro-\nposed framework scales well with the number of parallel\nworkers, making ef\ufb01cient use of resources.\nSomewhat surprisingly, asynchronous one-step Q-learning\nand Sarsa algorithms exhibit superlinear speedups that\ncannot be explained by purely computational gains. We\nobserve that one-step methods (one-step Q and one-step\nSarsa) often require less data to achieve a particular score\nwhen using more parallel actor-learners. We believe this\nis due to positive effect of multiple threads to reduce the\nbias in one-step methods. These effects are shown more\nclearly in Figure 3, which shows plots of the average score\nagainst the total number of training frames for different"
      },
      {
        "header": "Score",
        "content": "A3C, Space Invaders\nFigure 2. Scatter plots of scores obtained by asynchronous advantage actor-critic on \ufb01ve games (Beamrider, Breakout, Pong, Q*bert,\nSpace Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for\nwhich all random initializations acheive good scores. This shows that A3C is quite robust to learning rates and initial random weights.\nnumbers of actor-learners and training methods on \ufb01ve\nAtari games, and Figure 4, which shows plots of the av-\nerage score against wall-clock time.\n5.6. Robustness and Stability\nFinally, we analyzed the stability and robustness of the\nfour proposed asynchronous algorithms. For each of the\nfour algorithms we trained models on \ufb01ve games (Break-\nout, Beamrider, Pong, Q*bert, Space Invaders) using 50\ndifferent learning rates and random initializations. Figure 2\nshows scatter plots of the resulting scores for A3C, while\nSupplementary Figure S11 shows plots for the other three\nmethods. There is usually a range of learning rates for each\nmethod and game combination that leads to good scores,\nindicating that all methods are quite robust to the choice of\nlearning rate and random initialization. The fact that there\nare virtually no points with scores of 0 in regions with good\nlearning rates indicates that the methods are stable and do\nnot collapse or diverge once they are learning."
      },
      {
        "header": "We have presented asynchronous versions of four standard",
        "content": "reinforcement learning algorithms and showed that they\nare able to train neural network controllers on a variety\nof domains in a stable manner. Our results show that in\nour proposed framework stable training of neural networks\nthrough reinforcement learning is possible with both value-\nbased and policy-based methods, off-policy as well as on-\npolicy methods, and in discrete as well as continuous do-\nmains. When trained on the Atari domain using 16 CPU\ncores, the proposed asynchronous algorithms train faster\nthan DQN trained on an Nvidia K40 GPU, with A3C sur-\npassing the current state-of-the-art in half the training time.\nOne of our main \ufb01ndings is that using parallel actor-\nlearners to update a shared model had a stabilizing effect on\nthe learning process of the three value-based methods we\nconsidered. While this shows that stable online Q-learning\nis possible without experience replay, which was used for\nthis purpose in DQN, it does not mean that experience re-\nplay is not useful."
      },
      {
        "header": "Incorporating experience replay into",
        "content": "the asynchronous reinforcement learning framework could\nsubstantially improve the data ef\ufb01ciency of these methods\nby reusing old data. This could in turn lead to much faster\ntraining times in domains like TORCS where interacting\nwith the environment is more expensive than updating the\nmodel for the architecture we used.\nCombining other existing reinforcement learning meth-\nods or recent advances in deep reinforcement learning\nwith our asynchronous framework presents many possibil-\nities for immediate improvements to the methods we pre-\nsented. While our n-step methods operate in the forward\nview (Sutton & Barto, 1998) by using corrected n-step re-\nturns directly as targets, it has been more common to use\nthe backward view to implicitly combine different returns\nthrough eligibility traces (Watkins, 1989; Sutton & Barto,\n1998; Peng & Williams, 1996).\nThe asynchronous ad-\nvantage actor-critic method could be potentially improved\nby using other ways of estimating the advantage function,\nsuch as generalized advantage estimation of (Schulman\net al., 2015b). All of the value-based methods we inves-\ntigated could bene\ufb01t from different ways of reducing over-\nestimation bias of Q-values (Van Hasselt et al., 2015; Belle-\nmare et al., 2016). Yet another, more speculative, direction\nis to try and combine the recent work on true online tempo-\nral difference methods (van Seijen et al., 2015) with non-\nlinear function approximation.\nIn addition to these algorithmic improvements, a number\nof complementary improvements to the neural network ar-\nchitecture are possible. The dueling architecture of (Wang\net al., 2015) has been shown to produce more accurate es-\ntimates of Q-values by including separate streams for the\nstate value and advantage in the network. The spatial soft-\nmax proposed by (Levine et al., 2015) could improve both\nvalue-based and policy-based methods by making it easier\nfor the network to represent feature coordinates."
      },
      {
        "header": "Sasha Vezhnevets and Joseph Modayil for many helpful",
        "content": "discussions, suggestions and comments on the paper. We\nalso thank the DeepMind evaluation team for setting up the\nenvironments used to evaluate the agents in the paper."
      },
      {
        "header": "Beamrider",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Breakout",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Pong",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Score",
        "content": "Q*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Space Invaders",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Beamrider",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Breakout",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Pong",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Score",
        "content": "Q*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Space Invaders",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Space Invaders",
        "content": "A3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 3. Data ef\ufb01ciency comparison of different numbers of actor-learners for three asynchronous methods on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis\nshows the average score. Each curve shows the average over the three best learning rates. Single step methods show increased data\nef\ufb01ciency from more parallel workers. Results for Sarsa are shown in Supplementary Figure S9.\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000"
      },
      {
        "header": "Beamrider",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250"
      },
      {
        "header": "Breakout",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15"
      },
      {
        "header": "Pong",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500"
      },
      {
        "header": "Score",
        "content": "Q*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700"
      },
      {
        "header": "Space Invaders",
        "content": "1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000"
      },
      {
        "header": "Beamrider",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300"
      },
      {
        "header": "Breakout",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15"
      },
      {
        "header": "Pong",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000"
      },
      {
        "header": "Score",
        "content": "Q*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700"
      },
      {
        "header": "Space Invaders",
        "content": "n-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000"
      },
      {
        "header": "Beamrider",
        "content": "A3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n100\n200\n300\n400\n500"
      },
      {
        "header": "Breakout",
        "content": "A3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n30\n20\n10\n0\n10\n20"
      },
      {
        "header": "Pong",
        "content": "A3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000"
      },
      {
        "header": "Score",
        "content": "Q*bert\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400"
      },
      {
        "header": "Space Invaders",
        "content": "A3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 4. Training speed comparison of different numbers of actor-learners on \ufb01ve Atari games. The x-axis shows training time in\nhours while the y-axis shows the average score. Each curve shows the average over the three best learning rates. All asynchronous\nmethods show signi\ufb01cant speedups from using greater numbers of parallel actor-learners. Results for Sarsa are shown in Supplementary\nFigure S10."
      },
      {
        "header": "References",
        "content": "Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and\nBowling, Michael.\nThe arcade learning environment:\nAn evaluation platform for general agents. Journal of\nArti\ufb01cial Intelligence Research, 2012.\nBellemare, Marc G., Ostrovski, Georg, Guez, Arthur,\nThomas, Philip S., and Munos, R\u00e9mi. Increasing the ac-\ntion gap: New operators for reinforcement learning. In\nProceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence, 2016.\nBertsekas, Dimitri P. Distributed dynamic programming.\nAutomatic Control, IEEE Transactions on, 27(3):610\u2013\n616, 1982.\nChavez, Kevin, Ong, Hao Yi, and Hong, Augustus. Dis-\ntributed deep q-learning. Technical report, Stanford Uni-\nversity, June 2015.\nDegris, Thomas, Pilarski, Patrick M, and Sutton, Richard S.\nModel-free reinforcement learning with continuous ac-\ntion in practice. In American Control Conference (ACC),\n2012, pp. 2177\u20132182. IEEE, 2012.\nGrounds, Matthew and Kudenko, Daniel.\nParallel rein-\nforcement learning with linear function approximation.\nIn Proceedings of the 5th, 6th and 7th European Confer-\nence on Adaptive and Learning Agents and Multi-agent\nSystems: Adaptation and Multi-agent Learning, pp. 60\u2013\n74. Springer-Verlag, 2008.\nKoutn\u00edk, Jan, Schmidhuber, J\u00fcrgen, and Gomez, Faustino."
      },
      {
        "header": "Evolving deep unsupervised convolutional networks for",
        "content": "vision-based reinforcement learning. In Proceedings of\nthe 2014 conference on Genetic and evolutionary com-\nputation, pp. 541\u2013548. ACM, 2014.\nLevine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,\nPieter. End-to-end training of deep visuomotor policies.\narXiv preprint arXiv:1504.00702, 2015.\nLi, Yuxi and Schuurmans, Dale. Mapreduce for parallel re-\ninforcement learning. In Recent Advances in Reinforce-\nment Learning - 9th European Workshop, EWRL 2011,\nAthens, Greece, September 9-11, 2011, Revised Selected\nPapers, pp. 309\u2013320, 2011.\nLillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,\nHeess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,\nand Wierstra, Daan. Continuous control with deep re-\ninforcement learning. arXiv preprint arXiv:1509.02971,\n2015.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nGraves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and\nRiedmiller, Martin. Playing atari with deep reinforce-\nment learning. In NIPS Deep Learning Workshop. 2013.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nRusu, Andrei A., Veness, Joel, Bellemare, Marc G.,\nGraves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,\nOstrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,\nAmir, Antonoglou, Ioannis, King, Helen, Kumaran,\nDharshan, Wierstra, Daan, Legg, Shane, and Hassabis,\nDemis. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529\u2013533, 02 2015. URL\nhttp://dx.doi.org/10.1038/nature14236.\nNair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alci-\ncek, Cagdas, Fearon, Rory, Maria, Alessandro De, Pan-\nneershelvam, Vedavyas, Suleyman, Mustafa, Beattie,\nCharles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr,\nKavukcuoglu, Koray, and Silver, David. Massively par-\nallel methods for deep reinforcement learning. In ICML\nDeep Learning Workshop. 2015.\nPeng, Jing and Williams, Ronald J. Incremental multi-step\nq-learning. Machine Learning, 22(1-3):283\u2013290, 1996.\nRecht, Benjamin, Re, Christopher, Wright, Stephen, and\nNiu, Feng. Hogwild: A lock-free approach to paralleliz-\ning stochastic gradient descent. In Advances in Neural\nInformation Processing Systems, pp. 693\u2013701, 2011.\nRiedmiller, Martin. Neural \ufb01tted q iteration\u2013\ufb01rst experi-\nences with a data ef\ufb01cient neural reinforcement learning\nmethod. In Machine Learning: ECML 2005, pp. 317\u2013\n328. Springer Berlin Heidelberg, 2005.\nRummery, Gavin A and Niranjan, Mahesan.\nOn-line q-\nlearning using connectionist systems. 1994.\nSchaul, Tom, Quan, John, Antonoglou, Ioannis, and Sil-\nver, David. Prioritized experience replay. arXiv preprint\narXiv:1511.05952, 2015.\nSchulman, John, Levine, Sergey, Moritz, Philipp, Jordan,\nMichael I, and Abbeel, Pieter. Trust region policy op-\ntimization."
      },
      {
        "header": "In International Conference on Machine",
        "content": "Learning (ICML), 2015a.\nSchulman, John, Moritz, Philipp, Levine, Sergey, Jordan,\nMichael, and Abbeel, Pieter.\nHigh-dimensional con-\ntinuous control using generalized advantage estimation.\narXiv preprint arXiv:1506.02438, 2015b.\nSutton, R. and Barto, A. Reinforcement Learning: an In-\ntroduction. MIT Press, 1998.\nTieleman, Tijmen and Hinton, Geoffrey.\nLecture 6.5-\nrmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural Networks for\nMachine Learning, 4, 2012.\nTodorov, E. MuJoCo: Modeling, Simulation and Visual-\nization of Multi-Joint Dynamics with Contact (ed 1.0).\nRoboti Publishing, 2015."
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "Tomassini, Marco. Parallel and distributed evolutionary al-\ngorithms: A review. Technical report, 1999.\nTsitsiklis, John N. Asynchronous stochastic approxima-\ntion and q-learning. Machine Learning, 16(3):185\u2013202,\n1994.\nVan Hasselt, Hado, Guez, Arthur, and Silver, David. Deep\nreinforcement learning with double q-learning.\narXiv\npreprint arXiv:1509.06461, 2015.\nvan Seijen, H., Rupam Mahmood, A., Pilarski, P. M.,\nMachado, M. C., and Sutton, R. S."
      },
      {
        "header": "True Online",
        "content": "Temporal-Difference Learning. ArXiv e-prints, Decem-\nber 2015.\nWang, Z., de Freitas, N., and Lanctot, M. Dueling Network\nArchitectures for Deep Reinforcement Learning. ArXiv\ne-prints, November 2015.\nWatkins, Christopher John Cornish Hellaby. Learning from\ndelayed rewards. PhD thesis, University of Cambridge\nEngland, 1989.\nWilliams, R.J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning.\nMa-\nchine Learning, 8(3):229\u2013256, 1992.\nWilliams, Ronald J and Peng, Jing. Function optimization\nusing connectionist reinforcement learning algorithms.\nConnection Science, 3(3):241\u2013268, 1991.\nWymann, B., Espi\u00c3l\u2019, E., Guionneau, C., Dimitrakakis, C.,\nCoulom, R., and Sumner, A. Torcs: The open racing car\nsimulator, v1.3.5, 2013.\n\n\nSupplementary Material for \"Asynchronous Methods for Deep\nReinforcement Learning\"\nJune 17, 2016"
      },
      {
        "header": "Optimization Details",
        "content": "We investigated two different optimization algorithms with our asynchronous framework \u2013 stochastic gradient\ndescent and RMSProp. Our implementations of these algorithms do not use any locking in order to maximize\nthroughput when using a large number of threads.\nMomentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and\nwell studied (Recht et al., 2011). Let \u03b8 be the parameter vector that is shared across all threads and let \u2206\u03b8i\nbe the accumulated gradients of the loss with respect to parameters \u03b8 computed by thread number i. Each\nthread i independently applies the standard momentum SGD update mi = \u03b1mi + (1 \u2212\u03b1)\u2206\u03b8i followed by\n\u03b8 \u2190\u03b8 \u2212\u03b7mi with learning rate \u03b7, momentum \u03b1 and without any locks. Note that in this setting, each thread\nmaintains its own separate gradient and momentum vector.\nRMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature,\nit has not been extensively studied in the asynchronous optimization setting. The standard non-centered"
      },
      {
        "header": "RMSProp update is given by",
        "content": "g = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82\n(S2)\n\u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(S3)\nwhere all operations are performed elementwise. In order to apply RMSProp in the asynchronous optimiza-\ntion setting one must decide whether the moving average of elementwise squared gradients g is shared or\nper-thread. We experimented with two versions of the algorithm. In one version, which we refer to as RM-\nSProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared\nRMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing\nstatistics among threads also reduces memory requirements by using one fewer copy of the parameter vector\nper thread.\nWe compared these three asynchronous optimization algorithms in terms of their sensitivity to different learn-\ning rates and random network initializations. Figure S5 shows a comparison of the methods for two different\nreinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games\n(Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the scores for 50 experiments that\ncorrespond to 50 different random learning rates and initializations. The x-axis shows the rank of the model\nafter sorting in descending order by \ufb01nal average score and the y-axis shows the \ufb01nal average score achieved\nby the corresponding model. In this representation, the algorithm that performs better would achieve higher\nmaximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizon-\ntal, thus maximizing the area under the curve. RMSProp with shared statistics tends to be more robust than\nRMSProp with per-thread statistics, which is in turn more robust than Momentum SGD."
      },
      {
        "header": "Experimental Setup",
        "content": "The experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS\nexperiments (Figure S6) used the following setup. Each experiment used 16 actor-learner threads running\non a single machine and no GPUs. All methods performed updates after every 5 actions (tmax = 5 and\nIUpdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods\nused a shared target network that was updated every 40000 frames. The Atari experiments used the same\ninput preprocessing as (Mnih et al., 2015) and an action repeat of 4. The agents used the network architecture\nfrom (Mnih et al., 2013). The network used a convolutional layer with 16 \ufb01lters of size 8 \u00d7 8 with stride\n4, followed by a convolutional layer with with 32 \ufb01lters of size 4 \u00d7 4 with stride 2, followed by a fully\nconnected layer with 256 hidden units. All three hidden layers were followed by a recti\ufb01er nonlinearity. The\nvalue-based methods had a single linear output unit for each action representing the action-value. The model\nused by actor-critic agents had two set of outputs \u2013 a softmax output with one entry per action representing the\nprobability of selecting the action, and a single linear output representing the value function. All experiments\nused a discount of \u03b3 = 0.99 and an RMSProp decay factor of \u03b1 = 0.99.\nThe value based methods sampled the exploration rate \u03f5 from a distribution taking three values \u03f51, \u03f52, \u03f53 with\nprobabilities 0.4, 0.3, 0.3. The values of \u03f51, \u03f52, \u03f53 were annealed from 1 to 0.1, 0.01, 0.5 respectively over\nthe \ufb01rst four million frames. Advantage actor-critic used entropy regularization with a weight \u03b2 = 0.01 for\nall Atari and TORCS experiments. We performed a set of 50 experiments for \ufb01ve Atari games and every\nTORCS level, each using a different random initialization and initial learning rate. The initial learning rate\nwas sampled from a LogUniform(10\u22124, 10\u22122) distribution and annealed to 0 over the course of training.\nNote that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used\n\ufb01xed hyperparameters."
      },
      {
        "header": "Continuous Action Control Using the MuJoCo Physics Simulator",
        "content": "To apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly\nidentical to that used in the discrete action domains, so here we enumerate only the differences required for\nthe continuous action domains. The essential elements for many of the tasks (i.e. the physics models and\ntask objectives) are near identical to the tasks examined in (Lillicrap et al., 2015). However, the rewards and\nthus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco\nwhich altered the contact model.\nFor all the domains we attempted to learn the task using the physical state as input. The physical state\nconsisted of the joint positions and velocities as well as the target position if the task required a target. In\naddition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from\nRGB pixel inputs. In the low dimensional physical state case, the inputs are mapped to a hidden state using\none hidden layer with 200 ReLU units. In the cases where we used pixels, the input was passed through two\nlayers of spatial convolutions without any non-linearity or pooling. In either case, the output of the encoder\nlayers were fed to a single layer of 128 LSTM cells. The most important difference in the architecture is in the\nthe output layer of the policy network. Unlike the discrete action domain where the action output is a Softmax,\nhere the two outputs of the policy network are two real number vectors which we treat as the mean vector \u00b5\nand scalar variance \u03c32 of a multidimensional normal distribution with a spherical covariance. To act, the input\nis passed through the model to the output layer where we sample from the normal distribution determined by\n\u00b5 and \u03c32. In practice, \u00b5 is modeled by a linear layer and \u03c32 by a SoftPlus operation, log(1 + exp(x)), as the\nactivation computed as a function of the output of a linear layer. In our experiments with continuous control\nproblems the networks for policy network and value network do not share any parameters, though this detail\nis unlikely to be crucial. Finally, since the episodes were typically at most several hundred time steps long,\nwe did not use any bootstrapping in the policy or value function updates and batched each episode into a\nsingle update.\nAs in the discrete action case, we included an entropy cost which encouraged exploration. In the continuous"
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "case the we used a cost on the differential entropy of the normal distribution de\ufb01ned by the output of the\nactor network, \u22121\n2(log(2\u03c0\u03c32) + 1), we used a constant multiplier of 10\u22124 for this cost across all of the tasks\nexamined. The asynchronous advantage actor-critic algorithm \ufb01nds solutions for all the domains. Figure S8\nshows learning curves against wall-clock time, and demonstrates that most of the domains from states can be\nsolved within a few hours. All of the experiments, including those done from pixel based observations, were\nrun on CPU. Even in the case of solving the domains directly from pixel inputs we found that it was possible\nto reliably discover solutions within 24 hours. Figure S7 shows scatter plots of the top scores against the\nsampled learning rates. In most of the domains there is large range of learning rates that consistently achieve\ngood performance on the task.\nAlgorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread.\n// Assume global shared parameter vector \u03b8.\n// Assume global shared target parameter vector \u03b8\u2212.\n// Assume global shared counter T = 0.\nInitialize thread step counter t \u21901\nInitialize target network parameters \u03b8\u2212\u2190\u03b8\nInitialize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\nInitialize network gradients d\u03b8 \u21900\nrepeat\nClear gradients d\u03b8 \u21900\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\ntstart = t"
      },
      {
        "header": "Get state st",
        "content": "repeat\nTake action at according to the \u03f5-greedy policy based on Q(st, a; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a 0\nfor terminal st\nmaxa Q(st, a; \u03b8\u2212)\nfor non-terminal st\nfor i \u2208{t \u22121, . . . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 +\n\u2202(R\u2212Q(si,ai;\u03b8\u2032))\n2\n\u2202\u03b8\u2032\nend for\nPerform asynchronous update of \u03b8 using d\u03b8.\nif T\nmod Itarget == 0 then\n\u03b8\u2212\u2190\u03b8\nend if\nuntil T > Tmax"
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "Algorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.\n// Assume global shared parameter vectors \u03b8 and \u03b8v and global shared counter T = 0\n// Assume thread-speci\ufb01c parameter vectors \u03b8\u2032 and \u03b8\u2032\nv\nInitialize thread step counter t \u21901\nrepeat\nReset gradients: d\u03b8 \u21900 and d\u03b8v \u21900.\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8 and \u03b8\u2032\nv = \u03b8v\ntstart = t"
      },
      {
        "header": "Get state st",
        "content": "repeat\nPerform at according to policy \u03c0(at|st; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a\n0\nfor terminal st\nV (st, \u03b8\u2032\nv)\nfor non-terminal st// Bootstrap from last state\nfor i \u2208{t \u22121, . . . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 + \u2207\u03b8\u2032 log \u03c0(ai|si; \u03b8\u2032)(R \u2212V (si; \u03b8\u2032\nv))\nAccumulate gradients wrt \u03b8\u2032\nv: d\u03b8v \u2190d\u03b8v + \u2202(R \u2212V (si; \u03b8\u2032\nv))2/\u2202\u03b8\u2032\nv\nend for\nPerform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v.\nuntil T > Tmax"
      },
      {
        "header": "Space Invaders",
        "content": "A3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\nFigure S5. Comparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested\nusing two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Break-\nout, Beamrider, Seaquest and Space Invaders). Each curve shows the \ufb01nal scores for 50 experiments sorted in descending\norder that covers a search over 50 random initializations and learning rates. The top row shows results using Async n-step\nQ algorithm and bottom row shows results with Async Advantage Actor-Critic. Each individual graph shows results for\none of the four games and three different optimization methods. Shared RMSProp tends to be more robust to different\nlearning rates and random initializations than Momentum SGD and RMSProp without sharing.\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000"
      },
      {
        "header": "Human tester",
        "content": "Figure S6. Comparison of algorithms on the TORCS car racing simulator. Four different con\ufb01gurations of car speed and\nopponent presence or absence are shown. In each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and\nAdvantage Actor-Critic) are compared on score vs training time in wall clock hours. Multi-step algorithms achieve better\npolicies much faster than one-step algorithms on all four levels. The curves show averages over the 5 best runs from 50\nexperiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed."
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "Figure S7. Performance for the Mujoco continuous action domains. Scatter plot of the best score obtained against\nlearning rates sampled from LogUniform(10\u22125, 10\u22121). For nearly all of the tasks there is a wide range of learning\nrates that lead to good performance on the task."
      },
      {
        "header": "Asynchronous Methods for Deep Reinforcement Learning",
        "content": "Figure S8. Score per episode vs wall-clock time plots for the Mujoco domains. Each plot shows error bars for the top 5\nexperiments.\n0\n10\n20\n30"
      },
      {
        "header": "Beamrider",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Breakout",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Pong",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Score",
        "content": "Q*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30"
      },
      {
        "header": "Space Invaders",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S9. Data ef\ufb01ciency comparison of different numbers of actor-learners one-step Sarsa on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads).\nThe y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over\n50 random learning rates. Sarsa shows increased data ef\ufb01ciency with increased numbers of parallel workers."
      },
      {
        "header": "Beamrider",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300"
      },
      {
        "header": "Breakout",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15"
      },
      {
        "header": "Pong",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000"
      },
      {
        "header": "Score",
        "content": "Q*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700"
      },
      {
        "header": "Space Invaders",
        "content": "1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S10. Training speed comparison of different numbers of actor-learners for all one-step Sarsa on \ufb01ve Atari games.\nThe x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average of the\nthree best performing agents from a search over 50 random learning rates. Sarsa shows signi\ufb01cant speedups from using\ngreater numbers of parallel actor-learners.\n10-4\n10-3\n10-2"
      },
      {
        "header": "Score",
        "content": "n-step Q, Space Invaders\nFigure S11. Scatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on \ufb01ve games (Beamrider,\nBreakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. All algorithms exhibit\nsome level of robustness to the choice of learning rate."
      },
      {
        "header": "Gravitar",
        "content": "216.5\n538.4\n200.5\n297.0\n218.0\n269.5\n303.5\n320.0\nH.E.R.O.\n12952.5\n8963.4\n14892.5\n15207.9\n20506.4\n28765.8\n32464.1\n28889.5"
      },
      {
        "header": "Krull",
        "content": "3864.0\n6363.1\n6796.1\n8051.6\n7406.5\n8066.6\n5560.0\n5911.4\nKung-Fu Master\n11875.0\n20620.0\n30207.0\n24288.0\n31244.0\n3046.0\n28819.0\n40835.0\nMontezuma\u2019s Revenge\n50.0\n84.0\n42.0\n22.0\n13.0\n53.0\n67.0\n41.0\nMs. Pacman\n763.5\n1263.0\n1241.3\n2250.6\n1824.6\n594.4\n653.7\n850.7"
      },
      {
        "header": "Private Eye",
        "content": "298.2\n2598.6\n-575.5\n292.6\n179.0\n194.4\n206.9\n421.1\nQ*Bert\n4589.8\n7089.8\n11020.8\n14175.8\n11277.0\n13752.3\n15148.8\n21307.5"
      },
      {
        "header": "Zaxxon",
        "content": "831.0\n6159.4\n8593.0\n10164.0\n9501.0\n2659.0\n24622.0\n23519.0\nTable S3. Raw scores for the human start condition (30 minutes emulator time). DQN scores taken from (Nair et al.,\n2015). Double DQN scores taken from (Van Hasselt et al., 2015), Dueling scores from (Wang et al., 2015) and Prioritized\nscores taken from (Schaul et al., 2015)"
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "author": "Volodymyr Mnih1, Adri\u00e0 Puigdom\u00e8nech Badia1, Mehdi Mirza1,2, Alex Graves1, Tim Harley1, Timothy P. Lillicrap1, David Silver1, Koray Kavukcuoglu 1",
      "subject": "Proceedings of the International Conference on Machine Learning 2016",
      "keywords": "",
      "creator": "LaTeX with hyperref package",
      "producer": "pdfTeX-1.40.12",
      "creationDate": "D:20160617001528Z",
      "modDate": "D:20160617001528Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 19,
    "pages": [
      "Asynchronous Methods for Deep Reinforcement Learning\nVolodymyr Mnih1\nVMNIH@GOOGLE.COM\nAdri\u00e0 Puigdom\u00e8nech Badia1\nADRIAP@GOOGLE.COM\nMehdi Mirza1,2\nMIRZAMOM@IRO.UMONTREAL.CA\nAlex Graves1\nGRAVESA@GOOGLE.COM\nTim Harley1\nTHARLEY@GOOGLE.COM\nTimothy P. Lillicrap1\nCOUNTZERO@GOOGLE.COM\nDavid Silver1\nDAVIDSILVER@GOOGLE.COM\nKoray Kavukcuoglu 1\nKORAYK@GOOGLE.COM\n1 Google DeepMind\n2 Montreal Institute for Learning Algorithms (MILA), University of Montreal\nAbstract\nWe\npropose\na\nconceptually\nsimple\nand\nlightweight\nframework\nfor\ndeep\nreinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers.\nThe best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input.\n1. Introduction\nDeep neural networks provide rich representations that can\nenable reinforcement learning (RL) algorithms to perform\neffectively. However, it was previously thought that the\ncombination of simple online RL algorithms with deep\nneural networks was fundamentally unstable. Instead, a va-\nriety of solutions have been proposed to stabilize the algo-\nrithm (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Has-\nselt et al., 2015; Schulman et al., 2015a). These approaches\nshare a common idea: the sequence of observed data en-\ncountered by an online RL agent is non-stationary, and on-\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s).\nline RL updates are strongly correlated.\nBy storing the\nagent\u2019s data in an experience replay memory, the data can\nbe batched (Riedmiller, 2005; Schulman et al., 2015a) or\nrandomly sampled (Mnih et al., 2013; 2015; Van Hasselt\net al., 2015) from different time-steps. Aggregating over\nmemory in this way reduces non-stationarity and decorre-\nlates updates, but at the same time limits the methods to\noff-policy reinforcement learning algorithms.\nDeep RL algorithms based on experience replay have\nachieved unprecedented success in challenging domains\nsuch as Atari 2600. However, experience replay has several\ndrawbacks: it uses more memory and computation per real\ninteraction; and it requires off-policy learning algorithms\nthat can update from data generated by an older policy.\nIn this paper we provide a very different paradigm for deep\nreinforcement learning. Instead of experience replay, we\nasynchronously execute multiple agents in parallel, on mul-\ntiple instances of the environment. This parallelism also\ndecorrelates the agents\u2019 data into a more stationary process,\nsince at any given time-step the parallel agents will be ex-\nperiencing a variety of different states. This simple idea\nenables a much larger spectrum of fundamental on-policy\nRL algorithms, such as Sarsa, n-step methods, and actor-\ncritic methods, as well as off-policy RL algorithms such\nas Q-learning, to be applied robustly and effectively using\ndeep neural networks.\nOur parallel reinforcement learning paradigm also offers\npractical bene\ufb01ts. Whereas previous approaches to deep re-\ninforcement learning rely heavily on specialized hardware\nsuch as GPUs (Mnih et al., 2015; Van Hasselt et al., 2015;\nSchaul et al., 2015) or massively distributed architectures\n(Nair et al., 2015), our experiments run on a single machine\nwith a standard multi-core CPU. When applied to a vari-\nety of Atari 2600 domains, on many games asynchronous\nreinforcement learning achieves better results, in far less\narXiv:1602.01783v2  [cs.LG]  16 Jun 2016\n",
      "Asynchronous Methods for Deep Reinforcement Learning\ntime than previous GPU-based algorithms, using far less\nresource than massively distributed approaches. The best\nof the proposed methods, asynchronous advantage actor-\ncritic (A3C), also mastered a variety of continuous motor\ncontrol tasks as well as learned general strategies for ex-\nploring 3D mazes purely from visual inputs. We believe\nthat the success of A3C on both 2D and 3D games, discrete\nand continuous action spaces, as well as its ability to train\nfeedforward and recurrent agents makes it the most general\nand successful reinforcement learning agent to date.\n2. Related Work\nThe General Reinforcement Learning Architecture (Gorila)\nof (Nair et al., 2015) performs asynchronous training of re-\ninforcement learning agents in a distributed setting. In Go-\nrila, each process contains an actor that acts in its own copy\nof the environment, a separate replay memory, and a learner\nthat samples data from the replay memory and computes\ngradients of the DQN loss (Mnih et al., 2015) with respect\nto the policy parameters. The gradients are asynchronously\nsent to a central parameter server which updates a central\ncopy of the model. The updated policy parameters are sent\nto the actor-learners at \ufb01xed intervals. By using 100 sep-\narate actor-learner processes and 30 parameter server in-\nstances, a total of 130 machines, Gorila was able to signif-\nicantly outperform DQN over 49 Atari games. On many\ngames Gorila reached the score achieved by DQN over 20\ntimes faster than DQN. We also note that a similar way of\nparallelizing DQN was proposed by (Chavez et al., 2015).\nIn earlier work, (Li & Schuurmans, 2011) applied the\nMap Reduce framework to parallelizing batch reinforce-\nment learning methods with linear function approximation.\nParallelism was used to speed up large matrix operations\nbut not to parallelize the collection of experience or sta-\nbilize learning. (Grounds & Kudenko, 2008) proposed a\nparallel version of the Sarsa algorithm that uses multiple\nseparate actor-learners to accelerate training. Each actor-\nlearner learns separately and periodically sends updates to\nweights that have changed signi\ufb01cantly to the other learn-\ners using peer-to-peer communication.\n(Tsitsiklis, 1994) studied convergence properties of Q-\nlearning in the asynchronous optimization setting. These\nresults show that Q-learning is still guaranteed to converge\nwhen some of the information is outdated as long as out-\ndated information is always eventually discarded and sev-\neral other technical assumptions are satis\ufb01ed. Even earlier,\n(Bertsekas, 1982) studied the related problem of distributed\ndynamic programming.\nAnother related area of work is in evolutionary meth-\nods, which are often straightforward to parallelize by dis-\ntributing \ufb01tness evaluations over multiple machines or\nthreads (Tomassini, 1999). Such parallel evolutionary ap-\nproaches have recently been applied to some visual rein-\nforcement learning tasks. In one example, (Koutn\u00edk et al.,\n2014) evolved convolutional neural network controllers for\nthe TORCS driving simulator by performing \ufb01tness evalu-\nations on 8 CPU cores in parallel.\n3. Reinforcement Learning Background\nWe consider the standard reinforcement learning setting\nwhere an agent interacts with an environment E over a\nnumber of discrete time steps. At each time step t, the\nagent receives a state st and selects an action at from some\nset of possible actions A according to its policy \u03c0, where\n\u03c0 is a mapping from states st to actions at. In return, the\nagent receives the next state st+1 and receives a scalar re-\nward rt. The process continues until the agent reaches a\nterminal state after which the process restarts. The return\nRt = P\u221e\nk=0 \u03b3krt+k is the total accumulated return from\ntime step t with discount factor \u03b3 \u2208(0, 1]. The goal of the\nagent is to maximize the expected return from each state st.\nThe action value Q\u03c0(s, a) = E [Rt|st = s, a] is the ex-\npected return for selecting action a in state s and follow-\ning policy \u03c0.\nThe optimal value function Q\u2217(s, a) =\nmax\u03c0 Q\u03c0(s, a) gives the maximum action value for state\ns and action a achievable by any policy.\nSimilarly, the\nvalue of state s under policy \u03c0 is de\ufb01ned as V \u03c0(s) =\nE [Rt|st = s] and is simply the expected return for follow-\ning policy \u03c0 from state s.\nIn value-based model-free reinforcement learning methods,\nthe action value function is represented using a function ap-\nproximator, such as a neural network. Let Q(s, a; \u03b8) be an\napproximate action-value function with parameters \u03b8. The\nupdates to \u03b8 can be derived from a variety of reinforcement\nlearning algorithms. One example of such an algorithm is\nQ-learning, which aims to directly approximate the optimal\naction value function: Q\u2217(s, a) \u2248Q(s, a; \u03b8). In one-step\nQ-learning, the parameters \u03b8 of the action value function\nQ(s, a; \u03b8) are learned by iteratively minimizing a sequence\nof loss functions, where the ith loss function de\ufb01ned as\nLi(\u03b8i) = E\n\u0010\nr + \u03b3 max\na\u2032 Q(s\u2032, a\u2032; \u03b8i\u22121) \u2212Q(s, a; \u03b8i)\n\u00112\nwhere s\u2032 is the state encountered after state s.\nWe refer to the above method as one-step Q-learning be-\ncause it updates the action value Q(s, a) toward the one-\nstep return r + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8). One drawback of us-\ning one-step methods is that obtaining a reward r only di-\nrectly affects the value of the state action pair s, a that led\nto the reward. The values of other state action pairs are\naffected only indirectly through the updated value Q(s, a).\nThis can make the learning process slow since many up-\ndates are required the propagate a reward to the relevant\npreceding states and actions.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nOne way of propagating rewards faster is by using n-\nstep returns (Watkins, 1989; Peng & Williams, 1996).\nIn n-step Q-learning, Q(s, a) is updated toward the n-\nstep return de\ufb01ned as rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3n\u22121rt+n\u22121 +\nmaxa \u03b3nQ(st+n, a). This results in a single reward r di-\nrectly affecting the values of n preceding state action pairs.\nThis makes the process of propagating rewards to relevant\nstate-action pairs potentially much more ef\ufb01cient.\nIn contrast to value-based methods, policy-based model-\nfree methods directly parameterize the policy \u03c0(a|s; \u03b8) and\nupdate the parameters \u03b8 by performing, typically approx-\nimate, gradient ascent on E[Rt].\nOne example of such\na method is the REINFORCE family of algorithms due\nto Williams (1992). Standard REINFORCE updates the\npolicy parameters \u03b8 in the direction \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt,\nwhich is an unbiased estimate of \u2207\u03b8E[Rt]. It is possible to\nreduce the variance of this estimate while keeping it unbi-\nased by subtracting a learned function of the state bt(st),\nknown as a baseline (Williams, 1992), from the return. The\nresulting gradient is \u2207\u03b8 log \u03c0(at|st; \u03b8) (Rt \u2212bt(st)).\nA learned estimate of the value function is commonly used\nas the baseline bt(st) \u2248V \u03c0(st) leading to a much lower\nvariance estimate of the policy gradient. When an approx-\nimate value function is used as the baseline, the quantity\nRt \u2212bt used to scale the policy gradient can be seen as\nan estimate of the advantage of action at in state st, or\nA(at, st) = Q(at, st)\u2212V (st), because Rt is an estimate of\nQ\u03c0(at, st) and bt is an estimate of V \u03c0(st). This approach\ncan be viewed as an actor-critic architecture where the pol-\nicy \u03c0 is the actor and the baseline bt is the critic (Sutton &\nBarto, 1998; Degris et al., 2012).\n4. Asynchronous RL Framework\nWe now present multi-threaded asynchronous variants of\none-step Sarsa, one-step Q-learning, n-step Q-learning, and\nadvantage actor-critic. The aim in designing these methods\nwas to \ufb01nd RL algorithms that can train deep neural net-\nwork policies reliably and without large resource require-\nments. While the underlying RL methods are quite dif-\nferent, with actor-critic being an on-policy policy search\nmethod and Q-learning being an off-policy value-based\nmethod, we use two main ideas to make all four algorithms\npractical given our design goal.\nFirst, we use asynchronous actor-learners, similarly to the\nGorila framework (Nair et al., 2015), but instead of using\nseparate machines and a parameter server, we use multi-\nple CPU threads on a single machine. Keeping the learn-\ners on a single machine removes the communication costs\nof sending gradients and parameters and enables us to use\nHogwild! (Recht et al., 2011) style updates for training.\nSecond, we make the observation that multiple actors-\nAlgorithm 1 Asynchronous one-step Q-learning - pseu-\ndocode for each actor-learner thread.\n// Assume global shared \u03b8, \u03b8\u2212, and counter T = 0.\nInitialize thread step counter t \u21900\nInitialize target network weights \u03b8\u2212\u2190\u03b8\nInitialize network gradients d\u03b8 \u21900\nGet initial state s\nrepeat\nTake action a with \u03f5-greedy policy based on Q(s, a; \u03b8)\nReceive new state s\u2032 and reward r\ny =\n\u001a r\nfor terminal s\u2032\nr + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8\u2212)\nfor non-terminal s\u2032\nAccumulate gradients wrt \u03b8: d\u03b8 \u2190d\u03b8 + \u2202(y\u2212Q(s,a;\u03b8))2\n\u2202\u03b8\ns = s\u2032\nT \u2190T + 1 and t \u2190t + 1\nif T\nmod Itarget == 0 then\nUpdate the target network \u03b8\u2212\u2190\u03b8\nend if\nif t mod IAsyncUpdate == 0 or s is terminal then\nPerform asynchronous update of \u03b8 using d\u03b8.\nClear gradients d\u03b8 \u21900.\nend if\nuntil T > Tmax\nlearners running in parallel are likely to be exploring dif-\nferent parts of the environment. Moreover, one can explic-\nitly use different exploration policies in each actor-learner\nto maximize this diversity.\nBy running different explo-\nration policies in different threads, the overall changes be-\ning made to the parameters by multiple actor-learners ap-\nplying online updates in parallel are likely to be less corre-\nlated in time than a single agent applying online updates.\nHence, we do not use a replay memory and rely on parallel\nactors employing different exploration policies to perform\nthe stabilizing role undertaken by experience replay in the\nDQN training algorithm.\nIn addition to stabilizing learning, using multiple parallel\nactor-learners has multiple practical bene\ufb01ts. First, we ob-\ntain a reduction in training time that is roughly linear in\nthe number of parallel actor-learners. Second, since we no\nlonger rely on experience replay for stabilizing learning we\nare able to use on-policy reinforcement learning methods\nsuch as Sarsa and actor-critic to train neural networks in a\nstable way. We now describe our variants of one-step Q-\nlearning, one-step Sarsa, n-step Q-learning and advantage\nactor-critic.\nAsynchronous one-step Q-learning: Pseudocode for our\nvariant of Q-learning, which we call Asynchronous one-\nstep Q-learning, is shown in Algorithm 1. Each thread in-\nteracts with its own copy of the environment and at each\nstep computes a gradient of the Q-learning loss. We use\na shared and slowly changing target network in comput-\ning the Q-learning loss, as was proposed in the DQN train-\ning method. We also accumulate gradients over multiple\ntimesteps before they are applied, which is similar to us-\n",
      "Asynchronous Methods for Deep Reinforcement Learning\ning minibatches. This reduces the chances of multiple ac-\ntor learners overwriting each other\u2019s updates. Accumulat-\ning updates over several steps also provides some ability to\ntrade off computational ef\ufb01ciency for data ef\ufb01ciency.\nFinally, we found that giving each thread a different explo-\nration policy helps improve robustness. Adding diversity\nto exploration in this manner also generally improves per-\nformance through better exploration. While there are many\npossible ways of making the exploration policies differ we\nexperiment with using \u03f5-greedy exploration with \u03f5 periodi-\ncally sampled from some distribution by each thread.\nAsynchronous one-step Sarsa: The asynchronous one-\nstep Sarsa algorithm is the same as asynchronous one-step\nQ-learning as given in Algorithm 1 except that it uses a dif-\nferent target value for Q(s, a). The target value used by\none-step Sarsa is r + \u03b3Q(s\u2032, a\u2032; \u03b8\u2212) where a\u2032 is the action\ntaken in state s\u2032 (Rummery & Niranjan, 1994; Sutton &\nBarto, 1998). We again use a target network and updates\naccumulated over multiple timesteps to stabilize learning.\nAsynchronous n-step Q-learning: Pseudocode for our\nvariant of multi-step Q-learning is shown in Supplementary\nAlgorithm S2. The algorithm is somewhat unusual because\nit operates in the forward view by explicitly computing n-\nstep returns, as opposed to the more common backward\nview used by techniques like eligibility traces (Sutton &\nBarto, 1998). We found that using the forward view is eas-\nier when training neural networks with momentum-based\nmethods and backpropagation through time. In order to\ncompute a single update, the algorithm \ufb01rst selects actions\nusing its exploration policy for up to tmax steps or until a\nterminal state is reached. This process results in the agent\nreceiving up to tmax rewards from the environment since\nits last update. The algorithm then computes gradients for\nn-step Q-learning updates for each of the state-action pairs\nencountered since the last update. Each n-step update uses\nthe longest possible n-step return resulting in a one-step\nupdate for the last state, a two-step update for the second\nlast state, and so on for a total of up to tmax updates. The\naccumulated updates are applied in a single gradient step.\nAsynchronous advantage actor-critic: The algorithm,\nwhich we call asynchronous advantage actor-critic (A3C),\nmaintains a policy \u03c0(at|st; \u03b8) and an estimate of the value\nfunction V (st; \u03b8v). Like our variant of n-step Q-learning,\nour variant of actor-critic also operates in the forward view\nand uses the same mix of n-step returns to update both the\npolicy and the value-function. The policy and the value\nfunction are updated after every tmax actions or when a\nterminal state is reached. The update performed by the al-\ngorithm can be seen as \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)A(st, at; \u03b8, \u03b8v)\nwhere A(st, at; \u03b8, \u03b8v) is an estimate of the advantage func-\ntion given by Pk\u22121\ni=0 \u03b3irt+i + \u03b3kV (st+k; \u03b8v) \u2212V (st; \u03b8v),\nwhere k can vary from state to state and is upper-bounded\nby tmax. The pseudocode for the algorithm is presented in\nSupplementary Algorithm S3.\nAs with the value-based methods we rely on parallel actor-\nlearners and accumulated updates for improving training\nstability. Note that while the parameters \u03b8 of the policy\nand \u03b8v of the value function are shown as being separate\nfor generality, we always share some of the parameters in\npractice. We typically use a convolutional neural network\nthat has one softmax output for the policy \u03c0(at|st; \u03b8) and\none linear output for the value function V (st; \u03b8v), with all\nnon-output layers shared.\nWe also found that adding the entropy of the policy \u03c0 to the\nobjective function improved exploration by discouraging\npremature convergence to suboptimal deterministic poli-\ncies. This technique was originally proposed by (Williams\n& Peng, 1991), who found that it was particularly help-\nful on tasks requiring hierarchical behavior.\nThe gradi-\nent of the full objective function including the entropy\nregularization term with respect to the policy parame-\nters takes the form \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)(Rt \u2212V (st; \u03b8v)) +\n\u03b2\u2207\u03b8\u2032H(\u03c0(st; \u03b8\u2032)), where H is the entropy. The hyperpa-\nrameter \u03b2 controls the strength of the entropy regulariza-\ntion term.\nOptimization: We investigated three different optimiza-\ntion algorithms in our asynchronous framework \u2013 SGD\nwith momentum, RMSProp (Tieleman & Hinton, 2012)\nwithout shared statistics, and RMSProp with shared statis-\ntics. We used the standard non-centered RMSProp update\ngiven by\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82 and \u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(1)\nwhere all operations are performed elementwise. A com-\nparison on a subset of Atari 2600 games showed that a vari-\nant of RMSProp where statistics g are shared across threads\nis considerably more robust than the other two methods.\nFull details of the methods and comparisons are included\nin Supplementary Section 7.\n5. Experiments\nWe use four different platforms for assessing the properties\nof the proposed framework. We perform most of our exper-\niments using the Arcade Learning Environment (Bellemare\net al., 2012), which provides a simulator for Atari 2600\ngames. This is one of the most commonly used benchmark\nenvironments for RL algorithms. We use the Atari domain\nto compare against state of the art results (Van Hasselt et al.,\n2015; Wang et al., 2015; Schaul et al., 2015; Nair et al.,\n2015; Mnih et al., 2015), as well as to carry out a detailed\nstability and scalability analysis of the proposed methods.\nWe performed further comparisons using the TORCS 3D\ncar racing simulator (Wymann et al., 2013). We also use\n",
      "Asynchronous Methods for Deep Reinforcement Learning\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nBeamrider\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n100\n200\n300\n400\n500\n600\nScore\nBreakout\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n30\n20\n10\n0\n10\n20\n30\nScore\nPong\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nQ*bert\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nScore\nSpace Invaders\nDQN\n1-step Q\n1-step SARSA\nn-step Q\nA3C\nFigure 1. Learning speed comparison for DQN and the new asynchronous algorithms on \ufb01ve Atari 2600 games. DQN was trained on\na single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores. The plots are averaged over 5 runs. In\nthe case of DQN the runs were for different seeds with \ufb01xed hyperparameters. For asynchronous methods we average over the best 5\nmodels from 50 experiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.\ntwo additional domains to evaluate only the A3C algorithm\n\u2013 Mujoco and Labyrinth. MuJoCo (Todorov, 2015) is a\nphysics simulator for evaluating agents on continuous mo-\ntor control tasks with contact dynamics. Labyrinth is a new\n3D environment where the agent must learn to \ufb01nd rewards\nin randomly generated mazes from a visual input. The pre-\ncise details of our experimental setup can be found in Sup-\nplementary Section 8.\n5.1. Atari 2600 Games\nWe \ufb01rst present results on a subset of Atari 2600 games to\ndemonstrate the training speed of the new methods. Fig-\nure 1 compares the learning speed of the DQN algorithm\ntrained on an Nvidia K40 GPU with the asynchronous\nmethods trained using 16 CPU cores on \ufb01ve Atari 2600\ngames. The results show that all four asynchronous meth-\nods we presented can successfully train neural network\ncontrollers on the Atari domain. The asynchronous meth-\nods tend to learn faster than DQN, with signi\ufb01cantly faster\nlearning on some games, while training on only 16 CPU\ncores. Additionally, the results suggest that n-step methods\nlearn faster than one-step methods on some games. Over-\nall, the policy-based advantage actor-critic method signi\ufb01-\ncantly outperforms all three value-based methods.\nWe then evaluated asynchronous advantage actor-critic on\n57 Atari games. In order to compare with the state of the\nart in Atari game playing, we largely followed the train-\ning and evaluation protocol of (Van Hasselt et al., 2015).\nSpeci\ufb01cally, we tuned hyperparameters (learning rate and\namount of gradient norm clipping) using a search on six\nAtari games (Beamrider, Breakout, Pong, Q*bert, Seaquest\nand Space Invaders) and then \ufb01xed all hyperparameters for\nall 57 games. We trained both a feedforward agent with the\nsame architecture as (Mnih et al., 2015; Nair et al., 2015;\nVan Hasselt et al., 2015) as well as a recurrent agent with an\nadditional 256 LSTM cells after the \ufb01nal hidden layer. We\nadditionally used the \ufb01nal network weights for evaluation\nto make the results more comparable to the original results\nMethod\nTraining Time\nMean\nMedian\nDQN\n8 days on GPU\n121.9%\n47.5%\nGorila\n4 days, 100 machines\n215.2%\n71.3%\nD-DQN\n8 days on GPU\n332.9%\n110.9%\nDueling D-DQN\n8 days on GPU\n343.8%\n117.1%\nPrioritized DQN\n8 days on GPU\n463.6%\n127.6%\nA3C, FF\n1 day on CPU\n344.1%\n68.2%\nA3C, FF\n4 days on CPU\n496.8%\n116.6%\nA3C, LSTM\n4 days on CPU\n623.0%\n112.6%\nTable 1. Mean and median human-normalized scores on 57 Atari\ngames using the human starts evaluation metric. Supplementary\nTable SS3 shows the raw scores for all games.\nfrom (Bellemare et al., 2012). We trained our agents for\nfour days using 16 CPU cores, while the other agents were\ntrained for 8 to 10 days on Nvidia K40 GPUs. Table 1\nshows the average and median human-normalized scores\nobtained by our agents trained by asynchronous advantage\nactor-critic (A3C) as well as the current state-of-the art.\nSupplementary Table S3 shows the scores on all games.\nA3C signi\ufb01cantly improves on state-of-the-art the average\nscore over 57 games in half the training time of the other\nmethods while using only 16 CPU cores and no GPU. Fur-\nthermore, after just one day of training, A3C matches the\naverage human normalized score of Dueling Double DQN\nand almost reaches the median human normalized score of\nGorila. We note that many of the improvements that are\npresented in Double DQN (Van Hasselt et al., 2015) and\nDueling Double DQN (Wang et al., 2015) can be incorpo-\nrated to 1-step Q and n-step Q methods presented in this\nwork with similar potential improvements.\n5.2. TORCS Car Racing Simulator\nWe also compared the four asynchronous methods on\nthe TORCS 3D car racing game (Wymann et al., 2013).\nTORCS not only has more realistic graphics than Atari\n2600 games, but also requires the agent to learn the dy-\nnamics of the car it is controlling. At each step, an agent\nreceived only a visual input in the form of an RGB image\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nof the current frame as well as a reward proportional to the\nagent\u2019s velocity along the center of the track at the agent\u2019s\ncurrent position. We used the same neural network archi-\ntecture as the one used in the Atari experiments speci\ufb01ed in\nSupplementary Section 8. We performed experiments us-\ning four different settings \u2013 the agent controlling a slow car\nwith and without opponent bots, and the agent controlling a\nfast car with and without opponent bots. Full results can be\nfound in Supplementary Figure S6. A3C was the best per-\nforming agent, reaching between roughly 75% and 90% of\nthe score obtained by a human tester on all four game con-\n\ufb01gurations in about 12 hours of training. A video showing\nthe learned driving behavior of the A3C agent can be found\nat https://youtu.be/0xo1Ldx3L5Q.\n5.3. Continuous Action Control Using the MuJoCo\nPhysics Simulator\nWe also examined a set of tasks where the action space\nis continuous. In particular, we looked at a set of rigid\nbody physics domains with contact dynamics where the\ntasks include many examples of manipulation and loco-\nmotion.\nThese tasks were simulated using the Mujoco\nphysics engine. We evaluated only the asynchronous ad-\nvantage actor-critic algorithm since, unlike the value-based\nmethods, it is easily extended to continuous actions. In all\nproblems, using either the physical state or pixels as in-\nput, Asynchronous Advantage-Critic found good solutions\nin less than 24 hours of training and typically in under a few\nhours. Some successful policies learned by our agent can\nbe seen in the following video https://youtu.be/\nAjjc08-iPx8. Further details about this experiment can\nbe found in Supplementary Section 9.\n5.4. Labyrinth\nWe performed an additional set of experiments with A3C\non a new 3D environment called Labyrinth. The speci\ufb01c\ntask we considered involved the agent learning to \ufb01nd re-\nwards in randomly generated mazes. At the beginning of\neach episode the agent was placed in a new randomly gen-\nerated maze consisting of rooms and corridors. Each maze\ncontained two types of objects that the agent was rewarded\nfor \ufb01nding \u2013 apples and portals. Picking up an apple led to\na reward of 1. Entering a portal led to a reward of 10 after\nwhich the agent was respawned in a new random location in\nthe maze and all previously collected apples were regener-\nated. An episode terminated after 60 seconds after which a\nnew episode would begin. The aim of the agent is to collect\nas many points as possible in the time limit and the optimal\nstrategy involves \ufb01rst \ufb01nding the portal and then repeatedly\ngoing back to it after each respawn. This task is much more\nchallenging than the TORCS driving domain because the\nagent is faced with a new maze in each episode and must\nlearn a general strategy for exploring random mazes.\nNumber of threads\nMethod\n1\n2\n4\n8\n16\n1-step Q\n1.0\n3.0\n6.3\n13.3\n24.1\n1-step SARSA\n1.0\n2.8\n5.9\n13.1\n22.1\nn-step Q\n1.0\n2.7\n5.9\n10.7\n17.2\nA3C\n1.0\n2.1\n3.7\n6.9\n12.5\nTable 2. The average training speedup for each method and num-\nber of threads averaged over seven Atari games. To compute the\ntraining speed-up on a single game we measured the time to re-\nquired reach a \ufb01xed reference score using each method and num-\nber of threads. The speedup from using n threads on a game was\nde\ufb01ned as the time required to reach a \ufb01xed reference score using\none thread divided the time required to reach the reference score\nusing n threads. The table shows the speedups averaged over\nseven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert,\nSeaquest, and Space Invaders).\nWe trained an A3C LSTM agent on this task using only\n84 \u00d7 84 RGB images as input. The \ufb01nal average score\nof around 50 indicates that the agent learned a reason-\nable strategy for exploring random 3D maxes using only\na visual input.\nA video showing one of the agents ex-\nploring previously unseen mazes is included at https:\n//youtu.be/nMR5mjCFZCw.\n5.5. Scalability and Data Ef\ufb01ciency\nWe analyzed the effectiveness of our proposed framework\nby looking at how the training time and data ef\ufb01ciency\nchanges with the number of parallel actor-learners. When\nusing multiple workers in parallel and updating a shared\nmodel, one would expect that in an ideal case, for a given\ntask and algorithm, the number of training steps to achieve\na certain score would remain the same with varying num-\nbers of workers. Therefore, the advantage would be solely\ndue to the ability of the system to consume more data in\nthe same amount of wall clock time and possibly improved\nexploration. Table 2 shows the training speed-up achieved\nby using increasing numbers of parallel actor-learners av-\neraged over seven Atari games. These results show that all\nfour methods achieve substantial speedups from using mul-\ntiple worker threads, with 16 threads leading to at least an\norder of magnitude speedup. This con\ufb01rms that our pro-\nposed framework scales well with the number of parallel\nworkers, making ef\ufb01cient use of resources.\nSomewhat surprisingly, asynchronous one-step Q-learning\nand Sarsa algorithms exhibit superlinear speedups that\ncannot be explained by purely computational gains. We\nobserve that one-step methods (one-step Q and one-step\nSarsa) often require less data to achieve a particular score\nwhen using more parallel actor-learners. We believe this\nis due to positive effect of multiple threads to reduce the\nbias in one-step methods. These effects are shown more\nclearly in Figure 3, which shows plots of the average score\nagainst the total number of training frames for different\n",
      "Asynchronous Methods for Deep Reinforcement Learning\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nA3C, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n200\n0\n200\n400\n600\n800\n1000\nScore\nA3C, Breakout\n10-4\n10-3\n10-2\nLearning rate\n30\n20\n10\n0\n10\n20\n30\nScore\nA3C, Pong\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nA3C, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n0\n200\n400\n600\n800\n1000\n1200\n1400\nScore\nA3C, Space Invaders\nFigure 2. Scatter plots of scores obtained by asynchronous advantage actor-critic on \ufb01ve games (Beamrider, Breakout, Pong, Q*bert,\nSpace Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for\nwhich all random initializations acheive good scores. This shows that A3C is quite robust to learning rates and initial random weights.\nnumbers of actor-learners and training methods on \ufb01ve\nAtari games, and Figure 4, which shows plots of the av-\nerage score against wall-clock time.\n5.6. Robustness and Stability\nFinally, we analyzed the stability and robustness of the\nfour proposed asynchronous algorithms. For each of the\nfour algorithms we trained models on \ufb01ve games (Break-\nout, Beamrider, Pong, Q*bert, Space Invaders) using 50\ndifferent learning rates and random initializations. Figure 2\nshows scatter plots of the resulting scores for A3C, while\nSupplementary Figure S11 shows plots for the other three\nmethods. There is usually a range of learning rates for each\nmethod and game combination that leads to good scores,\nindicating that all methods are quite robust to the choice of\nlearning rate and random initialization. The fact that there\nare virtually no points with scores of 0 in regions with good\nlearning rates indicates that the methods are stable and do\nnot collapse or diverge once they are learning.\n6. Conclusions and Discussion\nWe have presented asynchronous versions of four standard\nreinforcement learning algorithms and showed that they\nare able to train neural network controllers on a variety\nof domains in a stable manner. Our results show that in\nour proposed framework stable training of neural networks\nthrough reinforcement learning is possible with both value-\nbased and policy-based methods, off-policy as well as on-\npolicy methods, and in discrete as well as continuous do-\nmains. When trained on the Atari domain using 16 CPU\ncores, the proposed asynchronous algorithms train faster\nthan DQN trained on an Nvidia K40 GPU, with A3C sur-\npassing the current state-of-the-art in half the training time.\nOne of our main \ufb01ndings is that using parallel actor-\nlearners to update a shared model had a stabilizing effect on\nthe learning process of the three value-based methods we\nconsidered. While this shows that stable online Q-learning\nis possible without experience replay, which was used for\nthis purpose in DQN, it does not mean that experience re-\nplay is not useful.\nIncorporating experience replay into\nthe asynchronous reinforcement learning framework could\nsubstantially improve the data ef\ufb01ciency of these methods\nby reusing old data. This could in turn lead to much faster\ntraining times in domains like TORCS where interacting\nwith the environment is more expensive than updating the\nmodel for the architecture we used.\nCombining other existing reinforcement learning meth-\nods or recent advances in deep reinforcement learning\nwith our asynchronous framework presents many possibil-\nities for immediate improvements to the methods we pre-\nsented. While our n-step methods operate in the forward\nview (Sutton & Barto, 1998) by using corrected n-step re-\nturns directly as targets, it has been more common to use\nthe backward view to implicitly combine different returns\nthrough eligibility traces (Watkins, 1989; Sutton & Barto,\n1998; Peng & Williams, 1996).\nThe asynchronous ad-\nvantage actor-critic method could be potentially improved\nby using other ways of estimating the advantage function,\nsuch as generalized advantage estimation of (Schulman\net al., 2015b). All of the value-based methods we inves-\ntigated could bene\ufb01t from different ways of reducing over-\nestimation bias of Q-values (Van Hasselt et al., 2015; Belle-\nmare et al., 2016). Yet another, more speculative, direction\nis to try and combine the recent work on true online tempo-\nral difference methods (van Seijen et al., 2015) with non-\nlinear function approximation.\nIn addition to these algorithmic improvements, a number\nof complementary improvements to the neural network ar-\nchitecture are possible. The dueling architecture of (Wang\net al., 2015) has been shown to produce more accurate es-\ntimates of Q-values by including separate streams for the\nstate value and advantage in the network. The spatial soft-\nmax proposed by (Levine et al., 2015) could improve both\nvalue-based and policy-based methods by making it easier\nfor the network to represent feature coordinates.\nACKNOWLEDGMENTS\nWe thank Thomas Degris, Remi Munos, Marc Lanctot,\nSasha Vezhnevets and Joseph Modayil for many helpful\ndiscussions, suggestions and comments on the paper. We\nalso thank the DeepMind evaluation team for setting up the\nenvironments used to evaluate the agents in the paper.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\nScore\nBeamrider\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nScore\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nBeamrider\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nBreakout\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n30\n20\n10\n0\n10\n20\n30\nScore\nPong\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nQ*bert\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n200\n400\n600\n800\n1000\n1200\n1400\nScore\nSpace Invaders\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 3. Data ef\ufb01ciency comparison of different numbers of actor-learners for three asynchronous methods on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis\nshows the average score. Each curve shows the average over the three best learning rates. Single step methods show increased data\nef\ufb01ciency from more parallel workers. Results for Sarsa are shown in Supplementary Figure S9.\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\nScore\nBeamrider\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300\nScore\nBreakout\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nScore\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nScore\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nBeamrider\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n100\n200\n300\n400\n500\n600\nScore\nBreakout\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n30\n20\n10\n0\n10\n20\n30\nScore\nPong\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nQ*bert\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nScore\nSpace Invaders\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 4. Training speed comparison of different numbers of actor-learners on \ufb01ve Atari games. The x-axis shows training time in\nhours while the y-axis shows the average score. Each curve shows the average over the three best learning rates. All asynchronous\nmethods show signi\ufb01cant speedups from using greater numbers of parallel actor-learners. Results for Sarsa are shown in Supplementary\nFigure S10.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nReferences\nBellemare, Marc G, Naddaf, Yavar, Veness, Joel, and\nBowling, Michael.\nThe arcade learning environment:\nAn evaluation platform for general agents. Journal of\nArti\ufb01cial Intelligence Research, 2012.\nBellemare, Marc G., Ostrovski, Georg, Guez, Arthur,\nThomas, Philip S., and Munos, R\u00e9mi. Increasing the ac-\ntion gap: New operators for reinforcement learning. In\nProceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence, 2016.\nBertsekas, Dimitri P. Distributed dynamic programming.\nAutomatic Control, IEEE Transactions on, 27(3):610\u2013\n616, 1982.\nChavez, Kevin, Ong, Hao Yi, and Hong, Augustus. Dis-\ntributed deep q-learning. Technical report, Stanford Uni-\nversity, June 2015.\nDegris, Thomas, Pilarski, Patrick M, and Sutton, Richard S.\nModel-free reinforcement learning with continuous ac-\ntion in practice. In American Control Conference (ACC),\n2012, pp. 2177\u20132182. IEEE, 2012.\nGrounds, Matthew and Kudenko, Daniel.\nParallel rein-\nforcement learning with linear function approximation.\nIn Proceedings of the 5th, 6th and 7th European Confer-\nence on Adaptive and Learning Agents and Multi-agent\nSystems: Adaptation and Multi-agent Learning, pp. 60\u2013\n74. Springer-Verlag, 2008.\nKoutn\u00edk, Jan, Schmidhuber, J\u00fcrgen, and Gomez, Faustino.\nEvolving deep unsupervised convolutional networks for\nvision-based reinforcement learning. In Proceedings of\nthe 2014 conference on Genetic and evolutionary com-\nputation, pp. 541\u2013548. ACM, 2014.\nLevine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,\nPieter. End-to-end training of deep visuomotor policies.\narXiv preprint arXiv:1504.00702, 2015.\nLi, Yuxi and Schuurmans, Dale. Mapreduce for parallel re-\ninforcement learning. In Recent Advances in Reinforce-\nment Learning - 9th European Workshop, EWRL 2011,\nAthens, Greece, September 9-11, 2011, Revised Selected\nPapers, pp. 309\u2013320, 2011.\nLillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,\nHeess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,\nand Wierstra, Daan. Continuous control with deep re-\ninforcement learning. arXiv preprint arXiv:1509.02971,\n2015.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nGraves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and\nRiedmiller, Martin. Playing atari with deep reinforce-\nment learning. In NIPS Deep Learning Workshop. 2013.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nRusu, Andrei A., Veness, Joel, Bellemare, Marc G.,\nGraves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,\nOstrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,\nAmir, Antonoglou, Ioannis, King, Helen, Kumaran,\nDharshan, Wierstra, Daan, Legg, Shane, and Hassabis,\nDemis. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529\u2013533, 02 2015. URL\nhttp://dx.doi.org/10.1038/nature14236.\nNair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alci-\ncek, Cagdas, Fearon, Rory, Maria, Alessandro De, Pan-\nneershelvam, Vedavyas, Suleyman, Mustafa, Beattie,\nCharles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr,\nKavukcuoglu, Koray, and Silver, David. Massively par-\nallel methods for deep reinforcement learning. In ICML\nDeep Learning Workshop. 2015.\nPeng, Jing and Williams, Ronald J. Incremental multi-step\nq-learning. Machine Learning, 22(1-3):283\u2013290, 1996.\nRecht, Benjamin, Re, Christopher, Wright, Stephen, and\nNiu, Feng. Hogwild: A lock-free approach to paralleliz-\ning stochastic gradient descent. In Advances in Neural\nInformation Processing Systems, pp. 693\u2013701, 2011.\nRiedmiller, Martin. Neural \ufb01tted q iteration\u2013\ufb01rst experi-\nences with a data ef\ufb01cient neural reinforcement learning\nmethod. In Machine Learning: ECML 2005, pp. 317\u2013\n328. Springer Berlin Heidelberg, 2005.\nRummery, Gavin A and Niranjan, Mahesan.\nOn-line q-\nlearning using connectionist systems. 1994.\nSchaul, Tom, Quan, John, Antonoglou, Ioannis, and Sil-\nver, David. Prioritized experience replay. arXiv preprint\narXiv:1511.05952, 2015.\nSchulman, John, Levine, Sergey, Moritz, Philipp, Jordan,\nMichael I, and Abbeel, Pieter. Trust region policy op-\ntimization.\nIn International Conference on Machine\nLearning (ICML), 2015a.\nSchulman, John, Moritz, Philipp, Levine, Sergey, Jordan,\nMichael, and Abbeel, Pieter.\nHigh-dimensional con-\ntinuous control using generalized advantage estimation.\narXiv preprint arXiv:1506.02438, 2015b.\nSutton, R. and Barto, A. Reinforcement Learning: an In-\ntroduction. MIT Press, 1998.\nTieleman, Tijmen and Hinton, Geoffrey.\nLecture 6.5-\nrmsprop: Divide the gradient by a running average of\nits recent magnitude. COURSERA: Neural Networks for\nMachine Learning, 4, 2012.\nTodorov, E. MuJoCo: Modeling, Simulation and Visual-\nization of Multi-Joint Dynamics with Contact (ed 1.0).\nRoboti Publishing, 2015.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nTomassini, Marco. Parallel and distributed evolutionary al-\ngorithms: A review. Technical report, 1999.\nTsitsiklis, John N. Asynchronous stochastic approxima-\ntion and q-learning. Machine Learning, 16(3):185\u2013202,\n1994.\nVan Hasselt, Hado, Guez, Arthur, and Silver, David. Deep\nreinforcement learning with double q-learning.\narXiv\npreprint arXiv:1509.06461, 2015.\nvan Seijen, H., Rupam Mahmood, A., Pilarski, P. M.,\nMachado, M. C., and Sutton, R. S.\nTrue Online\nTemporal-Difference Learning. ArXiv e-prints, Decem-\nber 2015.\nWang, Z., de Freitas, N., and Lanctot, M. Dueling Network\nArchitectures for Deep Reinforcement Learning. ArXiv\ne-prints, November 2015.\nWatkins, Christopher John Cornish Hellaby. Learning from\ndelayed rewards. PhD thesis, University of Cambridge\nEngland, 1989.\nWilliams, R.J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning.\nMa-\nchine Learning, 8(3):229\u2013256, 1992.\nWilliams, Ronald J and Peng, Jing. Function optimization\nusing connectionist reinforcement learning algorithms.\nConnection Science, 3(3):241\u2013268, 1991.\nWymann, B., Espi\u00c3l\u2019, E., Guionneau, C., Dimitrakakis, C.,\nCoulom, R., and Sumner, A. Torcs: The open racing car\nsimulator, v1.3.5, 2013.\n",
      "Supplementary Material for \"Asynchronous Methods for Deep\nReinforcement Learning\"\nJune 17, 2016\n7. Optimization Details\nWe investigated two different optimization algorithms with our asynchronous framework \u2013 stochastic gradient\ndescent and RMSProp. Our implementations of these algorithms do not use any locking in order to maximize\nthroughput when using a large number of threads.\nMomentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and\nwell studied (Recht et al., 2011). Let \u03b8 be the parameter vector that is shared across all threads and let \u2206\u03b8i\nbe the accumulated gradients of the loss with respect to parameters \u03b8 computed by thread number i. Each\nthread i independently applies the standard momentum SGD update mi = \u03b1mi + (1 \u2212\u03b1)\u2206\u03b8i followed by\n\u03b8 \u2190\u03b8 \u2212\u03b7mi with learning rate \u03b7, momentum \u03b1 and without any locks. Note that in this setting, each thread\nmaintains its own separate gradient and momentum vector.\nRMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature,\nit has not been extensively studied in the asynchronous optimization setting. The standard non-centered\nRMSProp update is given by\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82\n(S2)\n\u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(S3)\nwhere all operations are performed elementwise. In order to apply RMSProp in the asynchronous optimiza-\ntion setting one must decide whether the moving average of elementwise squared gradients g is shared or\nper-thread. We experimented with two versions of the algorithm. In one version, which we refer to as RM-\nSProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared\nRMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing\nstatistics among threads also reduces memory requirements by using one fewer copy of the parameter vector\nper thread.\nWe compared these three asynchronous optimization algorithms in terms of their sensitivity to different learn-\ning rates and random network initializations. Figure S5 shows a comparison of the methods for two different\nreinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games\n(Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the scores for 50 experiments that\ncorrespond to 50 different random learning rates and initializations. The x-axis shows the rank of the model\nafter sorting in descending order by \ufb01nal average score and the y-axis shows the \ufb01nal average score achieved\nby the corresponding model. In this representation, the algorithm that performs better would achieve higher\nmaximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizon-\ntal, thus maximizing the area under the curve. RMSProp with shared statistics tends to be more robust than\nRMSProp with per-thread statistics, which is in turn more robust than Momentum SGD.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\n8. Experimental Setup\nThe experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS\nexperiments (Figure S6) used the following setup. Each experiment used 16 actor-learner threads running\non a single machine and no GPUs. All methods performed updates after every 5 actions (tmax = 5 and\nIUpdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods\nused a shared target network that was updated every 40000 frames. The Atari experiments used the same\ninput preprocessing as (Mnih et al., 2015) and an action repeat of 4. The agents used the network architecture\nfrom (Mnih et al., 2013). The network used a convolutional layer with 16 \ufb01lters of size 8 \u00d7 8 with stride\n4, followed by a convolutional layer with with 32 \ufb01lters of size 4 \u00d7 4 with stride 2, followed by a fully\nconnected layer with 256 hidden units. All three hidden layers were followed by a recti\ufb01er nonlinearity. The\nvalue-based methods had a single linear output unit for each action representing the action-value. The model\nused by actor-critic agents had two set of outputs \u2013 a softmax output with one entry per action representing the\nprobability of selecting the action, and a single linear output representing the value function. All experiments\nused a discount of \u03b3 = 0.99 and an RMSProp decay factor of \u03b1 = 0.99.\nThe value based methods sampled the exploration rate \u03f5 from a distribution taking three values \u03f51, \u03f52, \u03f53 with\nprobabilities 0.4, 0.3, 0.3. The values of \u03f51, \u03f52, \u03f53 were annealed from 1 to 0.1, 0.01, 0.5 respectively over\nthe \ufb01rst four million frames. Advantage actor-critic used entropy regularization with a weight \u03b2 = 0.01 for\nall Atari and TORCS experiments. We performed a set of 50 experiments for \ufb01ve Atari games and every\nTORCS level, each using a different random initialization and initial learning rate. The initial learning rate\nwas sampled from a LogUniform(10\u22124, 10\u22122) distribution and annealed to 0 over the course of training.\nNote that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used\n\ufb01xed hyperparameters.\n9. Continuous Action Control Using the MuJoCo Physics Simulator\nTo apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly\nidentical to that used in the discrete action domains, so here we enumerate only the differences required for\nthe continuous action domains. The essential elements for many of the tasks (i.e. the physics models and\ntask objectives) are near identical to the tasks examined in (Lillicrap et al., 2015). However, the rewards and\nthus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco\nwhich altered the contact model.\nFor all the domains we attempted to learn the task using the physical state as input. The physical state\nconsisted of the joint positions and velocities as well as the target position if the task required a target. In\naddition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from\nRGB pixel inputs. In the low dimensional physical state case, the inputs are mapped to a hidden state using\none hidden layer with 200 ReLU units. In the cases where we used pixels, the input was passed through two\nlayers of spatial convolutions without any non-linearity or pooling. In either case, the output of the encoder\nlayers were fed to a single layer of 128 LSTM cells. The most important difference in the architecture is in the\nthe output layer of the policy network. Unlike the discrete action domain where the action output is a Softmax,\nhere the two outputs of the policy network are two real number vectors which we treat as the mean vector \u00b5\nand scalar variance \u03c32 of a multidimensional normal distribution with a spherical covariance. To act, the input\nis passed through the model to the output layer where we sample from the normal distribution determined by\n\u00b5 and \u03c32. In practice, \u00b5 is modeled by a linear layer and \u03c32 by a SoftPlus operation, log(1 + exp(x)), as the\nactivation computed as a function of the output of a linear layer. In our experiments with continuous control\nproblems the networks for policy network and value network do not share any parameters, though this detail\nis unlikely to be crucial. Finally, since the episodes were typically at most several hundred time steps long,\nwe did not use any bootstrapping in the policy or value function updates and batched each episode into a\nsingle update.\nAs in the discrete action case, we included an entropy cost which encouraged exploration. In the continuous\n",
      "Asynchronous Methods for Deep Reinforcement Learning\ncase the we used a cost on the differential entropy of the normal distribution de\ufb01ned by the output of the\nactor network, \u22121\n2(log(2\u03c0\u03c32) + 1), we used a constant multiplier of 10\u22124 for this cost across all of the tasks\nexamined. The asynchronous advantage actor-critic algorithm \ufb01nds solutions for all the domains. Figure S8\nshows learning curves against wall-clock time, and demonstrates that most of the domains from states can be\nsolved within a few hours. All of the experiments, including those done from pixel based observations, were\nrun on CPU. Even in the case of solving the domains directly from pixel inputs we found that it was possible\nto reliably discover solutions within 24 hours. Figure S7 shows scatter plots of the top scores against the\nsampled learning rates. In most of the domains there is large range of learning rates that consistently achieve\ngood performance on the task.\nAlgorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread.\n// Assume global shared parameter vector \u03b8.\n// Assume global shared target parameter vector \u03b8\u2212.\n// Assume global shared counter T = 0.\nInitialize thread step counter t \u21901\nInitialize target network parameters \u03b8\u2212\u2190\u03b8\nInitialize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\nInitialize network gradients d\u03b8 \u21900\nrepeat\nClear gradients d\u03b8 \u21900\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\ntstart = t\nGet state st\nrepeat\nTake action at according to the \u03f5-greedy policy based on Q(st, a; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a 0\nfor terminal st\nmaxa Q(st, a; \u03b8\u2212)\nfor non-terminal st\nfor i \u2208{t \u22121, . . . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 +\n\u2202(R\u2212Q(si,ai;\u03b8\u2032))\n2\n\u2202\u03b8\u2032\nend for\nPerform asynchronous update of \u03b8 using d\u03b8.\nif T\nmod Itarget == 0 then\n\u03b8\u2212\u2190\u03b8\nend if\nuntil T > Tmax\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nAlgorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.\n// Assume global shared parameter vectors \u03b8 and \u03b8v and global shared counter T = 0\n// Assume thread-speci\ufb01c parameter vectors \u03b8\u2032 and \u03b8\u2032\nv\nInitialize thread step counter t \u21901\nrepeat\nReset gradients: d\u03b8 \u21900 and d\u03b8v \u21900.\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8 and \u03b8\u2032\nv = \u03b8v\ntstart = t\nGet state st\nrepeat\nPerform at according to policy \u03c0(at|st; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a\n0\nfor terminal st\nV (st, \u03b8\u2032\nv)\nfor non-terminal st// Bootstrap from last state\nfor i \u2208{t \u22121, . . . , tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 + \u2207\u03b8\u2032 log \u03c0(ai|si; \u03b8\u2032)(R \u2212V (si; \u03b8\u2032\nv))\nAccumulate gradients wrt \u03b8\u2032\nv: d\u03b8v \u2190d\u03b8v + \u2202(R \u2212V (si; \u03b8\u2032\nv))2/\u2202\u03b8\u2032\nv\nend for\nPerform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v.\nuntil T > Tmax\n",
      "Asynchronous Methods for Deep Reinforcement Learning\n10\n20\n30\n40\n50\nModel Rank\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\nBreakout\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n5000\n10000\n15000\n20000\n25000\nScore\nBeamrider\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nSeaquest\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\nScore\nSpace Invaders\nn-step Q, SGD\nn-step Q, RMSProp\nn-step Q, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\nScore\nBreakout\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n5000\n10000\n15000\n20000\n25000\nScore\nBeamrider\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\nScore\nSeaquest\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\n10\n20\n30\n40\n50\nModel Rank\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nScore\nSpace Invaders\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\nFigure S5. Comparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested\nusing two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Break-\nout, Beamrider, Seaquest and Space Invaders). Each curve shows the \ufb01nal scores for 50 experiments sorted in descending\norder that covers a search over 50 random initializations and learning rates. The top row shows results using Async n-step\nQ algorithm and bottom row shows results with Async Advantage Actor-Critic. Each individual graph shows results for\none of the four games and three different optimization methods. Shared RMSProp tends to be more robust to different\nlearning rates and random initializations than Momentum SGD and RMSProp without sharing.\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\nSlow car, no bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\nSlow car, bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nFast car, no bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000\n5000\n6000\nScore\nFast car, bots\nAsync 1-step Q\nAsync SARSA\nAsync n-step Q\nAsync actor-critic\nHuman tester\nFigure S6. Comparison of algorithms on the TORCS car racing simulator. Four different con\ufb01gurations of car speed and\nopponent presence or absence are shown. In each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and\nAdvantage Actor-Critic) are compared on score vs training time in wall clock hours. Multi-step algorithms achieve better\npolicies much faster than one-step algorithms on all four levels. The curves show averages over the 5 best runs from 50\nexperiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nFigure S7. Performance for the Mujoco continuous action domains. Scatter plot of the best score obtained against\nlearning rates sampled from LogUniform(10\u22125, 10\u22121). For nearly all of the tasks there is a wide range of learning\nrates that lead to good performance on the task.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nFigure S8. Score per episode vs wall-clock time plots for the Mujoco domains. Each plot shows error bars for the top 5\nexperiments.\n0\n10\n20\n30\n40\nTraining epochs\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nScore\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30\n40\nTraining epochs\n100\n200\n300\n400\n500\n600\n700\n800\n900\nScore\nSpace Invaders\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S9. Data ef\ufb01ciency comparison of different numbers of actor-learners one-step Sarsa on \ufb01ve Atari games. The\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads).\nThe y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over\n50 random learning rates. Sarsa shows increased data ef\ufb01ciency with increased numbers of parallel workers.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\nBeamrider\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300\n350\nScore\nBreakout\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nPong\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nScore\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700\n800\nScore\nSpace Invaders\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S10. Training speed comparison of different numbers of actor-learners for all one-step Sarsa on \ufb01ve Atari games.\nThe x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average of the\nthree best performing agents from a search over 50 random learning rates. Sarsa shows signi\ufb01cant speedups from using\ngreater numbers of parallel actor-learners.\n10-4\n10-3\n10-2\nLearning rate\n0\n2000\n4000\n6000\n8000\n10000\n12000\nScore\n1-step Q, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n50\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\n1-step Q, Breakout\n10-4\n10-3\n10-2\nLearning rate\n30\n20\n10\n0\n10\n20\n30\nScore\n1-step Q, Pong\n10-4\n10-3\n10-2\nLearning rate\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\n1-step Q, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n100\n200\n300\n400\n500\n600\n700\n800\nScore\n1-step Q, Space Invaders\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nScore\n1-step SARSA, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n50\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\n1-step SARSA, Breakout\n10-4\n10-3\n10-2\nLearning rate\n25\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\n1-step SARSA, Pong\n10-4\n10-3\n10-2\nLearning rate\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\n1-step SARSA, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n100\n200\n300\n400\n500\n600\n700\n800\n900\nScore\n1-step SARSA, Space Invaders\n10-4\n10-3\n10-2\nLearning rate\n2000\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nScore\nn-step Q, Beamrider\n10-4\n10-3\n10-2\nLearning rate\n50\n0\n50\n100\n150\n200\n250\n300\n350\n400\nScore\nn-step Q, Breakout\n10-4\n10-3\n10-2\nLearning rate\n30\n20\n10\n0\n10\n20\n30\nScore\nn-step Q, Pong\n10-4\n10-3\n10-2\nLearning rate\n1000\n0\n1000\n2000\n3000\n4000\n5000\nScore\nn-step Q, Q*bert\n10-4\n10-3\n10-2\nLearning rate\n300\n400\n500\n600\n700\n800\n900\n1000\nScore\nn-step Q, Space Invaders\nFigure S11. Scatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on \ufb01ve games (Beamrider,\nBreakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. All algorithms exhibit\nsome level of robustness to the choice of learning rate.\n",
      "Asynchronous Methods for Deep Reinforcement Learning\nGame\nDQN\nGorila\nDouble\nDueling\nPrioritized\nA3C FF, 1 day\nA3C FF\nA3C LSTM\nAlien\n570.2\n813.5\n1033.4\n1486.5\n900.5\n182.1\n518.4\n945.3\nAmidar\n133.4\n189.2\n169.1\n172.7\n218.4\n283.9\n263.9\n173.0\nAssault\n3332.3\n1195.8\n6060.8\n3994.8\n7748.5\n3746.1\n5474.9\n14497.9\nAsterix\n124.5\n3324.7\n16837.0\n15840.0\n31907.5\n6723.0\n22140.5\n17244.5\nAsteroids\n697.1\n933.6\n1193.2\n2035.4\n1654.0\n3009.4\n4474.5\n5093.1\nAtlantis\n76108.0\n629166.5\n319688.0\n445360.0\n593642.0\n772392.0\n911091.0\n875822.0\nBank Heist\n176.3\n399.4\n886.0\n1129.3\n816.8\n946.0\n970.1\n932.8\nBattle Zone\n17560.0\n19938.0\n24740.0\n31320.0\n29100.0\n11340.0\n12950.0\n20760.0\nBeam Rider\n8672.4\n3822.1\n17417.2\n14591.3\n26172.7\n13235.9\n22707.9\n24622.2\nBerzerk\n1011.1\n910.6\n1165.6\n1433.4\n817.9\n862.2\nBowling\n41.2\n54.0\n69.6\n65.7\n65.8\n36.2\n35.1\n41.8\nBoxing\n25.8\n74.2\n73.5\n77.3\n68.6\n33.7\n59.8\n37.3\nBreakout\n303.9\n313.0\n368.9\n411.6\n371.6\n551.6\n681.9\n766.8\nCentipede\n3773.1\n6296.9\n3853.5\n4881.0\n3421.9\n3306.5\n3755.8\n1997.0\nChopper Comman\n3046.0\n3191.8\n3495.0\n3784.0\n6604.0\n4669.0\n7021.0\n10150.0\nCrazy Climber\n50992.0\n65451.0\n113782.0\n124566.0\n131086.0\n101624.0\n112646.0\n138518.0\nDefender\n27510.0\n33996.0\n21093.5\n36242.5\n56533.0\n233021.5\nDemon Attack\n12835.2\n14880.1\n69803.4\n56322.8\n73185.8\n84997.5\n113308.4\n115201.9\nDouble Dunk\n-21.6\n-11.3\n-0.3\n-0.8\n2.7\n0.1\n-0.1\n0.1\nEnduro\n475.6\n71.0\n1216.6\n2077.4\n1884.4\n-82.2\n-82.5\n-82.5\nFishing Derby\n-2.3\n4.6\n3.2\n-4.1\n9.2\n13.6\n18.8\n22.6\nFreeway\n25.8\n10.2\n28.8\n0.2\n27.9\n0.1\n0.1\n0.1\nFrostbite\n157.4\n426.6\n1448.1\n2332.4\n2930.2\n180.1\n190.5\n197.6\nGopher\n2731.8\n4373.0\n15253.0\n20051.4\n57783.8\n8442.8\n10022.8\n17106.8\nGravitar\n216.5\n538.4\n200.5\n297.0\n218.0\n269.5\n303.5\n320.0\nH.E.R.O.\n12952.5\n8963.4\n14892.5\n15207.9\n20506.4\n28765.8\n32464.1\n28889.5\nIce Hockey\n-3.8\n-1.7\n-2.5\n-1.3\n-1.0\n-4.7\n-2.8\n-1.7\nJames Bond\n348.5\n444.0\n573.0\n835.5\n3511.5\n351.5\n541.0\n613.0\nKangaroo\n2696.0\n1431.0\n11204.0\n10334.0\n10241.0\n106.0\n94.0\n125.0\nKrull\n3864.0\n6363.1\n6796.1\n8051.6\n7406.5\n8066.6\n5560.0\n5911.4\nKung-Fu Master\n11875.0\n20620.0\n30207.0\n24288.0\n31244.0\n3046.0\n28819.0\n40835.0\nMontezuma\u2019s Revenge\n50.0\n84.0\n42.0\n22.0\n13.0\n53.0\n67.0\n41.0\nMs. Pacman\n763.5\n1263.0\n1241.3\n2250.6\n1824.6\n594.4\n653.7\n850.7\nName This Game\n5439.9\n9238.5\n8960.3\n11185.1\n11836.1\n5614.0\n10476.1\n12093.7\nPhoenix\n12366.5\n20410.5\n27430.1\n28181.8\n52894.1\n74786.7\nPit Fall\n-186.7\n-46.9\n-14.8\n-123.0\n-78.5\n-135.7\nPong\n16.2\n16.7\n19.1\n18.8\n18.9\n11.4\n5.6\n10.7\nPrivate Eye\n298.2\n2598.6\n-575.5\n292.6\n179.0\n194.4\n206.9\n421.1\nQ*Bert\n4589.8\n7089.8\n11020.8\n14175.8\n11277.0\n13752.3\n15148.8\n21307.5\nRiver Raid\n4065.3\n5310.3\n10838.4\n16569.4\n18184.4\n10001.2\n12201.8\n6591.9\nRoad Runner\n9264.0\n43079.8\n43156.0\n58549.0\n56990.0\n31769.0\n34216.0\n73949.0\nRobotank\n58.5\n61.8\n59.1\n62.0\n55.4\n2.3\n32.8\n2.6\nSeaquest\n2793.9\n10145.9\n14498.0\n37361.6\n39096.7\n2300.2\n2355.4\n1326.1\nSkiing\n-11490.4\n-11928.0\n-10852.8\n-13700.0\n-10911.1\n-14863.8\nSolaris\n810.0\n1768.4\n2238.2\n1884.8\n1956.0\n1936.4\nSpace Invaders\n1449.7\n1183.3\n2628.7\n5993.1\n9063.0\n2214.7\n15730.5\n23846.0\nStar Gunner\n34081.0\n14919.2\n58365.0\n90804.0\n51959.0\n64393.0\n138218.0\n164766.0\nSurround\n1.9\n4.0\n-0.9\n-9.6\n-9.7\n-8.3\nTennis\n-2.3\n-0.7\n-7.8\n4.4\n-2.0\n-10.2\n-6.3\n-6.4\nTime Pilot\n5640.0\n8267.8\n6608.0\n6601.0\n7448.0\n5825.0\n12679.0\n27202.0\nTutankham\n32.4\n118.5\n92.2\n48.0\n33.6\n26.1\n156.3\n144.2\nUp and Down\n3311.3\n8747.7\n19086.9\n24759.2\n29443.7\n54525.4\n74705.7\n105728.7\nVenture\n54.0\n523.4\n21.0\n200.0\n244.0\n19.0\n23.0\n25.0\nVideo Pinball\n20228.1\n112093.4\n367823.7\n110976.2\n374886.9\n185852.6\n331628.1\n470310.5\nWizard of Wor\n246.0\n10431.0\n6201.0\n7054.0\n7451.0\n5278.0\n17244.0\n18082.0\nYars Revenge\n6270.6\n25976.5\n5965.1\n7270.8\n7157.5\n5615.5\nZaxxon\n831.0\n6159.4\n8593.0\n10164.0\n9501.0\n2659.0\n24622.0\n23519.0\nTable S3. Raw scores for the human start condition (30 minutes emulator time). DQN scores taken from (Nair et al.,\n2015). Double DQN scores taken from (Van Hasselt et al., 2015), Dueling scores from (Wang et al., 2015) and Prioritized\nscores taken from (Schaul et al., 2015)\n"
    ],
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Soft Actor-Critic:\nOff-Policy Maximum Entropy Deep Reinforcement\nLearning with a Stochastic Actor\nTuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1\nAbstract\nModel-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods. By combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds.\n1. Introduction\nModel-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.\nof these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks.\nOne cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step.\nThis quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).\nWe explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces. To that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n\n\nSoft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016). SAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods.\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG.\n2. Related Work\nOur soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration.\nWe review prior works that draw on some of these ideas in\nthis section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function.\nAs such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n\n\nSoft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al. (2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization. Fur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin.\n3. Preliminaries\nWe \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks.\n3.1. Notation\nWe address policy learning in continuous action spaces.\nWe consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A.\nThe environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition. We will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st).\n3.2. Maximum Entropy Reinforcement Learning\nStandard RL maximizes the expected sum of rewards\nP\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) =\nT\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] .\n(1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121.\nThis objective has a number of conceptual and practical\nadvantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues.\nSecond, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework.\n4. From Soft Policy Iteration to Soft\nActor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory.\n\n\nSoft Actor-Critic\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section. We\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities.\nIn the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below.\nLemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.\nThen the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e.\nProof. See Appendix B.1.\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies.\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n.\n(4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2.\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. See Appendix B.2.\nThe full soft policy iteration algorithm alternates between\nthe soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1).\nAlthough this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic.\nTheorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e.\nProof. See Appendix B.3.\n4.2. Soft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st).\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors.\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3. This quantity can be\n\n\nSoft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer. The soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001.\n(9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014\nDKL\n\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n.\n(10)\nThere are several options for minimizing J\u03c0. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.\n(2018). Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer. In practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n\n\nSoft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nDDPG\nPPO\nSQL\nTD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.\nfor all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple.\n5. Experiments\nThe goal of our experimental evaluation is to understand\nhow the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements. We have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).\nThe source code of our SAC implementation1 and videos2\nare available online.\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n\n\nSoft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance.\nThe quantitative results attained by SAC in our experiments\nalso compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.\n5.2. Ablation Study\nThe results in the previous section suggest that algorithms\nbased on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.\nStochastic vs.\ndeterministic policy.\nSoft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5).\nTo compare how the stochasticity of the policy and entropy\nmaximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise. Figure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.\nseeds. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).\nPolicy evaluation.\nSince SAC converges to stochastic\npolicies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.\nReward scale.\nSoft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n\n\nSoft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n(c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue).\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.\nTarget network update.\nIt is common to use a separate\ntarget value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies. Large \u03c4 can lead to instabilities while small\n\u03c4 can make training slower. However, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost.\n6. Conclusion\nWe present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin. Our results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work.\nAcknowledgments\nWe would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George\nTucker for his valuable feedback on an early version of this\npaper. This work was supported by Siemens and Berkeley\nDeepDrive.\n\n\nSoft Actor-Critic\nReferences\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834\u2013846, 1983.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv\u00b4ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1204\u20131212, 2009.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym.\narXiv preprint arXiv:1606.01540, 2016.\nDuan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018.\nGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.\nThe reactor: A sample-ef\ufb01cient actor-critic architecture.\narXiv preprint arXiv:1704.04651, 2017.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic. arXiv preprint arXiv:1611.02247,\n2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352\u20131361, 2017.\nHasselt, H. V. Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613\u20132621,\n2010.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y. Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944\u20132952, 2015.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015.\nLevine, S. and Koltun, V. Guided policy search. In Interna-\ntional Conference on Machine Learning (ICML), pp. 1\u20139,\n2013.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1\u201340, 2016.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, 2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772\u20132782, 2017a.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nTrust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b.\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.\nPGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016.\nPeters, J. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682\u2013\n697, 2008.\nRawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS),\n2012.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n1889\u20131897, 2015.\n\n\nSoft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning. arXiv preprint\narXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484\u2013489, Jan 2016.\nISSN 0028-0836. Article.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P. Bias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n441\u2013448, 2014.\nTodorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. 4286\u20134292. IEEE, 2008.\nToussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. 1049\u20131056. ACM, 2009.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229\u2013256, 1992.\nZiebart, B. D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp.\n1433\u20131438, 2008.\n\n\nSoft Actor-Critic\nA. Maximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014). However, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.\n(14)\nThis objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\nevery state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy.\nB. Proofs\nB.1. Lemma 1\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e.\nProof. De\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded.\nB.2. Lemma 2\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. Let \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)).\n(16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st).\n(18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n...\n\u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18.\nConvergence to Q\u03c0new follows from Lemma 1.\n\n\nSoft Actor-Critic\nB.3. Theorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e.\nProof. Let \u03c0i be the policy at iteration i. By Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217. We will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217.\nUsing the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0.\nC. Enforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a \ufb01nite\ninterval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n.\n(20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212\nD\nX\ni=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u.\n\n\nSoft Actor-Critic\nD. Hyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the\nreward scale parameter that was tuned for each environment.\nTable 1. SAC Hyperparameters\nParameter\nValue\nShared\noptimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity\nReLU\nSAC\ntarget smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2. SAC Environment Speci\ufb01c Parameters\nEnvironment\nAction Dimensions\nReward Scale\nHopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n\n\nSoft Actor-Critic\nE. Additional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nSAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.\n\n\n",
    "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) al- gorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessi- tate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off- policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning frame- work. In this framework, the actor aims to maxi- mize expected reward while also maximizing en- tropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formu- lation, our method achieves state-of-the-art per- formance on a range of continuous control bench- mark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "sections": [
      {
        "header": "Abstract",
        "content": "Model-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods. By combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds."
      },
      {
        "header": "Introduction",
        "content": "Model-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.\nof these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks.\nOne cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step.\nThis quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).\nWe explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces. To that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n\n\nSoft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016). SAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods.\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG."
      },
      {
        "header": "Related Work",
        "content": "Our soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration."
      },
      {
        "header": "We review prior works that draw on some of these ideas in",
        "content": "this section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function.\nAs such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n\n\nSoft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al. (2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization. Fur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin."
      },
      {
        "header": "Preliminaries",
        "content": "We \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks.\n3.1. Notation\nWe address policy learning in continuous action spaces.\nWe consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A.\nThe environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition. We will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st).\n3.2. Maximum Entropy Reinforcement Learning"
      },
      {
        "header": "Standard RL maximizes the expected sum of rewards",
        "content": "P\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) ="
      },
      {
        "header": "T\nX",
        "content": "t=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] .\n(1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121."
      },
      {
        "header": "This objective has a number of conceptual and practical",
        "content": "advantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues.\nSecond, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework."
      },
      {
        "header": "From Soft Policy Iteration to Soft",
        "content": "Actor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory.\n\n\nSoft Actor-Critic\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section. We\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities.\nIn the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below.\nLemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.\nThen the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e.\nProof. See Appendix B.1.\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies.\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n.\n(4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2.\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. See Appendix B.2."
      },
      {
        "header": "The full soft policy iteration algorithm alternates between",
        "content": "the soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1).\nAlthough this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic.\nTheorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e.\nProof. See Appendix B.3.\n4.2. Soft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st).\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors.\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3. This quantity can be\n\n\nSoft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer. The soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001.\n(9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014"
      },
      {
        "header": "DKL",
        "content": "\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n.\n(10)\nThere are several options for minimizing J\u03c0. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.\n(2018). Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer. In practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n\n\nSoft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)"
      },
      {
        "header": "SQL",
        "content": "TD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.\nfor all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple."
      },
      {
        "header": "The goal of our experimental evaluation is to understand",
        "content": "how the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements. We have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).\nThe source code of our SAC implementation1 and videos2\nare available online.\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n\n\nSoft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance."
      },
      {
        "header": "The quantitative results attained by SAC in our experiments",
        "content": "also compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.\n5.2. Ablation Study"
      },
      {
        "header": "The results in the previous section suggest that algorithms",
        "content": "based on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.\nStochastic vs.\ndeterministic policy.\nSoft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5)."
      },
      {
        "header": "To compare how the stochasticity of the policy and entropy",
        "content": "maximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise. Figure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.\nseeds. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).\nPolicy evaluation."
      },
      {
        "header": "Since SAC converges to stochastic",
        "content": "policies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.\nReward scale.\nSoft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n\n\nSoft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n(c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue).\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.\nTarget network update."
      },
      {
        "header": "It is common to use a separate",
        "content": "target value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies. Large \u03c4 can lead to instabilities while small\n\u03c4 can make training slower. However, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost."
      },
      {
        "header": "Conclusion",
        "content": "We present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin. Our results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work."
      },
      {
        "header": "Acknowledgments",
        "content": "We would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George"
      },
      {
        "header": "References",
        "content": "Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834\u2013846, 1983.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv\u00b4ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation."
      },
      {
        "header": "In Advances in Neural Information Processing Systems",
        "content": "(NIPS), pp. 1204\u20131212, 2009.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym.\narXiv preprint arXiv:1606.01540, 2016.\nDuan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018.\nGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.\nThe reactor: A sample-ef\ufb01cient actor-critic architecture.\narXiv preprint arXiv:1704.04651, 2017.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic. arXiv preprint arXiv:1611.02247,\n2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352\u20131361, 2017.\nHasselt, H. V. Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613\u20132621,\n2010.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y. Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944\u20132952, 2015.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015.\nLevine, S. and Koltun, V. Guided policy search. In Interna-\ntional Conference on Machine Learning (ICML), pp. 1\u20139,\n2013.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1\u201340, 2016.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, 2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772\u20132782, 2017a.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nTrust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b.\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.\nPGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016.\nPeters, J. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682\u2013\n697, 2008.\nRawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS),\n2012.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n1889\u20131897, 2015.\n\n\nSoft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning. arXiv preprint\narXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484\u2013489, Jan 2016.\nISSN 0028-0836. Article.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P. Bias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n441\u2013448, 2014.\nTodorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. 4286\u20134292. IEEE, 2008.\nToussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. 1049\u20131056. ACM, 2009.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229\u2013256, 1992.\nZiebart, B. D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp.\n1433\u20131438, 2008.\n\n\nSoft Actor-Critic\nA. Maximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014). However, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.\n(14)"
      },
      {
        "header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
        "content": "every state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy.\nB. Proofs\nB.1. Lemma 1\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e.\nProof. De\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded.\nB.2. Lemma 2\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. Let \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)).\n(16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st).\n(18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n...\n\u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18.\nConvergence to Q\u03c0new follows from Lemma 1.\n\n\nSoft Actor-Critic\nB.3. Theorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e.\nProof. Let \u03c0i be the policy at iteration i. By Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217. We will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217.\nUsing the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0.\nC. Enforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a \ufb01nite\ninterval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n.\n(20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212"
      },
      {
        "header": "D\nX",
        "content": "i=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u.\n\n\nSoft Actor-Critic\nD. Hyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the\nreward scale parameter that was tuned for each environment.\nTable 1. SAC Hyperparameters"
      },
      {
        "header": "Shared",
        "content": "optimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity"
      },
      {
        "header": "SAC",
        "content": "target smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2. SAC Environment Speci\ufb01c Parameters"
      },
      {
        "header": "Reward Scale",
        "content": "Hopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n\n\nSoft Actor-Critic\nE. Additional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)"
      },
      {
        "header": "SAC",
        "content": "SAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor."
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "author": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine",
      "subject": "Proceedings of the International Conference on Machine Learning 2018",
      "keywords": "reinforcement learning, control as inference",
      "creator": "LaTeX with hyperref package",
      "producer": "pdfTeX-1.40.17",
      "creationDate": "D:20180810003613Z",
      "modDate": "D:20180810003613Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 14,
    "pages": [
      "Soft Actor-Critic:\nOff-Policy Maximum Entropy Deep Reinforcement\nLearning with a Stochastic Actor\nTuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1\nAbstract\nModel-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods. By combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds.\n1. Introduction\nModel-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.\nof these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks.\nOne cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step.\nThis quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).\nWe explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces. To that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n",
      "Soft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016). SAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods.\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG.\n2. Related Work\nOur soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration.\nWe review prior works that draw on some of these ideas in\nthis section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function.\nAs such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n",
      "Soft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al. (2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization. Fur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin.\n3. Preliminaries\nWe \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks.\n3.1. Notation\nWe address policy learning in continuous action spaces.\nWe consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A.\nThe environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition. We will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st).\n3.2. Maximum Entropy Reinforcement Learning\nStandard RL maximizes the expected sum of rewards\nP\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) =\nT\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] .\n(1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121.\nThis objective has a number of conceptual and practical\nadvantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues.\nSecond, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework.\n4. From Soft Policy Iteration to Soft\nActor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory.\n",
      "Soft Actor-Critic\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section. We\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities.\nIn the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below.\nLemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.\nThen the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e.\nProof. See Appendix B.1.\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies.\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n.\n(4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2.\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. See Appendix B.2.\nThe full soft policy iteration algorithm alternates between\nthe soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1).\nAlthough this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic.\nTheorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e.\nProof. See Appendix B.3.\n4.2. Soft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st).\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors.\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3. This quantity can be\n",
      "Soft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer. The soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001.\n(9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014\nDKL\n\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n.\n(10)\nThere are several options for minimizing J\u03c0. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.\n(2018). Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer. In practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n",
      "Soft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nDDPG\nPPO\nSQL\nTD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.\nfor all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple.\n5. Experiments\nThe goal of our experimental evaluation is to understand\nhow the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements. We have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).\nThe source code of our SAC implementation1 and videos2\nare available online.\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n",
      "Soft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance.\nThe quantitative results attained by SAC in our experiments\nalso compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.\n5.2. Ablation Study\nThe results in the previous section suggest that algorithms\nbased on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.\nStochastic vs.\ndeterministic policy.\nSoft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5).\nTo compare how the stochasticity of the policy and entropy\nmaximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise. Figure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.\nseeds. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).\nPolicy evaluation.\nSince SAC converges to stochastic\npolicies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.\nReward scale.\nSoft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n",
      "Soft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n(c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue).\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.\nTarget network update.\nIt is common to use a separate\ntarget value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies. Large \u03c4 can lead to instabilities while small\n\u03c4 can make training slower. However, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost.\n6. Conclusion\nWe present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin. Our results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work.\nAcknowledgments\nWe would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George\nTucker for his valuable feedback on an early version of this\npaper. This work was supported by Siemens and Berkeley\nDeepDrive.\n",
      "Soft Actor-Critic\nReferences\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834\u2013846, 1983.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv\u00b4ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1204\u20131212, 2009.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym.\narXiv preprint arXiv:1606.01540, 2016.\nDuan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018.\nGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.\nThe reactor: A sample-ef\ufb01cient actor-critic architecture.\narXiv preprint arXiv:1704.04651, 2017.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic. arXiv preprint arXiv:1611.02247,\n2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352\u20131361, 2017.\nHasselt, H. V. Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613\u20132621,\n2010.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y. Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944\u20132952, 2015.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015.\nLevine, S. and Koltun, V. Guided policy search. In Interna-\ntional Conference on Machine Learning (ICML), pp. 1\u20139,\n2013.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1\u201340, 2016.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, 2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772\u20132782, 2017a.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nTrust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b.\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.\nPGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016.\nPeters, J. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682\u2013\n697, 2008.\nRawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS),\n2012.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n1889\u20131897, 2015.\n",
      "Soft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning. arXiv preprint\narXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484\u2013489, Jan 2016.\nISSN 0028-0836. Article.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P. Bias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n441\u2013448, 2014.\nTodorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. 4286\u20134292. IEEE, 2008.\nToussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. 1049\u20131056. ACM, 2009.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229\u2013256, 1992.\nZiebart, B. D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp.\n1433\u20131438, 2008.\n",
      "Soft Actor-Critic\nA. Maximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014). However, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.\n(14)\nThis objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\nevery state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy.\nB. Proofs\nB.1. Lemma 1\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e.\nProof. De\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded.\nB.2. Lemma 2\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. Let \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)).\n(16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st).\n(18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n...\n\u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18.\nConvergence to Q\u03c0new follows from Lemma 1.\n",
      "Soft Actor-Critic\nB.3. Theorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e.\nProof. Let \u03c0i be the policy at iteration i. By Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217. We will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217.\nUsing the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0.\nC. Enforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a \ufb01nite\ninterval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n.\n(20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212\nD\nX\ni=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u.\n",
      "Soft Actor-Critic\nD. Hyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the\nreward scale parameter that was tuned for each environment.\nTable 1. SAC Hyperparameters\nParameter\nValue\nShared\noptimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity\nReLU\nSAC\ntarget smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2. SAC Environment Speci\ufb01c Parameters\nEnvironment\nAction Dimensions\nReward Scale\nHopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n",
      "Soft Actor-Critic\nE. Additional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nSAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.\n"
    ],
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n\n\n1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\n\nScaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n\u221adk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n\u221adk .\n3.2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4\n\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni\n\u2208Rdmodel\u00d7dk, W K\ni\n\u2208Rdmodel\u00d7dk, W V\ni\n\u2208Rdmodel\u00d7dv\nand W O \u2208Rhdv\u00d7dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221admodel.\n5\n\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 \u00b7 d)\nO(1)\nO(1)\nRecurrent\nO(n \u00b7 d2)\nO(n)\nO(n)\nConvolutional\nO(k \u00b7 n \u00b7 d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r \u00b7 n \u00b7 d)\nO(1)\nO(n/r)\n3.5\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4\nRegularization\nWe employ three types of regularization during training:\n7\n\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 \u00b7 1020\nGNMT + RL [38]\n24.6\n39.92\n2.3 \u00b7 1019\n1.4 \u00b7 1020\nConvS2S [9]\n25.16\n40.46\n9.6 \u00b7 1018\n1.5 \u00b7 1020\nMoE [32]\n26.03\n40.56\n2.0 \u00b7 1019\n1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39]\n40.4\n8.0 \u00b7 1020\nGNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 \u00b7 1020\n1.1 \u00b7 1021\nConvS2S Ensemble [9]\n26.36\n41.29\n7.7 \u00b7 1019\n1.2 \u00b7 1021\nTransformer (base model)\n27.3\n38.1\n3.3 \u00b7 1018\nTransformer (big)\n28.4\n41.8\n2.3 \u00b7 1019\nResidual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing\nDuring training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2\nModel Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\n\u03f5ls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3\nEnglish Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7\nConclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12\n\n\nAttention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\n\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n\n\n",
    "title": "Provided proper attribution is provided, Google hereby grants permission to",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. \u2020Work performed while at Google Brain. \u2021Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1706.03762v7 [cs.CL] 2 Aug 2023 1",
    "sections": [
      {
        "header": "The dominant sequence transduction models are based on complex recurrent or",
        "content": "convolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023"
      },
      {
        "header": "Introduction",
        "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15]."
      },
      {
        "header": "Recurrent models typically factor computation along the symbol positions of the input and output",
        "content": "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output."
      },
      {
        "header": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU",
        "content": "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9]."
      },
      {
        "header": "Model Architecture",
        "content": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1"
      },
      {
        "header": "Encoder and Decoder Stacks",
        "content": "Encoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2"
      },
      {
        "header": "Attention",
        "content": "An attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\n\nScaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n\u221adk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n\u221adk .\n3.2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4\n\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )"
      },
      {
        "header": "Where the projections are parameter matrices W Q",
        "content": "i\n\u2208Rdmodel\u00d7dk, W K\ni\n\u2208Rdmodel\u00d7dk, W V\ni\n\u2208Rdmodel\u00d7dv\nand W O \u2208Rhdv\u00d7dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3"
      },
      {
        "header": "Applications of Attention in our Model",
        "content": "The Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4"
      },
      {
        "header": "Embeddings and Softmax",
        "content": "Similarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221admodel.\n5\n\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention."
      },
      {
        "header": "Positional Encoding",
        "content": "Since our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences."
      },
      {
        "header": "Training Data and Batching",
        "content": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2"
      },
      {
        "header": "Hardware and Schedule",
        "content": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3"
      },
      {
        "header": "Optimizer",
        "content": "We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4"
      },
      {
        "header": "Regularization",
        "content": "We employ three types of regularization during training:\n7\n\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost."
      },
      {
        "header": "BLEU",
        "content": "Training Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 \u00b7 1020\nGNMT + RL [38]\n24.6\n39.92\n2.3 \u00b7 1019\n1.4 \u00b7 1020\nConvS2S [9]\n25.16\n40.46\n9.6 \u00b7 1018\n1.5 \u00b7 1020\nMoE [32]\n26.03\n40.56\n2.0 \u00b7 1019\n1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39]\n40.4\n8.0 \u00b7 1020\nGNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 \u00b7 1020\n1.1 \u00b7 1021\nConvS2S Ensemble [9]\n26.36\n41.29\n7.7 \u00b7 1019\n1.2 \u00b7 1021\nTransformer (base model)\n27.3\n38.1\n3.3 \u00b7 1018\nTransformer (big)\n28.4\n41.8\n2.3 \u00b7 1019"
      },
      {
        "header": "Residual Dropout",
        "content": "We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1."
      },
      {
        "header": "Label Smoothing",
        "content": "During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
      },
      {
        "header": "Machine Translation",
        "content": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2"
      },
      {
        "header": "Model Variations",
        "content": "To evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities."
      },
      {
        "header": "BLEU",
        "content": "params\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3"
      },
      {
        "header": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English",
        "content": "constituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)"
      },
      {
        "header": "Training",
        "content": "WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences."
      },
      {
        "header": "Conclusion",
        "content": "In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor."
      },
      {
        "header": "References",
        "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013."
      },
      {
        "header": "American",
        "content": "governments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>"
      },
      {
        "header": "American",
        "content": "governments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\n\n\nInput-Input Layer5"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\n\nInput-Input Layer5"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>"
      },
      {
        "header": "Law",
        "content": "will\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15"
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "",
      "author": "",
      "subject": "",
      "keywords": "",
      "creator": "LaTeX with hyperref",
      "producer": "pdfTeX-1.40.25",
      "creationDate": "D:20240410211143Z",
      "modDate": "D:20240410211143Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 15,
    "pages": [
      "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n",
      "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n",
      "Scaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n\u221adk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n\u221adk .\n3.2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni\n\u2208Rdmodel\u00d7dk, W K\ni\n\u2208Rdmodel\u00d7dk, W V\ni\n\u2208Rdmodel\u00d7dv\nand W O \u2208Rhdv\u00d7dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221admodel.\n5\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 \u00b7 d)\nO(1)\nO(1)\nRecurrent\nO(n \u00b7 d2)\nO(n)\nO(n)\nConvolutional\nO(k \u00b7 n \u00b7 d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r \u00b7 n \u00b7 d)\nO(1)\nO(n/r)\n3.5\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4\nRegularization\nWe employ three types of regularization during training:\n7\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 \u00b7 1020\nGNMT + RL [38]\n24.6\n39.92\n2.3 \u00b7 1019\n1.4 \u00b7 1020\nConvS2S [9]\n25.16\n40.46\n9.6 \u00b7 1018\n1.5 \u00b7 1020\nMoE [32]\n26.03\n40.56\n2.0 \u00b7 1019\n1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39]\n40.4\n8.0 \u00b7 1020\nGNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 \u00b7 1020\n1.1 \u00b7 1021\nConvS2S Ensemble [9]\n26.36\n41.29\n7.7 \u00b7 1019\n1.2 \u00b7 1021\nTransformer (base model)\n27.3\n38.1\n3.3 \u00b7 1018\nTransformer (big)\n28.4\n41.8\n2.3 \u00b7 1019\nResidual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing\nDuring training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2\nModel Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\n\u03f5ls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3\nEnglish Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7\nConclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n",
      "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n",
      "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12\n",
      "Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\n",
      "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n",
      "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n"
    ],
    "pdf_path": "data/papers/1706.03762v7.pdf"
  },
  {
    "text": "Evolution Strategies as a\nScalable Alternative to Reinforcement Learning\nTim Salimans\nJonathan Ho\nXi Chen\nSzymon Sidor\nIlya Sutskever\nOpenAI\nAbstract\nWe explore the use of Evolution Strategies (ES), a class of black box optimization\nalgorithms, as an alternative to popular MDP-based RL techniques such as Q-\nlearning and Policy Gradients. Experiments on MuJoCo and Atari show that ES\nis a viable solution strategy that scales extremely well with the number of CPUs\navailable: By using a novel communication strategy based on common random\nnumbers, our ES implementation only needs to communicate scalars, making it\npossible to scale to over a thousand parallel workers. This allows us to solve 3D\nhumanoid walking in 10 minutes and obtain competitive results on most Atari\ngames after one hour of training. In addition, we highlight several advantages of\nES as a black box optimization technique: it is invariant to action frequency and\ndelayed rewards, tolerant of extremely long horizons, and does not need temporal\ndiscounting or value function approximation.\n1\nIntroduction\nDeveloping agents that can accomplish challenging tasks in complex, uncertain environments is a key\ngoal of arti\ufb01cial intelligence. Recently, the most popular paradigm for analyzing such problems has\nbeen using a class of reinforcement learning (RL) algorithms based on the Markov Decision Process\n(MDP) formalism and the concept of value functions. Successes of this approach include systems\nthat learn to play Atari from pixels [Mnih et al., 2015], perform helicopter aerobatics Ng et al. [2006],\nor play expert-level Go [Silver et al., 2016].\nAn alternative approach to solving RL problems is using black-box optimization. This approach\nis known as direct policy search [Schmidhuber and Zhao, 1998], or neuro-evolution [Risi and\nTogelius, 2015], when applied to neural networks. In this paper, we study Evolution Strategies (ES)\n[Rechenberg and Eigen, 1973], a particular set of optimization algorithms in this class. We show\nthat ES can reliably train neural network policies, in a fashion well suited to be scaled up to modern\ndistributed computer systems, for controlling robots in the MuJoCo physics simulator [Todorov et al.,\n2012] and playing Atari games with pixel inputs [Mnih et al., 2015]. Our key \ufb01ndings are as follows:\n1. We found that the use of virtual batch normalization [Salimans et al., 2016] and other\nreparameterizations of the neural network policy (section 2.2) greatly improve the reliability\nof evolution strategies. Without these methods ES proved brittle in our experiments, but with\nthese reparameterizations we achieved strong results over a wide variety of environments.\n2. We found the evolution strategies method to be highly parallelizable: by introducing a novel\ncommunication strategy based on common random numbers, we are able to achieve linear\nspeedups in run time even when using over a thousand workers. In particular, using 1,440\nworkers, we have been able to solve the MuJoCo 3D humanoid task in under 10 minutes.\n3. The data ef\ufb01ciency of evolution strategies was surprisingly good: we were able to match\nthe \ufb01nal performance of A3C [Mnih et al., 2016] on most Atari environments while using\nbetween 3x and 10x as much data. The slight decrease in data ef\ufb01ciency is partly offset by a\narXiv:1703.03864v2  [stat.ML]  7 Sep 2017\n\n\nreduction in required computation of roughly 3x due to not performing backpropagation\nand not having a value function. Our 1-hour ES results require about the same amount of\ncomputation as the published 1-day results for A3C, while performing better on 23 games\ntested, and worse on 28. On MuJoCo tasks, we were able to match the learned policy\nperformance of Trust Region Policy Optimization [TRPO; Schulman et al., 2015], using no\nmore than 10x as much data.\n4. We found that ES exhibited better exploration behaviour than policy gradient methods like\nTRPO: on the MuJoCo humanoid task, ES has been able to learn a very wide variety of gaits\n(such as walking sideways or walking backwards). These unusual gaits are never observed\nwith TRPO, which suggests a qualitatively different exploration behavior.\n5. We found the evolution strategies method to be robust: we achieved the aforementioned\nresults using \ufb01xed hyperparameters for all the Atari environments, and a different set of\n\ufb01xed hyperparameters for all MuJoCo environments (with the exception of one binary hyper-\nparameter, which has not been held constant between the different MuJoCo environments).\nBlack-box optimization methods have several highly attractive properties: indifference to the distribu-\ntion of rewards (sparse or dense), no need for backpropagating gradients, and tolerance of potentially\narbitrarily long time horizons. However, they are perceived as less effective at solving hard RL\nproblems compared to techniques like Q-learning and policy gradients. The contribution of this work,\nwhich we hope will renew interest in this class of methods and lead to new useful applications, is\na demonstration that evolution strategies can be competitive with competing RL algorithms on the\nhardest environments studied by the deep RL community today, and that this approach can scale to\nmany more parallel workers.\n2\nEvolution Strategies\nEvolution Strategies (ES) is a class of black box optimization algorithms [Rechenberg and Eigen,\n1973, Schwefel, 1977] that are heuristic search procedures inspired by natural evolution: At every\niteration (\u201cgeneration\u201d), a population of parameter vectors (\u201cgenotypes\u201d) is perturbed (\u201cmutated\u201d)\nand their objective function value (\u201c\ufb01tness\u201d) is evaluated. The highest scoring parameter vectors are\nthen recombined to form the population for the next generation, and this procedure is iterated until the\nobjective is fully optimized. Algorithms in this class differ in how they represent the population and\nhow they perform mutation and recombination. The most widely known member of the ES class is\nthe covariance matrix adaptation evolution strategy [CMA-ES; Hansen and Ostermeier, 2001], which\nrepresents the population by a full-covariance multivariate Gaussian. CMA-ES has been extremely\nsuccessful in solving optimization problems in low to medium dimension.\nThe version of ES we use in this work belongs to the class of natural evolution strategies (NES)\n[Wierstra et al., 2008, 2014, Yi et al., 2009, Sun et al., 2009, Glasmachers et al., 2010a,b, Schaul et al.,\n2011] and is closely related to the work of Sehnke et al. [2010]. Let F denote the objective function\nacting on parameters \u03b8. NES algorithms represent the population with a distribution over parameters\np\u03c8(\u03b8)\u2014itself parameterized by \u03c8\u2014and proceed to maximize the average objective value E\u03b8\u223cp\u03c8F(\u03b8)\nover the population by searching for \u03c8 with stochastic gradient ascent. Speci\ufb01cally, using the score\nfunction estimator for \u2207\u03c8E\u03b8\u223cp\u03c8F(\u03b8) in a fashion similar to REINFORCE [Williams, 1992], NES\nalgorithms take gradient steps on \u03c8 with the following estimator:\n\u2207\u03c8E\u03b8\u223cp\u03c8F(\u03b8) = E\u03b8\u223cp\u03c8 {F(\u03b8)\u2207\u03c8 log p\u03c8(\u03b8)}\nFor the special case where p\u03c8 is factored Gaussian (as in this work), the resulting gradient estimator\nis also known as simultaneous perturbation stochastic approximation [Spall, 1992], parameter-\nexploring policy gradients [Sehnke et al., 2010], or zero-order gradient estimation [Nesterov and\nSpokoiny, 2011].\nIn this work, we focus on RL problems, so F(\u00b7) will be the stochastic return provided by an\nenvironment, and \u03b8 will be the parameters of a deterministic or stochastic policy \u03c0\u03b8 describing an\nagent acting in that environment, controlled by either discrete or continuous actions. Much of the\ninnovation in RL algorithms is focused on coping with the lack of access to or existence of derivatives\nof the environment or policy. Such non-smoothness can be addressed with ES as follows. We\ninstantiate the population distribution p\u03c8 as an isotropic multivariate Gaussian with mean \u03c8 and \ufb01xed\ncovariance \u03c32I, allowing us to write E\u03b8\u223cp\u03c8F(\u03b8) in terms of a mean parameter vector \u03b8 directly: we\n2\n\n\nset E\u03b8\u223cp\u03c8F(\u03b8) = E\u03f5\u223cN(0,I) F(\u03b8 + \u03c3\u03f5). With this setup, our stochastic objective can be viewed as\na Gaussian-blurred version of the original objective F, free of non-smoothness introduced by the\nenvironment or potentially discrete actions taken by the policy. Further discussion on how ES and\npolicy gradient methods cope with non-smoothness can be found in section 3.\nWith our objective de\ufb01ned in terms of \u03b8, we optimize over \u03b8 directly using stochastic gradient ascent\nwith the score function estimator:\n\u2207\u03b8 E\u03f5\u223cN(0,I) F(\u03b8 + \u03c3\u03f5) = 1\n\u03c3 E\u03f5\u223cN(0,I) {F(\u03b8 + \u03c3\u03f5) \u03f5}\nwhich can be approximated with samples. The resulting algorithm (1) repeatedly executes two phases:\n1) Stochastically perturbing the parameters of the policy and evaluating the resulting parameters by\nrunning an episode in the environment, and 2) Combining the results of these episodes, calculating a\nstochastic gradient estimate, and updating the parameters.\nAlgorithm 1 Evolution Strategies\n1: Input: Learning rate \u03b1, noise standard deviation \u03c3, initial policy parameters \u03b80\n2: for t = 0, 1, 2, . . . do\n3:\nSample \u03f51, . . . \u03f5n \u223cN(0, I)\n4:\nCompute returns Fi = F(\u03b8t + \u03c3\u03f5i) for i = 1, . . . , n\n5:\nSet \u03b8t+1 \u2190\u03b8t + \u03b1 1\nn\u03c3\nPn\ni=1 Fi\u03f5i\n6: end for\n2.1\nScaling and parallelizing ES\nES is well suited to be scaled up to many parallel workers: 1) It operates on complete episodes, thereby\nrequiring only infrequent communication between workers. 2) The only information obtained by each\nworker is the scalar return of an episode: if we synchronize random seeds between workers before\noptimization, each worker knows what perturbations the other workers used, so each worker only\nneeds to communicate a single scalar to and from each other worker to agree on a parameter update.\nES thus requires extremely low bandwidth, in sharp contrast to policy gradient methods, which\nrequire workers to communicate entire gradients. 3) It does not require value function approximations.\nRL with value function estimation is inherently sequential: To improve upon a given policy, multiple\nupdates to the value function are typically needed to get enough signal. Each time the policy is\nsigni\ufb01cantly changed, multiple iterations are necessary for the value function estimate to catch up.\nA simple parallel version of ES is given in Algorithm 2. The main novelty here is that the algo-\nrithm makes use of shared random seeds, which drastically reduces the bandwidth required for\ncommunication between the workers.\nAlgorithm 2 Parallelized Evolution Strategies\n1: Input: Learning rate \u03b1, noise standard deviation \u03c3, initial policy parameters \u03b80\n2: Initialize: n workers with known random seeds, and initial parameters \u03b80\n3: for t = 0, 1, 2, . . . do\n4:\nfor each worker i = 1, . . . , n do\n5:\nSample \u03f5i \u223cN(0, I)\n6:\nCompute returns Fi = F(\u03b8t + \u03c3\u03f5i)\n7:\nend for\n8:\nSend all scalar returns Fi from each worker to every other worker\n9:\nfor each worker i = 1, . . . , n do\n10:\nReconstruct all perturbations \u03f5j for j = 1, . . . , n using known random seeds\n11:\nSet \u03b8t+1 \u2190\u03b8t + \u03b1 1\nn\u03c3\nPn\nj=1 Fj\u03f5j\n12:\nend for\n13: end for\nIn practice, we implement sampling by having each worker instantiate a large block of Gaussian\nnoise at the start of training, and then perturbing its parameters by adding a randomly indexed subset\nof these noise variables at each iteration. Although this means that the perturbations are not strictly\n3\n\n\nindependent across iterations, we did not \ufb01nd this to be a problem in practice. Using this strategy,\nwe \ufb01nd that the second part of Algorithm 2 (lines 9-12) only takes up a small fraction of total time\nspend for all our experiments, even when using up to 1,440 parallel workers. When using many more\nworkers still, or when using very large neural networks, we can reduce the computation required for\nthis part of the algorithm by having workers only perturb a subset of the parameters \u03b8 rather than\nall of them: In this case the perturbation distribution p\u03c8 corresponds to a mixture of Gaussians, for\nwhich the update equations remain unchanged. At the very extreme, every worker would perturb\nonly a single coordinate of the parameter vector, which means that we would be using pure \ufb01nite\ndifferences.\nTo reduce variance, we use antithetic sampling Geweke [1988], also known as mirrored sampling\nBrockhoff et al. [2010] in the ES literature: that is, we always evaluate pairs of perturbations \u03f5, \u2212\u03f5,\nfor Gaussian noise vector \u03f5. We also \ufb01nd it useful to perform \ufb01tness shaping Wierstra et al. [2014] by\napplying a rank transformation to the returns before computing each parameter update. Doing so\nremoves the in\ufb02uence of outlier individuals in each population and decreases the tendency for ES to\nfall into local optima early in training. In addition, we apply weight decay to the parameters of our\npolicy network: this prevents the parameters from growing very large compared to the perturbations.\nUnlike Wierstra et al. [2014] we did not see bene\ufb01t from adapting \u03c3 during training, and we therefore\ntreat it as a \ufb01xed hyperparameter instead. We perform the optimization directly in parameter space;\nexploring indirect encodings Stanley et al. [2009], van Steenkiste et al. [2016] is left for future work.\nEvolution Strategies, as presented above, works with full-length episodes. In some rare cases this\ncan lead to low CPU utilization, as some episodes run for many more steps than others. For this\nreason, we cap episode length at a constant m steps for all workers, which we dynamically adjust as\ntraining progresses. For example, by setting m to be equal to twice the mean number of steps taken\nper episode, we can guarantee that CPU utilization stays above 50% in the worst case.\n2.2\nThe impact of network parameterization\nWhereas RL algorithms like Q-learning and policy gradients explore by sampling actions from a\nstochastic policy, Evolution Strategies derives learning signal from sampling instantiations of policy\nparameters. Exploration in ES is thus driven by parameter perturbation. For ES to improve upon\nparameters \u03b8, some members of the population must achieve better return than others: i.e. it is crucial\nthat Gaussian perturbation vectors \u03f5 occasionally lead to new individuals \u03b8 + \u03c3\u03f5 with better return.\nFor the Atari environments, we found that Gaussian parameter perturbations on DeepMind\u2019s con-\nvolutional architectures [Mnih et al., 2015] did not always lead to adequate exploration: For some\nenvironments, randomly perturbed parameters tended to encode policies that always took one speci\ufb01c\naction regardless of the state that was given as input. However, we discovered thatwe could match the\nperformance of policy gradient methods for most games by using virtual batch normalization [Sali-\nmans et al., 2016] in the policy speci\ufb01cation. Virtual batch normalization is precisely equivalent to\nbatch normalization [Ioffe and Szegedy, 2015] where the minibatch used for calculating normalizing\nstatistics is chosen at the start of training and is \ufb01xed. This change in parameterization makes the\npolicy more sensitive to very small changes in the input image at the early stages of training when the\nweights of the policy are random, ensuring that the policy takes a wide-enough variety of actions\nto gather occasional rewards. For most applications, a downside of virtual batch normalization is\nthat it makes training more expensive. For our application, however, the minibatch used to calculate\nthe normalizing statistics is much smaller than the number of steps taken during a typical episode,\nmeaning that the overhead is negligible.\nFor the MuJoCo tasks, we achieved good performance on nearly all the environments with the\nstandard multilayer perceptrons mapping to continuous actions. However, we observed that for some\nenvironments, we could encourage more exploration by discretizing the actions. This forced the\nactions to be non-smooth with respect to input observations and parameter perturbations, and thereby\nencouraged a wide variety of behaviors to be played out over the course of rollouts.\n3\nSmoothing in parameter space versus smoothing in action space\nAs mentioned in section 2, a large source of dif\ufb01culty in RL stems from the lack of informative\ngradients of policy performance: such gradients may not exist due to non-smoothness of the environ-\n4\n\n\nment or policy, or may only be available as high-variance estimates because the environment usually\ncan only be accessed via sampling. Explicitly, suppose we wish to solve general decision problems\nthat give a return R(a) after we take a sequence of actions a = {a1, . . . , aT }, where the actions are\ndetermined by a either a deterministic or a stochastic policy function at = \u03c0(s; \u03b8). The objective we\nwould like to optimize is thus\nF(\u03b8) = R(a(\u03b8)).\nSince the actions are allowed to be discrete and the policy is allowed to be deterministic, F(\u03b8)\ncan be non-smooth in \u03b8. More importantly, because we do not have explicit access to the under-\nlying state transition function of our decision problems, the gradients cannot be computed with\na backpropagation-like algorithm. This means we cannot directly use standard gradient-based\noptimization methods to \ufb01nd a good solution for \u03b8.\nIn order to both make the problem smooth and to have a means of to estimate its gradients, we need\nto add noise. Policy gradient methods add the noise in action space, which is done by sampling the\nactions from an appropriate distribution. For example, if the actions are discrete and \u03c0(s; \u03b8) calculates\na score for each action before selecting the best one, then we would sample an action a(\u03f5, \u03b8) (here \u03f5 is\nthe noise source) from a categorical distribution over actions at each time period, applying a softmax\nto the scores of each action. Doing so yields the objective FP G(\u03b8) = E\u03f5 R(a(\u03f5, \u03b8)), with gradients\n\u2207\u03b8FP G(\u03b8) = E\u03f5 {R(a(\u03f5, \u03b8))\u2207\u03b8 log p(a(\u03f5, \u03b8); \u03b8)} .\nEvolution strategies, on the other hand, add the noise in parameter space. That is, they perturb the\nparameters as \u02dc\u03b8 = \u03b8 + \u03be, with \u03be from a multivariate Gaussian distribution, and then pick actions\nas at = a(\u03be, \u03b8) = \u03c0(s; \u02dc\u03b8). It can be interpreted as adding a Gaussian blur to the original objective,\nwhich results in a smooth, differentiable cost FES(\u03b8) = E\u03be R(a(\u03be, \u03b8)), this time with gradients\n\u2207\u03b8FES(\u03b8) = E\u03be\nn\nR(a(\u03be, \u03b8))\u2207\u03b8 log p(\u02dc\u03b8(\u03be, \u03b8); \u03b8)\no\n.\nThe two methods for smoothing the decision problem are thus quite similar, and can be made even\nmore so by adding noise to both the parameters and the actions.\n3.1\nWhen is ES better than policy gradients?\nGiven these two methods of smoothing the decision problem, which should we use? The answer\ndepends strongly on the structure of the decision problem and on which type of Monte Carlo\nestimator is used to estimate the gradients \u2207\u03b8FP G(\u03b8) and \u2207\u03b8FES(\u03b8). Suppose the correlation\nbetween the return and the individual actions is low (as is true for any hard RL problem). Assuming\nwe approximate these gradients using simple Monte Carlo (REINFORCE) with a good baseline on\nthe return, we have\nVar[\u2207\u03b8FP G(\u03b8)] \u2248Var[R(a)] Var[\u2207\u03b8 log p(a; \u03b8)],\nVar[\u2207\u03b8FES(\u03b8)] \u2248Var[R(a)] Var[\u2207\u03b8 log p(\u02dc\u03b8; \u03b8)].\nIf both methods perform a similar amount of exploration, Var[R(a)] will be similar for both ex-\npressions. The difference will thus be in the second term. Here we have that \u2207\u03b8 log p(a; \u03b8) =\nPT\nt=1 \u2207\u03b8 log p(at; \u03b8) is a sum of T uncorrelated terms, so that the variance of the policy gradi-\nent estimator will grow nearly linearly with T. The corresponding term for evolution strategies,\n\u2207\u03b8 log p(\u02dc\u03b8; \u03b8), is independent of T. Evolution strategies will thus have an advantage compared to\npolicy gradients for long episodes with very many time steps. In practice, the effective number of\nsteps T is often reduced in policy gradient methods by discounting rewards. If the effects of actions\nare short-lasting, this allows us to dramatically reduce the variance in our gradient estimate, and\nthis has been critical to the success of applications such as Atari games. However, this discounting\nwill bias our gradient estimate if actions have long lasting effects. Another strategy for reducing the\neffective value of T is to use value function approximation. This has also been effective, but once\nagain runs the risk of biasing our gradient estimates. Evolution strategies is thus an attractive choice\nif the effective number of time steps T is long, actions have long-lasting effects, and if no good value\nfunction estimates are available.\n5\n\n\n3.2\nProblem dimensionality\nThe gradient estimate of ES can be interpreted as a method for randomized \ufb01nite differences in\nhigh-dimensional space. Indeed, using the fact that E\u03f5\u223cN(0,I) {F(\u03b8) \u03f5/\u03c3} = 0, we get\n\u2207\u03b8\u03b7(\u03b8) = E\u03f5\u223cN(0,I) {F(\u03b8 + \u03c3\u03f5) \u03f5/\u03c3} = E\u03f5\u223cN(0,I) {(F(\u03b8 + \u03c3\u03f5) \u2212F(\u03b8)) \u03f5/\u03c3}\nIt is now apparent that ES can be seen as computing a \ufb01nite difference derivative estimate in\na randomly chosen direction, especially as \u03c3 becomes small. The resemblance of ES to \ufb01nite\ndifferences suggests the method will scale poorly with the dimension of the parameters \u03b8. Theoretical\nanalysis indeed shows that for general non-smooth optimization problems, the required number of\noptimization steps scales linearly with the dimension [Nesterov and Spokoiny, 2011]. However, it\nis important to note that this does not mean that larger neural networks will perform worse than\nsmaller networks when optimized using ES: what matters is the dif\ufb01culty, or intrinsic dimension, of\nthe optimization problem. To see that the dimensionality of our model can be completely separate\nfrom the effective dimension of the optimization problem, consider a regression problem where we\napproximate a univariate variable y with a linear model \u02c6y = x \u00b7 w: if we double the number of\nfeatures and parameters in this model by concatenating x with itself (i.e. using features x\u2032 = (x, x)),\nthe problem does not become more dif\ufb01cult. The ES algorithm will do exactly the same thing when\napplied to this higher dimensional problem, as long as we divide the standard deviation of the noise\nby two, as well as the learning rate.\nIn practice, we observe slightly better results when using larger networks with ES. For example, we\ntried both the larger network and smaller network used in A3C [Mnih et al., 2016] for learning Atari\n2600 games, and on average obtained better results using the larger network. We hypothesize that this\nis due to the same effect that makes standard gradient-based optimization of large neural networks\neasier than for small ones: large networks have fewer local minima [Kawaguchi, 2016].\n3.3\nAdvantages of not calculating gradients\nIn addition to being easy to parallelize, and to having an advantage in cases with long action sequences\nand delayed rewards, black box optimization algorithms like ES have other advantages over RL\ntechniques that calculate gradients. The communication overhead of implementing ES in a distributed\nsetting is lower than for competing RL methods such as policy gradients and Q-learning, as the only\ninformation that needs to be communicated across processes are the scalar return and the random\nseed that was used to generate the perturbations \u03f5, rather than a full gradient. Also, ES can deal with\nmaximally sparse and delayed rewards; only the total return of an episode is used, whereas other\nmethods use individual rewards and their exact timing.\nBy not requiring backpropagation, black box optimizers reduce the amount of computation per\nepisode by about two thirds, and memory by potentially much more. In addition, not explicitly\ncalculating an analytical gradient protects against problems with exploding gradients that are common\nwhen working with recurrent neural networks. By smoothing the cost function in parameter space, we\nreduce the pathological curvature that causes these problems: bounded cost functions that are smooth\nenough can\u2019t have exploding gradients. At the extreme, ES allows us to incorporate non-differentiable\nelements into our architecture, such as modules that use hard attention [Xu et al., 2015].\nBlack box optimization methods are uniquely suited to low precision hardware for deep learning.\nLow precision arithmetic, such as in binary neural networks, can be performed much cheaper than\nat high precision. When optimizing such low precision architectures, biased low precision gradient\nestimates can be a problem when using gradient-based methods. Similarly, specialized hardware for\nneural network inference, such as TPUs [Jouppi et al., 2017], can be used directly when performing\noptimization using ES, while their limited memory usually makes backpropagation impossible.\nBy perturbing in parameter space instead of action space, black box optimizers are naturally invariant\nto the frequency at which our agent acts in the environment. For MDP-based reinforcement learning\nalgorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for\nthe optimization to succeed [Braylan et al., 2005]. While this is usually a solvable problem for games\nthat only require short-term planning and action, it is a problem for learning longer term strategic\nbehavior. For these problems, RL needs hierarchy to succeed [Parr and Russell, 1998], which is not\nas necessary when using black box optimization.\n6\n\n\n4\nExperiments\n4.1\nMuJoCo\nWe evaluated ES on a benchmark of continuous robotic control problems in the OpenAI Gym\n[Brockman et al., 2016] against a highly tuned implementation of Trust Region Policy Optimiza-\ntion [Schulman et al., 2015], a policy gradient algorithm designed to ef\ufb01ciently optimize neural\nnetwork policies. We tested on both classic problems, like balancing an inverted pendulum, and more\ndif\ufb01cult ones found in recent literature, like learning 2D hopping and walking gaits. The environments\nwere simulated by MuJoCo [Todorov et al., 2012].\nWe used both ES and TRPO to train policies with identical architectures: multilayer perceptrons with\ntwo 64-unit hidden layers separated by tanh nonlinearities. We found that ES occasionally bene\ufb01ted\nfrom discrete actions, since continuous actions could be too smooth with respect to parameter\nperturbation and could hamper exploration (see section 2.2). For the hopping and swimming tasks,\nwe discretized the actions for ES into 10 bins for each action component.\nWe found that ES was able to solve these tasks up to TRPO\u2019s \ufb01nal performance after 5 million\ntimesteps of environment interaction. To obtain this result, we ran ES over 6 random seeds and\ncompared the mean learning curves to similarly computed curves for TRPO. The exact sample\ncomplexity tradeoffs over the course of learning are listed in Table 1, and detailed results are listed\nin Table 3 of the supplement. Generally, we were able to solve the environments in less than 10x\npenalty in sample complexity on the hard environments (Hopper and Walker2d) compared to TRPO.\nOn simple environments, we achieved up to 3x better sample complexity than TRPO.\nTable 1: MuJoCo tasks: Ratio of ES timesteps to TRPO timesteps needed to reach various percentages\nof TRPO\u2019s learning progress at 5 million timesteps.\nEnvironment\n25%\n50%\n75%\n100%\nHalfCheetah\n0.15\n0.49\n0.42\n0.58\nHopper\n0.53\n3.64\n6.05\n6.94\nInvertedDoublePendulum\n0.46\n0.48\n0.49\n1.23\nInvertedPendulum\n0.28\n0.52\n0.78\n0.88\nSwimmer\n0.56\n0.47\n0.53\n0.30\nWalker2d\n0.41\n5.69\n8.02\n7.88\n4.2\nAtari\nWe ran our parallel implementation of Evolution Strategies, described in Algorithm 2, on 51 Atari\n2600 games available in OpenAI Gym [Brockman et al., 2016]. We used the same preprocessing\nand feedforward CNN architecture used by Mnih et al. [2016]. All games were trained for 1 billion\nframes, which requires about the same amount of neural network computation as the published 1-day\nresults for A3C [Mnih et al., 2016] which uses 320 million frames. The difference is due to the\nfact that ES does not perform backpropagation and does not use a value function. By parallelizing\nthe evaluation of perturbed parameters across 720 CPUs on Amazon EC2, we can bring down the\ntime required for the training process to about one hour per game. After training, we compared \ufb01nal\nperformance against the published A3C results and found that ES performed better in 23 games\ntested, while it performed worse in 28. The full results are in Table 2 in the supplementary material.\n4.3\nParallelization\nES is particularly amenable to parallelization because of its low communication bandwidth require-\nment (Section 2.1). We implemented a distributed version of Algorithm 2 to investigate how ES\nscales with the number of workers. Our distributed implementation did not rely on special networking\nsetup and was tested on public cloud computing service Amazon EC2.\nWe picked the 3D Humanoid walking task from OpenAI Gym [Brockman et al., 2016] as the test\nproblem for our scaling experiment, because it is one of the most challenging continuous control\nproblems solvable by state-of-the-art RL techniques, which require about a day to learn on modern\nhardware [Schulman et al., 2015, Duan et al., 2016a]. Solving 3D Humanoid with ES on one 18-\ncore machine takes about 11 hours, which is on par with RL. However, when distributed across 80\n7\n\n\n102\n103\n101\n102\n18 cores, 657 minutes\n1440 cores, 10 minutes\nNumber of CPU cores\nMedian time to solve (minutes)\nFigure 1: Time to reach a score of 6000 on\n3D Humanoid with different number of CPU\ncores. Experiments are repeated 7 times and\nmedian time is reported.\nFigure 2: Learning curves for Pong using\nvarying frame-skip parameters. Although per-\nformance is stochastic, each setting leads to\nabout equally fast learning, with each run con-\nverging in around 100 weight updates.\nmachines and 1, 440 CPU cores, ES can solve 3D Humanoid in just 10 minutes, reducing experiment\nturnaround time by two orders of magnitude. Figure 1 shows that, for this task, ES is able to achieve\nlinear speedup in the number of CPU cores.\n4.4\nInvariance to temporal resolution\nIt is common practice in RL to have the agent decide on its actions in a lower frequency than is\nused in the simulator that runs the environment. This action frequency, or frame-skip, is a crucial\nparameter in many RL algorithms [Braylan et al., 2005]. If the frame-skip is set too high, the agent\ncannot make its decisions at a \ufb01ne enough timeframe to perform well in the environment. If, on\nthe other hand, the frameskip is set too low, the effective time length of the episode increases too\nmuch, which deteriorates optimization performance as analyzed in section 3.1. An advantage of ES\nis that its gradient estimate is invariant to the length of the episode, which makes it much more robust\nto the action frequency. We demonstrate this by running the Atari game Pong using a frame skip\nparameter in {1, 2, 3, 4}. As can be seen in Figure 2, the learning curves for each setting indeed look\nvery similar.\n5\nRelated work\nThere have been many attempts at applying methods related to ES to train neural networks Risi and\nTogelius [2015]. For Atari, Hausknecht et al. [2014] obtain impressive results. Sehnke et al. [2010]\nproposed a method closely related the one investigated in our work. Koutn\u00edk et al. [2013, 2010] and\nSrivastava et al. [2012] have similarly applied an an ES method to RL problems with visual inputs,\nbut where the policy was compressed in a number of different ways. Natural evolution strategies\nhas been successfully applied to black box optimization Wierstra et al. [2008, 2014], as well as for\nthe training of the recurrent weights in recurrent neural networks Schmidhuber et al. [2007]. Stulp\nand Sigaud [2012] explored similar approaches to black box optimization. An interesting hybrid of\nblack-box optimization and policy gradient methods was recently explored by Usunier et al. [2016].\nHyper-Neat Stanley et al. [2009] is an alternative approach to evolving both the weights of the neural\nnetworks and their parameters. Derivative free optimization methods have also been analyzed in the\nconvex setting Duchi et al. [2015], Nesterov [2012].\nThe main contribution in our work is in showing that this class of algorithms is extremely scalable\nand ef\ufb01cient to use on distributed hardware. We have shown that ES, when carefully implemented, is\ncompetitive with competing RL algorithms in terms of performance on the hardest problems solvable\ntoday, and is surprisingly close in terms of data ef\ufb01ciency, while taking less wallclock time to train.\n8\n\n\n6\nConclusion\nWe have explored Evolution Strategies, a class of black-box optimization algorithms, as an alternative\nto popular MDP-based RL techniques such as Q-learning and policy gradients. Experiments on\nAtari and MuJoCo show that it is a viable option with some attractive features: it is invariant to\naction frequency and delayed rewards, and it does not need temporal discounting or value function\napproximation. Most importantly, ES is highly parallelizable, which allows us to make up for a\ndecreased data ef\ufb01ciency by scaling to more parallel workers.\nIn future work, we plan to apply evolution strategies to those problems for which MDP-based\nreinforcement learning is less well-suited: problems with long time horizons and complicated reward\nstructure. We are particularly interested in meta-learning, or learning-to-learn. A proof of concept\nfor meta-learning in an RL setting was given by Duan et al. [2016b]: Using black-box optimization\nwe hope to be able to extend these results. We also plan to examine combining ES with fast low\nprecision neural network implementations to fully make use of the gradient-free nature of ES.\nReferences\nAlex Braylan, Mark Hollenbeck, Elliot Meyerson, and Risto Miikkulainen. Frame skip is a powerful\nparameter for learning to play atari. Space, 1600:1800, 2005.\nDimo Brockhoff, Anne Auger, Nikolaus Hansen, Dirk V Arnold, and Tim Hohm. Mirrored sampling\nand sequential selection for evolution strategies. In International Conference on Parallel Problem\nSolving from Nature, pages 11\u201321. Springer, 2010.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep\nreinforcement learning for continuous control. In Proceedings of the 33rd International Conference\non Machine Learning (ICML), 2016a.\nYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016b.\nJohn C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for\nzero-order convex optimization: The power of two function evaluations. IEEE Transactions on\nInformation Theory, 61(5):2788\u20132806, 2015.\nJohn Geweke. Antithetic acceleration of monte carlo integration in bayesian inference. Journal of\nEconometrics, 38(1-2):73\u201389, 1988.\nTobias Glasmachers, Tom Schaul, and J\u00fcrgen Schmidhuber. A natural evolution strategy for multi-\nobjective optimization. In International Conference on Parallel Problem Solving from Nature,\npages 627\u2013636. Springer, 2010a.\nTobias Glasmachers, Tom Schaul, Sun Yi, Daan Wierstra, and J\u00fcrgen Schmidhuber. Exponential natu-\nral evolution strategies. In Proceedings of the 12th annual conference on Genetic and evolutionary\ncomputation, pages 393\u2013400. ACM, 2010b.\nNikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution\nstrategies. Evolutionary computation, 9(2):159\u2013195, 2001.\nMatthew Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. A neuroevolution approach\nto general atari game playing. IEEE Transactions on Computational Intelligence and AI in Games,\n6(4):355\u2013366, 2014.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa,\nSarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of\na tensor processing unit. arXiv preprint arXiv:1704.04760, 2017.\n9\n\n\nKenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information\nProcessing Systems, pages 586\u2013594, 2016.\nJan Koutn\u00edk, Faustino Gomez, and J\u00fcrgen Schmidhuber. Evolving neural networks in compressed\nweight space. In Proceedings of the 12th annual conference on Genetic and evolutionary computa-\ntion, pages 619\u2013626. ACM, 2010.\nJan Koutn\u00edk, Giuseppe Cuccu, J\u00fcrgen Schmidhuber, and Faustino Gomez. Evolving large-scale neural\nnetworks for vision-based reinforcement learning. In Proceedings of the 15th annual conference\non Genetic and evolutionary computation, pages 1061\u20131068. ACM, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. In International Conference on Machine Learning, 2016.\nYurii Nesterov. Ef\ufb01ciency of coordinate descent methods on huge-scale optimization problems. SIAM\nJournal on Optimization, 22(2):341\u2013362, 2012.\nYurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.\nFoundations of Computational Mathematics, pages 1\u201340, 2011.\nAndrew Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and\nEric Liang. Autonomous inverted helicopter \ufb02ight via reinforcement learning. Experimental\nRobotics IX, pages 363\u2013372, 2006.\nRonald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in\nneural information processing systems, pages 1043\u20131049, 1998.\nI. Rechenberg and M. Eigen. Evolutionsstrategie: Optimierung Technischer Systeme nach Prinzipien\nder Biologischen Evolution. Frommann-Holzboog Stuttgart, 1973.\nSebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open challenges.\nIEEE Transactions on Computational Intelligence and AI in Games, 2015.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. In Advances in Neural Information Processing Systems,\npages 2226\u20132234, 2016.\nTom Schaul, Tobias Glasmachers, and J\u00fcrgen Schmidhuber. High dimensions and heavy tails\nfor natural evolution strategies. In Proceedings of the 13th annual conference on Genetic and\nevolutionary computation, pages 845\u2013852. ACM, 2011.\nJuergen Schmidhuber and Jieyu Zhao. Direct policy search and uncertain policy evaluation. In Aaai\nspring symposium on search under uncertain and incomplete information, stanford univ, pages\n119\u2013124, 1998.\nJ\u00fcrgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, and Faustino Gomez. Training recurrent\nnetworks by evolino. Neural computation, 19(3):757\u2013779, 2007.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region\npolicy optimization. In ICML, pages 1889\u20131897, 2015.\nH.-P. Schwefel. Numerische optimierung von computer-modellen mittels der evolutionsstrategie.\n1977.\nFrank Sehnke, Christian Osendorfer, Thomas R\u00fcckstie\u00df, Alex Graves, Jan Peters, and J\u00fcrgen Schmid-\nhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551\u2013559, 2010.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.\n10\n\n\nJames C Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient\napproximation. IEEE transactions on automatic control, 37(3):332\u2013341, 1992.\nRupesh Kumar Srivastava, J\u00fcrgen Schmidhuber, and Faustino Gomez. Generalized compressed\nnetwork search. In International Conference on Parallel Problem Solving from Nature, pages\n337\u2013346. Springer, 2012.\nKenneth O Stanley, David B D\u2019Ambrosio, and Jason Gauci. A hypercube-based encoding for evolving\nlarge-scale neural networks. Arti\ufb01cial life, 15(2):185\u2013212, 2009.\nFreek Stulp and Olivier Sigaud. Policy improvement methods: Between black-box optimization and\nepisodic reinforcement learning. 2012.\nYi Sun, Daan Wierstra, Tom Schaul, and Juergen Schmidhuber. Ef\ufb01cient natural evolution strategies.\nIn Proceedings of the 11th Annual conference on Genetic and evolutionary computation, pages\n539\u2013546. ACM, 2009.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages\n5026\u20135033. IEEE, 2012.\nNicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic exploration for\ndeep deterministic policies: An application to starcraft micromanagement tasks. arXiv preprint\narXiv:1609.02993, 2016.\nSjoerd van Steenkiste, Jan Koutn\u00edk, Kurt Driessens, and J\u00fcrgen Schmidhuber. A wavelet-based\nencoding for neuroevolution. In Proceedings of the 2016 on Genetic and Evolutionary Computation\nConference, pages 517\u2013524. ACM, 2016.\nDaan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In\nEvolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelli-\ngence). IEEE Congress on, pages 3381\u20133387. IEEE, 2008.\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J\u00fcrgen Schmidhuber.\nNatural evolution strategies. Journal of Machine Learning Research, 15(1):949\u2013980, 2014.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229\u2013256, 1992.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov,\nRichard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation\nwith visual attention. In ICML, volume 14, pages 77\u201381, 2015.\nSun Yi, Daan Wierstra, Tom Schaul, and J\u00fcrgen Schmidhuber. Stochastic search using the natural\ngradient. In Proceedings of the 26th Annual International Conference on Machine Learning, pages\n1161\u20131168. ACM, 2009.\n11\n\n\nGame\nDQN\nA3C FF, 1 day\nHyperNEAT\nES FF, 1 hour\nA2C FF\nAmidar\n133.4\n283.9\n184.4\n112.0\n548.2\nAssault\n3332.3\n3746.1\n912.6\n1673.9\n2026.6\nAsterix\n124.5\n6723.0\n2340.0\n1440.0\n3779.7\nAsteroids\n697.1\n3009.4\n1694.0\n1562.0\n1733.4\nAtlantis\n76108.0\n772392.0\n61260.0\n1267410.0\n2872644.8\nBank Heist\n176.3\n946.0\n214.0\n225.0\n724.1\nBattle Zone\n17560.0\n11340.0\n36200.0\n16600.0\n8406.2\nBeam Rider\n8672.4\n13235.9\n1412.8\n744.0\n4438.9\nBerzerk\n1433.4\n1394.0\n686.0\n720.6\nBowling\n41.2\n36.2\n135.8\n30.0\n28.9\nBoxing\n25.8\n33.7\n16.4\n49.8\n95.8\nBreakout\n303.9\n551.6\n2.8\n9.5\n368.5\nCentipede\n3773.1\n3306.5\n25275.2\n7783.9\n2773.3\nChopper Command\n3046.0\n4669.0\n3960.0\n3710.0\n1700.0\nCrazy Climber\n50992.0\n101624.0\n0.0\n26430.0\n100034.4\nDemon Attack\n12835.2\n84997.5\n14620.0\n1166.5\n23657.7\nDouble Dunk\n21.6\n0.1\n2.0\n0.2\n3.2\nEnduro\n475.6\n82.2\n93.6\n95.0\n0.0\nFishing Derby\n2.3\n13.6\n49.8\n49.0\n33.9\nFreeway\n25.8\n0.1\n29.0\n31.0\n0.0\nFrostbite\n157.4\n180.1\n2260.0\n370.0\n266.6\nGopher\n2731.8\n8442.8\n364.0\n582.0\n6266.2\nGravitar\n216.5\n269.5\n370.0\n805.0\n256.2\nIce Hockey\n3.8\n4.7\n10.6\n4.1\n4.9\nKangaroo\n2696.0\n106.0\n800.0\n11200.0\n1357.6\nKrull\n3864.0\n8066.6\n12601.4\n8647.2\n6411.5\nMontezuma\u2019s Revenge\n50.0\n53.0\n0.0\n0.0\n0.0\nName This Game\n5439.9\n5614.0\n6742.0\n4503.0\n5532.8\nPhoenix\n28181.8\n1762.0\n4041.0\n14104.7\nPit Fall\n123.0\n0.0\n0.0\n8.2\nPong\n16.2\n11.4\n17.4\n21.0\n20.8\nPrivate Eye\n298.2\n194.4\n10747.4\n100.0\n100.0\nQ*Bert\n4589.8\n13752.3\n695.0\n147.5\n15758.6\nRiver Raid\n4065.3\n10001.2\n2616.0\n5009.0\n9856.9\nRoad Runner\n9264.0\n31769.0\n3220.0\n16590.0\n33846.9\nRobotank\n58.5\n2.3\n43.8\n11.9\n2.2\nSeaquest\n2793.9\n2300.2\n716.0\n1390.0\n1763.7\nSkiing\n13700.0\n7983.6\n15442.5\n15245.8\nSolaris\n1884.8\n160.0\n2090.0\n2265.0\nSpace Invaders\n1449.7\n2214.7\n1251.0\n678.5\n951.9\nStar Gunner\n34081.0\n64393.0\n2720.0\n1470.0\n40065.6\nTennis\n2.3\n10.2\n0.0\n4.5\n11.2\nTime Pilot\n5640.0\n5825.0\n7340.0\n4970.0\n4637.5\nTutankham\n32.4\n26.1\n23.6\n130.3\n194.3\nUp and Down\n3311.3\n54525.4\n43734.0\n67974.0\n75785.9\nVenture\n54.0\n19.0\n0.0\n760.0\n0.0\nVideo Pinball\n20228.1\n185852.6\n0.0\n22834.8\n46470.1\nWizard of Wor\n246.0\n5278.0\n3360.0\n3480.0\n1587.5\nYars Revenge\n7270.8\n24096.4\n16401.7\n8963.5\nZaxxon\n831.0\n2659.0\n3000.0\n6380.0\n5.6\nTable 2: Final results obtained using Evolution Strategies on Atari 2600 games (feedforward CNN\npolicy, deterministic policy evaluation, averaged over 10 re-runs with up to 30 random initial no-ops),\nand compared to results for DQN and A3C from Mnih et al. [2016] and HyperNEAT from Hausknecht\net al. [2014]. A2C is our synchronous variant of A3C, and its reported scores are obtained with 320M\ntraining frames with the same evaluation setup as for the ES results. All methods were trained on raw\npixel input.\n12\n\n\nTable 3: MuJoCo tasks: Ratio of ES timesteps to TRPO timesteps needed to reach various percentages of TRPO\u2019s learning progress at 5 million timesteps. These\nresults were computed from ES learning curves averaged over 6 reruns.\nEnvironment\n% TRPO \ufb01nal score\nTRPO score\nTRPO timesteps\nES timesteps\nES timesteps / TRPO timesteps\nHalfCheetah\n25%\n-1.35\n9.05e+05\n1.36e+05\n0.15\n50%\n793.55\n1.70e+06\n8.28e+05\n0.49\n75%\n1589.83\n3.34e+06\n1.42e+06\n0.42\n100%\n2385.79\n5.00e+06\n2.88e+06\n0.58\nHopper\n25%\n877.45\n7.29e+05\n3.83e+05\n0.53\n50%\n1718.16\n1.03e+06\n3.73e+06\n3.64\n75%\n2561.11\n1.59e+06\n9.63e+06\n6.05\n100%\n3403.46\n4.56e+06\n3.16e+07\n6.94\nInvertedDoublePendulum\n25%\n2358.98\n8.73e+05\n3.98e+05\n0.46\n50%\n4609.68\n9.65e+05\n4.66e+05\n0.48\n75%\n6874.03\n1.07e+06\n5.30e+05\n0.49\n100%\n9104.07\n4.39e+06\n5.39e+06\n1.23\nInvertedPendulum\n25%\n276.59\n2.21e+05\n6.25e+04\n0.28\n50%\n519.15\n2.73e+05\n1.43e+05\n0.52\n75%\n753.17\n3.25e+05\n2.55e+05\n0.78\n100%\n1000.00\n5.17e+05\n4.55e+05\n0.88\nSwimmer\n25%\n41.97\n1.04e+06\n5.88e+05\n0.56\n50%\n70.73\n1.82e+06\n8.52e+05\n0.47\n75%\n99.68\n2.33e+06\n1.23e+06\n0.53\n100%\n128.25\n4.59e+06\n1.39e+06\n0.30\nWalker2d\n25%\n957.68\n1.55e+06\n6.43e+05\n0.41\n50%\n1916.48\n2.27e+06\n1.29e+07\n5.69\n75%\n2872.81\n2.89e+06\n2.31e+07\n8.02\n100%\n3830.03\n4.81e+06\n3.79e+07\n7.88\n13\n\n\n",
    "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
    "abstract": "We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q- learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation. 1",
    "sections": [
      {
        "header": "Abstract",
        "content": "We explore the use of Evolution Strategies (ES), a class of black box optimization\nalgorithms, as an alternative to popular MDP-based RL techniques such as Q-\nlearning and Policy Gradients. Experiments on MuJoCo and Atari show that ES\nis a viable solution strategy that scales extremely well with the number of CPUs\navailable: By using a novel communication strategy based on common random\nnumbers, our ES implementation only needs to communicate scalars, making it\npossible to scale to over a thousand parallel workers. This allows us to solve 3D\nhumanoid walking in 10 minutes and obtain competitive results on most Atari\ngames after one hour of training. In addition, we highlight several advantages of\nES as a black box optimization technique: it is invariant to action frequency and\ndelayed rewards, tolerant of extremely long horizons, and does not need temporal\ndiscounting or value function approximation."
      },
      {
        "header": "Introduction",
        "content": "Developing agents that can accomplish challenging tasks in complex, uncertain environments is a key\ngoal of arti\ufb01cial intelligence. Recently, the most popular paradigm for analyzing such problems has\nbeen using a class of reinforcement learning (RL) algorithms based on the Markov Decision Process\n(MDP) formalism and the concept of value functions. Successes of this approach include systems\nthat learn to play Atari from pixels [Mnih et al., 2015], perform helicopter aerobatics Ng et al. [2006],\nor play expert-level Go [Silver et al., 2016].\nAn alternative approach to solving RL problems is using black-box optimization. This approach\nis known as direct policy search [Schmidhuber and Zhao, 1998], or neuro-evolution [Risi and\nTogelius, 2015], when applied to neural networks. In this paper, we study Evolution Strategies (ES)\n[Rechenberg and Eigen, 1973], a particular set of optimization algorithms in this class. We show\nthat ES can reliably train neural network policies, in a fashion well suited to be scaled up to modern\ndistributed computer systems, for controlling robots in the MuJoCo physics simulator [Todorov et al.,\n2012] and playing Atari games with pixel inputs [Mnih et al., 2015]. Our key \ufb01ndings are as follows:\n1. We found that the use of virtual batch normalization [Salimans et al., 2016] and other\nreparameterizations of the neural network policy (section 2.2) greatly improve the reliability\nof evolution strategies. Without these methods ES proved brittle in our experiments, but with\nthese reparameterizations we achieved strong results over a wide variety of environments.\n2. We found the evolution strategies method to be highly parallelizable: by introducing a novel\ncommunication strategy based on common random numbers, we are able to achieve linear\nspeedups in run time even when using over a thousand workers. In particular, using 1,440\nworkers, we have been able to solve the MuJoCo 3D humanoid task in under 10 minutes.\n3. The data ef\ufb01ciency of evolution strategies was surprisingly good: we were able to match\nthe \ufb01nal performance of A3C [Mnih et al., 2016] on most Atari environments while using\nbetween 3x and 10x as much data. The slight decrease in data ef\ufb01ciency is partly offset by a\narXiv:1703.03864v2  [stat.ML]  7 Sep 2017\n\n\nreduction in required computation of roughly 3x due to not performing backpropagation\nand not having a value function. Our 1-hour ES results require about the same amount of\ncomputation as the published 1-day results for A3C, while performing better on 23 games\ntested, and worse on 28. On MuJoCo tasks, we were able to match the learned policy\nperformance of Trust Region Policy Optimization [TRPO; Schulman et al., 2015], using no\nmore than 10x as much data."
      },
      {
        "header": "We found that ES exhibited better exploration behaviour than policy gradient methods like",
        "content": "TRPO: on the MuJoCo humanoid task, ES has been able to learn a very wide variety of gaits\n(such as walking sideways or walking backwards). These unusual gaits are never observed\nwith TRPO, which suggests a qualitatively different exploration behavior.\n5. We found the evolution strategies method to be robust: we achieved the aforementioned\nresults using \ufb01xed hyperparameters for all the Atari environments, and a different set of\n\ufb01xed hyperparameters for all MuJoCo environments (with the exception of one binary hyper-\nparameter, which has not been held constant between the different MuJoCo environments).\nBlack-box optimization methods have several highly attractive properties: indifference to the distribu-\ntion of rewards (sparse or dense), no need for backpropagating gradients, and tolerance of potentially\narbitrarily long time horizons. However, they are perceived as less effective at solving hard RL\nproblems compared to techniques like Q-learning and policy gradients. The contribution of this work,\nwhich we hope will renew interest in this class of methods and lead to new useful applications, is\na demonstration that evolution strategies can be competitive with competing RL algorithms on the\nhardest environments studied by the deep RL community today, and that this approach can scale to\nmany more parallel workers."
      },
      {
        "header": "Evolution Strategies",
        "content": "Evolution Strategies (ES) is a class of black box optimization algorithms [Rechenberg and Eigen,\n1973, Schwefel, 1977] that are heuristic search procedures inspired by natural evolution: At every\niteration (\u201cgeneration\u201d), a population of parameter vectors (\u201cgenotypes\u201d) is perturbed (\u201cmutated\u201d)\nand their objective function value (\u201c\ufb01tness\u201d) is evaluated. The highest scoring parameter vectors are\nthen recombined to form the population for the next generation, and this procedure is iterated until the\nobjective is fully optimized. Algorithms in this class differ in how they represent the population and\nhow they perform mutation and recombination. The most widely known member of the ES class is\nthe covariance matrix adaptation evolution strategy [CMA-ES; Hansen and Ostermeier, 2001], which\nrepresents the population by a full-covariance multivariate Gaussian. CMA-ES has been extremely\nsuccessful in solving optimization problems in low to medium dimension.\nThe version of ES we use in this work belongs to the class of natural evolution strategies (NES)\n[Wierstra et al., 2008, 2014, Yi et al., 2009, Sun et al., 2009, Glasmachers et al., 2010a,b, Schaul et al.,\n2011] and is closely related to the work of Sehnke et al. [2010]. Let F denote the objective function\nacting on parameters \u03b8. NES algorithms represent the population with a distribution over parameters\np\u03c8(\u03b8)\u2014itself parameterized by \u03c8\u2014and proceed to maximize the average objective value E\u03b8\u223cp\u03c8F(\u03b8)\nover the population by searching for \u03c8 with stochastic gradient ascent. Speci\ufb01cally, using the score\nfunction estimator for \u2207\u03c8E\u03b8\u223cp\u03c8F(\u03b8) in a fashion similar to REINFORCE [Williams, 1992], NES\nalgorithms take gradient steps on \u03c8 with the following estimator:\n\u2207\u03c8E\u03b8\u223cp\u03c8F(\u03b8) = E\u03b8\u223cp\u03c8 {F(\u03b8)\u2207\u03c8 log p\u03c8(\u03b8)}\nFor the special case where p\u03c8 is factored Gaussian (as in this work), the resulting gradient estimator\nis also known as simultaneous perturbation stochastic approximation [Spall, 1992], parameter-\nexploring policy gradients [Sehnke et al., 2010], or zero-order gradient estimation [Nesterov and\nSpokoiny, 2011].\nIn this work, we focus on RL problems, so F(\u00b7) will be the stochastic return provided by an\nenvironment, and \u03b8 will be the parameters of a deterministic or stochastic policy \u03c0\u03b8 describing an\nagent acting in that environment, controlled by either discrete or continuous actions. Much of the\ninnovation in RL algorithms is focused on coping with the lack of access to or existence of derivatives\nof the environment or policy. Such non-smoothness can be addressed with ES as follows. We\ninstantiate the population distribution p\u03c8 as an isotropic multivariate Gaussian with mean \u03c8 and \ufb01xed\ncovariance \u03c32I, allowing us to write E\u03b8\u223cp\u03c8F(\u03b8) in terms of a mean parameter vector \u03b8 directly: we\n2\n\n\nset E\u03b8\u223cp\u03c8F(\u03b8) = E\u03f5\u223cN(0,I) F(\u03b8 + \u03c3\u03f5). With this setup, our stochastic objective can be viewed as\na Gaussian-blurred version of the original objective F, free of non-smoothness introduced by the\nenvironment or potentially discrete actions taken by the policy. Further discussion on how ES and\npolicy gradient methods cope with non-smoothness can be found in section 3.\nWith our objective de\ufb01ned in terms of \u03b8, we optimize over \u03b8 directly using stochastic gradient ascent\nwith the score function estimator:\n\u2207\u03b8 E\u03f5\u223cN(0,I) F(\u03b8 + \u03c3\u03f5) = 1\n\u03c3 E\u03f5\u223cN(0,I) {F(\u03b8 + \u03c3\u03f5) \u03f5}\nwhich can be approximated with samples. The resulting algorithm (1) repeatedly executes two phases:\n1) Stochastically perturbing the parameters of the policy and evaluating the resulting parameters by\nrunning an episode in the environment, and 2) Combining the results of these episodes, calculating a\nstochastic gradient estimate, and updating the parameters.\nAlgorithm 1 Evolution Strategies\n1: Input: Learning rate \u03b1, noise standard deviation \u03c3, initial policy parameters \u03b80\n2: for t = 0, 1, 2, . . . do\n3:\nSample \u03f51, . . . \u03f5n \u223cN(0, I)\n4:\nCompute returns Fi = F(\u03b8t + \u03c3\u03f5i) for i = 1, . . . , n\n5:\nSet \u03b8t+1 \u2190\u03b8t + \u03b1 1\nn\u03c3"
      },
      {
        "header": "Scaling and parallelizing ES",
        "content": "ES is well suited to be scaled up to many parallel workers: 1) It operates on complete episodes, thereby\nrequiring only infrequent communication between workers. 2) The only information obtained by each\nworker is the scalar return of an episode: if we synchronize random seeds between workers before\noptimization, each worker knows what perturbations the other workers used, so each worker only\nneeds to communicate a single scalar to and from each other worker to agree on a parameter update.\nES thus requires extremely low bandwidth, in sharp contrast to policy gradient methods, which\nrequire workers to communicate entire gradients. 3) It does not require value function approximations.\nRL with value function estimation is inherently sequential: To improve upon a given policy, multiple\nupdates to the value function are typically needed to get enough signal. Each time the policy is\nsigni\ufb01cantly changed, multiple iterations are necessary for the value function estimate to catch up.\nA simple parallel version of ES is given in Algorithm 2. The main novelty here is that the algo-\nrithm makes use of shared random seeds, which drastically reduces the bandwidth required for\ncommunication between the workers.\nAlgorithm 2 Parallelized Evolution Strategies\n1: Input: Learning rate \u03b1, noise standard deviation \u03c3, initial policy parameters \u03b80\n2: Initialize: n workers with known random seeds, and initial parameters \u03b80\n3: for t = 0, 1, 2, . . . do\n4:\nfor each worker i = 1, . . . , n do\n5:\nSample \u03f5i \u223cN(0, I)\n6:\nCompute returns Fi = F(\u03b8t + \u03c3\u03f5i)\n7:\nend for\n8:"
      },
      {
        "header": "Send all scalar returns Fi from each worker to every other worker",
        "content": "9:\nfor each worker i = 1, . . . , n do\n10:\nReconstruct all perturbations \u03f5j for j = 1, . . . , n using known random seeds\n11:\nSet \u03b8t+1 \u2190\u03b8t + \u03b1 1\nn\u03c3"
      },
      {
        "header": "Pn",
        "content": "j=1 Fj\u03f5j\n12:\nend for\n13: end for\nIn practice, we implement sampling by having each worker instantiate a large block of Gaussian\nnoise at the start of training, and then perturbing its parameters by adding a randomly indexed subset\nof these noise variables at each iteration. Although this means that the perturbations are not strictly\n3\n\n\nindependent across iterations, we did not \ufb01nd this to be a problem in practice. Using this strategy,\nwe \ufb01nd that the second part of Algorithm 2 (lines 9-12) only takes up a small fraction of total time\nspend for all our experiments, even when using up to 1,440 parallel workers. When using many more\nworkers still, or when using very large neural networks, we can reduce the computation required for\nthis part of the algorithm by having workers only perturb a subset of the parameters \u03b8 rather than\nall of them: In this case the perturbation distribution p\u03c8 corresponds to a mixture of Gaussians, for\nwhich the update equations remain unchanged. At the very extreme, every worker would perturb\nonly a single coordinate of the parameter vector, which means that we would be using pure \ufb01nite\ndifferences.\nTo reduce variance, we use antithetic sampling Geweke [1988], also known as mirrored sampling\nBrockhoff et al. [2010] in the ES literature: that is, we always evaluate pairs of perturbations \u03f5, \u2212\u03f5,\nfor Gaussian noise vector \u03f5. We also \ufb01nd it useful to perform \ufb01tness shaping Wierstra et al. [2014] by\napplying a rank transformation to the returns before computing each parameter update. Doing so\nremoves the in\ufb02uence of outlier individuals in each population and decreases the tendency for ES to\nfall into local optima early in training. In addition, we apply weight decay to the parameters of our\npolicy network: this prevents the parameters from growing very large compared to the perturbations.\nUnlike Wierstra et al. [2014] we did not see bene\ufb01t from adapting \u03c3 during training, and we therefore\ntreat it as a \ufb01xed hyperparameter instead. We perform the optimization directly in parameter space;\nexploring indirect encodings Stanley et al. [2009], van Steenkiste et al. [2016] is left for future work.\nEvolution Strategies, as presented above, works with full-length episodes. In some rare cases this\ncan lead to low CPU utilization, as some episodes run for many more steps than others. For this\nreason, we cap episode length at a constant m steps for all workers, which we dynamically adjust as\ntraining progresses. For example, by setting m to be equal to twice the mean number of steps taken\nper episode, we can guarantee that CPU utilization stays above 50% in the worst case.\n2.2"
      },
      {
        "header": "The impact of network parameterization",
        "content": "Whereas RL algorithms like Q-learning and policy gradients explore by sampling actions from a\nstochastic policy, Evolution Strategies derives learning signal from sampling instantiations of policy\nparameters. Exploration in ES is thus driven by parameter perturbation. For ES to improve upon\nparameters \u03b8, some members of the population must achieve better return than others: i.e. it is crucial\nthat Gaussian perturbation vectors \u03f5 occasionally lead to new individuals \u03b8 + \u03c3\u03f5 with better return.\nFor the Atari environments, we found that Gaussian parameter perturbations on DeepMind\u2019s con-\nvolutional architectures [Mnih et al., 2015] did not always lead to adequate exploration: For some\nenvironments, randomly perturbed parameters tended to encode policies that always took one speci\ufb01c\naction regardless of the state that was given as input. However, we discovered thatwe could match the\nperformance of policy gradient methods for most games by using virtual batch normalization [Sali-\nmans et al., 2016] in the policy speci\ufb01cation. Virtual batch normalization is precisely equivalent to\nbatch normalization [Ioffe and Szegedy, 2015] where the minibatch used for calculating normalizing\nstatistics is chosen at the start of training and is \ufb01xed. This change in parameterization makes the\npolicy more sensitive to very small changes in the input image at the early stages of training when the\nweights of the policy are random, ensuring that the policy takes a wide-enough variety of actions\nto gather occasional rewards. For most applications, a downside of virtual batch normalization is\nthat it makes training more expensive. For our application, however, the minibatch used to calculate\nthe normalizing statistics is much smaller than the number of steps taken during a typical episode,\nmeaning that the overhead is negligible.\nFor the MuJoCo tasks, we achieved good performance on nearly all the environments with the\nstandard multilayer perceptrons mapping to continuous actions. However, we observed that for some\nenvironments, we could encourage more exploration by discretizing the actions. This forced the\nactions to be non-smooth with respect to input observations and parameter perturbations, and thereby\nencouraged a wide variety of behaviors to be played out over the course of rollouts."
      },
      {
        "header": "Smoothing in parameter space versus smoothing in action space",
        "content": "As mentioned in section 2, a large source of dif\ufb01culty in RL stems from the lack of informative\ngradients of policy performance: such gradients may not exist due to non-smoothness of the environ-\n4\n\n\nment or policy, or may only be available as high-variance estimates because the environment usually\ncan only be accessed via sampling. Explicitly, suppose we wish to solve general decision problems\nthat give a return R(a) after we take a sequence of actions a = {a1, . . . , aT }, where the actions are\ndetermined by a either a deterministic or a stochastic policy function at = \u03c0(s; \u03b8). The objective we\nwould like to optimize is thus\nF(\u03b8) = R(a(\u03b8)).\nSince the actions are allowed to be discrete and the policy is allowed to be deterministic, F(\u03b8)\ncan be non-smooth in \u03b8. More importantly, because we do not have explicit access to the under-\nlying state transition function of our decision problems, the gradients cannot be computed with\na backpropagation-like algorithm. This means we cannot directly use standard gradient-based\noptimization methods to \ufb01nd a good solution for \u03b8.\nIn order to both make the problem smooth and to have a means of to estimate its gradients, we need\nto add noise. Policy gradient methods add the noise in action space, which is done by sampling the\nactions from an appropriate distribution. For example, if the actions are discrete and \u03c0(s; \u03b8) calculates\na score for each action before selecting the best one, then we would sample an action a(\u03f5, \u03b8) (here \u03f5 is\nthe noise source) from a categorical distribution over actions at each time period, applying a softmax\nto the scores of each action. Doing so yields the objective FP G(\u03b8) = E\u03f5 R(a(\u03f5, \u03b8)), with gradients\n\u2207\u03b8FP G(\u03b8) = E\u03f5 {R(a(\u03f5, \u03b8))\u2207\u03b8 log p(a(\u03f5, \u03b8); \u03b8)} .\nEvolution strategies, on the other hand, add the noise in parameter space. That is, they perturb the\nparameters as \u02dc\u03b8 = \u03b8 + \u03be, with \u03be from a multivariate Gaussian distribution, and then pick actions\nas at = a(\u03be, \u03b8) = \u03c0(s; \u02dc\u03b8). It can be interpreted as adding a Gaussian blur to the original objective,\nwhich results in a smooth, differentiable cost FES(\u03b8) = E\u03be R(a(\u03be, \u03b8)), this time with gradients\n\u2207\u03b8FES(\u03b8) = E\u03be\nn\nR(a(\u03be, \u03b8))\u2207\u03b8 log p(\u02dc\u03b8(\u03be, \u03b8); \u03b8)\no\n.\nThe two methods for smoothing the decision problem are thus quite similar, and can be made even\nmore so by adding noise to both the parameters and the actions.\n3.1\nWhen is ES better than policy gradients?\nGiven these two methods of smoothing the decision problem, which should we use? The answer\ndepends strongly on the structure of the decision problem and on which type of Monte Carlo\nestimator is used to estimate the gradients \u2207\u03b8FP G(\u03b8) and \u2207\u03b8FES(\u03b8). Suppose the correlation\nbetween the return and the individual actions is low (as is true for any hard RL problem). Assuming\nwe approximate these gradients using simple Monte Carlo (REINFORCE) with a good baseline on\nthe return, we have\nVar[\u2207\u03b8FP G(\u03b8)] \u2248Var[R(a)] Var[\u2207\u03b8 log p(a; \u03b8)],\nVar[\u2207\u03b8FES(\u03b8)] \u2248Var[R(a)] Var[\u2207\u03b8 log p(\u02dc\u03b8; \u03b8)].\nIf both methods perform a similar amount of exploration, Var[R(a)] will be similar for both ex-\npressions. The difference will thus be in the second term. Here we have that \u2207\u03b8 log p(a; \u03b8) ="
      },
      {
        "header": "PT",
        "content": "t=1 \u2207\u03b8 log p(at; \u03b8) is a sum of T uncorrelated terms, so that the variance of the policy gradi-\nent estimator will grow nearly linearly with T. The corresponding term for evolution strategies,\n\u2207\u03b8 log p(\u02dc\u03b8; \u03b8), is independent of T. Evolution strategies will thus have an advantage compared to\npolicy gradients for long episodes with very many time steps. In practice, the effective number of\nsteps T is often reduced in policy gradient methods by discounting rewards. If the effects of actions\nare short-lasting, this allows us to dramatically reduce the variance in our gradient estimate, and\nthis has been critical to the success of applications such as Atari games. However, this discounting\nwill bias our gradient estimate if actions have long lasting effects. Another strategy for reducing the\neffective value of T is to use value function approximation. This has also been effective, but once\nagain runs the risk of biasing our gradient estimates. Evolution strategies is thus an attractive choice\nif the effective number of time steps T is long, actions have long-lasting effects, and if no good value\nfunction estimates are available.\n5\n\n\n3.2"
      },
      {
        "header": "Problem dimensionality",
        "content": "The gradient estimate of ES can be interpreted as a method for randomized \ufb01nite differences in\nhigh-dimensional space. Indeed, using the fact that E\u03f5\u223cN(0,I) {F(\u03b8) \u03f5/\u03c3} = 0, we get\n\u2207\u03b8\u03b7(\u03b8) = E\u03f5\u223cN(0,I) {F(\u03b8 + \u03c3\u03f5) \u03f5/\u03c3} = E\u03f5\u223cN(0,I) {(F(\u03b8 + \u03c3\u03f5) \u2212F(\u03b8)) \u03f5/\u03c3}\nIt is now apparent that ES can be seen as computing a \ufb01nite difference derivative estimate in\na randomly chosen direction, especially as \u03c3 becomes small. The resemblance of ES to \ufb01nite\ndifferences suggests the method will scale poorly with the dimension of the parameters \u03b8. Theoretical\nanalysis indeed shows that for general non-smooth optimization problems, the required number of\noptimization steps scales linearly with the dimension [Nesterov and Spokoiny, 2011]. However, it\nis important to note that this does not mean that larger neural networks will perform worse than\nsmaller networks when optimized using ES: what matters is the dif\ufb01culty, or intrinsic dimension, of\nthe optimization problem. To see that the dimensionality of our model can be completely separate\nfrom the effective dimension of the optimization problem, consider a regression problem where we\napproximate a univariate variable y with a linear model \u02c6y = x \u00b7 w: if we double the number of\nfeatures and parameters in this model by concatenating x with itself (i.e. using features x\u2032 = (x, x)),\nthe problem does not become more dif\ufb01cult. The ES algorithm will do exactly the same thing when\napplied to this higher dimensional problem, as long as we divide the standard deviation of the noise\nby two, as well as the learning rate.\nIn practice, we observe slightly better results when using larger networks with ES. For example, we\ntried both the larger network and smaller network used in A3C [Mnih et al., 2016] for learning Atari\n2600 games, and on average obtained better results using the larger network. We hypothesize that this\nis due to the same effect that makes standard gradient-based optimization of large neural networks\neasier than for small ones: large networks have fewer local minima [Kawaguchi, 2016].\n3.3"
      },
      {
        "header": "Advantages of not calculating gradients",
        "content": "In addition to being easy to parallelize, and to having an advantage in cases with long action sequences\nand delayed rewards, black box optimization algorithms like ES have other advantages over RL\ntechniques that calculate gradients. The communication overhead of implementing ES in a distributed\nsetting is lower than for competing RL methods such as policy gradients and Q-learning, as the only\ninformation that needs to be communicated across processes are the scalar return and the random\nseed that was used to generate the perturbations \u03f5, rather than a full gradient. Also, ES can deal with\nmaximally sparse and delayed rewards; only the total return of an episode is used, whereas other\nmethods use individual rewards and their exact timing.\nBy not requiring backpropagation, black box optimizers reduce the amount of computation per\nepisode by about two thirds, and memory by potentially much more. In addition, not explicitly\ncalculating an analytical gradient protects against problems with exploding gradients that are common\nwhen working with recurrent neural networks. By smoothing the cost function in parameter space, we\nreduce the pathological curvature that causes these problems: bounded cost functions that are smooth\nenough can\u2019t have exploding gradients. At the extreme, ES allows us to incorporate non-differentiable\nelements into our architecture, such as modules that use hard attention [Xu et al., 2015].\nBlack box optimization methods are uniquely suited to low precision hardware for deep learning.\nLow precision arithmetic, such as in binary neural networks, can be performed much cheaper than\nat high precision. When optimizing such low precision architectures, biased low precision gradient\nestimates can be a problem when using gradient-based methods. Similarly, specialized hardware for\nneural network inference, such as TPUs [Jouppi et al., 2017], can be used directly when performing\noptimization using ES, while their limited memory usually makes backpropagation impossible.\nBy perturbing in parameter space instead of action space, black box optimizers are naturally invariant\nto the frequency at which our agent acts in the environment. For MDP-based reinforcement learning\nalgorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for\nthe optimization to succeed [Braylan et al., 2005]. While this is usually a solvable problem for games\nthat only require short-term planning and action, it is a problem for learning longer term strategic\nbehavior. For these problems, RL needs hierarchy to succeed [Parr and Russell, 1998], which is not\nas necessary when using black box optimization.\n6"
      },
      {
        "header": "We evaluated ES on a benchmark of continuous robotic control problems in the OpenAI Gym",
        "content": "[Brockman et al., 2016] against a highly tuned implementation of Trust Region Policy Optimiza-\ntion [Schulman et al., 2015], a policy gradient algorithm designed to ef\ufb01ciently optimize neural\nnetwork policies. We tested on both classic problems, like balancing an inverted pendulum, and more\ndif\ufb01cult ones found in recent literature, like learning 2D hopping and walking gaits. The environments\nwere simulated by MuJoCo [Todorov et al., 2012].\nWe used both ES and TRPO to train policies with identical architectures: multilayer perceptrons with\ntwo 64-unit hidden layers separated by tanh nonlinearities. We found that ES occasionally bene\ufb01ted\nfrom discrete actions, since continuous actions could be too smooth with respect to parameter\nperturbation and could hamper exploration (see section 2.2). For the hopping and swimming tasks,\nwe discretized the actions for ES into 10 bins for each action component.\nWe found that ES was able to solve these tasks up to TRPO\u2019s \ufb01nal performance after 5 million\ntimesteps of environment interaction. To obtain this result, we ran ES over 6 random seeds and\ncompared the mean learning curves to similarly computed curves for TRPO. The exact sample\ncomplexity tradeoffs over the course of learning are listed in Table 1, and detailed results are listed\nin Table 3 of the supplement. Generally, we were able to solve the environments in less than 10x\npenalty in sample complexity on the hard environments (Hopper and Walker2d) compared to TRPO.\nOn simple environments, we achieved up to 3x better sample complexity than TRPO.\nTable 1: MuJoCo tasks: Ratio of ES timesteps to TRPO timesteps needed to reach various percentages\nof TRPO\u2019s learning progress at 5 million timesteps."
      },
      {
        "header": "Atari",
        "content": "We ran our parallel implementation of Evolution Strategies, described in Algorithm 2, on 51 Atari\n2600 games available in OpenAI Gym [Brockman et al., 2016]. We used the same preprocessing\nand feedforward CNN architecture used by Mnih et al. [2016]. All games were trained for 1 billion\nframes, which requires about the same amount of neural network computation as the published 1-day\nresults for A3C [Mnih et al., 2016] which uses 320 million frames. The difference is due to the\nfact that ES does not perform backpropagation and does not use a value function. By parallelizing\nthe evaluation of perturbed parameters across 720 CPUs on Amazon EC2, we can bring down the\ntime required for the training process to about one hour per game. After training, we compared \ufb01nal\nperformance against the published A3C results and found that ES performed better in 23 games\ntested, while it performed worse in 28. The full results are in Table 2 in the supplementary material.\n4.3"
      },
      {
        "header": "Parallelization",
        "content": "ES is particularly amenable to parallelization because of its low communication bandwidth require-\nment (Section 2.1). We implemented a distributed version of Algorithm 2 to investigate how ES\nscales with the number of workers. Our distributed implementation did not rely on special networking\nsetup and was tested on public cloud computing service Amazon EC2.\nWe picked the 3D Humanoid walking task from OpenAI Gym [Brockman et al., 2016] as the test\nproblem for our scaling experiment, because it is one of the most challenging continuous control\nproblems solvable by state-of-the-art RL techniques, which require about a day to learn on modern\nhardware [Schulman et al., 2015, Duan et al., 2016a]. Solving 3D Humanoid with ES on one 18-\ncore machine takes about 11 hours, which is on par with RL. However, when distributed across 80\n7\n\n\n102\n103\n101\n102\n18 cores, 657 minutes\n1440 cores, 10 minutes"
      },
      {
        "header": "D Humanoid with different number of CPU",
        "content": "cores. Experiments are repeated 7 times and\nmedian time is reported.\nFigure 2: Learning curves for Pong using\nvarying frame-skip parameters. Although per-\nformance is stochastic, each setting leads to\nabout equally fast learning, with each run con-\nverging in around 100 weight updates.\nmachines and 1, 440 CPU cores, ES can solve 3D Humanoid in just 10 minutes, reducing experiment\nturnaround time by two orders of magnitude. Figure 1 shows that, for this task, ES is able to achieve\nlinear speedup in the number of CPU cores.\n4.4"
      },
      {
        "header": "It is common practice in RL to have the agent decide on its actions in a lower frequency than is",
        "content": "used in the simulator that runs the environment. This action frequency, or frame-skip, is a crucial\nparameter in many RL algorithms [Braylan et al., 2005]. If the frame-skip is set too high, the agent\ncannot make its decisions at a \ufb01ne enough timeframe to perform well in the environment. If, on\nthe other hand, the frameskip is set too low, the effective time length of the episode increases too\nmuch, which deteriorates optimization performance as analyzed in section 3.1. An advantage of ES\nis that its gradient estimate is invariant to the length of the episode, which makes it much more robust\nto the action frequency. We demonstrate this by running the Atari game Pong using a frame skip\nparameter in {1, 2, 3, 4}. As can be seen in Figure 2, the learning curves for each setting indeed look\nvery similar."
      },
      {
        "header": "There have been many attempts at applying methods related to ES to train neural networks Risi and",
        "content": "Togelius [2015]. For Atari, Hausknecht et al. [2014] obtain impressive results. Sehnke et al. [2010]\nproposed a method closely related the one investigated in our work. Koutn\u00edk et al. [2013, 2010] and\nSrivastava et al. [2012] have similarly applied an an ES method to RL problems with visual inputs,\nbut where the policy was compressed in a number of different ways. Natural evolution strategies\nhas been successfully applied to black box optimization Wierstra et al. [2008, 2014], as well as for\nthe training of the recurrent weights in recurrent neural networks Schmidhuber et al. [2007]. Stulp\nand Sigaud [2012] explored similar approaches to black box optimization. An interesting hybrid of\nblack-box optimization and policy gradient methods was recently explored by Usunier et al. [2016].\nHyper-Neat Stanley et al. [2009] is an alternative approach to evolving both the weights of the neural\nnetworks and their parameters. Derivative free optimization methods have also been analyzed in the\nconvex setting Duchi et al. [2015], Nesterov [2012]."
      },
      {
        "header": "The main contribution in our work is in showing that this class of algorithms is extremely scalable",
        "content": "and ef\ufb01cient to use on distributed hardware. We have shown that ES, when carefully implemented, is\ncompetitive with competing RL algorithms in terms of performance on the hardest problems solvable\ntoday, and is surprisingly close in terms of data ef\ufb01ciency, while taking less wallclock time to train.\n8"
      },
      {
        "header": "Conclusion",
        "content": "We have explored Evolution Strategies, a class of black-box optimization algorithms, as an alternative\nto popular MDP-based RL techniques such as Q-learning and policy gradients. Experiments on\nAtari and MuJoCo show that it is a viable option with some attractive features: it is invariant to\naction frequency and delayed rewards, and it does not need temporal discounting or value function\napproximation. Most importantly, ES is highly parallelizable, which allows us to make up for a\ndecreased data ef\ufb01ciency by scaling to more parallel workers.\nIn future work, we plan to apply evolution strategies to those problems for which MDP-based\nreinforcement learning is less well-suited: problems with long time horizons and complicated reward\nstructure. We are particularly interested in meta-learning, or learning-to-learn. A proof of concept\nfor meta-learning in an RL setting was given by Duan et al. [2016b]: Using black-box optimization\nwe hope to be able to extend these results. We also plan to examine combining ES with fast low\nprecision neural network implementations to fully make use of the gradient-free nature of ES."
      },
      {
        "header": "References",
        "content": "Alex Braylan, Mark Hollenbeck, Elliot Meyerson, and Risto Miikkulainen. Frame skip is a powerful\nparameter for learning to play atari. Space, 1600:1800, 2005.\nDimo Brockhoff, Anne Auger, Nikolaus Hansen, Dirk V Arnold, and Tim Hohm. Mirrored sampling\nand sequential selection for evolution strategies. In International Conference on Parallel Problem\nSolving from Nature, pages 11\u201321. Springer, 2010.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep\nreinforcement learning for continuous control. In Proceedings of the 33rd International Conference\non Machine Learning (ICML), 2016a.\nYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016b.\nJohn C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for\nzero-order convex optimization: The power of two function evaluations. IEEE Transactions on\nInformation Theory, 61(5):2788\u20132806, 2015.\nJohn Geweke. Antithetic acceleration of monte carlo integration in bayesian inference. Journal of\nEconometrics, 38(1-2):73\u201389, 1988.\nTobias Glasmachers, Tom Schaul, and J\u00fcrgen Schmidhuber. A natural evolution strategy for multi-\nobjective optimization. In International Conference on Parallel Problem Solving from Nature,\npages 627\u2013636. Springer, 2010a.\nTobias Glasmachers, Tom Schaul, Sun Yi, Daan Wierstra, and J\u00fcrgen Schmidhuber. Exponential natu-\nral evolution strategies. In Proceedings of the 12th annual conference on Genetic and evolutionary\ncomputation, pages 393\u2013400. ACM, 2010b.\nNikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution\nstrategies. Evolutionary computation, 9(2):159\u2013195, 2001.\nMatthew Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. A neuroevolution approach\nto general atari game playing. IEEE Transactions on Computational Intelligence and AI in Games,\n6(4):355\u2013366, 2014.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa,\nSarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of\na tensor processing unit. arXiv preprint arXiv:1704.04760, 2017.\n9\n\n\nKenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information\nProcessing Systems, pages 586\u2013594, 2016.\nJan Koutn\u00edk, Faustino Gomez, and J\u00fcrgen Schmidhuber. Evolving neural networks in compressed\nweight space. In Proceedings of the 12th annual conference on Genetic and evolutionary computa-\ntion, pages 619\u2013626. ACM, 2010.\nJan Koutn\u00edk, Giuseppe Cuccu, J\u00fcrgen Schmidhuber, and Faustino Gomez. Evolving large-scale neural\nnetworks for vision-based reinforcement learning. In Proceedings of the 15th annual conference\non Genetic and evolutionary computation, pages 1061\u20131068. ACM, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. In International Conference on Machine Learning, 2016.\nYurii Nesterov. Ef\ufb01ciency of coordinate descent methods on huge-scale optimization problems. SIAM\nJournal on Optimization, 22(2):341\u2013362, 2012.\nYurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.\nFoundations of Computational Mathematics, pages 1\u201340, 2011.\nAndrew Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and\nEric Liang. Autonomous inverted helicopter \ufb02ight via reinforcement learning. Experimental\nRobotics IX, pages 363\u2013372, 2006.\nRonald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in\nneural information processing systems, pages 1043\u20131049, 1998.\nI. Rechenberg and M. Eigen. Evolutionsstrategie: Optimierung Technischer Systeme nach Prinzipien\nder Biologischen Evolution. Frommann-Holzboog Stuttgart, 1973.\nSebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open challenges.\nIEEE Transactions on Computational Intelligence and AI in Games, 2015.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. In Advances in Neural Information Processing Systems,\npages 2226\u20132234, 2016.\nTom Schaul, Tobias Glasmachers, and J\u00fcrgen Schmidhuber. High dimensions and heavy tails\nfor natural evolution strategies. In Proceedings of the 13th annual conference on Genetic and\nevolutionary computation, pages 845\u2013852. ACM, 2011.\nJuergen Schmidhuber and Jieyu Zhao. Direct policy search and uncertain policy evaluation. In Aaai\nspring symposium on search under uncertain and incomplete information, stanford univ, pages\n119\u2013124, 1998.\nJ\u00fcrgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, and Faustino Gomez. Training recurrent\nnetworks by evolino. Neural computation, 19(3):757\u2013779, 2007.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region\npolicy optimization. In ICML, pages 1889\u20131897, 2015.\nH.-P. Schwefel. Numerische optimierung von computer-modellen mittels der evolutionsstrategie.\n1977.\nFrank Sehnke, Christian Osendorfer, Thomas R\u00fcckstie\u00df, Alex Graves, Jan Peters, and J\u00fcrgen Schmid-\nhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551\u2013559, 2010.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.\n10\n\n\nJames C Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient\napproximation. IEEE transactions on automatic control, 37(3):332\u2013341, 1992.\nRupesh Kumar Srivastava, J\u00fcrgen Schmidhuber, and Faustino Gomez. Generalized compressed\nnetwork search. In International Conference on Parallel Problem Solving from Nature, pages\n337\u2013346. Springer, 2012.\nKenneth O Stanley, David B D\u2019Ambrosio, and Jason Gauci. A hypercube-based encoding for evolving\nlarge-scale neural networks. Arti\ufb01cial life, 15(2):185\u2013212, 2009.\nFreek Stulp and Olivier Sigaud. Policy improvement methods: Between black-box optimization and\nepisodic reinforcement learning. 2012.\nYi Sun, Daan Wierstra, Tom Schaul, and Juergen Schmidhuber. Ef\ufb01cient natural evolution strategies.\nIn Proceedings of the 11th Annual conference on Genetic and evolutionary computation, pages\n539\u2013546. ACM, 2009.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages\n5026\u20135033. IEEE, 2012.\nNicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic exploration for\ndeep deterministic policies: An application to starcraft micromanagement tasks. arXiv preprint\narXiv:1609.02993, 2016.\nSjoerd van Steenkiste, Jan Koutn\u00edk, Kurt Driessens, and J\u00fcrgen Schmidhuber. A wavelet-based\nencoding for neuroevolution. In Proceedings of the 2016 on Genetic and Evolutionary Computation\nConference, pages 517\u2013524. ACM, 2016.\nDaan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In\nEvolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelli-\ngence). IEEE Congress on, pages 3381\u20133387. IEEE, 2008.\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J\u00fcrgen Schmidhuber.\nNatural evolution strategies. Journal of Machine Learning Research, 15(1):949\u2013980, 2014.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229\u2013256, 1992.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov,\nRichard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation\nwith visual attention. In ICML, volume 14, pages 77\u201381, 2015.\nSun Yi, Daan Wierstra, Tom Schaul, and J\u00fcrgen Schmidhuber. Stochastic search using the natural\ngradient. In Proceedings of the 26th Annual International Conference on Machine Learning, pages\n1161\u20131168. ACM, 2009."
      },
      {
        "header": "Zaxxon",
        "content": "831.0\n2659.0\n3000.0\n6380.0\n5.6\nTable 2: Final results obtained using Evolution Strategies on Atari 2600 games (feedforward CNN\npolicy, deterministic policy evaluation, averaged over 10 re-runs with up to 30 random initial no-ops),\nand compared to results for DQN and A3C from Mnih et al. [2016] and HyperNEAT from Hausknecht\net al. [2014]. A2C is our synchronous variant of A3C, and its reported scores are obtained with 320M\ntraining frames with the same evaluation setup as for the ES results. All methods were trained on raw\npixel input.\n12\n\n\nTable 3: MuJoCo tasks: Ratio of ES timesteps to TRPO timesteps needed to reach various percentages of TRPO\u2019s learning progress at 5 million timesteps. These\nresults were computed from ES learning curves averaged over 6 reruns."
      },
      {
        "header": "HalfCheetah",
        "content": "25%\n-1.35\n9.05e+05\n1.36e+05\n0.15\n50%\n793.55\n1.70e+06\n8.28e+05\n0.49\n75%\n1589.83\n3.34e+06\n1.42e+06\n0.42\n100%\n2385.79\n5.00e+06\n2.88e+06\n0.58"
      },
      {
        "header": "Hopper",
        "content": "25%\n877.45\n7.29e+05\n3.83e+05\n0.53\n50%\n1718.16\n1.03e+06\n3.73e+06\n3.64\n75%\n2561.11\n1.59e+06\n9.63e+06\n6.05\n100%\n3403.46\n4.56e+06\n3.16e+07\n6.94"
      },
      {
        "header": "InvertedDoublePendulum",
        "content": "25%\n2358.98\n8.73e+05\n3.98e+05\n0.46\n50%\n4609.68\n9.65e+05\n4.66e+05\n0.48\n75%\n6874.03\n1.07e+06\n5.30e+05\n0.49\n100%\n9104.07\n4.39e+06\n5.39e+06\n1.23"
      },
      {
        "header": "InvertedPendulum",
        "content": "25%\n276.59\n2.21e+05\n6.25e+04\n0.28\n50%\n519.15\n2.73e+05\n1.43e+05\n0.52\n75%\n753.17\n3.25e+05\n2.55e+05\n0.78\n100%\n1000.00\n5.17e+05\n4.55e+05\n0.88"
      },
      {
        "header": "Swimmer",
        "content": "25%\n41.97\n1.04e+06\n5.88e+05\n0.56\n50%\n70.73\n1.82e+06\n8.52e+05\n0.47\n75%\n99.68\n2.33e+06\n1.23e+06\n0.53\n100%\n128.25\n4.59e+06\n1.39e+06\n0.30\nWalker2d\n25%\n957.68\n1.55e+06\n6.43e+05\n0.41\n50%\n1916.48\n2.27e+06\n1.29e+07\n5.69\n75%\n2872.81\n2.89e+06\n2.31e+07\n8.02\n100%\n3830.03\n4.81e+06\n3.79e+07\n7.88\n13"
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
      "author": "Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, Ilya Sutskever",
      "subject": "",
      "keywords": "",
      "creator": "LaTeX with hyperref package",
      "producer": "pdfTeX-1.40.17",
      "creationDate": "D:20170911000624Z",
      "modDate": "D:20170911000624Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 13,
    "pages": [
      "Evolution Strategies as a\nScalable Alternative to Reinforcement Learning\nTim Salimans\nJonathan Ho\nXi Chen\nSzymon Sidor\nIlya Sutskever\nOpenAI\nAbstract\nWe explore the use of Evolution Strategies (ES), a class of black box optimization\nalgorithms, as an alternative to popular MDP-based RL techniques such as Q-\nlearning and Policy Gradients. Experiments on MuJoCo and Atari show that ES\nis a viable solution strategy that scales extremely well with the number of CPUs\navailable: By using a novel communication strategy based on common random\nnumbers, our ES implementation only needs to communicate scalars, making it\npossible to scale to over a thousand parallel workers. This allows us to solve 3D\nhumanoid walking in 10 minutes and obtain competitive results on most Atari\ngames after one hour of training. In addition, we highlight several advantages of\nES as a black box optimization technique: it is invariant to action frequency and\ndelayed rewards, tolerant of extremely long horizons, and does not need temporal\ndiscounting or value function approximation.\n1\nIntroduction\nDeveloping agents that can accomplish challenging tasks in complex, uncertain environments is a key\ngoal of arti\ufb01cial intelligence. Recently, the most popular paradigm for analyzing such problems has\nbeen using a class of reinforcement learning (RL) algorithms based on the Markov Decision Process\n(MDP) formalism and the concept of value functions. Successes of this approach include systems\nthat learn to play Atari from pixels [Mnih et al., 2015], perform helicopter aerobatics Ng et al. [2006],\nor play expert-level Go [Silver et al., 2016].\nAn alternative approach to solving RL problems is using black-box optimization. This approach\nis known as direct policy search [Schmidhuber and Zhao, 1998], or neuro-evolution [Risi and\nTogelius, 2015], when applied to neural networks. In this paper, we study Evolution Strategies (ES)\n[Rechenberg and Eigen, 1973], a particular set of optimization algorithms in this class. We show\nthat ES can reliably train neural network policies, in a fashion well suited to be scaled up to modern\ndistributed computer systems, for controlling robots in the MuJoCo physics simulator [Todorov et al.,\n2012] and playing Atari games with pixel inputs [Mnih et al., 2015]. Our key \ufb01ndings are as follows:\n1. We found that the use of virtual batch normalization [Salimans et al., 2016] and other\nreparameterizations of the neural network policy (section 2.2) greatly improve the reliability\nof evolution strategies. Without these methods ES proved brittle in our experiments, but with\nthese reparameterizations we achieved strong results over a wide variety of environments.\n2. We found the evolution strategies method to be highly parallelizable: by introducing a novel\ncommunication strategy based on common random numbers, we are able to achieve linear\nspeedups in run time even when using over a thousand workers. In particular, using 1,440\nworkers, we have been able to solve the MuJoCo 3D humanoid task in under 10 minutes.\n3. The data ef\ufb01ciency of evolution strategies was surprisingly good: we were able to match\nthe \ufb01nal performance of A3C [Mnih et al., 2016] on most Atari environments while using\nbetween 3x and 10x as much data. The slight decrease in data ef\ufb01ciency is partly offset by a\narXiv:1703.03864v2  [stat.ML]  7 Sep 2017\n",
      "reduction in required computation of roughly 3x due to not performing backpropagation\nand not having a value function. Our 1-hour ES results require about the same amount of\ncomputation as the published 1-day results for A3C, while performing better on 23 games\ntested, and worse on 28. On MuJoCo tasks, we were able to match the learned policy\nperformance of Trust Region Policy Optimization [TRPO; Schulman et al., 2015], using no\nmore than 10x as much data.\n4. We found that ES exhibited better exploration behaviour than policy gradient methods like\nTRPO: on the MuJoCo humanoid task, ES has been able to learn a very wide variety of gaits\n(such as walking sideways or walking backwards). These unusual gaits are never observed\nwith TRPO, which suggests a qualitatively different exploration behavior.\n5. We found the evolution strategies method to be robust: we achieved the aforementioned\nresults using \ufb01xed hyperparameters for all the Atari environments, and a different set of\n\ufb01xed hyperparameters for all MuJoCo environments (with the exception of one binary hyper-\nparameter, which has not been held constant between the different MuJoCo environments).\nBlack-box optimization methods have several highly attractive properties: indifference to the distribu-\ntion of rewards (sparse or dense), no need for backpropagating gradients, and tolerance of potentially\narbitrarily long time horizons. However, they are perceived as less effective at solving hard RL\nproblems compared to techniques like Q-learning and policy gradients. The contribution of this work,\nwhich we hope will renew interest in this class of methods and lead to new useful applications, is\na demonstration that evolution strategies can be competitive with competing RL algorithms on the\nhardest environments studied by the deep RL community today, and that this approach can scale to\nmany more parallel workers.\n2\nEvolution Strategies\nEvolution Strategies (ES) is a class of black box optimization algorithms [Rechenberg and Eigen,\n1973, Schwefel, 1977] that are heuristic search procedures inspired by natural evolution: At every\niteration (\u201cgeneration\u201d), a population of parameter vectors (\u201cgenotypes\u201d) is perturbed (\u201cmutated\u201d)\nand their objective function value (\u201c\ufb01tness\u201d) is evaluated. The highest scoring parameter vectors are\nthen recombined to form the population for the next generation, and this procedure is iterated until the\nobjective is fully optimized. Algorithms in this class differ in how they represent the population and\nhow they perform mutation and recombination. The most widely known member of the ES class is\nthe covariance matrix adaptation evolution strategy [CMA-ES; Hansen and Ostermeier, 2001], which\nrepresents the population by a full-covariance multivariate Gaussian. CMA-ES has been extremely\nsuccessful in solving optimization problems in low to medium dimension.\nThe version of ES we use in this work belongs to the class of natural evolution strategies (NES)\n[Wierstra et al., 2008, 2014, Yi et al., 2009, Sun et al., 2009, Glasmachers et al., 2010a,b, Schaul et al.,\n2011] and is closely related to the work of Sehnke et al. [2010]. Let F denote the objective function\nacting on parameters \u03b8. NES algorithms represent the population with a distribution over parameters\np\u03c8(\u03b8)\u2014itself parameterized by \u03c8\u2014and proceed to maximize the average objective value E\u03b8\u223cp\u03c8F(\u03b8)\nover the population by searching for \u03c8 with stochastic gradient ascent. Speci\ufb01cally, using the score\nfunction estimator for \u2207\u03c8E\u03b8\u223cp\u03c8F(\u03b8) in a fashion similar to REINFORCE [Williams, 1992], NES\nalgorithms take gradient steps on \u03c8 with the following estimator:\n\u2207\u03c8E\u03b8\u223cp\u03c8F(\u03b8) = E\u03b8\u223cp\u03c8 {F(\u03b8)\u2207\u03c8 log p\u03c8(\u03b8)}\nFor the special case where p\u03c8 is factored Gaussian (as in this work), the resulting gradient estimator\nis also known as simultaneous perturbation stochastic approximation [Spall, 1992], parameter-\nexploring policy gradients [Sehnke et al., 2010], or zero-order gradient estimation [Nesterov and\nSpokoiny, 2011].\nIn this work, we focus on RL problems, so F(\u00b7) will be the stochastic return provided by an\nenvironment, and \u03b8 will be the parameters of a deterministic or stochastic policy \u03c0\u03b8 describing an\nagent acting in that environment, controlled by either discrete or continuous actions. Much of the\ninnovation in RL algorithms is focused on coping with the lack of access to or existence of derivatives\nof the environment or policy. Such non-smoothness can be addressed with ES as follows. We\ninstantiate the population distribution p\u03c8 as an isotropic multivariate Gaussian with mean \u03c8 and \ufb01xed\ncovariance \u03c32I, allowing us to write E\u03b8\u223cp\u03c8F(\u03b8) in terms of a mean parameter vector \u03b8 directly: we\n2\n",
      "set E\u03b8\u223cp\u03c8F(\u03b8) = E\u03f5\u223cN(0,I) F(\u03b8 + \u03c3\u03f5). With this setup, our stochastic objective can be viewed as\na Gaussian-blurred version of the original objective F, free of non-smoothness introduced by the\nenvironment or potentially discrete actions taken by the policy. Further discussion on how ES and\npolicy gradient methods cope with non-smoothness can be found in section 3.\nWith our objective de\ufb01ned in terms of \u03b8, we optimize over \u03b8 directly using stochastic gradient ascent\nwith the score function estimator:\n\u2207\u03b8 E\u03f5\u223cN(0,I) F(\u03b8 + \u03c3\u03f5) = 1\n\u03c3 E\u03f5\u223cN(0,I) {F(\u03b8 + \u03c3\u03f5) \u03f5}\nwhich can be approximated with samples. The resulting algorithm (1) repeatedly executes two phases:\n1) Stochastically perturbing the parameters of the policy and evaluating the resulting parameters by\nrunning an episode in the environment, and 2) Combining the results of these episodes, calculating a\nstochastic gradient estimate, and updating the parameters.\nAlgorithm 1 Evolution Strategies\n1: Input: Learning rate \u03b1, noise standard deviation \u03c3, initial policy parameters \u03b80\n2: for t = 0, 1, 2, . . . do\n3:\nSample \u03f51, . . . \u03f5n \u223cN(0, I)\n4:\nCompute returns Fi = F(\u03b8t + \u03c3\u03f5i) for i = 1, . . . , n\n5:\nSet \u03b8t+1 \u2190\u03b8t + \u03b1 1\nn\u03c3\nPn\ni=1 Fi\u03f5i\n6: end for\n2.1\nScaling and parallelizing ES\nES is well suited to be scaled up to many parallel workers: 1) It operates on complete episodes, thereby\nrequiring only infrequent communication between workers. 2) The only information obtained by each\nworker is the scalar return of an episode: if we synchronize random seeds between workers before\noptimization, each worker knows what perturbations the other workers used, so each worker only\nneeds to communicate a single scalar to and from each other worker to agree on a parameter update.\nES thus requires extremely low bandwidth, in sharp contrast to policy gradient methods, which\nrequire workers to communicate entire gradients. 3) It does not require value function approximations.\nRL with value function estimation is inherently sequential: To improve upon a given policy, multiple\nupdates to the value function are typically needed to get enough signal. Each time the policy is\nsigni\ufb01cantly changed, multiple iterations are necessary for the value function estimate to catch up.\nA simple parallel version of ES is given in Algorithm 2. The main novelty here is that the algo-\nrithm makes use of shared random seeds, which drastically reduces the bandwidth required for\ncommunication between the workers.\nAlgorithm 2 Parallelized Evolution Strategies\n1: Input: Learning rate \u03b1, noise standard deviation \u03c3, initial policy parameters \u03b80\n2: Initialize: n workers with known random seeds, and initial parameters \u03b80\n3: for t = 0, 1, 2, . . . do\n4:\nfor each worker i = 1, . . . , n do\n5:\nSample \u03f5i \u223cN(0, I)\n6:\nCompute returns Fi = F(\u03b8t + \u03c3\u03f5i)\n7:\nend for\n8:\nSend all scalar returns Fi from each worker to every other worker\n9:\nfor each worker i = 1, . . . , n do\n10:\nReconstruct all perturbations \u03f5j for j = 1, . . . , n using known random seeds\n11:\nSet \u03b8t+1 \u2190\u03b8t + \u03b1 1\nn\u03c3\nPn\nj=1 Fj\u03f5j\n12:\nend for\n13: end for\nIn practice, we implement sampling by having each worker instantiate a large block of Gaussian\nnoise at the start of training, and then perturbing its parameters by adding a randomly indexed subset\nof these noise variables at each iteration. Although this means that the perturbations are not strictly\n3\n",
      "independent across iterations, we did not \ufb01nd this to be a problem in practice. Using this strategy,\nwe \ufb01nd that the second part of Algorithm 2 (lines 9-12) only takes up a small fraction of total time\nspend for all our experiments, even when using up to 1,440 parallel workers. When using many more\nworkers still, or when using very large neural networks, we can reduce the computation required for\nthis part of the algorithm by having workers only perturb a subset of the parameters \u03b8 rather than\nall of them: In this case the perturbation distribution p\u03c8 corresponds to a mixture of Gaussians, for\nwhich the update equations remain unchanged. At the very extreme, every worker would perturb\nonly a single coordinate of the parameter vector, which means that we would be using pure \ufb01nite\ndifferences.\nTo reduce variance, we use antithetic sampling Geweke [1988], also known as mirrored sampling\nBrockhoff et al. [2010] in the ES literature: that is, we always evaluate pairs of perturbations \u03f5, \u2212\u03f5,\nfor Gaussian noise vector \u03f5. We also \ufb01nd it useful to perform \ufb01tness shaping Wierstra et al. [2014] by\napplying a rank transformation to the returns before computing each parameter update. Doing so\nremoves the in\ufb02uence of outlier individuals in each population and decreases the tendency for ES to\nfall into local optima early in training. In addition, we apply weight decay to the parameters of our\npolicy network: this prevents the parameters from growing very large compared to the perturbations.\nUnlike Wierstra et al. [2014] we did not see bene\ufb01t from adapting \u03c3 during training, and we therefore\ntreat it as a \ufb01xed hyperparameter instead. We perform the optimization directly in parameter space;\nexploring indirect encodings Stanley et al. [2009], van Steenkiste et al. [2016] is left for future work.\nEvolution Strategies, as presented above, works with full-length episodes. In some rare cases this\ncan lead to low CPU utilization, as some episodes run for many more steps than others. For this\nreason, we cap episode length at a constant m steps for all workers, which we dynamically adjust as\ntraining progresses. For example, by setting m to be equal to twice the mean number of steps taken\nper episode, we can guarantee that CPU utilization stays above 50% in the worst case.\n2.2\nThe impact of network parameterization\nWhereas RL algorithms like Q-learning and policy gradients explore by sampling actions from a\nstochastic policy, Evolution Strategies derives learning signal from sampling instantiations of policy\nparameters. Exploration in ES is thus driven by parameter perturbation. For ES to improve upon\nparameters \u03b8, some members of the population must achieve better return than others: i.e. it is crucial\nthat Gaussian perturbation vectors \u03f5 occasionally lead to new individuals \u03b8 + \u03c3\u03f5 with better return.\nFor the Atari environments, we found that Gaussian parameter perturbations on DeepMind\u2019s con-\nvolutional architectures [Mnih et al., 2015] did not always lead to adequate exploration: For some\nenvironments, randomly perturbed parameters tended to encode policies that always took one speci\ufb01c\naction regardless of the state that was given as input. However, we discovered thatwe could match the\nperformance of policy gradient methods for most games by using virtual batch normalization [Sali-\nmans et al., 2016] in the policy speci\ufb01cation. Virtual batch normalization is precisely equivalent to\nbatch normalization [Ioffe and Szegedy, 2015] where the minibatch used for calculating normalizing\nstatistics is chosen at the start of training and is \ufb01xed. This change in parameterization makes the\npolicy more sensitive to very small changes in the input image at the early stages of training when the\nweights of the policy are random, ensuring that the policy takes a wide-enough variety of actions\nto gather occasional rewards. For most applications, a downside of virtual batch normalization is\nthat it makes training more expensive. For our application, however, the minibatch used to calculate\nthe normalizing statistics is much smaller than the number of steps taken during a typical episode,\nmeaning that the overhead is negligible.\nFor the MuJoCo tasks, we achieved good performance on nearly all the environments with the\nstandard multilayer perceptrons mapping to continuous actions. However, we observed that for some\nenvironments, we could encourage more exploration by discretizing the actions. This forced the\nactions to be non-smooth with respect to input observations and parameter perturbations, and thereby\nencouraged a wide variety of behaviors to be played out over the course of rollouts.\n3\nSmoothing in parameter space versus smoothing in action space\nAs mentioned in section 2, a large source of dif\ufb01culty in RL stems from the lack of informative\ngradients of policy performance: such gradients may not exist due to non-smoothness of the environ-\n4\n",
      "ment or policy, or may only be available as high-variance estimates because the environment usually\ncan only be accessed via sampling. Explicitly, suppose we wish to solve general decision problems\nthat give a return R(a) after we take a sequence of actions a = {a1, . . . , aT }, where the actions are\ndetermined by a either a deterministic or a stochastic policy function at = \u03c0(s; \u03b8). The objective we\nwould like to optimize is thus\nF(\u03b8) = R(a(\u03b8)).\nSince the actions are allowed to be discrete and the policy is allowed to be deterministic, F(\u03b8)\ncan be non-smooth in \u03b8. More importantly, because we do not have explicit access to the under-\nlying state transition function of our decision problems, the gradients cannot be computed with\na backpropagation-like algorithm. This means we cannot directly use standard gradient-based\noptimization methods to \ufb01nd a good solution for \u03b8.\nIn order to both make the problem smooth and to have a means of to estimate its gradients, we need\nto add noise. Policy gradient methods add the noise in action space, which is done by sampling the\nactions from an appropriate distribution. For example, if the actions are discrete and \u03c0(s; \u03b8) calculates\na score for each action before selecting the best one, then we would sample an action a(\u03f5, \u03b8) (here \u03f5 is\nthe noise source) from a categorical distribution over actions at each time period, applying a softmax\nto the scores of each action. Doing so yields the objective FP G(\u03b8) = E\u03f5 R(a(\u03f5, \u03b8)), with gradients\n\u2207\u03b8FP G(\u03b8) = E\u03f5 {R(a(\u03f5, \u03b8))\u2207\u03b8 log p(a(\u03f5, \u03b8); \u03b8)} .\nEvolution strategies, on the other hand, add the noise in parameter space. That is, they perturb the\nparameters as \u02dc\u03b8 = \u03b8 + \u03be, with \u03be from a multivariate Gaussian distribution, and then pick actions\nas at = a(\u03be, \u03b8) = \u03c0(s; \u02dc\u03b8). It can be interpreted as adding a Gaussian blur to the original objective,\nwhich results in a smooth, differentiable cost FES(\u03b8) = E\u03be R(a(\u03be, \u03b8)), this time with gradients\n\u2207\u03b8FES(\u03b8) = E\u03be\nn\nR(a(\u03be, \u03b8))\u2207\u03b8 log p(\u02dc\u03b8(\u03be, \u03b8); \u03b8)\no\n.\nThe two methods for smoothing the decision problem are thus quite similar, and can be made even\nmore so by adding noise to both the parameters and the actions.\n3.1\nWhen is ES better than policy gradients?\nGiven these two methods of smoothing the decision problem, which should we use? The answer\ndepends strongly on the structure of the decision problem and on which type of Monte Carlo\nestimator is used to estimate the gradients \u2207\u03b8FP G(\u03b8) and \u2207\u03b8FES(\u03b8). Suppose the correlation\nbetween the return and the individual actions is low (as is true for any hard RL problem). Assuming\nwe approximate these gradients using simple Monte Carlo (REINFORCE) with a good baseline on\nthe return, we have\nVar[\u2207\u03b8FP G(\u03b8)] \u2248Var[R(a)] Var[\u2207\u03b8 log p(a; \u03b8)],\nVar[\u2207\u03b8FES(\u03b8)] \u2248Var[R(a)] Var[\u2207\u03b8 log p(\u02dc\u03b8; \u03b8)].\nIf both methods perform a similar amount of exploration, Var[R(a)] will be similar for both ex-\npressions. The difference will thus be in the second term. Here we have that \u2207\u03b8 log p(a; \u03b8) =\nPT\nt=1 \u2207\u03b8 log p(at; \u03b8) is a sum of T uncorrelated terms, so that the variance of the policy gradi-\nent estimator will grow nearly linearly with T. The corresponding term for evolution strategies,\n\u2207\u03b8 log p(\u02dc\u03b8; \u03b8), is independent of T. Evolution strategies will thus have an advantage compared to\npolicy gradients for long episodes with very many time steps. In practice, the effective number of\nsteps T is often reduced in policy gradient methods by discounting rewards. If the effects of actions\nare short-lasting, this allows us to dramatically reduce the variance in our gradient estimate, and\nthis has been critical to the success of applications such as Atari games. However, this discounting\nwill bias our gradient estimate if actions have long lasting effects. Another strategy for reducing the\neffective value of T is to use value function approximation. This has also been effective, but once\nagain runs the risk of biasing our gradient estimates. Evolution strategies is thus an attractive choice\nif the effective number of time steps T is long, actions have long-lasting effects, and if no good value\nfunction estimates are available.\n5\n",
      "3.2\nProblem dimensionality\nThe gradient estimate of ES can be interpreted as a method for randomized \ufb01nite differences in\nhigh-dimensional space. Indeed, using the fact that E\u03f5\u223cN(0,I) {F(\u03b8) \u03f5/\u03c3} = 0, we get\n\u2207\u03b8\u03b7(\u03b8) = E\u03f5\u223cN(0,I) {F(\u03b8 + \u03c3\u03f5) \u03f5/\u03c3} = E\u03f5\u223cN(0,I) {(F(\u03b8 + \u03c3\u03f5) \u2212F(\u03b8)) \u03f5/\u03c3}\nIt is now apparent that ES can be seen as computing a \ufb01nite difference derivative estimate in\na randomly chosen direction, especially as \u03c3 becomes small. The resemblance of ES to \ufb01nite\ndifferences suggests the method will scale poorly with the dimension of the parameters \u03b8. Theoretical\nanalysis indeed shows that for general non-smooth optimization problems, the required number of\noptimization steps scales linearly with the dimension [Nesterov and Spokoiny, 2011]. However, it\nis important to note that this does not mean that larger neural networks will perform worse than\nsmaller networks when optimized using ES: what matters is the dif\ufb01culty, or intrinsic dimension, of\nthe optimization problem. To see that the dimensionality of our model can be completely separate\nfrom the effective dimension of the optimization problem, consider a regression problem where we\napproximate a univariate variable y with a linear model \u02c6y = x \u00b7 w: if we double the number of\nfeatures and parameters in this model by concatenating x with itself (i.e. using features x\u2032 = (x, x)),\nthe problem does not become more dif\ufb01cult. The ES algorithm will do exactly the same thing when\napplied to this higher dimensional problem, as long as we divide the standard deviation of the noise\nby two, as well as the learning rate.\nIn practice, we observe slightly better results when using larger networks with ES. For example, we\ntried both the larger network and smaller network used in A3C [Mnih et al., 2016] for learning Atari\n2600 games, and on average obtained better results using the larger network. We hypothesize that this\nis due to the same effect that makes standard gradient-based optimization of large neural networks\neasier than for small ones: large networks have fewer local minima [Kawaguchi, 2016].\n3.3\nAdvantages of not calculating gradients\nIn addition to being easy to parallelize, and to having an advantage in cases with long action sequences\nand delayed rewards, black box optimization algorithms like ES have other advantages over RL\ntechniques that calculate gradients. The communication overhead of implementing ES in a distributed\nsetting is lower than for competing RL methods such as policy gradients and Q-learning, as the only\ninformation that needs to be communicated across processes are the scalar return and the random\nseed that was used to generate the perturbations \u03f5, rather than a full gradient. Also, ES can deal with\nmaximally sparse and delayed rewards; only the total return of an episode is used, whereas other\nmethods use individual rewards and their exact timing.\nBy not requiring backpropagation, black box optimizers reduce the amount of computation per\nepisode by about two thirds, and memory by potentially much more. In addition, not explicitly\ncalculating an analytical gradient protects against problems with exploding gradients that are common\nwhen working with recurrent neural networks. By smoothing the cost function in parameter space, we\nreduce the pathological curvature that causes these problems: bounded cost functions that are smooth\nenough can\u2019t have exploding gradients. At the extreme, ES allows us to incorporate non-differentiable\nelements into our architecture, such as modules that use hard attention [Xu et al., 2015].\nBlack box optimization methods are uniquely suited to low precision hardware for deep learning.\nLow precision arithmetic, such as in binary neural networks, can be performed much cheaper than\nat high precision. When optimizing such low precision architectures, biased low precision gradient\nestimates can be a problem when using gradient-based methods. Similarly, specialized hardware for\nneural network inference, such as TPUs [Jouppi et al., 2017], can be used directly when performing\noptimization using ES, while their limited memory usually makes backpropagation impossible.\nBy perturbing in parameter space instead of action space, black box optimizers are naturally invariant\nto the frequency at which our agent acts in the environment. For MDP-based reinforcement learning\nalgorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for\nthe optimization to succeed [Braylan et al., 2005]. While this is usually a solvable problem for games\nthat only require short-term planning and action, it is a problem for learning longer term strategic\nbehavior. For these problems, RL needs hierarchy to succeed [Parr and Russell, 1998], which is not\nas necessary when using black box optimization.\n6\n",
      "4\nExperiments\n4.1\nMuJoCo\nWe evaluated ES on a benchmark of continuous robotic control problems in the OpenAI Gym\n[Brockman et al., 2016] against a highly tuned implementation of Trust Region Policy Optimiza-\ntion [Schulman et al., 2015], a policy gradient algorithm designed to ef\ufb01ciently optimize neural\nnetwork policies. We tested on both classic problems, like balancing an inverted pendulum, and more\ndif\ufb01cult ones found in recent literature, like learning 2D hopping and walking gaits. The environments\nwere simulated by MuJoCo [Todorov et al., 2012].\nWe used both ES and TRPO to train policies with identical architectures: multilayer perceptrons with\ntwo 64-unit hidden layers separated by tanh nonlinearities. We found that ES occasionally bene\ufb01ted\nfrom discrete actions, since continuous actions could be too smooth with respect to parameter\nperturbation and could hamper exploration (see section 2.2). For the hopping and swimming tasks,\nwe discretized the actions for ES into 10 bins for each action component.\nWe found that ES was able to solve these tasks up to TRPO\u2019s \ufb01nal performance after 5 million\ntimesteps of environment interaction. To obtain this result, we ran ES over 6 random seeds and\ncompared the mean learning curves to similarly computed curves for TRPO. The exact sample\ncomplexity tradeoffs over the course of learning are listed in Table 1, and detailed results are listed\nin Table 3 of the supplement. Generally, we were able to solve the environments in less than 10x\npenalty in sample complexity on the hard environments (Hopper and Walker2d) compared to TRPO.\nOn simple environments, we achieved up to 3x better sample complexity than TRPO.\nTable 1: MuJoCo tasks: Ratio of ES timesteps to TRPO timesteps needed to reach various percentages\nof TRPO\u2019s learning progress at 5 million timesteps.\nEnvironment\n25%\n50%\n75%\n100%\nHalfCheetah\n0.15\n0.49\n0.42\n0.58\nHopper\n0.53\n3.64\n6.05\n6.94\nInvertedDoublePendulum\n0.46\n0.48\n0.49\n1.23\nInvertedPendulum\n0.28\n0.52\n0.78\n0.88\nSwimmer\n0.56\n0.47\n0.53\n0.30\nWalker2d\n0.41\n5.69\n8.02\n7.88\n4.2\nAtari\nWe ran our parallel implementation of Evolution Strategies, described in Algorithm 2, on 51 Atari\n2600 games available in OpenAI Gym [Brockman et al., 2016]. We used the same preprocessing\nand feedforward CNN architecture used by Mnih et al. [2016]. All games were trained for 1 billion\nframes, which requires about the same amount of neural network computation as the published 1-day\nresults for A3C [Mnih et al., 2016] which uses 320 million frames. The difference is due to the\nfact that ES does not perform backpropagation and does not use a value function. By parallelizing\nthe evaluation of perturbed parameters across 720 CPUs on Amazon EC2, we can bring down the\ntime required for the training process to about one hour per game. After training, we compared \ufb01nal\nperformance against the published A3C results and found that ES performed better in 23 games\ntested, while it performed worse in 28. The full results are in Table 2 in the supplementary material.\n4.3\nParallelization\nES is particularly amenable to parallelization because of its low communication bandwidth require-\nment (Section 2.1). We implemented a distributed version of Algorithm 2 to investigate how ES\nscales with the number of workers. Our distributed implementation did not rely on special networking\nsetup and was tested on public cloud computing service Amazon EC2.\nWe picked the 3D Humanoid walking task from OpenAI Gym [Brockman et al., 2016] as the test\nproblem for our scaling experiment, because it is one of the most challenging continuous control\nproblems solvable by state-of-the-art RL techniques, which require about a day to learn on modern\nhardware [Schulman et al., 2015, Duan et al., 2016a]. Solving 3D Humanoid with ES on one 18-\ncore machine takes about 11 hours, which is on par with RL. However, when distributed across 80\n7\n",
      "102\n103\n101\n102\n18 cores, 657 minutes\n1440 cores, 10 minutes\nNumber of CPU cores\nMedian time to solve (minutes)\nFigure 1: Time to reach a score of 6000 on\n3D Humanoid with different number of CPU\ncores. Experiments are repeated 7 times and\nmedian time is reported.\nFigure 2: Learning curves for Pong using\nvarying frame-skip parameters. Although per-\nformance is stochastic, each setting leads to\nabout equally fast learning, with each run con-\nverging in around 100 weight updates.\nmachines and 1, 440 CPU cores, ES can solve 3D Humanoid in just 10 minutes, reducing experiment\nturnaround time by two orders of magnitude. Figure 1 shows that, for this task, ES is able to achieve\nlinear speedup in the number of CPU cores.\n4.4\nInvariance to temporal resolution\nIt is common practice in RL to have the agent decide on its actions in a lower frequency than is\nused in the simulator that runs the environment. This action frequency, or frame-skip, is a crucial\nparameter in many RL algorithms [Braylan et al., 2005]. If the frame-skip is set too high, the agent\ncannot make its decisions at a \ufb01ne enough timeframe to perform well in the environment. If, on\nthe other hand, the frameskip is set too low, the effective time length of the episode increases too\nmuch, which deteriorates optimization performance as analyzed in section 3.1. An advantage of ES\nis that its gradient estimate is invariant to the length of the episode, which makes it much more robust\nto the action frequency. We demonstrate this by running the Atari game Pong using a frame skip\nparameter in {1, 2, 3, 4}. As can be seen in Figure 2, the learning curves for each setting indeed look\nvery similar.\n5\nRelated work\nThere have been many attempts at applying methods related to ES to train neural networks Risi and\nTogelius [2015]. For Atari, Hausknecht et al. [2014] obtain impressive results. Sehnke et al. [2010]\nproposed a method closely related the one investigated in our work. Koutn\u00edk et al. [2013, 2010] and\nSrivastava et al. [2012] have similarly applied an an ES method to RL problems with visual inputs,\nbut where the policy was compressed in a number of different ways. Natural evolution strategies\nhas been successfully applied to black box optimization Wierstra et al. [2008, 2014], as well as for\nthe training of the recurrent weights in recurrent neural networks Schmidhuber et al. [2007]. Stulp\nand Sigaud [2012] explored similar approaches to black box optimization. An interesting hybrid of\nblack-box optimization and policy gradient methods was recently explored by Usunier et al. [2016].\nHyper-Neat Stanley et al. [2009] is an alternative approach to evolving both the weights of the neural\nnetworks and their parameters. Derivative free optimization methods have also been analyzed in the\nconvex setting Duchi et al. [2015], Nesterov [2012].\nThe main contribution in our work is in showing that this class of algorithms is extremely scalable\nand ef\ufb01cient to use on distributed hardware. We have shown that ES, when carefully implemented, is\ncompetitive with competing RL algorithms in terms of performance on the hardest problems solvable\ntoday, and is surprisingly close in terms of data ef\ufb01ciency, while taking less wallclock time to train.\n8\n",
      "6\nConclusion\nWe have explored Evolution Strategies, a class of black-box optimization algorithms, as an alternative\nto popular MDP-based RL techniques such as Q-learning and policy gradients. Experiments on\nAtari and MuJoCo show that it is a viable option with some attractive features: it is invariant to\naction frequency and delayed rewards, and it does not need temporal discounting or value function\napproximation. Most importantly, ES is highly parallelizable, which allows us to make up for a\ndecreased data ef\ufb01ciency by scaling to more parallel workers.\nIn future work, we plan to apply evolution strategies to those problems for which MDP-based\nreinforcement learning is less well-suited: problems with long time horizons and complicated reward\nstructure. We are particularly interested in meta-learning, or learning-to-learn. A proof of concept\nfor meta-learning in an RL setting was given by Duan et al. [2016b]: Using black-box optimization\nwe hope to be able to extend these results. We also plan to examine combining ES with fast low\nprecision neural network implementations to fully make use of the gradient-free nature of ES.\nReferences\nAlex Braylan, Mark Hollenbeck, Elliot Meyerson, and Risto Miikkulainen. Frame skip is a powerful\nparameter for learning to play atari. Space, 1600:1800, 2005.\nDimo Brockhoff, Anne Auger, Nikolaus Hansen, Dirk V Arnold, and Tim Hohm. Mirrored sampling\nand sequential selection for evolution strategies. In International Conference on Parallel Problem\nSolving from Nature, pages 11\u201321. Springer, 2010.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep\nreinforcement learning for continuous control. In Proceedings of the 33rd International Conference\non Machine Learning (ICML), 2016a.\nYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016b.\nJohn C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for\nzero-order convex optimization: The power of two function evaluations. IEEE Transactions on\nInformation Theory, 61(5):2788\u20132806, 2015.\nJohn Geweke. Antithetic acceleration of monte carlo integration in bayesian inference. Journal of\nEconometrics, 38(1-2):73\u201389, 1988.\nTobias Glasmachers, Tom Schaul, and J\u00fcrgen Schmidhuber. A natural evolution strategy for multi-\nobjective optimization. In International Conference on Parallel Problem Solving from Nature,\npages 627\u2013636. Springer, 2010a.\nTobias Glasmachers, Tom Schaul, Sun Yi, Daan Wierstra, and J\u00fcrgen Schmidhuber. Exponential natu-\nral evolution strategies. In Proceedings of the 12th annual conference on Genetic and evolutionary\ncomputation, pages 393\u2013400. ACM, 2010b.\nNikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution\nstrategies. Evolutionary computation, 9(2):159\u2013195, 2001.\nMatthew Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. A neuroevolution approach\nto general atari game playing. IEEE Transactions on Computational Intelligence and AI in Games,\n6(4):355\u2013366, 2014.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa,\nSarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of\na tensor processing unit. arXiv preprint arXiv:1704.04760, 2017.\n9\n",
      "Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information\nProcessing Systems, pages 586\u2013594, 2016.\nJan Koutn\u00edk, Faustino Gomez, and J\u00fcrgen Schmidhuber. Evolving neural networks in compressed\nweight space. In Proceedings of the 12th annual conference on Genetic and evolutionary computa-\ntion, pages 619\u2013626. ACM, 2010.\nJan Koutn\u00edk, Giuseppe Cuccu, J\u00fcrgen Schmidhuber, and Faustino Gomez. Evolving large-scale neural\nnetworks for vision-based reinforcement learning. In Proceedings of the 15th annual conference\non Genetic and evolutionary computation, pages 1061\u20131068. ACM, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. In International Conference on Machine Learning, 2016.\nYurii Nesterov. Ef\ufb01ciency of coordinate descent methods on huge-scale optimization problems. SIAM\nJournal on Optimization, 22(2):341\u2013362, 2012.\nYurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.\nFoundations of Computational Mathematics, pages 1\u201340, 2011.\nAndrew Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and\nEric Liang. Autonomous inverted helicopter \ufb02ight via reinforcement learning. Experimental\nRobotics IX, pages 363\u2013372, 2006.\nRonald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in\nneural information processing systems, pages 1043\u20131049, 1998.\nI. Rechenberg and M. Eigen. Evolutionsstrategie: Optimierung Technischer Systeme nach Prinzipien\nder Biologischen Evolution. Frommann-Holzboog Stuttgart, 1973.\nSebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open challenges.\nIEEE Transactions on Computational Intelligence and AI in Games, 2015.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. In Advances in Neural Information Processing Systems,\npages 2226\u20132234, 2016.\nTom Schaul, Tobias Glasmachers, and J\u00fcrgen Schmidhuber. High dimensions and heavy tails\nfor natural evolution strategies. In Proceedings of the 13th annual conference on Genetic and\nevolutionary computation, pages 845\u2013852. ACM, 2011.\nJuergen Schmidhuber and Jieyu Zhao. Direct policy search and uncertain policy evaluation. In Aaai\nspring symposium on search under uncertain and incomplete information, stanford univ, pages\n119\u2013124, 1998.\nJ\u00fcrgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, and Faustino Gomez. Training recurrent\nnetworks by evolino. Neural computation, 19(3):757\u2013779, 2007.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region\npolicy optimization. In ICML, pages 1889\u20131897, 2015.\nH.-P. Schwefel. Numerische optimierung von computer-modellen mittels der evolutionsstrategie.\n1977.\nFrank Sehnke, Christian Osendorfer, Thomas R\u00fcckstie\u00df, Alex Graves, Jan Peters, and J\u00fcrgen Schmid-\nhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551\u2013559, 2010.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.\n10\n",
      "James C Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient\napproximation. IEEE transactions on automatic control, 37(3):332\u2013341, 1992.\nRupesh Kumar Srivastava, J\u00fcrgen Schmidhuber, and Faustino Gomez. Generalized compressed\nnetwork search. In International Conference on Parallel Problem Solving from Nature, pages\n337\u2013346. Springer, 2012.\nKenneth O Stanley, David B D\u2019Ambrosio, and Jason Gauci. A hypercube-based encoding for evolving\nlarge-scale neural networks. Arti\ufb01cial life, 15(2):185\u2013212, 2009.\nFreek Stulp and Olivier Sigaud. Policy improvement methods: Between black-box optimization and\nepisodic reinforcement learning. 2012.\nYi Sun, Daan Wierstra, Tom Schaul, and Juergen Schmidhuber. Ef\ufb01cient natural evolution strategies.\nIn Proceedings of the 11th Annual conference on Genetic and evolutionary computation, pages\n539\u2013546. ACM, 2009.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages\n5026\u20135033. IEEE, 2012.\nNicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic exploration for\ndeep deterministic policies: An application to starcraft micromanagement tasks. arXiv preprint\narXiv:1609.02993, 2016.\nSjoerd van Steenkiste, Jan Koutn\u00edk, Kurt Driessens, and J\u00fcrgen Schmidhuber. A wavelet-based\nencoding for neuroevolution. In Proceedings of the 2016 on Genetic and Evolutionary Computation\nConference, pages 517\u2013524. ACM, 2016.\nDaan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies. In\nEvolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelli-\ngence). IEEE Congress on, pages 3381\u20133387. IEEE, 2008.\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J\u00fcrgen Schmidhuber.\nNatural evolution strategies. Journal of Machine Learning Research, 15(1):949\u2013980, 2014.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229\u2013256, 1992.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov,\nRichard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation\nwith visual attention. In ICML, volume 14, pages 77\u201381, 2015.\nSun Yi, Daan Wierstra, Tom Schaul, and J\u00fcrgen Schmidhuber. Stochastic search using the natural\ngradient. In Proceedings of the 26th Annual International Conference on Machine Learning, pages\n1161\u20131168. ACM, 2009.\n11\n",
      "Game\nDQN\nA3C FF, 1 day\nHyperNEAT\nES FF, 1 hour\nA2C FF\nAmidar\n133.4\n283.9\n184.4\n112.0\n548.2\nAssault\n3332.3\n3746.1\n912.6\n1673.9\n2026.6\nAsterix\n124.5\n6723.0\n2340.0\n1440.0\n3779.7\nAsteroids\n697.1\n3009.4\n1694.0\n1562.0\n1733.4\nAtlantis\n76108.0\n772392.0\n61260.0\n1267410.0\n2872644.8\nBank Heist\n176.3\n946.0\n214.0\n225.0\n724.1\nBattle Zone\n17560.0\n11340.0\n36200.0\n16600.0\n8406.2\nBeam Rider\n8672.4\n13235.9\n1412.8\n744.0\n4438.9\nBerzerk\n1433.4\n1394.0\n686.0\n720.6\nBowling\n41.2\n36.2\n135.8\n30.0\n28.9\nBoxing\n25.8\n33.7\n16.4\n49.8\n95.8\nBreakout\n303.9\n551.6\n2.8\n9.5\n368.5\nCentipede\n3773.1\n3306.5\n25275.2\n7783.9\n2773.3\nChopper Command\n3046.0\n4669.0\n3960.0\n3710.0\n1700.0\nCrazy Climber\n50992.0\n101624.0\n0.0\n26430.0\n100034.4\nDemon Attack\n12835.2\n84997.5\n14620.0\n1166.5\n23657.7\nDouble Dunk\n21.6\n0.1\n2.0\n0.2\n3.2\nEnduro\n475.6\n82.2\n93.6\n95.0\n0.0\nFishing Derby\n2.3\n13.6\n49.8\n49.0\n33.9\nFreeway\n25.8\n0.1\n29.0\n31.0\n0.0\nFrostbite\n157.4\n180.1\n2260.0\n370.0\n266.6\nGopher\n2731.8\n8442.8\n364.0\n582.0\n6266.2\nGravitar\n216.5\n269.5\n370.0\n805.0\n256.2\nIce Hockey\n3.8\n4.7\n10.6\n4.1\n4.9\nKangaroo\n2696.0\n106.0\n800.0\n11200.0\n1357.6\nKrull\n3864.0\n8066.6\n12601.4\n8647.2\n6411.5\nMontezuma\u2019s Revenge\n50.0\n53.0\n0.0\n0.0\n0.0\nName This Game\n5439.9\n5614.0\n6742.0\n4503.0\n5532.8\nPhoenix\n28181.8\n1762.0\n4041.0\n14104.7\nPit Fall\n123.0\n0.0\n0.0\n8.2\nPong\n16.2\n11.4\n17.4\n21.0\n20.8\nPrivate Eye\n298.2\n194.4\n10747.4\n100.0\n100.0\nQ*Bert\n4589.8\n13752.3\n695.0\n147.5\n15758.6\nRiver Raid\n4065.3\n10001.2\n2616.0\n5009.0\n9856.9\nRoad Runner\n9264.0\n31769.0\n3220.0\n16590.0\n33846.9\nRobotank\n58.5\n2.3\n43.8\n11.9\n2.2\nSeaquest\n2793.9\n2300.2\n716.0\n1390.0\n1763.7\nSkiing\n13700.0\n7983.6\n15442.5\n15245.8\nSolaris\n1884.8\n160.0\n2090.0\n2265.0\nSpace Invaders\n1449.7\n2214.7\n1251.0\n678.5\n951.9\nStar Gunner\n34081.0\n64393.0\n2720.0\n1470.0\n40065.6\nTennis\n2.3\n10.2\n0.0\n4.5\n11.2\nTime Pilot\n5640.0\n5825.0\n7340.0\n4970.0\n4637.5\nTutankham\n32.4\n26.1\n23.6\n130.3\n194.3\nUp and Down\n3311.3\n54525.4\n43734.0\n67974.0\n75785.9\nVenture\n54.0\n19.0\n0.0\n760.0\n0.0\nVideo Pinball\n20228.1\n185852.6\n0.0\n22834.8\n46470.1\nWizard of Wor\n246.0\n5278.0\n3360.0\n3480.0\n1587.5\nYars Revenge\n7270.8\n24096.4\n16401.7\n8963.5\nZaxxon\n831.0\n2659.0\n3000.0\n6380.0\n5.6\nTable 2: Final results obtained using Evolution Strategies on Atari 2600 games (feedforward CNN\npolicy, deterministic policy evaluation, averaged over 10 re-runs with up to 30 random initial no-ops),\nand compared to results for DQN and A3C from Mnih et al. [2016] and HyperNEAT from Hausknecht\net al. [2014]. A2C is our synchronous variant of A3C, and its reported scores are obtained with 320M\ntraining frames with the same evaluation setup as for the ES results. All methods were trained on raw\npixel input.\n12\n",
      "Table 3: MuJoCo tasks: Ratio of ES timesteps to TRPO timesteps needed to reach various percentages of TRPO\u2019s learning progress at 5 million timesteps. These\nresults were computed from ES learning curves averaged over 6 reruns.\nEnvironment\n% TRPO \ufb01nal score\nTRPO score\nTRPO timesteps\nES timesteps\nES timesteps / TRPO timesteps\nHalfCheetah\n25%\n-1.35\n9.05e+05\n1.36e+05\n0.15\n50%\n793.55\n1.70e+06\n8.28e+05\n0.49\n75%\n1589.83\n3.34e+06\n1.42e+06\n0.42\n100%\n2385.79\n5.00e+06\n2.88e+06\n0.58\nHopper\n25%\n877.45\n7.29e+05\n3.83e+05\n0.53\n50%\n1718.16\n1.03e+06\n3.73e+06\n3.64\n75%\n2561.11\n1.59e+06\n9.63e+06\n6.05\n100%\n3403.46\n4.56e+06\n3.16e+07\n6.94\nInvertedDoublePendulum\n25%\n2358.98\n8.73e+05\n3.98e+05\n0.46\n50%\n4609.68\n9.65e+05\n4.66e+05\n0.48\n75%\n6874.03\n1.07e+06\n5.30e+05\n0.49\n100%\n9104.07\n4.39e+06\n5.39e+06\n1.23\nInvertedPendulum\n25%\n276.59\n2.21e+05\n6.25e+04\n0.28\n50%\n519.15\n2.73e+05\n1.43e+05\n0.52\n75%\n753.17\n3.25e+05\n2.55e+05\n0.78\n100%\n1000.00\n5.17e+05\n4.55e+05\n0.88\nSwimmer\n25%\n41.97\n1.04e+06\n5.88e+05\n0.56\n50%\n70.73\n1.82e+06\n8.52e+05\n0.47\n75%\n99.68\n2.33e+06\n1.23e+06\n0.53\n100%\n128.25\n4.59e+06\n1.39e+06\n0.30\nWalker2d\n25%\n957.68\n1.55e+06\n6.43e+05\n0.41\n50%\n1916.48\n2.27e+06\n1.29e+07\n5.69\n75%\n2872.81\n2.89e+06\n2.31e+07\n8.02\n100%\n3830.03\n4.81e+06\n3.79e+07\n7.88\n13\n"
    ],
    "pdf_path": "data/papers/1703.03864v2.pdf"
  },
  {
    "text": "Soft Actor-Critic:\nOff-Policy Maximum Entropy Deep Reinforcement\nLearning with a Stochastic Actor\nTuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1\nAbstract\nModel-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods. By combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds.\n1. Introduction\nModel-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.\nof these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks.\nOne cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step.\nThis quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).\nWe explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces. To that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n\n\nSoft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016). SAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods.\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG.\n2. Related Work\nOur soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration.\nWe review prior works that draw on some of these ideas in\nthis section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function.\nAs such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n\n\nSoft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al. (2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization. Fur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin.\n3. Preliminaries\nWe \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks.\n3.1. Notation\nWe address policy learning in continuous action spaces.\nWe consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A.\nThe environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition. We will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st).\n3.2. Maximum Entropy Reinforcement Learning\nStandard RL maximizes the expected sum of rewards\nP\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) =\nT\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] .\n(1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121.\nThis objective has a number of conceptual and practical\nadvantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues.\nSecond, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework.\n4. From Soft Policy Iteration to Soft\nActor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory.\n\n\nSoft Actor-Critic\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section. We\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities.\nIn the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below.\nLemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.\nThen the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e.\nProof. See Appendix B.1.\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies.\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n.\n(4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2.\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. See Appendix B.2.\nThe full soft policy iteration algorithm alternates between\nthe soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1).\nAlthough this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic.\nTheorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e.\nProof. See Appendix B.3.\n4.2. Soft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st).\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors.\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3. This quantity can be\n\n\nSoft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer. The soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001.\n(9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014\nDKL\n\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n.\n(10)\nThere are several options for minimizing J\u03c0. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.\n(2018). Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer. In practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n\n\nSoft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nDDPG\nPPO\nSQL\nTD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.\nfor all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple.\n5. Experiments\nThe goal of our experimental evaluation is to understand\nhow the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements. We have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).\nThe source code of our SAC implementation1 and videos2\nare available online.\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n\n\nSoft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance.\nThe quantitative results attained by SAC in our experiments\nalso compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.\n5.2. Ablation Study\nThe results in the previous section suggest that algorithms\nbased on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.\nStochastic vs.\ndeterministic policy.\nSoft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5).\nTo compare how the stochasticity of the policy and entropy\nmaximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise. Figure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.\nseeds. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).\nPolicy evaluation.\nSince SAC converges to stochastic\npolicies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.\nReward scale.\nSoft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n\n\nSoft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n(c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue).\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.\nTarget network update.\nIt is common to use a separate\ntarget value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies. Large \u03c4 can lead to instabilities while small\n\u03c4 can make training slower. However, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost.\n6. Conclusion\nWe present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin. Our results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work.\nAcknowledgments\nWe would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George\nTucker for his valuable feedback on an early version of this\npaper. This work was supported by Siemens and Berkeley\nDeepDrive.\n\n\nSoft Actor-Critic\nReferences\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834\u2013846, 1983.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv\u00b4ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1204\u20131212, 2009.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym.\narXiv preprint arXiv:1606.01540, 2016.\nDuan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018.\nGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.\nThe reactor: A sample-ef\ufb01cient actor-critic architecture.\narXiv preprint arXiv:1704.04651, 2017.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic. arXiv preprint arXiv:1611.02247,\n2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352\u20131361, 2017.\nHasselt, H. V. Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613\u20132621,\n2010.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y. Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944\u20132952, 2015.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015.\nLevine, S. and Koltun, V. Guided policy search. In Interna-\ntional Conference on Machine Learning (ICML), pp. 1\u20139,\n2013.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1\u201340, 2016.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, 2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772\u20132782, 2017a.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nTrust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b.\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.\nPGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016.\nPeters, J. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682\u2013\n697, 2008.\nRawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS),\n2012.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n1889\u20131897, 2015.\n\n\nSoft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning. arXiv preprint\narXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484\u2013489, Jan 2016.\nISSN 0028-0836. Article.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P. Bias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n441\u2013448, 2014.\nTodorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. 4286\u20134292. IEEE, 2008.\nToussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. 1049\u20131056. ACM, 2009.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229\u2013256, 1992.\nZiebart, B. D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp.\n1433\u20131438, 2008.\n\n\nSoft Actor-Critic\nA. Maximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014). However, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.\n(14)\nThis objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\nevery state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy.\nB. Proofs\nB.1. Lemma 1\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e.\nProof. De\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded.\nB.2. Lemma 2\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. Let \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)).\n(16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st).\n(18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n...\n\u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18.\nConvergence to Q\u03c0new follows from Lemma 1.\n\n\nSoft Actor-Critic\nB.3. Theorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e.\nProof. Let \u03c0i be the policy at iteration i. By Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217. We will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217.\nUsing the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0.\nC. Enforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a \ufb01nite\ninterval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n.\n(20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212\nD\nX\ni=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u.\n\n\nSoft Actor-Critic\nD. Hyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the\nreward scale parameter that was tuned for each environment.\nTable 1. SAC Hyperparameters\nParameter\nValue\nShared\noptimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity\nReLU\nSAC\ntarget smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2. SAC Environment Speci\ufb01c Parameters\nEnvironment\nAction Dimensions\nReward Scale\nHopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n\n\nSoft Actor-Critic\nE. Additional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nSAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.\n\n\n",
    "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) al- gorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessi- tate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off- policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning frame- work. In this framework, the actor aims to maxi- mize expected reward while also maximizing en- tropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formu- lation, our method achieves state-of-the-art per- formance on a range of continuous control bench- mark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "sections": [
      {
        "header": "Abstract",
        "content": "Model-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods. By combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds."
      },
      {
        "header": "Introduction",
        "content": "Model-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.\nof these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks.\nOne cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step.\nThis quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).\nWe explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces. To that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n\n\nSoft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016). SAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods.\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG."
      },
      {
        "header": "Related Work",
        "content": "Our soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration."
      },
      {
        "header": "We review prior works that draw on some of these ideas in",
        "content": "this section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function.\nAs such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n\n\nSoft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al. (2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization. Fur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin."
      },
      {
        "header": "Preliminaries",
        "content": "We \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks.\n3.1. Notation\nWe address policy learning in continuous action spaces.\nWe consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A.\nThe environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition. We will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st).\n3.2. Maximum Entropy Reinforcement Learning"
      },
      {
        "header": "Standard RL maximizes the expected sum of rewards",
        "content": "P\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) ="
      },
      {
        "header": "T\nX",
        "content": "t=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] .\n(1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121."
      },
      {
        "header": "This objective has a number of conceptual and practical",
        "content": "advantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues.\nSecond, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework."
      },
      {
        "header": "From Soft Policy Iteration to Soft",
        "content": "Actor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory.\n\n\nSoft Actor-Critic\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section. We\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities.\nIn the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below.\nLemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.\nThen the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e.\nProof. See Appendix B.1.\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies.\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n.\n(4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2.\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. See Appendix B.2."
      },
      {
        "header": "The full soft policy iteration algorithm alternates between",
        "content": "the soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1).\nAlthough this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic.\nTheorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e.\nProof. See Appendix B.3.\n4.2. Soft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st).\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors.\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3. This quantity can be\n\n\nSoft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer. The soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001.\n(9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014"
      },
      {
        "header": "DKL",
        "content": "\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n.\n(10)\nThere are several options for minimizing J\u03c0. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.\n(2018). Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer. In practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n\n\nSoft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)"
      },
      {
        "header": "SQL",
        "content": "TD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.\nfor all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple."
      },
      {
        "header": "The goal of our experimental evaluation is to understand",
        "content": "how the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements. We have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).\nThe source code of our SAC implementation1 and videos2\nare available online.\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n\n\nSoft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance."
      },
      {
        "header": "The quantitative results attained by SAC in our experiments",
        "content": "also compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.\n5.2. Ablation Study"
      },
      {
        "header": "The results in the previous section suggest that algorithms",
        "content": "based on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.\nStochastic vs.\ndeterministic policy.\nSoft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5)."
      },
      {
        "header": "To compare how the stochasticity of the policy and entropy",
        "content": "maximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise. Figure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.\nseeds. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).\nPolicy evaluation."
      },
      {
        "header": "Since SAC converges to stochastic",
        "content": "policies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.\nReward scale.\nSoft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n\n\nSoft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n(c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue).\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.\nTarget network update."
      },
      {
        "header": "It is common to use a separate",
        "content": "target value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies. Large \u03c4 can lead to instabilities while small\n\u03c4 can make training slower. However, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost."
      },
      {
        "header": "Conclusion",
        "content": "We present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin. Our results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work."
      },
      {
        "header": "Acknowledgments",
        "content": "We would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George"
      },
      {
        "header": "References",
        "content": "Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834\u2013846, 1983.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv\u00b4ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation."
      },
      {
        "header": "In Advances in Neural Information Processing Systems",
        "content": "(NIPS), pp. 1204\u20131212, 2009.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym.\narXiv preprint arXiv:1606.01540, 2016.\nDuan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018.\nGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.\nThe reactor: A sample-ef\ufb01cient actor-critic architecture.\narXiv preprint arXiv:1704.04651, 2017.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic. arXiv preprint arXiv:1611.02247,\n2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352\u20131361, 2017.\nHasselt, H. V. Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613\u20132621,\n2010.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y. Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944\u20132952, 2015.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015.\nLevine, S. and Koltun, V. Guided policy search. In Interna-\ntional Conference on Machine Learning (ICML), pp. 1\u20139,\n2013.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1\u201340, 2016.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, 2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772\u20132782, 2017a.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nTrust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b.\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.\nPGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016.\nPeters, J. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682\u2013\n697, 2008.\nRawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS),\n2012.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n1889\u20131897, 2015.\n\n\nSoft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning. arXiv preprint\narXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484\u2013489, Jan 2016.\nISSN 0028-0836. Article.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P. Bias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n441\u2013448, 2014.\nTodorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. 4286\u20134292. IEEE, 2008.\nToussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. 1049\u20131056. ACM, 2009.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229\u2013256, 1992.\nZiebart, B. D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp.\n1433\u20131438, 2008.\n\n\nSoft Actor-Critic\nA. Maximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014). However, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.\n(14)"
      },
      {
        "header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
        "content": "every state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy.\nB. Proofs\nB.1. Lemma 1\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e.\nProof. De\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded.\nB.2. Lemma 2\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. Let \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)).\n(16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st).\n(18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n...\n\u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18.\nConvergence to Q\u03c0new follows from Lemma 1.\n\n\nSoft Actor-Critic\nB.3. Theorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e.\nProof. Let \u03c0i be the policy at iteration i. By Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217. We will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217.\nUsing the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0.\nC. Enforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a \ufb01nite\ninterval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n.\n(20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212"
      },
      {
        "header": "D\nX",
        "content": "i=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u.\n\n\nSoft Actor-Critic\nD. Hyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the\nreward scale parameter that was tuned for each environment.\nTable 1. SAC Hyperparameters"
      },
      {
        "header": "Shared",
        "content": "optimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity"
      },
      {
        "header": "SAC",
        "content": "target smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2. SAC Environment Speci\ufb01c Parameters"
      },
      {
        "header": "Reward Scale",
        "content": "Hopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n\n\nSoft Actor-Critic\nE. Additional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)"
      },
      {
        "header": "SAC",
        "content": "SAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor."
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "author": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine",
      "subject": "Proceedings of the International Conference on Machine Learning 2018",
      "keywords": "reinforcement learning, control as inference",
      "creator": "LaTeX with hyperref package",
      "producer": "pdfTeX-1.40.17",
      "creationDate": "D:20180810003613Z",
      "modDate": "D:20180810003613Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 14,
    "pages": [
      "Soft Actor-Critic:\nOff-Policy Maximum Entropy Deep Reinforcement\nLearning with a Stochastic Actor\nTuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1\nAbstract\nModel-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning. Both of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork. In this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy. That is, to succeed at the task while acting\nas randomly as possible. Prior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods. By combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods. Furthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds.\n1. Introduction\nModel-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA. Correspondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.\nof these methods in real-world domains has been hampered\nby two major challenges. First, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity. Even relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results. Both of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks.\nOne cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step.\nThis quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity. Off-\npolicy algorithms aim to reuse past experience. This is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning. A\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).\nWe explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces. To that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n",
      "Soft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017). More importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017). However, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability. This algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016). SAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods.\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG.\n2. Related Work\nOur soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration.\nWe review prior works that draw on some of these ideas in\nthis section. Actor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017). On-policy training tends to improve stability but\nresults in poor sample complexity.\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016). How-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency. A particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function.\nAs such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm. Unfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016). Our method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective. We \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG. A similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015). However, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n",
      "Soft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a). While\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al. (2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al. (2017)\nwith a sampling network trained to draw samples from the\noptimal policy. Although the soft Q-learning algorithm pro-\nposed by Haarnoja et al. (2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution. Hence, Haarnoja et al. (2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm. Crucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior. In contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization. Fur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning. In our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin.\n3. Preliminaries\nWe \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks.\n3.1. Notation\nWe address policy learning in continuous action spaces.\nWe consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A.\nThe environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition. We will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st).\n3.2. Maximum Entropy Reinforcement Learning\nStandard RL maximizes the expected sum of rewards\nP\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)]. We will consider a more gen-\neral maximum entropy objective (see e.g. Ziebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) =\nT\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] .\n(1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy. The maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920. For the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121.\nThis objective has a number of conceptual and practical\nadvantages. First, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues.\nSecond, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions. Lastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function. We can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework.\n4. From Soft Policy Iteration to Soft\nActor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory.\n",
      "Soft Actor-Critic\n4.1. Derivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework. Our derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section. We\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities.\nIn the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below.\nLemma 1 (Soft Policy Evaluation). Consider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.\nThen the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e.\nProof. See Appendix B.1.\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value. Since in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies.\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence. In the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n.\n(4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section. For this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1. We formalize this result in Lemma 2.\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. See Appendix B.2.\nThe full soft policy iteration algorithm alternates between\nthe soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1).\nAlthough this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase. Therefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive. The approximation gives rise to a new practical\nalgorithm, called soft actor-critic.\nTheorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e.\nProof. See Appendix B.3.\n4.2. Soft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration. To\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st).\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6. For\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks. We will next\nderive update rules for these parameter vectors.\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3. This quantity can be\n",
      "Soft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer. The gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer. The soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001.\n(9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015). Alternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E). Finally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014\nDKL\n\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n.\n(10)\nThere are several options for minimizing J\u03c0. A typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks. However, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator. To that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian. We can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted. We can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st). This unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.\n(2018). Although our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks. The complete algo-\nrithm is described in Algorithm 1. The method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer. In practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n",
      "Soft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nDDPG\nPPO\nSQL\nTD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1. Training curves on continuous control benchmarks. Soft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.\nfor all hyperparameter). Using off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data. The algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple.\n5. Experiments\nThe goal of our experimental evaluation is to understand\nhow the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms. We compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016). The stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies. Our SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments. We addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation. This is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements. We have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E. We\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).\nThe source code of our SAC implementation1 and videos2\nare available online.\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n",
      "Soft Actor-Critic\n5.1. Comparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3. We train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps. The solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance. For example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016). SAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance.\nThe quantitative results attained by SAC in our experiments\nalso compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.\n5.2. Ablation Study\nThe results in the previous section suggest that algorithms\nbased on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks. In this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance. We also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.\nStochastic vs.\ndeterministic policy.\nSoft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive. The entropy appears in both the policy and value\nfunction. In the policy, it prevents premature convergence of\nthe policy variance (Equation 10). In the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5).\nTo compare how the stochasticity of the policy and entropy\nmaximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise. Figure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy.\nseeds. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability. As\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training. This\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging. In this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).\nPolicy evaluation.\nSince SAC converges to stochastic\npolicies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance. For evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution. Figure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance. It should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.\nReward scale.\nSoft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity. Larger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n",
      "Soft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task. (a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.\n(c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue).\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.\nTarget network update.\nIt is common to use a separate\ntarget value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015). A value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all. In Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies. Large \u03c4 can lead to instabilities while small\n\u03c4 can make training slower. However, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks. In Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost.\n6. Conclusion\nWe present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm. In fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin. Our results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work.\nAcknowledgments\nWe would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George\nTucker for his valuable feedback on an early version of this\npaper. This work was supported by Siemens and Berkeley\nDeepDrive.\n",
      "Soft Actor-Critic\nReferences\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems. IEEE transactions on systems, man, and\ncybernetics, pp. 834\u2013846, 1983.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei,\nH. R., and Szepesv\u00b4ari, C. Convergent temporal-difference\nlearning with arbitrary smooth function approximation.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1204\u20131212, 2009.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. OpenAI gym.\narXiv preprint arXiv:1606.01540, 2016.\nDuan, Y., Chen, X. Houthooft, R., Schulman, J., and Abbeel,\nP. Benchmarking deep reinforcement learning for contin-\nuous control. In International Conference on Machine\nLearning (ICML), 2016.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016.\nFujimoto, S., van Hoof, H., and Meger, D. Addressing func-\ntion approximation error in actor-critic methods. arXiv\npreprint arXiv:1802.09477, 2018.\nGruslys, A., Azar, M. G., Bellemare, M. G., and Munos, R.\nThe reactor: A sample-ef\ufb01cient actor-critic architecture.\narXiv preprint arXiv:1704.04651, 2017.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and\nLevine, S. Q-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic. arXiv preprint arXiv:1611.02247,\n2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-\nforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning (ICML),\npp. 1352\u20131361, 2017.\nHasselt, H. V. Double Q-learning. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 2613\u20132621,\n2010.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y. Learning continuous control policies by stochas-\ntic value gradients. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2944\u20132952, 2015.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560, 2017.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference for Learning\nPresentations (ICLR), 2015.\nLevine, S. and Koltun, V. Guided policy search. In Interna-\ntional Conference on Machine Learning (ICML), pp. 1\u20139,\n2013.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies. Journal of Machine\nLearning Research, 17(39):1\u201340, 2016.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, 2015.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational Conference on Machine Learning (ICML),\n2016.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems (NIPS), pp. 2772\u20132782, 2017a.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nTrust-PCL: An off-policy trust region method for contin-\nuous control. arXiv preprint arXiv:1707.01891, 2017b.\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.\nPGQ: Combining policy gradient and Q-learning. arXiv\npreprint arXiv:1611.01626, 2016.\nPeters, J. and Schaal, S. Reinforcement learning of motor\nskills with policy gradients. Neural networks, 21(4):682\u2013\n697, 2008.\nRawlik, K., Toussaint, M., and Vijayakumar, S. On stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference. Robotics: Science and Systems (RSS),\n2012.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and\nMoritz, P. Trust region policy optimization. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n1889\u20131897, 2015.\n",
      "Soft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X. Equivalence be-\ntween policy gradients and soft Q-learning. arXiv preprint\narXiv:1704.06440, 2017a.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017b.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M. Deterministic policy gradient algo-\nrithms. In International Conference on Machine Learning\n(ICML), 2014.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search. Nature, 529(7587):484\u2013489, Jan 2016.\nISSN 0028-0836. Article.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge, 1998.\nThomas, P. Bias in natural actor-critic algorithms. In Inter-\nnational Conference on Machine Learning (ICML), pp.\n441\u2013448, 2014.\nTodorov, E. General duality between optimal control and\nestimation. In IEEE Conference on Decision and Control\n(CDC), pp. 4286\u20134292. IEEE, 2008.\nToussaint, M. Robot trajectory optimization using approxi-\nmate inference. In International Conference on Machine\nLearning (ICML), pp. 1049\u20131056. ACM, 2009.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 8(3-4):229\u2013256, 1992.\nZiebart, B. D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp.\n1433\u20131438, 2008.\n",
      "Soft Actor-Critic\nA. Maximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014). However, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.\n(14)\nThis objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\nevery state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy.\nB. Proofs\nB.1. Lemma 1\nLemma 1 (Soft Policy Evaluation). Consider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk. Then the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e.\nProof. De\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded.\nB.2. Lemma 2\nLemma 2 (Soft Policy Improvement). Let \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4. Then Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.\nProof. Let \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)).\n(16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st).\n(18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n...\n\u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18.\nConvergence to Q\u03c0new follows from Lemma 1.\n",
      "Soft Actor-Critic\nB.3. Theorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e.\nProof. Let \u03c0i be the policy at iteration i. By Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217. We will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217.\nUsing the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0.\nC. Enforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution. However, in practice, the actions needs to be bounded to a \ufb01nite\ninterval. To that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions. In the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n.\n(20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212\nD\nX\ni=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u.\n",
      "Soft Actor-Critic\nD. Hyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4. Table 2 lists the\nreward scale parameter that was tuned for each environment.\nTable 1. SAC Hyperparameters\nParameter\nValue\nShared\noptimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity\nReLU\nSAC\ntarget smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2. SAC Environment Speci\ufb01c Parameters\nEnvironment\nAction Dimensions\nReward Scale\nHopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n",
      "Soft Actor-Critic\nE. Additional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4. Trust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer. The \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned. Both of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nSAC\nSAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants. Soft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions. It is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.\n"
    ],
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Reinforcement Learning with Competitive Ensembles\nof Information-Constrained Primitives\nAnirudh Goyal1, Shagun Sodhani1, Jonathan Binas1, Xue Bin Peng2\nSergey Levine2, Yoshua Bengio1\u2020\n1 Mila, Universit\u00e9 de Montr\u00e9al\n2 University of California, Berkeley\n\u2020CIFAR Senior Fellow.\nAbstract\nReinforcement learning agents that operate in diverse and complex environments\ncan bene\ufb01t from the structured decomposition of their behavior. Often, this is\naddressed in the context of hierarchical reinforcement learning, where the aim is\nto decompose a policy into lower-level primitives or options, and a higher-level\nmeta-policy that triggers the appropriate behaviors for a given situation. However,\nthe meta-policy must still produce appropriate decisions in all states. In this\nwork, we propose a policy design that decomposes into primitives, similarly to\nhierarchical reinforcement learning, but without a high-level meta-policy. Instead,\neach primitive can decide for themselves whether they wish to act in the current\nstate. We use an information-theoretic mechanism for enabling this decentralized\ndecision: each primitive chooses how much information it needs about the current\nstate to make a decision and the primitive that requests the most information about\nthe current state acts in the world. The primitives are regularized to use as little\ninformation as possible, which leads to natural competition and specialization. We\nexperimentally demonstrate that this policy architecture improves over both \ufb02at\nand hierarchical policies in terms of generalization.\n1\nIntroduction\nLearning policies that generalize to new environments or tasks is a fundamental challenge in rein-\nforcement learning. While deep reinforcement learning has enabled training powerful policies, which\noutperform humans on speci\ufb01c, well-de\ufb01ned tasks [24], their performance often diminishes when the\nproperties of the environment or the task change to regimes not encountered during training.\nThis is in stark contrast to how humans learn, plan, and act: humans can seamlessly switch between\ndifferent aspects of a task, transfer knowledge to new tasks from remotely related but essentially\ndistinct prior experience, and combine primitives (or skills) used for distinct aspects of different tasks\nin meaningful ways to solve new problems. A hypothesis hinting at the reasons for this discrepancy\nis that the world is inherently compositional, such that its features can be described by compositions\nof small sets of primitive mechanisms [26]. Since humans seem to bene\ufb01t from learning skills and\nlearning to combine skills, it might be a useful inductive bias for the learning models as well.\nThis is addressed to some extent by the hierarchical reinforcement learning (HRL) methods, which\nfocus on learning representations at multiple spatial and temporal scales, thus enabling better explo-\nration strategies and improved generalization performance [9, 36, 10, 19]. However, hierarchical\napproaches rely on some form of learned high-level controller, which decides when to activate\ndifferent components in the hierarchy. While low-level sub-policies can specialize to smaller portions\nof the state space, the top-level controller (or master policy) needs to know how to deal with any\ngiven state. That is, it should provide optimal behavior for the entire accessible state space. As the\nPreprint. Under review.\narXiv:1906.10667v1  [cs.LG]  25 Jun 2019\n\n\nx\nx\nx\n\u03c01\n\u03c02\n\u03c03\na\n\u03c01\n\u03c02\n\u03c03\na\n\u03c01\n\u03c02\n\u03c03\na\nAction\nselection\nCompetition\nmechanism\nCompositional\nenvironment\n1\n2\n3\nFigure 1: Illustration of our model. An intrinsic competition mechanism, based on the amount of\ninformation each primitive provides, is used to select a primitive to be active for a given input. Each\nprimitive focuses on distinct features of the environment; in this case, one policy focuses on boxes, a\nsecond one on gates, and the third one on spheres.\nmaster policy is trained on a particular state distribution, learning it in a way that generalizes to new\nenvironments effectively can, therefore, become the bottleneck for such approaches [31, 3].\nWe argue, and empirically show, that in order to achieve better generalization, the interaction between\nthe low-level primitives and the selection thereof should itself be performed without requiring a\nsingle centralized network that understands the entire state space. We, therefore, propose a fully\ndecentralized approach as an alternative to standard HRL, where we only learn a set of low-level\nprimitives without learning a high-level controller. We construct a factorized representation of the\npolicy by learning simple \u201cprimitive\u201d policies, which focus on distinct regions of the state space.\nRather than being gated by a single meta-policy, the primitives directly compete with one another\nto determine which one should be active at any given time, based on the degree to which their state\nencoders \u201crecognize\u201d the current state input.\nWe frame the problem as one of information transfer between the current state and a dynamically\nselected primitive policy. Each policy can by itself decide to request information about the current\nstate, and the amount of information requested is used to determine which primitive acts in the\ncurrent state. Since the amount of state information that a single primitive can access is limited, each\nprimitive is encouraged to use its resources wisely. Constraining the amount of accessible information\nin this way naturally leads to a decentralized competition and decision mechanism where individual\nprimitives specialize in smaller regions of the state space. We formalize this information-driven\nobjective based on the variational information bottleneck. The resulting set of competing primitives\nachieves both a meaningful factorization of the policy and an effective decision mechanism for which\nprimitives to use. Importantly, not relying on a centralized meta-policy means that individual primitive\nmechanisms can be recombined in a \u201cplug-and-play\u201d fashion, and can be transferred seamlessly to\nnew environments.\nContributions:\nIn summary, the contributions of our work are as follows: (1) We propose a method\nfor learning and operating a set of functional primitives in a fully decentralized way, without requiring\na high-level meta-controller to select active primitives (see Figure 1 for illustration). (2) We introduce\nan information-theoretic objective, the effects of which are twofold: a) it leads to the specialization of\nindividual primitives to distinct regions of the state space, and b) it enables a competition mechanism,\nwhich is used to select active primitives in a decentralized manner. (3) We demonstrate the superior\ntransfer learning performance of our model, which is due to the \ufb02exibility of the proposed framework\nregarding the dynamic addition, removal, and recombination of primitives. Decentralized primitives\ncan be successfully transferred to larger or previously unseen environments, and outperform models\nwith an explicit meta-controller for primitive selection.\n2\n\n\n2\nPreliminaries\nWe consider a Markov decision process (MDP) de\ufb01ned by the tuple (S, A, P, r, \u03b3), where the state\nspace S and the action space A may be discrete or continuous. The environment emits a bounded\nreward r : S \u00d7 A \u2192[rmin, rmax] on each transition and \u03b3 \u2208[0, 1) is the discount factor. \u03c0(.|s)\ndenotes a policy over the actions given the current state s. R(\u03c0) = E\u03c0[P\nt \u03b3tr(st)] denotes the\nexpected total return when the policy \u03c0 is followed. The standard objective in reinforcement learning\nis to maximize the expected total return R(\u03c0). We use the concept of the information bottleneck [39] to\nlearn compressed representations. The information bottleneck objective is formalized as minimizing\nthe mutual information of a bottleneck representation layer with the input while maximizing its\nmutual information with the corresponding output. This type of input compression has been shown to\nimprove generalization [39, 1, 2]. Computing the mutual information is generally intractable, but can\nbe approximated using variational inference [2].\n3\nInformation-Theoretic Decentralized Learning of Distinct Primitives\nOur goal is to learn a policy, composed of multiple primitive sub-policies, to maximize average\nreturns over T-step interactions for a distribution of tasks. Simple primitives which focus on solving\na part of the given task (and not the complete task) should generalize more effectively, as they can\nbe applied to similar aspects of different tasks (subtasks) even if the overall objective of the tasks\nare drastically different. Learning primitives in this way can also be viewed as learning a factorized\nrepresentation of a policy, which is composed of several independent policies.\nOur proposed approach consists of three components: 1) a mechanism for restricting a particular\nprimitive to a subset of the state space; 2) a competition mechanism between primitives to select the\nmost effective primitive for a given state; 3) a regularization mechanism to improve the generalization\nperformance of the policy as a whole. We consider experiments with both \ufb01xed and variable sets of\nprimitives and show that our method allows for primitives to be added or removed during training, or\nrecombined in new ways. Each primitive is represented by a differentiable, parameterized function\napproximator, such as a neural network.\n3.1\nPrimitives with an Information Bottleneck\nstate s\n(z1, . . . , zK)\n(L1, . . . , LK)\n(\u03b11, . . . , \u03b1K)\nE\u03b1\u03c0k\naction a\nencoder\nDKL(\u00b7||N)\ndecoder\nsoftmax\nFigure 2:\nThe primitive-selection\nmechanism of our model. The primi-\ntive with most information acts in the\nenvironment, and gets the reward.\nTo encourage each primitive to encode information from a\nparticular part of state space, we limit the amount of informa-\ntion each primitive can access from the state. In particular,\neach primitive has an information bottleneck with respect to\nthe input state, preventing it from using all the information\nfrom the state.\nTo implement an information bottleneck, we design each of\nthe K primitives to be composed of an encoder penc(Zk | S)\nand a decoder pdec(A | Zk), together forming the primitive\npolicy,\n\u03c0k\n\u03b8(A | S) =\nR\nz penc(zk | S) pdec(A | zk) dzk .1\nThe encoder output Z is meant to represent the information\nabout the current state S that an individual primitive believes\nis important to access in order to perform well. The decoder\ntakes this encoded information and produces a distribution\nover the actions A. Following the variational information bottleneck objective [2], we penalize the\nKL divergence of Z and the prior,\nLk = DKL(penc(Zk|S)||N(0, 1)) .\n(1)\nIn other words, a primitive pays an \u201cinformation cost\u201d proportional to Lk for accessing the information\nabout the current state.\n1In practice, we estimate the marginalization over Z using a single sample throughout our experiments.\n3\n\n\nIn the experiments below, we \ufb01x the prior to be a unit Gaussian. In the general case, we can learn the\nprior as well and include its parameters in \u03b8. The information bottleneck encourages each primitive to\nlimit its knowledge about the current state, but it will not prevent multiple primitives from specializing\nto similar parts of the state space. To mitigate this redundancy, and to make individual primitives focus\non different regions of the state space, we introduce an information-based competition mechanism to\nencourage diversity among the primitives, as described in the next section.\n3.2\nCompeting Information-Constrained Primitives\nWe can use the information measure from equation 1 to de\ufb01ne a selection mechanism for the primitives\nwithout having to learn a centralized meta-policy. The idea is that the information content of an\nindividual primitive encodes its effectiveness in a given state s such that the primitive with the highest\nvalue Lk\nshould be activated in that particular state. We compute normalized weights \u03b1k for each of the\nk = 1, . . . , K primitives by applying the softmax operator,\n\u03b1k = exp(Lk)/ P\nj exp(Lj) .\n(2)\nThe resulting weights \u03b1k can be treated as a probability distribution that can be used in different ways:\nform a mixture of primitives, sample a primitive using from the distribution, or simply select the\nprimitive with the maximum weight. The selected primitive is then allowed to act in the environment.\nTrading Reward and Information: To make the different primitives compete for competence in\nthe different regions of the state space, the environment reward is distributed according to their\nparticipation in the global decision, i.e. the reward rk given to the kth primitive is weighted by\nits selection coef\ufb01cient, such that rk = \u03b1kr, with r = P\nk rk. Hence, a primitive gets a higher\nreward for accessing more information about the current state, but that primitive also pays the price\n(equal to information cost) for accessing the state information. Hence, a primitive that does not\naccess any state information is not going to get any reward. The information bottleneck and the\ncompetition mechanism, when combined with the overall reward maximization objective, should lead\nto specialization of individual primitives to distinct regions in the state space.\nThat is, each primitive should specialize in a part of the state space that it can reliably associate\nrewards with. Since the entire ensemble still needs to understand all of the state space for the given\ntask, different primitives need to encode and focus on different parts of the state space.\n3.3\nRegularization of the Combined Representation\nTo encourage a diverse set of primitive con\ufb01gurations and to make sure that the model does not col-\nlapse to a single primitive (which remains active at all times), we introduce an additional regularization\nterm,\nLreg = P\nk \u03b1kLk .\n(3)\nThis can be rewritten (see Appendix A) as\nLreg = \u2212H(\u03b1) + LSE(L1, . . . , LK) ,\n(4)\nwhere H(\u03b1) is the entropy of the \u03b1 distribution, and LSE is the LogSumExp function,\nLSE(x) = log(P\nj exj). The desired behavior is achieved by minimizing Lreg. Increasing the entropy\nof \u03b1 leads to a diverse set of primitive selections, ensuring that different combinations of the primitives\nare used. On the other hand, LSE approximates the maximum of its arguments, LSE(x) \u2248maxj xj,\nand, therefore, penalizes the dominating Lk terms, thus equalizing their magnitudes.\n3.4\nObjective and Algorithm Summary\nOur overall objective function consists of 3 terms,\n1. The expected return from the standard RL objective, R(\u03c0) which is distributed to the\nprimitives according to their participation,\n2. The individual bottleneck terms leading the individual primitives to focus on speci\ufb01c parts\nof the state space, Lk for k = 1, . . . , K,\n4\n\n\n3. The regularization term applied to the combined model, Lreg.\nThe overall objective for the kth primitive thus takes the form:\nJk(\u03b8) \u2261E\u03c0\u03b8[rk] \u2212\u03b2indLk \u2212\u03b2regLreg ,\n(5)\nwhere E\u03c0\u03b8 denotes an expectation over the state trajectories generated by the agent\u2019s policy, rk = \u03b1kr\nis the reward given to the kth primitive, and \u03b2ind, \u03b2reg are the parameters controlling the impact of the\nrespective terms.\nImplementation: In our experiments, the encoders penc(zk|S) and decoders pdec(A|zk) are repre-\nsented by neural networks, the parameters of which we denote by \u03b8. Actions are sampled through\neach primitive every step. While our approach is compatible with any RL method, we maximize\nJ(\u03b8) computed on-policy from the sampled trajectories using a score function estimator [42, 35]\nspeci\ufb01cally A2C [25] (unless otherwise noted). Every experimental result reported has been averaged\nover 5 random seeds. Our model introduces 2 extra hyper-parameters \u03b2ind, \u03b2reg.\n4\nRelated Work\nThere are a wide variety of hierarchical reinforcement learning approaches[34, 9, 10]. One of the\nmost widely applied HRL framework is the Options framework ([36]). An option can be thought of\nas an action that extends over multiple timesteps thus providing the notion of temporal abstraction or\nsubroutines in an MDP. Each option has its own policy (which is followed if the option is selected)\nand the termination condition (to stop the execution of that option). Many strategies are proposed for\ndiscovering options using task-speci\ufb01c hierarchies, such as pre-de\ufb01ned sub-goals [16], hand-designed\nfeatures [12], or diversity-promoting priors [8, 11]. These approaches do not generalize well to new\ntasks. [4] proposed an approach to learn options in an end-to-end manner by parameterizing the\nintra-option policy as well as the policy and termination condition for all the options. Eigen-options\n[21] use the eigenvalues of the Laplacian (for the transition graph induced by the MDP) to derive an\nintrinsic reward for discovering options as well as learning an intra-option policy.\nIn this work, we consider sparse reward setup with high dimensional action spaces. In such a scenario,\nperforming unsupervised pretraining or using auxiliary rewards leads to much better performance\n[13, 12, 16]. Auxiliary tasks such as motion imitation have been applied to learn motor primitives\nthat are capable of performing a variety of sophisticated skills [20, 28, 23, 22].\nOur work is also related to the Neural Module Network family of architectures [3, 17, 30] where\nthe idea is to learn modules that can perform some useful computation like solving a subtask and\na controller that can learn to combine these modules for solving novel tasks. The key difference\nbetween our approach and all the works mentioned above is that we learn functional primitives in a\nfully decentralized way without requiring any high-level meta-controller or master policy.\n5\nExperimental Results\nIn this section, we brie\ufb02y outline the tasks that we used to evaluate our proposed method and direct\nthe reader to the appendix for the complete details of each task along with the hyperparameters used\nfor the model. The code is provided with the supplementary material. We designed experiments to\naddress the following questions: a) Learning primitives \u2013 Can an ensemble of primitives be learned\nover a distribution of tasks? b) Transfer Learning using primitives \u2013 Can the learned primitives\nbe transferred to unseen/unsolvable sparse environments? c) Comparison to centralized methods\n\u2013 How does our method compare to approaches where the primitives are trained using an explicit\nmeta-controller, in a centralized way?\nBaselines.\nWe compare our proposed method to the following baselines:\na) Option Critic [4] \u2013 We extended the author\u2019s implementation 2 of the Option Critic architecture\nand experimented with multiple variations in the terms of hyperparameters and state/goal encoding.\nNone of these yielded reasonable performance in partially observed tasks, so we omit it from the\nresults.\n2https://github.com/jeanharb/option_critic\n5\n\n\nb) MLSH (Meta-Learning Shared Hierarchy) [13] \u2013 This method uses meta-learning to learn sub-\npolicies that are shared across tasks along with learning a task-speci\ufb01c high-level master. It also\nrequires a phase-wise training schedule between the master and the sub-policies to stabilize training.\nWe use the MLSH implementation provided by the authors 3.\nc) Transfer A2C: In this method, we \ufb01rst learn a single policy on the one task and then transfer the\npolicy to another task, followed by \ufb01ne-tuning in the second task.\n5.1\nMulti-Task Training\nWe evaluate our model in a partially-observable 2D multi-task environment called Minigrid, similar to\nthe one introduced in [6]. The environment is a two-dimensional grid with a single agent, impassable\nwalls, and many objects scattered in the environment. The agent is provided with a natural language\nstring that speci\ufb01es the task that the agent needs to complete. The setup is partially observable\nand the agent only gets the small, egocentric view of the grid (along with the natural language task\ndescription). We consider three tasks here: the Pickup task (A), where the agent is required to pick up\nan object speci\ufb01ed by the goal string, the Unlock task (B) where the agent needs to unlock the door\n(there could be multiple keys in the environment and the agent needs to use the key which matches\nthe color of the door) and the UnlockPickup task (C), where the agent \ufb01rst needs to unlock a door\nthat leads to another room. In this room, the agent needs to \ufb01nd and pick up the object speci\ufb01ed by\nthe goal string. Additional implementation details of the environment are provided in appendix D.\nDetails on the agent model can be found in appendix D.3.\nWe train agents with varying numbers of primitives on various tasks \u2013 concurrently, as well as in\ntransfer settings. The different experiments are summarized in Figs. 3 and 5. An advantage of the\nmulti-task setting is that it allows for quantitative interpretability as to when and which primitives are\nbeing used. The results indicate that a system composed of multiple primitives generalizes more easily\nto a new task, as compared to a single policy. We further demonstrate that several primitives can be\ncombined dynamically and that the individual primitives respond to stimuli from new environments\nwhen trained on related environments.\n5.2\nDo Learned Primitives Help in Transfer Learning?\nWe now evaluate our approach in the settings where the adaptation to the changes in the task is vital.\nThe argument in the favor of modularity is that it enables better knowledge transfer between related\ntask. This transfer is more effective when the tasks are closely related as the model would only have\nto learn how to compose the already learned primitives. In general, it is dif\ufb01cult to determine how\n\u201cclosely\u201d related two tasks are and the inductive bias of modularity could be harmful if the two tasks\nare quite different. In such cases, we could add new primitives (which would have to be learned) and\nstill obtain a sample-ef\ufb01cient transfer as some part of the task structure would already have been\ncaptured by the pretrained primitives. This approach can be extended by adding primitives during\ntraining which provides a seamless way to combine knowledge about different tasks to solve more\ncomplex tasks. We investigate here the transfer properties of a primitive trained in one environment\nand transferred to a different one.\nContinuous control for ant maze\nWe evaluate the transfer performance of pretrained primitives\non the cross maze environment [15]. Here, a quadrupedal robot must walk to the different goals along\nthe different paths (see Appendix G for details). The goal is randomly chosen from a set of available\ngoals at the start of each episode. We pretrain a policy (see model details in Appendix G.1) with a\nmotion reward in an environment which does not have any walls (similar to [15]), and then transfer\nthe policy to the second task where the ant has to navigate to a random goal chosen from one of the 3\n(or 10) available goal options. For our model, we make four copies of the pretrained policies and\nthen \ufb01netune the model using the pretrained policies as primitives. We compare to both MLSH [13]\nand option-critic [4]. All these baselines have been pretrained in the same manner. As evident from\nFigure 5, our method outperforms the other approaches. The fact that the initial policies successfully\nadapt to the transfer environment underlines the \ufb02exibility of our approach.\n3https://github.com/openai/mlsh\n6\n\n\nA\n\u25e6\u25e6\nB\n\u25e6\u25e6\nC\n\u25e6\u25e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\n0.0\n0.2\n0.4\n0.6\n0.8\nNumber of frames\n1e7\n0.2\n0.4\n0.6\n0.8\nRelative frequency of activation\nidx 1\nidx 2\nA\n\u25e6\u25e6\nB\n\u25e6\u25e6\nretrain\nA\nA\n\u25e6\u25e6\n\u25e6\u25e6\nB\n\u25e6\u25e6\u25e6\u25e6\ncopy\ncombine\nC\nA\nB\n\u25e6\u25e6\n\u25e6\u25e6\n\u25e6\u25e6\nzero shot generalisation\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPercentage of episodes completed\n1e2\nTransfer A2C\nOur approach\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPercentage of episodes completed\n1e2\nTransfer A2C\nOption Critic / MLSH\nOur approach\n0\n1\n2\n3\n4\n5\nNumber of frames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\nidx 3\nidx 4\nFigure 3: Multitask training. Each panel corresponds to a different training setup, where different\ntasks are denoted A, B, C, ..., and a rectangle with n circles corresponds to an agent composed of\nn primitives trained on the respective tasks. Top row: activation of primitives for agents trained\non single tasks. Bottom row: Retrain: two primitives are trained on A and transferred to B. The\nresults (success rates) indicate that the multi-primitive model is substantially more sample ef\ufb01cient\nthan the baseline (transfer A2C). Copy and Combine: more primitives are added to the model over\ntime in a plug-and-play fashion (2 primitives are trained on A; the model is extended with a copy of\nitself; the resulting four-primitive model is trained on B.) This is more sample ef\ufb01cient than other\nstrong baselines, such as [13, 4]. Zero-Shot Generalization: A set of primitives is trained on C, and\nzero-shot generalization to A and B is evaluated. The primitives learn a form of spatial decomposition\nwhich allows them to be active in both target tasks, A and B.\n5.3\nLearning Ensembles of Functional Primitives\nWe evaluate our approach on a number of RL environments to show that we can indeed learn sets of\nprimitive policies focusing on different aspects of a task and collectively solving it.\nMotion Imitation.\nTo test the scalability of the proposed method, we present a series of tasks from\nthe motion imitation domain. In these tasks, we train a simulated 2D biped character to perform\na variety of highly dynamic skills by imitating motion capture clips recorded from human actors.\nEach mocap clip is represented by a target state trajectory \u03c4 \u2217= {s\u2217\n0, s\u2217\n1, ..., s\u2217\nT }, where s\u2217\nt denotes\nthe target state at timestep t. The input to the policy is augmented with a goal gt = {s\u2217\nt+1, s\u2217\nt+2},\nwhich speci\ufb01es the the target states for the next two timesteps. Both the state st and goal gt are then\nprocessed by the encoder penc(zt|st, gt). The repertoire of skills consists of 8 clips depicting different\ntypes of walks, runs, jumps, and \ufb02ips. The motion imitation approach closely follows Peng et al. [29].\nSnapshots of some of the learned motions are shown in Figure 6.4 To analyze the specialization of the\nvarious primitives, we computed 2D embeddings of states and goals which each primitive is active in,\nand the actions proposed by the primitives. Figure 7 illustrates the embeddings computed with t-SNE\n[41]. The embeddings show distinct clusters for the primitives, suggesting a degree of specialization\nof each primitive to certain states, goals, and actions.\n4See supplementary information for video material.\n7\n\n\n0.0\n0.5\n1.0\n1.5\n2.0\nNumber of frames\n1e6\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\nRelative frequency of activation\nidx 1\nidx 2\n2 goals\n4 goals\n8 goals\n0.0\n0.5\n1.0\n1.5\n2.0\nNumber of training frames\n1e6\nTransfer Baseline\nOur Approach\nFigure 4: Continual Learning Scenario: We consider a continual learning scenario where we train\n2 primitives for 2 goal positions, then transfer (and \ufb01netune) on 4 goal positions then transfer (and\n\ufb01netune) on 8 goals positions. The plot on the left shows the primitives remain activated. The solid\ngreen line shows the boundary between the tasks, The plot on the right shows the number of samples\ntaken by our model and the transfer baseline model across different tasks. We observe that the\nproposed model takes fewer steps than the baseline (an A2C policy trained in a similar way) and the\ngap in terms of the number of samples keeps increasing as tasks become harder.\nA\nB\nC\nD\n\u25e6\u25e6\u25e6\u25e6\u25e6\u25e6\u25e6\u25e6\n0\n1\n2\n3\n4\n5\nNumber of frames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\nRelative frequency of activation\nMethod\n3 goals\n10 goals\nFlat Policy (PPO)\n11 \u00b1 5 %\n4 \u00b1 2 %\nOption critic\n18 \u00b1 10 %\n7 \u00b1 3 %\nMLSH\n32 \u00b1 3 %\n5 \u00b1 3 %\nExplicit high level policy\n21 \u00b1 5 %\n11 \u00b1 2 %\nProposed method\n68 \u00b1 3%\n40 \u00b1 3%\nFigure 5: Left: Multitask setup, where we show that we are able to train 8 primitives when training on\na mixture of 4 tasks. Right: Success rates on the different Ant Maze tasks. Success rate is measured\nas the number of times the ant is able to reach the goal (based on 500 sampled trajectories).\n6\nSummary and Discussion\nWe present a framework for learning an ensemble of primitive policies which can collectively solve\ntasks in a decentralized fashion. Rather than relying on a centralized, learned meta-controller, the\nselection of active primitives is implemented through an information-theoretic mechanism. The\nlearned primitives can be \ufb02exibly recombined to solve more complex tasks. Our experiments show\nthat, on a partially observed \u201cMinigrid\u201d task and a continuous control \u201cant maze\u201d walking task, our\nmethod can enable better transfer than \ufb02at policies and hierarchical RL baselines, including the\nFigure 6: Snapshots of motions learned by the policy. Top: Reference motion clip. Middle:\nSimulated character imitating the reference motion. Bottom: Probability of selecting each primitive.\n8\n\n\nS\nG\nA\nFigure 7: Embeddings visualizing the states (S) and goals (G) which each primitive is active in, and\nthe actions (A) proposed by the primitives for the motion imitation tasks. A total of four primitives\nare trained. The primitives produce distinct clusters.\nMeta-learning Shared Hierarchies model and the Option-Critic framework. On Minigrid, we show\nhow primitives trained with our method can transfer much more successfully to new tasks and on the\nant maze, we show that primitives initialized from a pretrained walking control can learn to walk to\ndifferent goals in a stochastic, multi-modal environment with nearly double the success rate of a more\nconventional hierarchical RL approach, which uses the same pretraining but a centralized high-level\npolicy.\nThe proposed framework could be very attractive for continual learning settings, where one could\nadd more primitive policies over time. Thereby, the already learned primitives would keep their focus\non particular aspects of the task, and newly added ones could specialize on novel aspects.\nAcknowledgements\nThe authors acknowledge the important role played by their colleagues at Mila throughout the\nduration of this work. The authors would like to thank Greg Wayne, Mike Mozer, Matthew Botvnick\nfor very useful discussions. The authors would also like to thank Nasim Rahaman, Samarth Sinha,\nNithin Vasisth, Hugo Larochelle, Jordan Hoffman, Ankesh Anand for feedback on the draft. The\nauthors are grateful to NSERC, CIFAR, Google, Samsung, Nuance, IBM, Canada Research Chairs,\nCanada Graduate Scholarship Program, Nvidia for funding, and Compute Canada for computing\nresources. We are very grateful to Google for giving Google Cloud credits used in this project.\n9\n\n\nReferences\n[1] Alessandro Achille and Stefano Soatto. Information dropout: learning optimal representations\nthrough noise. CoRR, abs/1611.01353, 2016. URL http://arxiv.org/abs/1611.01353.\n[2] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational\ninformation bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.\n00410.\n[3] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with\npolicy sketches. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pages 166\u2013175. JMLR. org, 2017.\n[4] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pages\n1726\u20131734, 2017.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym, 2016.\n[6] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environ-\nment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.\n[7] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n[8] Christian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search.\nIn Arti\ufb01cial Intelligence and Statistics, pages 273\u2013281, 2012.\n[9] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural\ninformation processing systems, pages 271\u2013278, 1993.\n[10] Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function\ndecomposition. Journal of Arti\ufb01cial Intelligence Research, 13:227\u2013303, 2000.\n[11] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you\nneed: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.\n[12] Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical\nreinforcement learning. arXiv preprint arXiv:1704.03012, 2017.\n[13] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman. Meta Learning Shared Hierarchies. arXiv\ne-prints, October 2017.\n[14] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared\nhierarchies. arXiv preprint arXiv:1710.09767, 2017.\n[15] Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies\nfor hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808, 2018.\n[16] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom\nErez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in\nrich environments. arXiv preprint arXiv:1707.02286, 2017.\n[17] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei,\nC Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual rea-\nsoning. In Proceedings of the IEEE International Conference on Computer Vision, pages\n2989\u20132998, 2017.\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[19] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical\ndeep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In\nAdvances in neural information processing systems, pages 3675\u20133683, 2016.\n10\n\n\n[20] Libin Liu and Jessica Hodgins. Learning to schedule control fragments for physics-based\ncharacters using deep q-learning. ACM Transactions on Graphics, 36(3), 2017.\n[21] Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for\noption discovery in reinforcement learning. arXiv preprint arXiv:1703.00956, 2017.\n[22] Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas\nHeess, and Greg Wayne. Hierarchical visuomotor control of humanoids. In International\nConference on Learning Representations, 2019. URL https://openreview.net/forum?\nid=BJfYvo09Y7.\n[23] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne,\nYee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control.\nIn International Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=BJl6TjRcY7.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n[25] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on machine learning, pages 1928\u20131937,\n2016.\n[26] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Sch\u00f6lkopf.\nLearning independent causal mechanisms. arXiv preprint arXiv:1712.00961, 2017.\n[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\nPyTorch. In NIPS Autodiff Workshop, 2017.\n[28] Xue Bin Peng, Glen Berseth, Kangkang Yin, and Michiel Van De Panne. Deeploco: Dynamic\nlocomotion skills using hierarchical deep reinforcement learning. ACM Trans. Graph., 36\n(4):41:1\u201341:13, July 2017. ISSN 0730-0301. doi: 10.1145/3072959.3073602. URL http:\n//doi.acm.org/10.1145/3072959.3073602.\n[29] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-\nguided deep reinforcement learning of physics-based character skills. ACM Trans. Graph.,\n37(4):143:1\u2013143:14, July 2018. ISSN 0730-0301. doi: 10.1145/3197517.3201311. URL\nhttp://doi.acm.org/10.1145/3197517.3201311.\n[30] Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing networks and\nthe challenges of modular and compositional computation. arXiv preprint arXiv:1904.12774,\n2019.\n[31] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg,\nDavid Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning.\narXiv preprint arXiv:1703.01161, 2017.\n[32] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation.\narXiv preprint\narXiv:1506.02438, 2015.\n[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[34] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,\n1998.\n[35] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Proceedings of the 12th\nInternational Conference on Neural Information Processing Systems, NIPS\u201999, pages 1057\u2013\n1063, Cambridge, MA, USA, 1999. MIT Press. URL http://dl.acm.org/citation.cfm?\nid=3009657.3009806.\n11\n\n\n[36] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):\n181\u2013211, 1999.\n[37] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):\n181\u2013211, 1999.\n[38] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for\nmachine learning. University of Toronto, Technical Report, 2012.\n[39] Naftali Tishby, Fernando C. N. Pereira, and William Bialek. The information bottleneck method.\nCoRR, physics/0004057, 2000. URL http://arxiv.org/abs/physics/0004057.\n[40] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n5026\u20135033. IEEE, 2012.\n[41] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of\nMachine Learning Research, 9:2579\u20132605, 2008. URL http://www.jmlr.org/papers/v9/\nvandermaaten08a.html.\n[42] Ronald J. Williams.\nSimple Statistical Gradient-Following Algorithms for Connectionist\nReinforcement Learning. Machine Learning, 8(3-4):229\u2013256, 1992. ISSN 0885-6125. doi:\n10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.\n[43] Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region\nmethod for deep reinforcement learning using kronecker-factored approximation. In Advances\nin neural information processing systems, pages 5279\u20135288, 2017.\nA\nInterpretation of the regularization term\nThe regularization term is given by\nLreg =\nX\nk\n\u03b1kLk ,\nwhere\n\u03b1k = eLk/\nX\nj\neLj ,\nand thus\nlog \u03b1k = Lk \u2212log\nX\nj\neLj ,\nor\nLk = log \u03b1k + LSE(L1, . . . , LK) ,\nwhere LSE(L1, . . . , LK) = log P\nj eLj is independent of k.\nPlugging this in, and using P \u03b1k = 1, we get\nLreg =\nX\nk\n\u03b1k log \u03b1k + LSE(L1, . . . , LK) = \u2212H(\u03b1) + LSE(L1, . . . , LK) .\nInformation-theoretic interpretation\nNotably, Lreg also represents an upper bound to the KL-\ndivergence of a mixture of the currently active primitives and a prior,\nLreg \u2265DKL(\nX\nk\n\u03b1kpenc(Zk|S)||N(0, 1)) ,\nand thus can be regarded as a term limiting the information content of the mixture of all active\nprimitives. This arises from the convexity properties of the KL divergence, which directly lead to\nDKL(\nX\nk\n\u03b1kfk||g) \u2264\nX\nk\n\u03b1kDKL(fk||g) .\n12\n\n\nB\nAdditional Results\nB.1\n2D Bandits Environment\nIn order to test if our approach can learn distinct primitives, we used the 2D moving bandits tasks\n(introduced in [14]). In this task, the agent is placed in a 2D world and is shown the position of two\nrandomly placed points. One of these points is the goal point but the agent does not know which. We\nuse the sparse reward setup where the agent receives the reward of 1 if it is within a certain distance\nof the goal point and 0 at all other times. Each episode lasts for 50 steps and to get the reward, the\nlearning agent must reach near the goal point in those 50 steps. The agent\u2019s action space consists of 5\nactions - moving in one of the four cardinal directions (top, down, left, right) and staying still.\nB.1.1\nResults for 2D Bandits\nWe want to answer the following questions:\n1. Can our proposed approach learn primitives which remain active throughout training?\n2. Can our proposed approach learn primitives which can solve the task?\nWe train two primitives on the 2D Bandits tasks and evaluate the relative frequency of activation of\nthe primitives throughout the training. It is important that both the primitives remain active. If only 1\nprimitive is acting most of the time, its effect would be the same as training a \ufb02at policy. We evaluate\nthe effectiveness of our model by comparing the success rate with a \ufb02at A2C baseline. Figure 8 shows\nthat not only do both the primitives remain active throughout training, our approach also outperforms\nthe baseline approach.\n0\n1\n2\n3\n4\n5\nNumber of frames\n1e6\n20\n40\n60\n80\nPercentage of episodes completed\nFlat Policy\nDecentralized Policy\n0\n1\n2\n3\n4\nNumber of frames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\nFigure 8: Performance on the 2D bandits task. Left: The comparison of our model (blue curve -\ndecentralized policy) with the baseline (red curve - \ufb02at policy) in terms of success rate shows the\neffectiveness of our proposed approach. Right: Relative frequency of activation of the primitives\n(normalized to sum up to 1). Both primitives are utilized throughout the training.\nB.2\nFour-rooms Environment\nWe consider the Four-rooms gridworld environment [37] where the agent has to navigate its way\nthrough a grid of four interconnected rooms to reach a goal position within the grid. The agent\ncan perform one of the following four actions: move up, move down, move left, move right. The\nenvironment is stochastic and with 1/3 probability, the agent\u2019s chosen action is ignored and a new\naction (randomly selected from the remaining 3 actions) is executed ie the agent\u2019s selected action\nis executed with a probability of only 2/3 and the agent takes any of the 3 remaining actions with a\nprobability of 1/9 each.\nB.2.1\nTask distribution for the Four-room Environment\nIn the Four-room environment, the agent has to navigate to a goal position which is randomly selected\nfrom a set of goal positions. We can use the size of this set of goal positions to de\ufb01ne a curriculum\nof task distributions. Since the environment does not provide any information about the goal state,\nthe larger the goal set, harder is the task as the now goal could be any element from a larger set.\nThe choice of the set of goal states and the choice of curriculum does not affect the environment\n13\n\n\ndynamics. Speci\ufb01cally, we consider three tasks - Fourroom-v0, Fourroom-v1 and Fourroom-v2 with\nthe set of 2, 4 and 8 goal positions respectively. The set of goal positions for each task is \ufb01xed but not\nknown to the learning agent. We expect, and empirically verify, that the Fourroom-v0 environment\nrequires the least number of samples to be learned, followed by the Fourroom-v1 and the Fourroom-v2\nenvironment (\ufb01gure 6 in the paper).\nB.2.2\nResults for Four-rooms environment\nWe want to answer the following questions:\n1. Can our proposed approach learn primitives that remain active when training the agent over\na sequence of tasks?\n2. Can our proposed approach be used to improve the sample ef\ufb01ciency of the agent over a\nsequence of tasks?\nTo answer these questions, we consider two setups. In the baseline setup, we train a \ufb02at A2C policy\non Fourrooms-v0 till it achieves a 100 % success rate during evaluation. Then we transfer this policy\nto Fourrooms-v1 and continue to train till it achieves a 100 % success rate during the evaluation\non Fourrooms-v1. We transfer the policy one more time to Fourrooms-v2 and continue to train the\npolicy until it reaches a 60% success rate. In the last task(Fourrooms-v2), we do not use 100% as the\nthreshold as the models do not achieve 100% for training even after training for 10M frames. We use\n60% as the baseline models generally converge around this value.\nIn the second setup, we repeat this exercise of training on one task and transferring to the next task\nwith our proposed model. Note that even though our proposed model converges to a higher value\nthan 60% in the last task(Fourrooms-v2), we compare the number of samples required to reach 60%\nsuccess rate to provide a fair comparison with the baseline.\nC\nImplementation Details\nIn this section, we describe the implementation details which are common for all the models. Other\ntask-speci\ufb01c details are covered in the respective task sections.\n1. All the models (proposed as well as the baselines) are implemented in Pytorch 1.1 unless\nstated otherwise. [27].\n2. For Meta-Learning Shared Hierarchies [14] and Option-Critic [4], we adapted the author\u2019s\nimplementations 5for our environments.\n3. During the evaluation, we use 10 processes in parallel to run 500 episodes and compute the\npercentage of times the agent solves the task within the prescribed time limit. This metric is\nreferred to as the \u201csuccess rate\u201d.\n4. The default time limit is 500 steps for all the tasks unless speci\ufb01ed otherwise.\n5. All the feedforward networks are initialized with the orthogonal initialization where the\ninput tensor is \ufb01lled with a (semi) orthogonal matrix.\n6. For all the embedding layers, the weights are initialized using the unit-Gaussian distribution.\n7. The weights and biases for all the GRU model are initialized using the uniform distribution\nfrom U(\u2212\n\u221a\nk,\n\u221a\nk) where k =\n1\nhidden_size.\n8. During training, we perform 64 rollouts in parallel to collect 5-step trajectories.\n9. The \u03b2ind and \u03b2reg parameters are both selected from the set {0.001, 0.005, 0.009} by\nperforming validation.\nIn section D.4.2, we explain all the components of the model architecture along with the implemen-\ntation details in the context of the MiniGrid Environment. For the subsequent environments, we\ndescribe only those components and implementation details which are different than their counterpart\nin the MiniGrid setup and do not describe the components which work identically.\n5https://github.com/openai/mlsh, https://github.com/jeanharb/option_critic\n14\n\n\nD\nMiniGrid Environment\nWe use the MiniGrid environment [6] which is an open-source, grid-world environment package 6.\nIt provides a family of customizable reinforcement learning environments that are compatible with\nthe OpenAI Gym framework [5]. Since the environments can be easily extended and modi\ufb01ed, it is\nstraightforward to control the complexity of the task (eg controlling the size of the grid, the number\nof rooms or the number of objects in the grid, etc). Such \ufb02exibility is very useful when experimenting\nwith curriculum learning or testing for generalization.\nD.1\nThe World\nIn MiniGrid, the world (environment for the learning agent) is a rectangular grid of size say MxN.\nEach tile in the grid contains either zero or one object. The possible object types are wall, \ufb02oor, lava,\ndoor, key, ball, box and goal. Each object has an associated string (which denote the object type) and\nan associated discrete color (could be red, green, blue, purple, yellow and grey). By default, walls are\nalways grey and goal squares are always green. Certain objects have special effects. For example, a\nkey can unlock a door of the same color.\nD.1.1\nReward Function\nWe consider the sparse reward setup where the agent gets a reward (of 1) only if it completes the task\nand 0 at all other time steps. We also apply a time limit of 500 steps on all the tasks ie the agent must\ncomplete the task in 500 steps. A task is terminated either when the agent solves the task or when the\ntime limit is reached - whichever happens \ufb01rst.\nD.1.2\nAction Space\nThe agent can perform one of the following seven actions per timestep: turn left, turn right, move\nforward, pick up an object, drop the object being carried, toggle, done (optional action).\nThe agent can use the turn left and turn right actions to rotate around and face one of the 4 possible\ndirections (north, south, east, west). The move forward action makes the agent move from its current\ntile onto the tile in the direction it is currently facing, provided there is nothing on that tile, or that the\ntile contains an open door. The toggle actions enable the agent to interact with other objects in the\nworld. For example, the agent can use the toggle action to open the door if they are right in front of it\nand have the key of matching color.\nD.1.3\nObservation Space\nThe MiniGrid environment provides partial and egocentric observations. For all our experiments, the\nagent sees the view of a square of 4x4 tiles in the direction it is facing. The view includes the tile on\nwhich the agent is standing. The observations are provided as a tensor of shape 4x4x3. However,\nnote that this tensor does not represent RGB images. The \ufb01rst two channels denote the view size and\nthe third channel encodes three integer values. The \ufb01rst integer value describes the type of the object,\nthe second value describes the color of the object and the third value describes if the doors are open\nor closed. The bene\ufb01t of using this encoding over the RGB encoding is that this encoding is more\nspace-ef\ufb01cient and enables faster training. For human viewing, the fully observable, RGB image\nview of the environments is also provided and we use that view as an example in the paper.\nAdditionally, the environment also provides a natural language description of the goal. An example\nof the goal description is: \u201cUnlock the door and pick up the red ball\u201d. The learning agent and the\nenvironment use a shared vocabulary where different words are assigned numbers and the environment\nprovides a number-encoded goal description along with each observation. Since different instructions\ncan be of different lengths, the environment pads the goal description with <unk> tokens to ensure\nthat the sequence length is the same. When encoding the instruction, the agent ignores the padded\nsub-sequence in the instruction.\n15\n\n\nFigure 9: RGB view of the Fetch environment.\nFigure 10: RGB view of the Unlock environment.\nD.2\nTasks in MiniGrid Environment\nWe consider the following tasks in the MiniGrid environment:\n1. Fetch: In the Fetch task, the agent spawns at an arbitrary position in a 8 \u00d7 8 grid (\ufb01gure 9 ).\nIt is provided with a natural language goal description of the form \u201cgo fetch a yellow box\u201d.\nThe agent has to navigate to the object being referred to in the goal description and pick it\nup.\n2. Unlock: In the Unlock task, the agent spawns at an arbitrary position in a two-room grid\nenvironment. Each room is 8 \u00d7 8 square (\ufb01gure 10 ). It is provided with a natural language\n6https://github.com/maximecb/gym-minigrid\nFigure 11: RGB view of the UnlockPickup environment.\n16\n\n\ngoal description of the form \u201copen the door\u201d. The agent has to \ufb01nd the key that corresponds\nto the color of the door, navigate to that key and use that key to open the door.\n3. UnlockPickup: This task is basically a union of the Unlock and the Fetch tasks. The agent\nspawns at an arbitrary position in a two-room grid environment. Each room is 8 \u00d7 8 square\n(\ufb01gure 11 ). It is provided with a natural language goal description of the form \u201copen the\ndoor and pick up the yellow box\u201d. The agent has to \ufb01nd the key that corresponds to the color\nof the door, navigate to that key, use that key to open the door, enter the other room and pick\nup the object mentioned in the goal description.\nD.3\nModel Architecture\nD.3.1\nTraining Setup\nConsider an agent training on any task in the MiniGrid suite of environments. At the beginning\nof an episode, the learning agent spawns at a random position. At each step, the environment\nprovides observations in two modalities - a 4 \u00d7 4 \u00d7 3 tensor xt (an egocentric view of the state of the\nenvironment) and a variable length goal description g. We describe the design of the learning agent\nin terms of an encoder-decoder architecture.\nD.3.2\nEncoder Architecture\nThe agent\u2019s encoder network consists of two models - a CNN+GRU based observation encoder and\na GRU [7] based goal encoder\nObservation Encoder:\nIt is a three layer CNN with the output channel sizes set to 16, 16 and 32 respectively (with ReLU\nlayers in between) and kernel size set to 2 \u00d7 2 for all the layers. The output of the CNN is \ufb02attened\nand fed to a GRU model (referred to as the observation-rnn) with 128-dimensional hidden state. The\noutput from the observation-rnn represents the encoding of the observation.\nGoal Encoder:\nIt comprises of an embedding layer followed by a unidirectional GRU model. The dimension of the\nembedding layer and the hidden and the output layer of the GRU model are all set to 128.\nThe concatenated output of the observation encoder and the goal encoder represents the output of\nthe encoder.\nD.3.3\nDecoder\nThe decoder network comprises the action network and the critic network - both of which are\nimplemented as feedforward networks. We now describe the design of these networks.\nD.3.4\nValue Network\n1. Two-layer feedforward network with the tanh non-linearity.\n2. Input: Concatenation of z and the current hidden state of the observation-rnn.\n3. Size of the input to the \ufb01rst layer and the second layer of the policy network are 320 and 64\nrespectively.\n4. Produces a scalar output.\nD.4\nComponents speci\ufb01c to the proposed model\nThe components that we described so far are used by both the baselines as well as our proposed\nmodel. We now describe the components that are speci\ufb01c to our proposed model. Our proposed\nmodel consists of an ensemble of primitives and the components we describe apply to each of those\nprimitives.\n17\n\n\nD.4.1\nInformation Bottleneck\nGiven that we want to control and regularize the amount of information that the encoder encodes, we\ncompute the KL divergence between the output of the action-feature encoder network and a diagonal\nunit Gaussian distribution. More is the KL divergence, more is the information that is being encoded\nwith respect to the Gaussian prior and vice-versa. Thus we regularize the primitives to minimize the\nKL divergence.\nD.4.2\nHyperparameters\nTable 1 lists the different hyperparameters for the MiniGrid tasks.\nParameter\nValue\nLearning Algorithm\nA2C [43]\nOpitimizer \u2018\nRMSProp[38]\nlearning rate\n7 \u00b7 10\u22124\nbatch size\n64\ndiscount\n0.99\nlambda (for GAE [32])\n0.95\nentropy coef\ufb01cient\n10\u22122\nloss coef\ufb01cient\n0.5\nMaximum gradient norm\n0.5\nTable 1: Hyperparameters\nE\n2D Bandits Environment\nE.0.1\nObservation Space\nThe 2D bandits task provides a 6-dimensional \ufb02at observation. The \ufb01rst two dimensions correspond\nto the (x, y) coordinates of the current position of the agent and the remaining four dimensions\ncorrespond to the (x, y) coordinates of the two randomly chosen points.\nE.1\nModel Architecture\nE.1.1\nTraining Setup\nConsider an agent training on the 2D bandits tasks. The learning agent spawns at a \ufb01xed position and\nis randomly assigned two points. At each step, the environmental observation provides the current\nposition of the agent as well the position of the two points. We describe the design of the learning\nagent in terms of an encoder-decoder architecture.\nE.1.2\nEncoder Architecture\nThe agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn) with a hidden state size of 128. The 6-dimensional observation from the environment is the input\nto the GRU model. The output from the observation-rnn represents the encoding of the observation.\nE.2\nHyperparameters\nThe implementation details for the 2D Bandits environment are the same as that for MiniGrid\nenvironment and are described in detail in section D.4.2. In the table below, we list the values of the\ntask-speci\ufb01c hyperparameters.\n18\n\n\nParameter\nValue\nLearning Algorithm\nPPO [33]\nepochs per update (PPO)\n10\nOptimizer \u2018\nAdam[18]\nlearning rate\n3 \u00b7 10\u22125\n\u03b21\n0.9\n\u03b22\n0.999\nbatch size\n64\ndiscount\n0.99\nentropy coef\ufb01cient\n0\nloss coef\ufb01cient\n1.0\nMaximum gradient norm\n0.5\nTable 2: Hyperparameters\nF\nFour-rooms Environment\nF.1\nThe World\nIn the Four-rooms setup, the world (environment for the learning agent) is a square grid of say 11\u00d711.\nThe grid is divided into 4 rooms such that each room is connected with two other rooms via hallways.\nThe layout of the rooms is shown in \ufb01gure 12. The agent spawns at a random position and has to\nnavigate to a goal position within 500 steps.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nFigure 12: View of the four-room environment\nF.1.1\nReward Function\nWe consider the sparse reward setup where the agent gets a reward (of 1) only if it completes the task\n(and reaches the goal position) and 0 at all other time steps. We also apply a time limit of 300 steps\non all the tasks ie the agent must complete the task in 300 steps. A task is terminate either when the\nagent solves the task or when the time limit is reached - whichever happens \ufb01rst.\nF.1.2\nObservation Space\nThe environment is a 11 \u00d7 11 grid divided into 4 interconnected rooms. As such, the environment has\na total of 104 states (or cells) that can be occupied. These states are mapped to integer identi\ufb01ers. At\nany time t, the environment observation is a one-hot representation of the identi\ufb01er corresponding to\nthe state (or the cell) the agent is in right now. ie the environment returns a vectors of zeros with only\none entry being 1 and the index of this entry gives the current position of the agent. The environment\ndoes not return any information about the goal state.\n19\n\n\nF.2\nModel Architecture for Four-room Environment\nF.2.1\nTraining Setup\nConsider an agent training on any task in the Four-room suite of environments. At the beginning\nof an episode, the learning agent spawns at a random position and the environment selects a goal\nposition for the agent. At each step, the environment provides a one-hot representation of the agent\u2019s\ncurrent position (without including any information about the goal state). We describe the design of\nthe learning agent in terms of an encoder-decoder architecture.\nF.3\nEncoder Architecture\nThe agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn with a hidden state size of 128. The 104-dimensional one-hot input from the environment is fed\nto the GRU model. The output from the observation-rnn represents the encoding of the observation.\nThe implementation details for the Four-rooms environment are the same as that for MiniGrid\nenvironment and are described in detail in section D.4.2.\nG\nAnt Maze Environment\nWe use the Mujoco-based quadruple ant [40] to evaluate the transfer performance of our approach\non the cross maze environment [15]. The training happens in two phases. In the \ufb01rst phase, we\ntrain the ant to walk on a surface using a motion reward and using just 1 primitive. In the second\nphase, we make 4 copies of this trained policy and train the agent to navigate to a goal position in a\nmaze (Figure 13). The goal position is chosen from a set of 3 (or 10) goals. The environment is a\ncontinuous control environment and the agent can directly manipulate the movement of joints and\nlimbs.\nFigure 13: View of the Ant Maze environment with 3 goals\nG.0.1\nObservation Space\nIn the \ufb01rst phase (training the ant to walk), the observations from the environment correspond to the\nstate-space representation ie a real-valued vector that describes the state of the ant in mechanical\nterms - position, velocity, acceleration, angle, etc of the joints and limbs. In the second phase (training\nthe ant to navigate the maze), the observation from the environment also contains the location of the\ngoal position along with the mechanical state of the ant.\nG.1\nModel Architecture for Ant Maze Environment\nG.1.1\nTraining Setup\nWe describe the design of the learning agent in terms of an encoder-decoder architecture.\nG.1.2\nEncoder Architecture\nThe agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn with a hidden state size of 128. The real-valued state vector from the environment is fed to the\nGRU model. The output from the observation-rnn represents the encoding of the observation. Note\n20\n\n\nthat in the case of phase 1 vs phase 2, only the size of the input to the observation-rnn changes and\nthe encoder architecture remains the same.\nG.1.3\nDecoder\nThe decoder network comprises the action network and the critic network. All these networks are\nimplemented as feedforward networks. The design of these networks is very similar to that of the\ndecoder model for the MiniGrid environment as described in section D.3.3 with just one difference.\nIn this case, the action space is continuous so the action-feature decoder network produces the mean\nand log-standard-deviation for a diagonal Gaussian policy. This is used to sample a real-valued action\nto execute in the environment.\n21\n\n\n",
    "title": "Reinforcement Learning with Competitive Ensembles",
    "abstract": "Reinforcement learning agents that operate in diverse and complex environments can bene\ufb01t from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both \ufb02at and hierarchical policies in terms of generalization. 1",
    "sections": [
      {
        "header": "Reinforcement Learning with Competitive Ensembles",
        "content": "of Information-Constrained Primitives\nAnirudh Goyal1, Shagun Sodhani1, Jonathan Binas1, Xue Bin Peng2\nSergey Levine2, Yoshua Bengio1\u2020\n1 Mila, Universit\u00e9 de Montr\u00e9al\n2 University of California, Berkeley\n\u2020CIFAR Senior Fellow."
      },
      {
        "header": "Reinforcement learning agents that operate in diverse and complex environments",
        "content": "can bene\ufb01t from the structured decomposition of their behavior. Often, this is\naddressed in the context of hierarchical reinforcement learning, where the aim is\nto decompose a policy into lower-level primitives or options, and a higher-level\nmeta-policy that triggers the appropriate behaviors for a given situation. However,\nthe meta-policy must still produce appropriate decisions in all states. In this\nwork, we propose a policy design that decomposes into primitives, similarly to\nhierarchical reinforcement learning, but without a high-level meta-policy. Instead,\neach primitive can decide for themselves whether they wish to act in the current\nstate. We use an information-theoretic mechanism for enabling this decentralized\ndecision: each primitive chooses how much information it needs about the current\nstate to make a decision and the primitive that requests the most information about\nthe current state acts in the world. The primitives are regularized to use as little\ninformation as possible, which leads to natural competition and specialization. We\nexperimentally demonstrate that this policy architecture improves over both \ufb02at\nand hierarchical policies in terms of generalization."
      },
      {
        "header": "Introduction",
        "content": "Learning policies that generalize to new environments or tasks is a fundamental challenge in rein-\nforcement learning. While deep reinforcement learning has enabled training powerful policies, which\noutperform humans on speci\ufb01c, well-de\ufb01ned tasks [24], their performance often diminishes when the\nproperties of the environment or the task change to regimes not encountered during training.\nThis is in stark contrast to how humans learn, plan, and act: humans can seamlessly switch between\ndifferent aspects of a task, transfer knowledge to new tasks from remotely related but essentially\ndistinct prior experience, and combine primitives (or skills) used for distinct aspects of different tasks\nin meaningful ways to solve new problems. A hypothesis hinting at the reasons for this discrepancy\nis that the world is inherently compositional, such that its features can be described by compositions\nof small sets of primitive mechanisms [26]. Since humans seem to bene\ufb01t from learning skills and\nlearning to combine skills, it might be a useful inductive bias for the learning models as well.\nThis is addressed to some extent by the hierarchical reinforcement learning (HRL) methods, which\nfocus on learning representations at multiple spatial and temporal scales, thus enabling better explo-\nration strategies and improved generalization performance [9, 36, 10, 19]. However, hierarchical\napproaches rely on some form of learned high-level controller, which decides when to activate\ndifferent components in the hierarchy. While low-level sub-policies can specialize to smaller portions\nof the state space, the top-level controller (or master policy) needs to know how to deal with any\ngiven state. That is, it should provide optimal behavior for the entire accessible state space. As the\nPreprint. Under review.\narXiv:1906.10667v1  [cs.LG]  25 Jun 2019\n\n\nx\nx\nx\n\u03c01\n\u03c02\n\u03c03\na\n\u03c01\n\u03c02\n\u03c03\na\n\u03c01\n\u03c02\n\u03c03\na"
      },
      {
        "header": "Compositional",
        "content": "environment\n1\n2\n3\nFigure 1: Illustration of our model. An intrinsic competition mechanism, based on the amount of\ninformation each primitive provides, is used to select a primitive to be active for a given input. Each\nprimitive focuses on distinct features of the environment; in this case, one policy focuses on boxes, a\nsecond one on gates, and the third one on spheres.\nmaster policy is trained on a particular state distribution, learning it in a way that generalizes to new\nenvironments effectively can, therefore, become the bottleneck for such approaches [31, 3].\nWe argue, and empirically show, that in order to achieve better generalization, the interaction between\nthe low-level primitives and the selection thereof should itself be performed without requiring a\nsingle centralized network that understands the entire state space. We, therefore, propose a fully\ndecentralized approach as an alternative to standard HRL, where we only learn a set of low-level\nprimitives without learning a high-level controller. We construct a factorized representation of the\npolicy by learning simple \u201cprimitive\u201d policies, which focus on distinct regions of the state space.\nRather than being gated by a single meta-policy, the primitives directly compete with one another\nto determine which one should be active at any given time, based on the degree to which their state\nencoders \u201crecognize\u201d the current state input."
      },
      {
        "header": "We frame the problem as one of information transfer between the current state and a dynamically",
        "content": "selected primitive policy. Each policy can by itself decide to request information about the current\nstate, and the amount of information requested is used to determine which primitive acts in the\ncurrent state. Since the amount of state information that a single primitive can access is limited, each\nprimitive is encouraged to use its resources wisely. Constraining the amount of accessible information\nin this way naturally leads to a decentralized competition and decision mechanism where individual\nprimitives specialize in smaller regions of the state space. We formalize this information-driven\nobjective based on the variational information bottleneck. The resulting set of competing primitives\nachieves both a meaningful factorization of the policy and an effective decision mechanism for which\nprimitives to use. Importantly, not relying on a centralized meta-policy means that individual primitive\nmechanisms can be recombined in a \u201cplug-and-play\u201d fashion, and can be transferred seamlessly to\nnew environments.\nContributions:\nIn summary, the contributions of our work are as follows: (1) We propose a method\nfor learning and operating a set of functional primitives in a fully decentralized way, without requiring\na high-level meta-controller to select active primitives (see Figure 1 for illustration). (2) We introduce\nan information-theoretic objective, the effects of which are twofold: a) it leads to the specialization of\nindividual primitives to distinct regions of the state space, and b) it enables a competition mechanism,\nwhich is used to select active primitives in a decentralized manner. (3) We demonstrate the superior\ntransfer learning performance of our model, which is due to the \ufb02exibility of the proposed framework\nregarding the dynamic addition, removal, and recombination of primitives. Decentralized primitives\ncan be successfully transferred to larger or previously unseen environments, and outperform models\nwith an explicit meta-controller for primitive selection.\n2"
      },
      {
        "header": "Preliminaries",
        "content": "We consider a Markov decision process (MDP) de\ufb01ned by the tuple (S, A, P, r, \u03b3), where the state\nspace S and the action space A may be discrete or continuous. The environment emits a bounded\nreward r : S \u00d7 A \u2192[rmin, rmax] on each transition and \u03b3 \u2208[0, 1) is the discount factor. \u03c0(.|s)\ndenotes a policy over the actions given the current state s. R(\u03c0) = E\u03c0[P\nt \u03b3tr(st)] denotes the\nexpected total return when the policy \u03c0 is followed. The standard objective in reinforcement learning\nis to maximize the expected total return R(\u03c0). We use the concept of the information bottleneck [39] to\nlearn compressed representations. The information bottleneck objective is formalized as minimizing\nthe mutual information of a bottleneck representation layer with the input while maximizing its\nmutual information with the corresponding output. This type of input compression has been shown to\nimprove generalization [39, 1, 2]. Computing the mutual information is generally intractable, but can\nbe approximated using variational inference [2].\n3\nInformation-Theoretic Decentralized Learning of Distinct Primitives\nOur goal is to learn a policy, composed of multiple primitive sub-policies, to maximize average\nreturns over T-step interactions for a distribution of tasks. Simple primitives which focus on solving\na part of the given task (and not the complete task) should generalize more effectively, as they can\nbe applied to similar aspects of different tasks (subtasks) even if the overall objective of the tasks\nare drastically different. Learning primitives in this way can also be viewed as learning a factorized\nrepresentation of a policy, which is composed of several independent policies.\nOur proposed approach consists of three components: 1) a mechanism for restricting a particular\nprimitive to a subset of the state space; 2) a competition mechanism between primitives to select the\nmost effective primitive for a given state; 3) a regularization mechanism to improve the generalization\nperformance of the policy as a whole. We consider experiments with both \ufb01xed and variable sets of\nprimitives and show that our method allows for primitives to be added or removed during training, or\nrecombined in new ways. Each primitive is represented by a differentiable, parameterized function\napproximator, such as a neural network.\n3.1"
      },
      {
        "header": "Primitives with an Information Bottleneck",
        "content": "state s\n(z1, . . . , zK)\n(L1, . . . , LK)\n(\u03b11, . . . , \u03b1K)\nE\u03b1\u03c0k\naction a\nencoder\nDKL(\u00b7||N)\ndecoder\nsoftmax\nFigure 2:\nThe primitive-selection\nmechanism of our model. The primi-\ntive with most information acts in the\nenvironment, and gets the reward."
      },
      {
        "header": "To encourage each primitive to encode information from a",
        "content": "particular part of state space, we limit the amount of informa-\ntion each primitive can access from the state. In particular,\neach primitive has an information bottleneck with respect to\nthe input state, preventing it from using all the information\nfrom the state.\nTo implement an information bottleneck, we design each of\nthe K primitives to be composed of an encoder penc(Zk | S)\nand a decoder pdec(A | Zk), together forming the primitive\npolicy,\n\u03c0k\n\u03b8(A | S) =\nR\nz penc(zk | S) pdec(A | zk) dzk .1"
      },
      {
        "header": "The encoder output Z is meant to represent the information",
        "content": "about the current state S that an individual primitive believes\nis important to access in order to perform well. The decoder\ntakes this encoded information and produces a distribution\nover the actions A. Following the variational information bottleneck objective [2], we penalize the\nKL divergence of Z and the prior,\nLk = DKL(penc(Zk|S)||N(0, 1)) .\n(1)\nIn other words, a primitive pays an \u201cinformation cost\u201d proportional to Lk for accessing the information\nabout the current state.\n1In practice, we estimate the marginalization over Z using a single sample throughout our experiments.\n3\n\n\nIn the experiments below, we \ufb01x the prior to be a unit Gaussian. In the general case, we can learn the\nprior as well and include its parameters in \u03b8. The information bottleneck encourages each primitive to\nlimit its knowledge about the current state, but it will not prevent multiple primitives from specializing\nto similar parts of the state space. To mitigate this redundancy, and to make individual primitives focus\non different regions of the state space, we introduce an information-based competition mechanism to\nencourage diversity among the primitives, as described in the next section.\n3.2\nCompeting Information-Constrained Primitives\nWe can use the information measure from equation 1 to de\ufb01ne a selection mechanism for the primitives\nwithout having to learn a centralized meta-policy. The idea is that the information content of an\nindividual primitive encodes its effectiveness in a given state s such that the primitive with the highest\nvalue Lk\nshould be activated in that particular state. We compute normalized weights \u03b1k for each of the\nk = 1, . . . , K primitives by applying the softmax operator,\n\u03b1k = exp(Lk)/ P\nj exp(Lj) .\n(2)\nThe resulting weights \u03b1k can be treated as a probability distribution that can be used in different ways:\nform a mixture of primitives, sample a primitive using from the distribution, or simply select the\nprimitive with the maximum weight. The selected primitive is then allowed to act in the environment.\nTrading Reward and Information: To make the different primitives compete for competence in\nthe different regions of the state space, the environment reward is distributed according to their\nparticipation in the global decision, i.e. the reward rk given to the kth primitive is weighted by\nits selection coef\ufb01cient, such that rk = \u03b1kr, with r = P\nk rk. Hence, a primitive gets a higher\nreward for accessing more information about the current state, but that primitive also pays the price\n(equal to information cost) for accessing the state information. Hence, a primitive that does not\naccess any state information is not going to get any reward. The information bottleneck and the\ncompetition mechanism, when combined with the overall reward maximization objective, should lead\nto specialization of individual primitives to distinct regions in the state space.\nThat is, each primitive should specialize in a part of the state space that it can reliably associate\nrewards with. Since the entire ensemble still needs to understand all of the state space for the given\ntask, different primitives need to encode and focus on different parts of the state space.\n3.3"
      },
      {
        "header": "Regularization of the Combined Representation",
        "content": "To encourage a diverse set of primitive con\ufb01gurations and to make sure that the model does not col-\nlapse to a single primitive (which remains active at all times), we introduce an additional regularization\nterm,\nLreg = P\nk \u03b1kLk .\n(3)\nThis can be rewritten (see Appendix A) as\nLreg = \u2212H(\u03b1) + LSE(L1, . . . , LK) ,\n(4)\nwhere H(\u03b1) is the entropy of the \u03b1 distribution, and LSE is the LogSumExp function,\nLSE(x) = log(P\nj exj). The desired behavior is achieved by minimizing Lreg. Increasing the entropy\nof \u03b1 leads to a diverse set of primitive selections, ensuring that different combinations of the primitives\nare used. On the other hand, LSE approximates the maximum of its arguments, LSE(x) \u2248maxj xj,\nand, therefore, penalizes the dominating Lk terms, thus equalizing their magnitudes.\n3.4"
      },
      {
        "header": "Objective and Algorithm Summary",
        "content": "Our overall objective function consists of 3 terms,\n1. The expected return from the standard RL objective, R(\u03c0) which is distributed to the\nprimitives according to their participation,\n2. The individual bottleneck terms leading the individual primitives to focus on speci\ufb01c parts\nof the state space, Lk for k = 1, . . . , K,\n4\n\n\n3. The regularization term applied to the combined model, Lreg.\nThe overall objective for the kth primitive thus takes the form:\nJk(\u03b8) \u2261E\u03c0\u03b8[rk] \u2212\u03b2indLk \u2212\u03b2regLreg ,\n(5)\nwhere E\u03c0\u03b8 denotes an expectation over the state trajectories generated by the agent\u2019s policy, rk = \u03b1kr\nis the reward given to the kth primitive, and \u03b2ind, \u03b2reg are the parameters controlling the impact of the\nrespective terms.\nImplementation: In our experiments, the encoders penc(zk|S) and decoders pdec(A|zk) are repre-\nsented by neural networks, the parameters of which we denote by \u03b8. Actions are sampled through\neach primitive every step. While our approach is compatible with any RL method, we maximize\nJ(\u03b8) computed on-policy from the sampled trajectories using a score function estimator [42, 35]\nspeci\ufb01cally A2C [25] (unless otherwise noted). Every experimental result reported has been averaged\nover 5 random seeds. Our model introduces 2 extra hyper-parameters \u03b2ind, \u03b2reg."
      },
      {
        "header": "Related Work",
        "content": "There are a wide variety of hierarchical reinforcement learning approaches[34, 9, 10]. One of the\nmost widely applied HRL framework is the Options framework ([36]). An option can be thought of\nas an action that extends over multiple timesteps thus providing the notion of temporal abstraction or\nsubroutines in an MDP. Each option has its own policy (which is followed if the option is selected)\nand the termination condition (to stop the execution of that option). Many strategies are proposed for\ndiscovering options using task-speci\ufb01c hierarchies, such as pre-de\ufb01ned sub-goals [16], hand-designed\nfeatures [12], or diversity-promoting priors [8, 11]. These approaches do not generalize well to new\ntasks. [4] proposed an approach to learn options in an end-to-end manner by parameterizing the\nintra-option policy as well as the policy and termination condition for all the options. Eigen-options\n[21] use the eigenvalues of the Laplacian (for the transition graph induced by the MDP) to derive an\nintrinsic reward for discovering options as well as learning an intra-option policy.\nIn this work, we consider sparse reward setup with high dimensional action spaces. In such a scenario,\nperforming unsupervised pretraining or using auxiliary rewards leads to much better performance\n[13, 12, 16]. Auxiliary tasks such as motion imitation have been applied to learn motor primitives\nthat are capable of performing a variety of sophisticated skills [20, 28, 23, 22].\nOur work is also related to the Neural Module Network family of architectures [3, 17, 30] where\nthe idea is to learn modules that can perform some useful computation like solving a subtask and\na controller that can learn to combine these modules for solving novel tasks. The key difference\nbetween our approach and all the works mentioned above is that we learn functional primitives in a\nfully decentralized way without requiring any high-level meta-controller or master policy."
      },
      {
        "header": "Experimental Results",
        "content": "In this section, we brie\ufb02y outline the tasks that we used to evaluate our proposed method and direct\nthe reader to the appendix for the complete details of each task along with the hyperparameters used\nfor the model. The code is provided with the supplementary material. We designed experiments to\naddress the following questions: a) Learning primitives \u2013 Can an ensemble of primitives be learned\nover a distribution of tasks? b) Transfer Learning using primitives \u2013 Can the learned primitives\nbe transferred to unseen/unsolvable sparse environments? c) Comparison to centralized methods\n\u2013 How does our method compare to approaches where the primitives are trained using an explicit\nmeta-controller, in a centralized way?\nBaselines.\nWe compare our proposed method to the following baselines:\na) Option Critic [4] \u2013 We extended the author\u2019s implementation 2 of the Option Critic architecture\nand experimented with multiple variations in the terms of hyperparameters and state/goal encoding.\nNone of these yielded reasonable performance in partially observed tasks, so we omit it from the\nresults.\n2https://github.com/jeanharb/option_critic\n5\n\n\nb) MLSH (Meta-Learning Shared Hierarchy) [13] \u2013 This method uses meta-learning to learn sub-\npolicies that are shared across tasks along with learning a task-speci\ufb01c high-level master. It also\nrequires a phase-wise training schedule between the master and the sub-policies to stabilize training.\nWe use the MLSH implementation provided by the authors 3.\nc) Transfer A2C: In this method, we \ufb01rst learn a single policy on the one task and then transfer the\npolicy to another task, followed by \ufb01ne-tuning in the second task.\n5.1\nMulti-Task Training\nWe evaluate our model in a partially-observable 2D multi-task environment called Minigrid, similar to\nthe one introduced in [6]. The environment is a two-dimensional grid with a single agent, impassable\nwalls, and many objects scattered in the environment. The agent is provided with a natural language\nstring that speci\ufb01es the task that the agent needs to complete. The setup is partially observable\nand the agent only gets the small, egocentric view of the grid (along with the natural language task\ndescription). We consider three tasks here: the Pickup task (A), where the agent is required to pick up\nan object speci\ufb01ed by the goal string, the Unlock task (B) where the agent needs to unlock the door\n(there could be multiple keys in the environment and the agent needs to use the key which matches\nthe color of the door) and the UnlockPickup task (C), where the agent \ufb01rst needs to unlock a door\nthat leads to another room. In this room, the agent needs to \ufb01nd and pick up the object speci\ufb01ed by\nthe goal string. Additional implementation details of the environment are provided in appendix D.\nDetails on the agent model can be found in appendix D.3.\nWe train agents with varying numbers of primitives on various tasks \u2013 concurrently, as well as in\ntransfer settings. The different experiments are summarized in Figs. 3 and 5. An advantage of the\nmulti-task setting is that it allows for quantitative interpretability as to when and which primitives are\nbeing used. The results indicate that a system composed of multiple primitives generalizes more easily\nto a new task, as compared to a single policy. We further demonstrate that several primitives can be\ncombined dynamically and that the individual primitives respond to stimuli from new environments\nwhen trained on related environments.\n5.2\nDo Learned Primitives Help in Transfer Learning?\nWe now evaluate our approach in the settings where the adaptation to the changes in the task is vital."
      },
      {
        "header": "The argument in the favor of modularity is that it enables better knowledge transfer between related",
        "content": "task. This transfer is more effective when the tasks are closely related as the model would only have\nto learn how to compose the already learned primitives. In general, it is dif\ufb01cult to determine how\n\u201cclosely\u201d related two tasks are and the inductive bias of modularity could be harmful if the two tasks\nare quite different. In such cases, we could add new primitives (which would have to be learned) and\nstill obtain a sample-ef\ufb01cient transfer as some part of the task structure would already have been\ncaptured by the pretrained primitives. This approach can be extended by adding primitives during\ntraining which provides a seamless way to combine knowledge about different tasks to solve more\ncomplex tasks. We investigate here the transfer properties of a primitive trained in one environment\nand transferred to a different one."
      },
      {
        "header": "We evaluate the transfer performance of pretrained primitives",
        "content": "on the cross maze environment [15]. Here, a quadrupedal robot must walk to the different goals along\nthe different paths (see Appendix G for details). The goal is randomly chosen from a set of available\ngoals at the start of each episode. We pretrain a policy (see model details in Appendix G.1) with a\nmotion reward in an environment which does not have any walls (similar to [15]), and then transfer\nthe policy to the second task where the ant has to navigate to a random goal chosen from one of the 3\n(or 10) available goal options. For our model, we make four copies of the pretrained policies and\nthen \ufb01netune the model using the pretrained policies as primitives. We compare to both MLSH [13]\nand option-critic [4]. All these baselines have been pretrained in the same manner. As evident from\nFigure 5, our method outperforms the other approaches. The fact that the initial policies successfully\nadapt to the transfer environment underlines the \ufb02exibility of our approach.\n3https://github.com/openai/mlsh\n6\n\n\nA\n\u25e6\u25e6\nB\n\u25e6\u25e6\nC\n\u25e6\u25e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0"
      },
      {
        "header": "Relative frequency of activation",
        "content": "idx 1\nidx 2\nidx 3\nidx 4\nFigure 3: Multitask training. Each panel corresponds to a different training setup, where different\ntasks are denoted A, B, C, ..., and a rectangle with n circles corresponds to an agent composed of\nn primitives trained on the respective tasks. Top row: activation of primitives for agents trained\non single tasks. Bottom row: Retrain: two primitives are trained on A and transferred to B. The\nresults (success rates) indicate that the multi-primitive model is substantially more sample ef\ufb01cient\nthan the baseline (transfer A2C). Copy and Combine: more primitives are added to the model over\ntime in a plug-and-play fashion (2 primitives are trained on A; the model is extended with a copy of\nitself; the resulting four-primitive model is trained on B.) This is more sample ef\ufb01cient than other\nstrong baselines, such as [13, 4]. Zero-Shot Generalization: A set of primitives is trained on C, and\nzero-shot generalization to A and B is evaluated. The primitives learn a form of spatial decomposition\nwhich allows them to be active in both target tasks, A and B.\n5.3"
      },
      {
        "header": "We evaluate our approach on a number of RL environments to show that we can indeed learn sets of",
        "content": "primitive policies focusing on different aspects of a task and collectively solving it.\nMotion Imitation.\nTo test the scalability of the proposed method, we present a series of tasks from\nthe motion imitation domain. In these tasks, we train a simulated 2D biped character to perform\na variety of highly dynamic skills by imitating motion capture clips recorded from human actors.\nEach mocap clip is represented by a target state trajectory \u03c4 \u2217= {s\u2217\n0, s\u2217\n1, ..., s\u2217\nT }, where s\u2217\nt denotes\nthe target state at timestep t. The input to the policy is augmented with a goal gt = {s\u2217\nt+1, s\u2217\nt+2},\nwhich speci\ufb01es the the target states for the next two timesteps. Both the state st and goal gt are then\nprocessed by the encoder penc(zt|st, gt). The repertoire of skills consists of 8 clips depicting different\ntypes of walks, runs, jumps, and \ufb02ips. The motion imitation approach closely follows Peng et al. [29].\nSnapshots of some of the learned motions are shown in Figure 6.4 To analyze the specialization of the\nvarious primitives, we computed 2D embeddings of states and goals which each primitive is active in,\nand the actions proposed by the primitives. Figure 7 illustrates the embeddings computed with t-SNE\n[41]. The embeddings show distinct clusters for the primitives, suggesting a degree of specialization\nof each primitive to certain states, goals, and actions.\n4See supplementary information for video material.\n7\n\n\n0.0\n0.5\n1.0\n1.5\n2.0"
      },
      {
        "header": "Our Approach",
        "content": "Figure 4: Continual Learning Scenario: We consider a continual learning scenario where we train\n2 primitives for 2 goal positions, then transfer (and \ufb01netune) on 4 goal positions then transfer (and\n\ufb01netune) on 8 goals positions. The plot on the left shows the primitives remain activated. The solid\ngreen line shows the boundary between the tasks, The plot on the right shows the number of samples\ntaken by our model and the transfer baseline model across different tasks. We observe that the\nproposed model takes fewer steps than the baseline (an A2C policy trained in a similar way) and the\ngap in terms of the number of samples keeps increasing as tasks become harder."
      },
      {
        "header": "Proposed method",
        "content": "68 \u00b1 3%\n40 \u00b1 3%\nFigure 5: Left: Multitask setup, where we show that we are able to train 8 primitives when training on\na mixture of 4 tasks. Right: Success rates on the different Ant Maze tasks. Success rate is measured\nas the number of times the ant is able to reach the goal (based on 500 sampled trajectories)."
      },
      {
        "header": "We present a framework for learning an ensemble of primitive policies which can collectively solve",
        "content": "tasks in a decentralized fashion. Rather than relying on a centralized, learned meta-controller, the\nselection of active primitives is implemented through an information-theoretic mechanism. The\nlearned primitives can be \ufb02exibly recombined to solve more complex tasks. Our experiments show\nthat, on a partially observed \u201cMinigrid\u201d task and a continuous control \u201cant maze\u201d walking task, our\nmethod can enable better transfer than \ufb02at policies and hierarchical RL baselines, including the\nFigure 6: Snapshots of motions learned by the policy. Top: Reference motion clip. Middle:\nSimulated character imitating the reference motion. Bottom: Probability of selecting each primitive."
      },
      {
        "header": "S\nG",
        "content": "A\nFigure 7: Embeddings visualizing the states (S) and goals (G) which each primitive is active in, and\nthe actions (A) proposed by the primitives for the motion imitation tasks. A total of four primitives\nare trained. The primitives produce distinct clusters.\nMeta-learning Shared Hierarchies model and the Option-Critic framework. On Minigrid, we show\nhow primitives trained with our method can transfer much more successfully to new tasks and on the\nant maze, we show that primitives initialized from a pretrained walking control can learn to walk to\ndifferent goals in a stochastic, multi-modal environment with nearly double the success rate of a more\nconventional hierarchical RL approach, which uses the same pretraining but a centralized high-level\npolicy.\nThe proposed framework could be very attractive for continual learning settings, where one could\nadd more primitive policies over time. Thereby, the already learned primitives would keep their focus\non particular aspects of the task, and newly added ones could specialize on novel aspects."
      },
      {
        "header": "The authors acknowledge the important role played by their colleagues at Mila throughout the",
        "content": "duration of this work. The authors would like to thank Greg Wayne, Mike Mozer, Matthew Botvnick\nfor very useful discussions. The authors would also like to thank Nasim Rahaman, Samarth Sinha,\nNithin Vasisth, Hugo Larochelle, Jordan Hoffman, Ankesh Anand for feedback on the draft. The\nauthors are grateful to NSERC, CIFAR, Google, Samsung, Nuance, IBM, Canada Research Chairs,\nCanada Graduate Scholarship Program, Nvidia for funding, and Compute Canada for computing\nresources. We are very grateful to Google for giving Google Cloud credits used in this project."
      },
      {
        "header": "References",
        "content": "[1] Alessandro Achille and Stefano Soatto. Information dropout: learning optimal representations\nthrough noise. CoRR, abs/1611.01353, 2016. URL http://arxiv.org/abs/1611.01353.\n[2] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational\ninformation bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.\n00410.\n[3] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with\npolicy sketches. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pages 166\u2013175. JMLR. org, 2017.\n[4] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pages\n1726\u20131734, 2017.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym, 2016.\n[6] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environ-\nment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.\n[7] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n[8] Christian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search.\nIn Arti\ufb01cial Intelligence and Statistics, pages 273\u2013281, 2012.\n[9] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural\ninformation processing systems, pages 271\u2013278, 1993.\n[10] Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function\ndecomposition. Journal of Arti\ufb01cial Intelligence Research, 13:227\u2013303, 2000.\n[11] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you\nneed: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.\n[12] Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical\nreinforcement learning. arXiv preprint arXiv:1704.03012, 2017.\n[13] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman. Meta Learning Shared Hierarchies. arXiv\ne-prints, October 2017.\n[14] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared\nhierarchies. arXiv preprint arXiv:1710.09767, 2017.\n[15] Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies\nfor hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808, 2018.\n[16] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom\nErez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in\nrich environments. arXiv preprint arXiv:1707.02286, 2017.\n[17] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei,\nC Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual rea-\nsoning. In Proceedings of the IEEE International Conference on Computer Vision, pages\n2989\u20132998, 2017.\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[19] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical\ndeep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In\nAdvances in neural information processing systems, pages 3675\u20133683, 2016.\n10\n\n\n[20] Libin Liu and Jessica Hodgins. Learning to schedule control fragments for physics-based\ncharacters using deep q-learning. ACM Transactions on Graphics, 36(3), 2017.\n[21] Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for\noption discovery in reinforcement learning. arXiv preprint arXiv:1703.00956, 2017.\n[22] Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas\nHeess, and Greg Wayne. Hierarchical visuomotor control of humanoids. In International\nConference on Learning Representations, 2019. URL https://openreview.net/forum?\nid=BJfYvo09Y7.\n[23] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne,\nYee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control.\nIn International Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=BJl6TjRcY7.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n[25] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on machine learning, pages 1928\u20131937,\n2016.\n[26] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Sch\u00f6lkopf.\nLearning independent causal mechanisms. arXiv preprint arXiv:1712.00961, 2017.\n[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\nPyTorch. In NIPS Autodiff Workshop, 2017.\n[28] Xue Bin Peng, Glen Berseth, Kangkang Yin, and Michiel Van De Panne. Deeploco: Dynamic\nlocomotion skills using hierarchical deep reinforcement learning. ACM Trans. Graph., 36\n(4):41:1\u201341:13, July 2017. ISSN 0730-0301. doi: 10.1145/3072959.3073602. URL http:\n//doi.acm.org/10.1145/3072959.3073602.\n[29] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-\nguided deep reinforcement learning of physics-based character skills. ACM Trans. Graph.,\n37(4):143:1\u2013143:14, July 2018. ISSN 0730-0301. doi: 10.1145/3197517.3201311. URL\nhttp://doi.acm.org/10.1145/3197517.3201311.\n[30] Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing networks and\nthe challenges of modular and compositional computation. arXiv preprint arXiv:1904.12774,\n2019.\n[31] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg,\nDavid Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning.\narXiv preprint arXiv:1703.01161, 2017.\n[32] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation.\narXiv preprint\narXiv:1506.02438, 2015.\n[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[34] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,\n1998.\n[35] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Proceedings of the 12th\nInternational Conference on Neural Information Processing Systems, NIPS\u201999, pages 1057\u2013\n1063, Cambridge, MA, USA, 1999. MIT Press. URL http://dl.acm.org/citation.cfm?\nid=3009657.3009806.\n11\n\n\n[36] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):\n181\u2013211, 1999.\n[37] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):\n181\u2013211, 1999.\n[38] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for\nmachine learning. University of Toronto, Technical Report, 2012.\n[39] Naftali Tishby, Fernando C. N. Pereira, and William Bialek. The information bottleneck method.\nCoRR, physics/0004057, 2000. URL http://arxiv.org/abs/physics/0004057.\n[40] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n5026\u20135033. IEEE, 2012.\n[41] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of\nMachine Learning Research, 9:2579\u20132605, 2008. URL http://www.jmlr.org/papers/v9/\nvandermaaten08a.html.\n[42] Ronald J. Williams.\nSimple Statistical Gradient-Following Algorithms for Connectionist\nReinforcement Learning. Machine Learning, 8(3-4):229\u2013256, 1992. ISSN 0885-6125. doi:\n10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.\n[43] Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region\nmethod for deep reinforcement learning using kronecker-factored approximation. In Advances\nin neural information processing systems, pages 5279\u20135288, 2017."
      },
      {
        "header": "X\nj",
        "content": "eLj ,\nor\nLk = log \u03b1k + LSE(L1, . . . , LK) ,\nwhere LSE(L1, . . . , LK) = log P\nj eLj is independent of k.\nPlugging this in, and using P \u03b1k = 1, we get\nLreg ="
      },
      {
        "header": "X\nk",
        "content": "\u03b1k log \u03b1k + LSE(L1, . . . , LK) = \u2212H(\u03b1) + LSE(L1, . . . , LK) .\nInformation-theoretic interpretation\nNotably, Lreg also represents an upper bound to the KL-\ndivergence of a mixture of the currently active primitives and a prior,\nLreg \u2265DKL("
      },
      {
        "header": "X\nk",
        "content": "\u03b1kpenc(Zk|S)||N(0, 1)) ,\nand thus can be regarded as a term limiting the information content of the mixture of all active\nprimitives. This arises from the convexity properties of the KL divergence, which directly lead to\nDKL("
      },
      {
        "header": "D Bandits Environment",
        "content": "In order to test if our approach can learn distinct primitives, we used the 2D moving bandits tasks\n(introduced in [14]). In this task, the agent is placed in a 2D world and is shown the position of two\nrandomly placed points. One of these points is the goal point but the agent does not know which. We\nuse the sparse reward setup where the agent receives the reward of 1 if it is within a certain distance\nof the goal point and 0 at all other times. Each episode lasts for 50 steps and to get the reward, the\nlearning agent must reach near the goal point in those 50 steps. The agent\u2019s action space consists of 5\nactions - moving in one of the four cardinal directions (top, down, left, right) and staying still.\nB.1.1\nResults for 2D Bandits\nWe want to answer the following questions:\n1. Can our proposed approach learn primitives which remain active throughout training?\n2. Can our proposed approach learn primitives which can solve the task?\nWe train two primitives on the 2D Bandits tasks and evaluate the relative frequency of activation of\nthe primitives throughout the training. It is important that both the primitives remain active. If only 1\nprimitive is acting most of the time, its effect would be the same as training a \ufb02at policy. We evaluate\nthe effectiveness of our model by comparing the success rate with a \ufb02at A2C baseline. Figure 8 shows\nthat not only do both the primitives remain active throughout training, our approach also outperforms\nthe baseline approach.\n0\n1\n2\n3\n4"
      },
      {
        "header": "Relative frequency of activation",
        "content": "idx 1\nidx 2\nFigure 8: Performance on the 2D bandits task. Left: The comparison of our model (blue curve -\ndecentralized policy) with the baseline (red curve - \ufb02at policy) in terms of success rate shows the\neffectiveness of our proposed approach. Right: Relative frequency of activation of the primitives\n(normalized to sum up to 1). Both primitives are utilized throughout the training.\nB.2\nFour-rooms Environment\nWe consider the Four-rooms gridworld environment [37] where the agent has to navigate its way\nthrough a grid of four interconnected rooms to reach a goal position within the grid. The agent\ncan perform one of the following four actions: move up, move down, move left, move right. The\nenvironment is stochastic and with 1/3 probability, the agent\u2019s chosen action is ignored and a new\naction (randomly selected from the remaining 3 actions) is executed ie the agent\u2019s selected action\nis executed with a probability of only 2/3 and the agent takes any of the 3 remaining actions with a\nprobability of 1/9 each.\nB.2.1\nTask distribution for the Four-room Environment\nIn the Four-room environment, the agent has to navigate to a goal position which is randomly selected\nfrom a set of goal positions. We can use the size of this set of goal positions to de\ufb01ne a curriculum\nof task distributions. Since the environment does not provide any information about the goal state,\nthe larger the goal set, harder is the task as the now goal could be any element from a larger set."
      },
      {
        "header": "The choice of the set of goal states and the choice of curriculum does not affect the environment",
        "content": "13\n\n\ndynamics. Speci\ufb01cally, we consider three tasks - Fourroom-v0, Fourroom-v1 and Fourroom-v2 with\nthe set of 2, 4 and 8 goal positions respectively. The set of goal positions for each task is \ufb01xed but not\nknown to the learning agent. We expect, and empirically verify, that the Fourroom-v0 environment\nrequires the least number of samples to be learned, followed by the Fourroom-v1 and the Fourroom-v2\nenvironment (\ufb01gure 6 in the paper).\nB.2.2\nResults for Four-rooms environment\nWe want to answer the following questions:"
      },
      {
        "header": "Can our proposed approach learn primitives that remain active when training the agent over",
        "content": "a sequence of tasks?\n2. Can our proposed approach be used to improve the sample ef\ufb01ciency of the agent over a\nsequence of tasks?\nTo answer these questions, we consider two setups. In the baseline setup, we train a \ufb02at A2C policy\non Fourrooms-v0 till it achieves a 100 % success rate during evaluation. Then we transfer this policy\nto Fourrooms-v1 and continue to train till it achieves a 100 % success rate during the evaluation\non Fourrooms-v1. We transfer the policy one more time to Fourrooms-v2 and continue to train the\npolicy until it reaches a 60% success rate. In the last task(Fourrooms-v2), we do not use 100% as the\nthreshold as the models do not achieve 100% for training even after training for 10M frames. We use\n60% as the baseline models generally converge around this value.\nIn the second setup, we repeat this exercise of training on one task and transferring to the next task\nwith our proposed model. Note that even though our proposed model converges to a higher value\nthan 60% in the last task(Fourrooms-v2), we compare the number of samples required to reach 60%\nsuccess rate to provide a fair comparison with the baseline."
      },
      {
        "header": "C\nImplementation Details",
        "content": "In this section, we describe the implementation details which are common for all the models. Other\ntask-speci\ufb01c details are covered in the respective task sections.\n1. All the models (proposed as well as the baselines) are implemented in Pytorch 1.1 unless\nstated otherwise. [27].\n2. For Meta-Learning Shared Hierarchies [14] and Option-Critic [4], we adapted the author\u2019s\nimplementations 5for our environments.\n3. During the evaluation, we use 10 processes in parallel to run 500 episodes and compute the\npercentage of times the agent solves the task within the prescribed time limit. This metric is\nreferred to as the \u201csuccess rate\u201d.\n4. The default time limit is 500 steps for all the tasks unless speci\ufb01ed otherwise."
      },
      {
        "header": "All the feedforward networks are initialized with the orthogonal initialization where the",
        "content": "input tensor is \ufb01lled with a (semi) orthogonal matrix.\n6. For all the embedding layers, the weights are initialized using the unit-Gaussian distribution."
      },
      {
        "header": "The weights and biases for all the GRU model are initialized using the uniform distribution",
        "content": "from U(\u2212\n\u221a\nk,\n\u221a\nk) where k =\n1\nhidden_size.\n8. During training, we perform 64 rollouts in parallel to collect 5-step trajectories.\n9. The \u03b2ind and \u03b2reg parameters are both selected from the set {0.001, 0.005, 0.009} by\nperforming validation.\nIn section D.4.2, we explain all the components of the model architecture along with the implemen-\ntation details in the context of the MiniGrid Environment. For the subsequent environments, we\ndescribe only those components and implementation details which are different than their counterpart\nin the MiniGrid setup and do not describe the components which work identically.\n5https://github.com/openai/mlsh, https://github.com/jeanharb/option_critic"
      },
      {
        "header": "It provides a family of customizable reinforcement learning environments that are compatible with",
        "content": "the OpenAI Gym framework [5]. Since the environments can be easily extended and modi\ufb01ed, it is\nstraightforward to control the complexity of the task (eg controlling the size of the grid, the number\nof rooms or the number of objects in the grid, etc). Such \ufb02exibility is very useful when experimenting\nwith curriculum learning or testing for generalization.\nD.1"
      },
      {
        "header": "The World",
        "content": "In MiniGrid, the world (environment for the learning agent) is a rectangular grid of size say MxN.\nEach tile in the grid contains either zero or one object. The possible object types are wall, \ufb02oor, lava,\ndoor, key, ball, box and goal. Each object has an associated string (which denote the object type) and\nan associated discrete color (could be red, green, blue, purple, yellow and grey). By default, walls are\nalways grey and goal squares are always green. Certain objects have special effects. For example, a\nkey can unlock a door of the same color.\nD.1.1"
      },
      {
        "header": "Reward Function",
        "content": "We consider the sparse reward setup where the agent gets a reward (of 1) only if it completes the task\nand 0 at all other time steps. We also apply a time limit of 500 steps on all the tasks ie the agent must\ncomplete the task in 500 steps. A task is terminated either when the agent solves the task or when the\ntime limit is reached - whichever happens \ufb01rst.\nD.1.2"
      },
      {
        "header": "Action Space",
        "content": "The agent can perform one of the following seven actions per timestep: turn left, turn right, move\nforward, pick up an object, drop the object being carried, toggle, done (optional action).\nThe agent can use the turn left and turn right actions to rotate around and face one of the 4 possible\ndirections (north, south, east, west). The move forward action makes the agent move from its current\ntile onto the tile in the direction it is currently facing, provided there is nothing on that tile, or that the\ntile contains an open door. The toggle actions enable the agent to interact with other objects in the\nworld. For example, the agent can use the toggle action to open the door if they are right in front of it\nand have the key of matching color.\nD.1.3"
      },
      {
        "header": "Observation Space",
        "content": "The MiniGrid environment provides partial and egocentric observations. For all our experiments, the\nagent sees the view of a square of 4x4 tiles in the direction it is facing. The view includes the tile on\nwhich the agent is standing. The observations are provided as a tensor of shape 4x4x3. However,\nnote that this tensor does not represent RGB images. The \ufb01rst two channels denote the view size and\nthe third channel encodes three integer values. The \ufb01rst integer value describes the type of the object,\nthe second value describes the color of the object and the third value describes if the doors are open\nor closed. The bene\ufb01t of using this encoding over the RGB encoding is that this encoding is more\nspace-ef\ufb01cient and enables faster training. For human viewing, the fully observable, RGB image\nview of the environments is also provided and we use that view as an example in the paper.\nAdditionally, the environment also provides a natural language description of the goal. An example\nof the goal description is: \u201cUnlock the door and pick up the red ball\u201d. The learning agent and the\nenvironment use a shared vocabulary where different words are assigned numbers and the environment\nprovides a number-encoded goal description along with each observation. Since different instructions\ncan be of different lengths, the environment pads the goal description with <unk> tokens to ensure\nthat the sequence length is the same. When encoding the instruction, the agent ignores the padded\nsub-sequence in the instruction.\n15\n\n\nFigure 9: RGB view of the Fetch environment.\nFigure 10: RGB view of the Unlock environment.\nD.2"
      },
      {
        "header": "Tasks in MiniGrid Environment",
        "content": "We consider the following tasks in the MiniGrid environment:\n1. Fetch: In the Fetch task, the agent spawns at an arbitrary position in a 8 \u00d7 8 grid (\ufb01gure 9 ).\nIt is provided with a natural language goal description of the form \u201cgo fetch a yellow box\u201d."
      },
      {
        "header": "The agent has to navigate to the object being referred to in the goal description and pick it",
        "content": "up.\n2. Unlock: In the Unlock task, the agent spawns at an arbitrary position in a two-room grid\nenvironment. Each room is 8 \u00d7 8 square (\ufb01gure 10 ). It is provided with a natural language\n6https://github.com/maximecb/gym-minigrid\nFigure 11: RGB view of the UnlockPickup environment.\n16\n\n\ngoal description of the form \u201copen the door\u201d. The agent has to \ufb01nd the key that corresponds\nto the color of the door, navigate to that key and use that key to open the door.\n3. UnlockPickup: This task is basically a union of the Unlock and the Fetch tasks. The agent\nspawns at an arbitrary position in a two-room grid environment. Each room is 8 \u00d7 8 square\n(\ufb01gure 11 ). It is provided with a natural language goal description of the form \u201copen the\ndoor and pick up the yellow box\u201d. The agent has to \ufb01nd the key that corresponds to the color\nof the door, navigate to that key, use that key to open the door, enter the other room and pick\nup the object mentioned in the goal description.\nD.3"
      },
      {
        "header": "Training Setup",
        "content": "Consider an agent training on any task in the MiniGrid suite of environments. At the beginning\nof an episode, the learning agent spawns at a random position. At each step, the environment\nprovides observations in two modalities - a 4 \u00d7 4 \u00d7 3 tensor xt (an egocentric view of the state of the\nenvironment) and a variable length goal description g. We describe the design of the learning agent\nin terms of an encoder-decoder architecture.\nD.3.2"
      },
      {
        "header": "Encoder Architecture",
        "content": "The agent\u2019s encoder network consists of two models - a CNN+GRU based observation encoder and\na GRU [7] based goal encoder\nObservation Encoder:\nIt is a three layer CNN with the output channel sizes set to 16, 16 and 32 respectively (with ReLU\nlayers in between) and kernel size set to 2 \u00d7 2 for all the layers. The output of the CNN is \ufb02attened\nand fed to a GRU model (referred to as the observation-rnn) with 128-dimensional hidden state. The\noutput from the observation-rnn represents the encoding of the observation.\nGoal Encoder:\nIt comprises of an embedding layer followed by a unidirectional GRU model. The dimension of the\nembedding layer and the hidden and the output layer of the GRU model are all set to 128."
      },
      {
        "header": "Decoder",
        "content": "The decoder network comprises the action network and the critic network - both of which are\nimplemented as feedforward networks. We now describe the design of these networks.\nD.3.4"
      },
      {
        "header": "Value Network",
        "content": "1. Two-layer feedforward network with the tanh non-linearity.\n2. Input: Concatenation of z and the current hidden state of the observation-rnn.\n3. Size of the input to the \ufb01rst layer and the second layer of the policy network are 320 and 64\nrespectively.\n4. Produces a scalar output.\nD.4\nComponents speci\ufb01c to the proposed model"
      },
      {
        "header": "The components that we described so far are used by both the baselines as well as our proposed",
        "content": "model. We now describe the components that are speci\ufb01c to our proposed model. Our proposed\nmodel consists of an ensemble of primitives and the components we describe apply to each of those\nprimitives.\n17\n\n\nD.4.1"
      },
      {
        "header": "Information Bottleneck",
        "content": "Given that we want to control and regularize the amount of information that the encoder encodes, we\ncompute the KL divergence between the output of the action-feature encoder network and a diagonal\nunit Gaussian distribution. More is the KL divergence, more is the information that is being encoded\nwith respect to the Gaussian prior and vice-versa. Thus we regularize the primitives to minimize the\nKL divergence.\nD.4.2"
      },
      {
        "header": "Learning Algorithm",
        "content": "A2C [43]\nOpitimizer \u2018\nRMSProp[38]\nlearning rate\n7 \u00b7 10\u22124\nbatch size\n64\ndiscount\n0.99\nlambda (for GAE [32])\n0.95\nentropy coef\ufb01cient\n10\u22122\nloss coef\ufb01cient\n0.5"
      },
      {
        "header": "Observation Space",
        "content": "The 2D bandits task provides a 6-dimensional \ufb02at observation. The \ufb01rst two dimensions correspond\nto the (x, y) coordinates of the current position of the agent and the remaining four dimensions\ncorrespond to the (x, y) coordinates of the two randomly chosen points.\nE.1"
      },
      {
        "header": "Training Setup",
        "content": "Consider an agent training on the 2D bandits tasks. The learning agent spawns at a \ufb01xed position and\nis randomly assigned two points. At each step, the environmental observation provides the current\nposition of the agent as well the position of the two points. We describe the design of the learning\nagent in terms of an encoder-decoder architecture.\nE.1.2"
      },
      {
        "header": "Encoder Architecture",
        "content": "The agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn) with a hidden state size of 128. The 6-dimensional observation from the environment is the input\nto the GRU model. The output from the observation-rnn represents the encoding of the observation.\nE.2"
      },
      {
        "header": "Hyperparameters",
        "content": "The implementation details for the 2D Bandits environment are the same as that for MiniGrid\nenvironment and are described in detail in section D.4.2. In the table below, we list the values of the\ntask-speci\ufb01c hyperparameters."
      },
      {
        "header": "Learning Algorithm",
        "content": "PPO [33]\nepochs per update (PPO)\n10\nOptimizer \u2018\nAdam[18]\nlearning rate\n3 \u00b7 10\u22125\n\u03b21\n0.9\n\u03b22\n0.999\nbatch size\n64\ndiscount\n0.99\nentropy coef\ufb01cient\n0\nloss coef\ufb01cient\n1.0"
      },
      {
        "header": "The World",
        "content": "In the Four-rooms setup, the world (environment for the learning agent) is a square grid of say 11\u00d711.\nThe grid is divided into 4 rooms such that each room is connected with two other rooms via hallways.\nThe layout of the rooms is shown in \ufb01gure 12. The agent spawns at a random position and has to\nnavigate to a goal position within 500 steps.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nFigure 12: View of the four-room environment\nF.1.1"
      },
      {
        "header": "Reward Function",
        "content": "We consider the sparse reward setup where the agent gets a reward (of 1) only if it completes the task\n(and reaches the goal position) and 0 at all other time steps. We also apply a time limit of 300 steps\non all the tasks ie the agent must complete the task in 300 steps. A task is terminate either when the\nagent solves the task or when the time limit is reached - whichever happens \ufb01rst.\nF.1.2"
      },
      {
        "header": "Observation Space",
        "content": "The environment is a 11 \u00d7 11 grid divided into 4 interconnected rooms. As such, the environment has\na total of 104 states (or cells) that can be occupied. These states are mapped to integer identi\ufb01ers. At\nany time t, the environment observation is a one-hot representation of the identi\ufb01er corresponding to\nthe state (or the cell) the agent is in right now. ie the environment returns a vectors of zeros with only\none entry being 1 and the index of this entry gives the current position of the agent. The environment\ndoes not return any information about the goal state.\n19\n\n\nF.2\nModel Architecture for Four-room Environment\nF.2.1"
      },
      {
        "header": "Training Setup",
        "content": "Consider an agent training on any task in the Four-room suite of environments. At the beginning\nof an episode, the learning agent spawns at a random position and the environment selects a goal\nposition for the agent. At each step, the environment provides a one-hot representation of the agent\u2019s\ncurrent position (without including any information about the goal state). We describe the design of\nthe learning agent in terms of an encoder-decoder architecture.\nF.3"
      },
      {
        "header": "Encoder Architecture",
        "content": "The agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn with a hidden state size of 128. The 104-dimensional one-hot input from the environment is fed\nto the GRU model. The output from the observation-rnn represents the encoding of the observation.\nThe implementation details for the Four-rooms environment are the same as that for MiniGrid\nenvironment and are described in detail in section D.4.2."
      },
      {
        "header": "G\nAnt Maze Environment",
        "content": "We use the Mujoco-based quadruple ant [40] to evaluate the transfer performance of our approach\non the cross maze environment [15]. The training happens in two phases. In the \ufb01rst phase, we\ntrain the ant to walk on a surface using a motion reward and using just 1 primitive. In the second\nphase, we make 4 copies of this trained policy and train the agent to navigate to a goal position in a\nmaze (Figure 13). The goal position is chosen from a set of 3 (or 10) goals. The environment is a\ncontinuous control environment and the agent can directly manipulate the movement of joints and\nlimbs.\nFigure 13: View of the Ant Maze environment with 3 goals\nG.0.1"
      },
      {
        "header": "Observation Space",
        "content": "In the \ufb01rst phase (training the ant to walk), the observations from the environment correspond to the\nstate-space representation ie a real-valued vector that describes the state of the ant in mechanical\nterms - position, velocity, acceleration, angle, etc of the joints and limbs. In the second phase (training\nthe ant to navigate the maze), the observation from the environment also contains the location of the\ngoal position along with the mechanical state of the ant.\nG.1"
      },
      {
        "header": "Encoder Architecture",
        "content": "The agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn with a hidden state size of 128. The real-valued state vector from the environment is fed to the\nGRU model. The output from the observation-rnn represents the encoding of the observation. Note\n20\n\n\nthat in the case of phase 1 vs phase 2, only the size of the input to the observation-rnn changes and\nthe encoder architecture remains the same.\nG.1.3"
      },
      {
        "header": "Decoder",
        "content": "The decoder network comprises the action network and the critic network. All these networks are\nimplemented as feedforward networks. The design of these networks is very similar to that of the\ndecoder model for the MiniGrid environment as described in section D.3.3 with just one difference.\nIn this case, the action space is continuous so the action-feature decoder network produces the mean\nand log-standard-deviation for a diagonal Gaussian policy. This is used to sample a real-valued action\nto execute in the environment.\n21"
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "",
      "author": "",
      "subject": "",
      "keywords": "",
      "creator": "LaTeX with hyperref package",
      "producer": "pdfTeX-1.40.17",
      "creationDate": "D:20190626004311Z",
      "modDate": "D:20190626004311Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 21,
    "pages": [
      "Reinforcement Learning with Competitive Ensembles\nof Information-Constrained Primitives\nAnirudh Goyal1, Shagun Sodhani1, Jonathan Binas1, Xue Bin Peng2\nSergey Levine2, Yoshua Bengio1\u2020\n1 Mila, Universit\u00e9 de Montr\u00e9al\n2 University of California, Berkeley\n\u2020CIFAR Senior Fellow.\nAbstract\nReinforcement learning agents that operate in diverse and complex environments\ncan bene\ufb01t from the structured decomposition of their behavior. Often, this is\naddressed in the context of hierarchical reinforcement learning, where the aim is\nto decompose a policy into lower-level primitives or options, and a higher-level\nmeta-policy that triggers the appropriate behaviors for a given situation. However,\nthe meta-policy must still produce appropriate decisions in all states. In this\nwork, we propose a policy design that decomposes into primitives, similarly to\nhierarchical reinforcement learning, but without a high-level meta-policy. Instead,\neach primitive can decide for themselves whether they wish to act in the current\nstate. We use an information-theoretic mechanism for enabling this decentralized\ndecision: each primitive chooses how much information it needs about the current\nstate to make a decision and the primitive that requests the most information about\nthe current state acts in the world. The primitives are regularized to use as little\ninformation as possible, which leads to natural competition and specialization. We\nexperimentally demonstrate that this policy architecture improves over both \ufb02at\nand hierarchical policies in terms of generalization.\n1\nIntroduction\nLearning policies that generalize to new environments or tasks is a fundamental challenge in rein-\nforcement learning. While deep reinforcement learning has enabled training powerful policies, which\noutperform humans on speci\ufb01c, well-de\ufb01ned tasks [24], their performance often diminishes when the\nproperties of the environment or the task change to regimes not encountered during training.\nThis is in stark contrast to how humans learn, plan, and act: humans can seamlessly switch between\ndifferent aspects of a task, transfer knowledge to new tasks from remotely related but essentially\ndistinct prior experience, and combine primitives (or skills) used for distinct aspects of different tasks\nin meaningful ways to solve new problems. A hypothesis hinting at the reasons for this discrepancy\nis that the world is inherently compositional, such that its features can be described by compositions\nof small sets of primitive mechanisms [26]. Since humans seem to bene\ufb01t from learning skills and\nlearning to combine skills, it might be a useful inductive bias for the learning models as well.\nThis is addressed to some extent by the hierarchical reinforcement learning (HRL) methods, which\nfocus on learning representations at multiple spatial and temporal scales, thus enabling better explo-\nration strategies and improved generalization performance [9, 36, 10, 19]. However, hierarchical\napproaches rely on some form of learned high-level controller, which decides when to activate\ndifferent components in the hierarchy. While low-level sub-policies can specialize to smaller portions\nof the state space, the top-level controller (or master policy) needs to know how to deal with any\ngiven state. That is, it should provide optimal behavior for the entire accessible state space. As the\nPreprint. Under review.\narXiv:1906.10667v1  [cs.LG]  25 Jun 2019\n",
      "x\nx\nx\n\u03c01\n\u03c02\n\u03c03\na\n\u03c01\n\u03c02\n\u03c03\na\n\u03c01\n\u03c02\n\u03c03\na\nAction\nselection\nCompetition\nmechanism\nCompositional\nenvironment\n1\n2\n3\nFigure 1: Illustration of our model. An intrinsic competition mechanism, based on the amount of\ninformation each primitive provides, is used to select a primitive to be active for a given input. Each\nprimitive focuses on distinct features of the environment; in this case, one policy focuses on boxes, a\nsecond one on gates, and the third one on spheres.\nmaster policy is trained on a particular state distribution, learning it in a way that generalizes to new\nenvironments effectively can, therefore, become the bottleneck for such approaches [31, 3].\nWe argue, and empirically show, that in order to achieve better generalization, the interaction between\nthe low-level primitives and the selection thereof should itself be performed without requiring a\nsingle centralized network that understands the entire state space. We, therefore, propose a fully\ndecentralized approach as an alternative to standard HRL, where we only learn a set of low-level\nprimitives without learning a high-level controller. We construct a factorized representation of the\npolicy by learning simple \u201cprimitive\u201d policies, which focus on distinct regions of the state space.\nRather than being gated by a single meta-policy, the primitives directly compete with one another\nto determine which one should be active at any given time, based on the degree to which their state\nencoders \u201crecognize\u201d the current state input.\nWe frame the problem as one of information transfer between the current state and a dynamically\nselected primitive policy. Each policy can by itself decide to request information about the current\nstate, and the amount of information requested is used to determine which primitive acts in the\ncurrent state. Since the amount of state information that a single primitive can access is limited, each\nprimitive is encouraged to use its resources wisely. Constraining the amount of accessible information\nin this way naturally leads to a decentralized competition and decision mechanism where individual\nprimitives specialize in smaller regions of the state space. We formalize this information-driven\nobjective based on the variational information bottleneck. The resulting set of competing primitives\nachieves both a meaningful factorization of the policy and an effective decision mechanism for which\nprimitives to use. Importantly, not relying on a centralized meta-policy means that individual primitive\nmechanisms can be recombined in a \u201cplug-and-play\u201d fashion, and can be transferred seamlessly to\nnew environments.\nContributions:\nIn summary, the contributions of our work are as follows: (1) We propose a method\nfor learning and operating a set of functional primitives in a fully decentralized way, without requiring\na high-level meta-controller to select active primitives (see Figure 1 for illustration). (2) We introduce\nan information-theoretic objective, the effects of which are twofold: a) it leads to the specialization of\nindividual primitives to distinct regions of the state space, and b) it enables a competition mechanism,\nwhich is used to select active primitives in a decentralized manner. (3) We demonstrate the superior\ntransfer learning performance of our model, which is due to the \ufb02exibility of the proposed framework\nregarding the dynamic addition, removal, and recombination of primitives. Decentralized primitives\ncan be successfully transferred to larger or previously unseen environments, and outperform models\nwith an explicit meta-controller for primitive selection.\n2\n",
      "2\nPreliminaries\nWe consider a Markov decision process (MDP) de\ufb01ned by the tuple (S, A, P, r, \u03b3), where the state\nspace S and the action space A may be discrete or continuous. The environment emits a bounded\nreward r : S \u00d7 A \u2192[rmin, rmax] on each transition and \u03b3 \u2208[0, 1) is the discount factor. \u03c0(.|s)\ndenotes a policy over the actions given the current state s. R(\u03c0) = E\u03c0[P\nt \u03b3tr(st)] denotes the\nexpected total return when the policy \u03c0 is followed. The standard objective in reinforcement learning\nis to maximize the expected total return R(\u03c0). We use the concept of the information bottleneck [39] to\nlearn compressed representations. The information bottleneck objective is formalized as minimizing\nthe mutual information of a bottleneck representation layer with the input while maximizing its\nmutual information with the corresponding output. This type of input compression has been shown to\nimprove generalization [39, 1, 2]. Computing the mutual information is generally intractable, but can\nbe approximated using variational inference [2].\n3\nInformation-Theoretic Decentralized Learning of Distinct Primitives\nOur goal is to learn a policy, composed of multiple primitive sub-policies, to maximize average\nreturns over T-step interactions for a distribution of tasks. Simple primitives which focus on solving\na part of the given task (and not the complete task) should generalize more effectively, as they can\nbe applied to similar aspects of different tasks (subtasks) even if the overall objective of the tasks\nare drastically different. Learning primitives in this way can also be viewed as learning a factorized\nrepresentation of a policy, which is composed of several independent policies.\nOur proposed approach consists of three components: 1) a mechanism for restricting a particular\nprimitive to a subset of the state space; 2) a competition mechanism between primitives to select the\nmost effective primitive for a given state; 3) a regularization mechanism to improve the generalization\nperformance of the policy as a whole. We consider experiments with both \ufb01xed and variable sets of\nprimitives and show that our method allows for primitives to be added or removed during training, or\nrecombined in new ways. Each primitive is represented by a differentiable, parameterized function\napproximator, such as a neural network.\n3.1\nPrimitives with an Information Bottleneck\nstate s\n(z1, . . . , zK)\n(L1, . . . , LK)\n(\u03b11, . . . , \u03b1K)\nE\u03b1\u03c0k\naction a\nencoder\nDKL(\u00b7||N)\ndecoder\nsoftmax\nFigure 2:\nThe primitive-selection\nmechanism of our model. The primi-\ntive with most information acts in the\nenvironment, and gets the reward.\nTo encourage each primitive to encode information from a\nparticular part of state space, we limit the amount of informa-\ntion each primitive can access from the state. In particular,\neach primitive has an information bottleneck with respect to\nthe input state, preventing it from using all the information\nfrom the state.\nTo implement an information bottleneck, we design each of\nthe K primitives to be composed of an encoder penc(Zk | S)\nand a decoder pdec(A | Zk), together forming the primitive\npolicy,\n\u03c0k\n\u03b8(A | S) =\nR\nz penc(zk | S) pdec(A | zk) dzk .1\nThe encoder output Z is meant to represent the information\nabout the current state S that an individual primitive believes\nis important to access in order to perform well. The decoder\ntakes this encoded information and produces a distribution\nover the actions A. Following the variational information bottleneck objective [2], we penalize the\nKL divergence of Z and the prior,\nLk = DKL(penc(Zk|S)||N(0, 1)) .\n(1)\nIn other words, a primitive pays an \u201cinformation cost\u201d proportional to Lk for accessing the information\nabout the current state.\n1In practice, we estimate the marginalization over Z using a single sample throughout our experiments.\n3\n",
      "In the experiments below, we \ufb01x the prior to be a unit Gaussian. In the general case, we can learn the\nprior as well and include its parameters in \u03b8. The information bottleneck encourages each primitive to\nlimit its knowledge about the current state, but it will not prevent multiple primitives from specializing\nto similar parts of the state space. To mitigate this redundancy, and to make individual primitives focus\non different regions of the state space, we introduce an information-based competition mechanism to\nencourage diversity among the primitives, as described in the next section.\n3.2\nCompeting Information-Constrained Primitives\nWe can use the information measure from equation 1 to de\ufb01ne a selection mechanism for the primitives\nwithout having to learn a centralized meta-policy. The idea is that the information content of an\nindividual primitive encodes its effectiveness in a given state s such that the primitive with the highest\nvalue Lk\nshould be activated in that particular state. We compute normalized weights \u03b1k for each of the\nk = 1, . . . , K primitives by applying the softmax operator,\n\u03b1k = exp(Lk)/ P\nj exp(Lj) .\n(2)\nThe resulting weights \u03b1k can be treated as a probability distribution that can be used in different ways:\nform a mixture of primitives, sample a primitive using from the distribution, or simply select the\nprimitive with the maximum weight. The selected primitive is then allowed to act in the environment.\nTrading Reward and Information: To make the different primitives compete for competence in\nthe different regions of the state space, the environment reward is distributed according to their\nparticipation in the global decision, i.e. the reward rk given to the kth primitive is weighted by\nits selection coef\ufb01cient, such that rk = \u03b1kr, with r = P\nk rk. Hence, a primitive gets a higher\nreward for accessing more information about the current state, but that primitive also pays the price\n(equal to information cost) for accessing the state information. Hence, a primitive that does not\naccess any state information is not going to get any reward. The information bottleneck and the\ncompetition mechanism, when combined with the overall reward maximization objective, should lead\nto specialization of individual primitives to distinct regions in the state space.\nThat is, each primitive should specialize in a part of the state space that it can reliably associate\nrewards with. Since the entire ensemble still needs to understand all of the state space for the given\ntask, different primitives need to encode and focus on different parts of the state space.\n3.3\nRegularization of the Combined Representation\nTo encourage a diverse set of primitive con\ufb01gurations and to make sure that the model does not col-\nlapse to a single primitive (which remains active at all times), we introduce an additional regularization\nterm,\nLreg = P\nk \u03b1kLk .\n(3)\nThis can be rewritten (see Appendix A) as\nLreg = \u2212H(\u03b1) + LSE(L1, . . . , LK) ,\n(4)\nwhere H(\u03b1) is the entropy of the \u03b1 distribution, and LSE is the LogSumExp function,\nLSE(x) = log(P\nj exj). The desired behavior is achieved by minimizing Lreg. Increasing the entropy\nof \u03b1 leads to a diverse set of primitive selections, ensuring that different combinations of the primitives\nare used. On the other hand, LSE approximates the maximum of its arguments, LSE(x) \u2248maxj xj,\nand, therefore, penalizes the dominating Lk terms, thus equalizing their magnitudes.\n3.4\nObjective and Algorithm Summary\nOur overall objective function consists of 3 terms,\n1. The expected return from the standard RL objective, R(\u03c0) which is distributed to the\nprimitives according to their participation,\n2. The individual bottleneck terms leading the individual primitives to focus on speci\ufb01c parts\nof the state space, Lk for k = 1, . . . , K,\n4\n",
      "3. The regularization term applied to the combined model, Lreg.\nThe overall objective for the kth primitive thus takes the form:\nJk(\u03b8) \u2261E\u03c0\u03b8[rk] \u2212\u03b2indLk \u2212\u03b2regLreg ,\n(5)\nwhere E\u03c0\u03b8 denotes an expectation over the state trajectories generated by the agent\u2019s policy, rk = \u03b1kr\nis the reward given to the kth primitive, and \u03b2ind, \u03b2reg are the parameters controlling the impact of the\nrespective terms.\nImplementation: In our experiments, the encoders penc(zk|S) and decoders pdec(A|zk) are repre-\nsented by neural networks, the parameters of which we denote by \u03b8. Actions are sampled through\neach primitive every step. While our approach is compatible with any RL method, we maximize\nJ(\u03b8) computed on-policy from the sampled trajectories using a score function estimator [42, 35]\nspeci\ufb01cally A2C [25] (unless otherwise noted). Every experimental result reported has been averaged\nover 5 random seeds. Our model introduces 2 extra hyper-parameters \u03b2ind, \u03b2reg.\n4\nRelated Work\nThere are a wide variety of hierarchical reinforcement learning approaches[34, 9, 10]. One of the\nmost widely applied HRL framework is the Options framework ([36]). An option can be thought of\nas an action that extends over multiple timesteps thus providing the notion of temporal abstraction or\nsubroutines in an MDP. Each option has its own policy (which is followed if the option is selected)\nand the termination condition (to stop the execution of that option). Many strategies are proposed for\ndiscovering options using task-speci\ufb01c hierarchies, such as pre-de\ufb01ned sub-goals [16], hand-designed\nfeatures [12], or diversity-promoting priors [8, 11]. These approaches do not generalize well to new\ntasks. [4] proposed an approach to learn options in an end-to-end manner by parameterizing the\nintra-option policy as well as the policy and termination condition for all the options. Eigen-options\n[21] use the eigenvalues of the Laplacian (for the transition graph induced by the MDP) to derive an\nintrinsic reward for discovering options as well as learning an intra-option policy.\nIn this work, we consider sparse reward setup with high dimensional action spaces. In such a scenario,\nperforming unsupervised pretraining or using auxiliary rewards leads to much better performance\n[13, 12, 16]. Auxiliary tasks such as motion imitation have been applied to learn motor primitives\nthat are capable of performing a variety of sophisticated skills [20, 28, 23, 22].\nOur work is also related to the Neural Module Network family of architectures [3, 17, 30] where\nthe idea is to learn modules that can perform some useful computation like solving a subtask and\na controller that can learn to combine these modules for solving novel tasks. The key difference\nbetween our approach and all the works mentioned above is that we learn functional primitives in a\nfully decentralized way without requiring any high-level meta-controller or master policy.\n5\nExperimental Results\nIn this section, we brie\ufb02y outline the tasks that we used to evaluate our proposed method and direct\nthe reader to the appendix for the complete details of each task along with the hyperparameters used\nfor the model. The code is provided with the supplementary material. We designed experiments to\naddress the following questions: a) Learning primitives \u2013 Can an ensemble of primitives be learned\nover a distribution of tasks? b) Transfer Learning using primitives \u2013 Can the learned primitives\nbe transferred to unseen/unsolvable sparse environments? c) Comparison to centralized methods\n\u2013 How does our method compare to approaches where the primitives are trained using an explicit\nmeta-controller, in a centralized way?\nBaselines.\nWe compare our proposed method to the following baselines:\na) Option Critic [4] \u2013 We extended the author\u2019s implementation 2 of the Option Critic architecture\nand experimented with multiple variations in the terms of hyperparameters and state/goal encoding.\nNone of these yielded reasonable performance in partially observed tasks, so we omit it from the\nresults.\n2https://github.com/jeanharb/option_critic\n5\n",
      "b) MLSH (Meta-Learning Shared Hierarchy) [13] \u2013 This method uses meta-learning to learn sub-\npolicies that are shared across tasks along with learning a task-speci\ufb01c high-level master. It also\nrequires a phase-wise training schedule between the master and the sub-policies to stabilize training.\nWe use the MLSH implementation provided by the authors 3.\nc) Transfer A2C: In this method, we \ufb01rst learn a single policy on the one task and then transfer the\npolicy to another task, followed by \ufb01ne-tuning in the second task.\n5.1\nMulti-Task Training\nWe evaluate our model in a partially-observable 2D multi-task environment called Minigrid, similar to\nthe one introduced in [6]. The environment is a two-dimensional grid with a single agent, impassable\nwalls, and many objects scattered in the environment. The agent is provided with a natural language\nstring that speci\ufb01es the task that the agent needs to complete. The setup is partially observable\nand the agent only gets the small, egocentric view of the grid (along with the natural language task\ndescription). We consider three tasks here: the Pickup task (A), where the agent is required to pick up\nan object speci\ufb01ed by the goal string, the Unlock task (B) where the agent needs to unlock the door\n(there could be multiple keys in the environment and the agent needs to use the key which matches\nthe color of the door) and the UnlockPickup task (C), where the agent \ufb01rst needs to unlock a door\nthat leads to another room. In this room, the agent needs to \ufb01nd and pick up the object speci\ufb01ed by\nthe goal string. Additional implementation details of the environment are provided in appendix D.\nDetails on the agent model can be found in appendix D.3.\nWe train agents with varying numbers of primitives on various tasks \u2013 concurrently, as well as in\ntransfer settings. The different experiments are summarized in Figs. 3 and 5. An advantage of the\nmulti-task setting is that it allows for quantitative interpretability as to when and which primitives are\nbeing used. The results indicate that a system composed of multiple primitives generalizes more easily\nto a new task, as compared to a single policy. We further demonstrate that several primitives can be\ncombined dynamically and that the individual primitives respond to stimuli from new environments\nwhen trained on related environments.\n5.2\nDo Learned Primitives Help in Transfer Learning?\nWe now evaluate our approach in the settings where the adaptation to the changes in the task is vital.\nThe argument in the favor of modularity is that it enables better knowledge transfer between related\ntask. This transfer is more effective when the tasks are closely related as the model would only have\nto learn how to compose the already learned primitives. In general, it is dif\ufb01cult to determine how\n\u201cclosely\u201d related two tasks are and the inductive bias of modularity could be harmful if the two tasks\nare quite different. In such cases, we could add new primitives (which would have to be learned) and\nstill obtain a sample-ef\ufb01cient transfer as some part of the task structure would already have been\ncaptured by the pretrained primitives. This approach can be extended by adding primitives during\ntraining which provides a seamless way to combine knowledge about different tasks to solve more\ncomplex tasks. We investigate here the transfer properties of a primitive trained in one environment\nand transferred to a different one.\nContinuous control for ant maze\nWe evaluate the transfer performance of pretrained primitives\non the cross maze environment [15]. Here, a quadrupedal robot must walk to the different goals along\nthe different paths (see Appendix G for details). The goal is randomly chosen from a set of available\ngoals at the start of each episode. We pretrain a policy (see model details in Appendix G.1) with a\nmotion reward in an environment which does not have any walls (similar to [15]), and then transfer\nthe policy to the second task where the ant has to navigate to a random goal chosen from one of the 3\n(or 10) available goal options. For our model, we make four copies of the pretrained policies and\nthen \ufb01netune the model using the pretrained policies as primitives. We compare to both MLSH [13]\nand option-critic [4]. All these baselines have been pretrained in the same manner. As evident from\nFigure 5, our method outperforms the other approaches. The fact that the initial policies successfully\nadapt to the transfer environment underlines the \ufb02exibility of our approach.\n3https://github.com/openai/mlsh\n6\n",
      "A\n\u25e6\u25e6\nB\n\u25e6\u25e6\nC\n\u25e6\u25e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\n0.0\n0.2\n0.4\n0.6\n0.8\nNumber of frames\n1e7\n0.2\n0.4\n0.6\n0.8\nRelative frequency of activation\nidx 1\nidx 2\nA\n\u25e6\u25e6\nB\n\u25e6\u25e6\nretrain\nA\nA\n\u25e6\u25e6\n\u25e6\u25e6\nB\n\u25e6\u25e6\u25e6\u25e6\ncopy\ncombine\nC\nA\nB\n\u25e6\u25e6\n\u25e6\u25e6\n\u25e6\u25e6\nzero shot generalisation\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPercentage of episodes completed\n1e2\nTransfer A2C\nOur approach\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nNumber of frames\n1e7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPercentage of episodes completed\n1e2\nTransfer A2C\nOption Critic / MLSH\nOur approach\n0\n1\n2\n3\n4\n5\nNumber of frames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\nidx 3\nidx 4\nFigure 3: Multitask training. Each panel corresponds to a different training setup, where different\ntasks are denoted A, B, C, ..., and a rectangle with n circles corresponds to an agent composed of\nn primitives trained on the respective tasks. Top row: activation of primitives for agents trained\non single tasks. Bottom row: Retrain: two primitives are trained on A and transferred to B. The\nresults (success rates) indicate that the multi-primitive model is substantially more sample ef\ufb01cient\nthan the baseline (transfer A2C). Copy and Combine: more primitives are added to the model over\ntime in a plug-and-play fashion (2 primitives are trained on A; the model is extended with a copy of\nitself; the resulting four-primitive model is trained on B.) This is more sample ef\ufb01cient than other\nstrong baselines, such as [13, 4]. Zero-Shot Generalization: A set of primitives is trained on C, and\nzero-shot generalization to A and B is evaluated. The primitives learn a form of spatial decomposition\nwhich allows them to be active in both target tasks, A and B.\n5.3\nLearning Ensembles of Functional Primitives\nWe evaluate our approach on a number of RL environments to show that we can indeed learn sets of\nprimitive policies focusing on different aspects of a task and collectively solving it.\nMotion Imitation.\nTo test the scalability of the proposed method, we present a series of tasks from\nthe motion imitation domain. In these tasks, we train a simulated 2D biped character to perform\na variety of highly dynamic skills by imitating motion capture clips recorded from human actors.\nEach mocap clip is represented by a target state trajectory \u03c4 \u2217= {s\u2217\n0, s\u2217\n1, ..., s\u2217\nT }, where s\u2217\nt denotes\nthe target state at timestep t. The input to the policy is augmented with a goal gt = {s\u2217\nt+1, s\u2217\nt+2},\nwhich speci\ufb01es the the target states for the next two timesteps. Both the state st and goal gt are then\nprocessed by the encoder penc(zt|st, gt). The repertoire of skills consists of 8 clips depicting different\ntypes of walks, runs, jumps, and \ufb02ips. The motion imitation approach closely follows Peng et al. [29].\nSnapshots of some of the learned motions are shown in Figure 6.4 To analyze the specialization of the\nvarious primitives, we computed 2D embeddings of states and goals which each primitive is active in,\nand the actions proposed by the primitives. Figure 7 illustrates the embeddings computed with t-SNE\n[41]. The embeddings show distinct clusters for the primitives, suggesting a degree of specialization\nof each primitive to certain states, goals, and actions.\n4See supplementary information for video material.\n7\n",
      "0.0\n0.5\n1.0\n1.5\n2.0\nNumber of frames\n1e6\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\nRelative frequency of activation\nidx 1\nidx 2\n2 goals\n4 goals\n8 goals\n0.0\n0.5\n1.0\n1.5\n2.0\nNumber of training frames\n1e6\nTransfer Baseline\nOur Approach\nFigure 4: Continual Learning Scenario: We consider a continual learning scenario where we train\n2 primitives for 2 goal positions, then transfer (and \ufb01netune) on 4 goal positions then transfer (and\n\ufb01netune) on 8 goals positions. The plot on the left shows the primitives remain activated. The solid\ngreen line shows the boundary between the tasks, The plot on the right shows the number of samples\ntaken by our model and the transfer baseline model across different tasks. We observe that the\nproposed model takes fewer steps than the baseline (an A2C policy trained in a similar way) and the\ngap in terms of the number of samples keeps increasing as tasks become harder.\nA\nB\nC\nD\n\u25e6\u25e6\u25e6\u25e6\u25e6\u25e6\u25e6\u25e6\n0\n1\n2\n3\n4\n5\nNumber of frames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\nRelative frequency of activation\nMethod\n3 goals\n10 goals\nFlat Policy (PPO)\n11 \u00b1 5 %\n4 \u00b1 2 %\nOption critic\n18 \u00b1 10 %\n7 \u00b1 3 %\nMLSH\n32 \u00b1 3 %\n5 \u00b1 3 %\nExplicit high level policy\n21 \u00b1 5 %\n11 \u00b1 2 %\nProposed method\n68 \u00b1 3%\n40 \u00b1 3%\nFigure 5: Left: Multitask setup, where we show that we are able to train 8 primitives when training on\na mixture of 4 tasks. Right: Success rates on the different Ant Maze tasks. Success rate is measured\nas the number of times the ant is able to reach the goal (based on 500 sampled trajectories).\n6\nSummary and Discussion\nWe present a framework for learning an ensemble of primitive policies which can collectively solve\ntasks in a decentralized fashion. Rather than relying on a centralized, learned meta-controller, the\nselection of active primitives is implemented through an information-theoretic mechanism. The\nlearned primitives can be \ufb02exibly recombined to solve more complex tasks. Our experiments show\nthat, on a partially observed \u201cMinigrid\u201d task and a continuous control \u201cant maze\u201d walking task, our\nmethod can enable better transfer than \ufb02at policies and hierarchical RL baselines, including the\nFigure 6: Snapshots of motions learned by the policy. Top: Reference motion clip. Middle:\nSimulated character imitating the reference motion. Bottom: Probability of selecting each primitive.\n8\n",
      "S\nG\nA\nFigure 7: Embeddings visualizing the states (S) and goals (G) which each primitive is active in, and\nthe actions (A) proposed by the primitives for the motion imitation tasks. A total of four primitives\nare trained. The primitives produce distinct clusters.\nMeta-learning Shared Hierarchies model and the Option-Critic framework. On Minigrid, we show\nhow primitives trained with our method can transfer much more successfully to new tasks and on the\nant maze, we show that primitives initialized from a pretrained walking control can learn to walk to\ndifferent goals in a stochastic, multi-modal environment with nearly double the success rate of a more\nconventional hierarchical RL approach, which uses the same pretraining but a centralized high-level\npolicy.\nThe proposed framework could be very attractive for continual learning settings, where one could\nadd more primitive policies over time. Thereby, the already learned primitives would keep their focus\non particular aspects of the task, and newly added ones could specialize on novel aspects.\nAcknowledgements\nThe authors acknowledge the important role played by their colleagues at Mila throughout the\nduration of this work. The authors would like to thank Greg Wayne, Mike Mozer, Matthew Botvnick\nfor very useful discussions. The authors would also like to thank Nasim Rahaman, Samarth Sinha,\nNithin Vasisth, Hugo Larochelle, Jordan Hoffman, Ankesh Anand for feedback on the draft. The\nauthors are grateful to NSERC, CIFAR, Google, Samsung, Nuance, IBM, Canada Research Chairs,\nCanada Graduate Scholarship Program, Nvidia for funding, and Compute Canada for computing\nresources. We are very grateful to Google for giving Google Cloud credits used in this project.\n9\n",
      "References\n[1] Alessandro Achille and Stefano Soatto. Information dropout: learning optimal representations\nthrough noise. CoRR, abs/1611.01353, 2016. URL http://arxiv.org/abs/1611.01353.\n[2] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational\ninformation bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.\n00410.\n[3] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with\npolicy sketches. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pages 166\u2013175. JMLR. org, 2017.\n[4] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pages\n1726\u20131734, 2017.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym, 2016.\n[6] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environ-\nment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.\n[7] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n[8] Christian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search.\nIn Arti\ufb01cial Intelligence and Statistics, pages 273\u2013281, 2012.\n[9] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural\ninformation processing systems, pages 271\u2013278, 1993.\n[10] Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function\ndecomposition. Journal of Arti\ufb01cial Intelligence Research, 13:227\u2013303, 2000.\n[11] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you\nneed: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.\n[12] Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical\nreinforcement learning. arXiv preprint arXiv:1704.03012, 2017.\n[13] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman. Meta Learning Shared Hierarchies. arXiv\ne-prints, October 2017.\n[14] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared\nhierarchies. arXiv preprint arXiv:1710.09767, 2017.\n[15] Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies\nfor hierarchical reinforcement learning. arXiv preprint arXiv:1804.02808, 2018.\n[16] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom\nErez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in\nrich environments. arXiv preprint arXiv:1707.02286, 2017.\n[17] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei,\nC Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual rea-\nsoning. In Proceedings of the IEEE International Conference on Computer Vision, pages\n2989\u20132998, 2017.\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[19] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical\ndeep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In\nAdvances in neural information processing systems, pages 3675\u20133683, 2016.\n10\n",
      "[20] Libin Liu and Jessica Hodgins. Learning to schedule control fragments for physics-based\ncharacters using deep q-learning. ACM Transactions on Graphics, 36(3), 2017.\n[21] Marlos C Machado, Marc G Bellemare, and Michael Bowling. A laplacian framework for\noption discovery in reinforcement learning. arXiv preprint arXiv:1703.00956, 2017.\n[22] Josh Merel, Arun Ahuja, Vu Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva Tirumala, Nicolas\nHeess, and Greg Wayne. Hierarchical visuomotor control of humanoids. In International\nConference on Learning Representations, 2019. URL https://openreview.net/forum?\nid=BJfYvo09Y7.\n[23] Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne,\nYee Whye Teh, and Nicolas Heess. Neural probabilistic motor primitives for humanoid control.\nIn International Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=BJl6TjRcY7.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n[25] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on machine learning, pages 1928\u20131937,\n2016.\n[26] Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Sch\u00f6lkopf.\nLearning independent causal mechanisms. arXiv preprint arXiv:1712.00961, 2017.\n[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\nPyTorch. In NIPS Autodiff Workshop, 2017.\n[28] Xue Bin Peng, Glen Berseth, Kangkang Yin, and Michiel Van De Panne. Deeploco: Dynamic\nlocomotion skills using hierarchical deep reinforcement learning. ACM Trans. Graph., 36\n(4):41:1\u201341:13, July 2017. ISSN 0730-0301. doi: 10.1145/3072959.3073602. URL http:\n//doi.acm.org/10.1145/3072959.3073602.\n[29] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-\nguided deep reinforcement learning of physics-based character skills. ACM Trans. Graph.,\n37(4):143:1\u2013143:14, July 2018. ISSN 0730-0301. doi: 10.1145/3197517.3201311. URL\nhttp://doi.acm.org/10.1145/3197517.3201311.\n[30] Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing networks and\nthe challenges of modular and compositional computation. arXiv preprint arXiv:1904.12774,\n2019.\n[31] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg,\nDavid Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning.\narXiv preprint arXiv:1703.01161, 2017.\n[32] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation.\narXiv preprint\narXiv:1506.02438, 2015.\n[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[34] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press,\n1998.\n[35] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Proceedings of the 12th\nInternational Conference on Neural Information Processing Systems, NIPS\u201999, pages 1057\u2013\n1063, Cambridge, MA, USA, 1999. MIT Press. URL http://dl.acm.org/citation.cfm?\nid=3009657.3009806.\n11\n",
      "[36] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):\n181\u2013211, 1999.\n[37] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):\n181\u2013211, 1999.\n[38] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for\nmachine learning. University of Toronto, Technical Report, 2012.\n[39] Naftali Tishby, Fernando C. N. Pereira, and William Bialek. The information bottleneck method.\nCoRR, physics/0004057, 2000. URL http://arxiv.org/abs/physics/0004057.\n[40] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n5026\u20135033. IEEE, 2012.\n[41] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of\nMachine Learning Research, 9:2579\u20132605, 2008. URL http://www.jmlr.org/papers/v9/\nvandermaaten08a.html.\n[42] Ronald J. Williams.\nSimple Statistical Gradient-Following Algorithms for Connectionist\nReinforcement Learning. Machine Learning, 8(3-4):229\u2013256, 1992. ISSN 0885-6125. doi:\n10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.\n[43] Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region\nmethod for deep reinforcement learning using kronecker-factored approximation. In Advances\nin neural information processing systems, pages 5279\u20135288, 2017.\nA\nInterpretation of the regularization term\nThe regularization term is given by\nLreg =\nX\nk\n\u03b1kLk ,\nwhere\n\u03b1k = eLk/\nX\nj\neLj ,\nand thus\nlog \u03b1k = Lk \u2212log\nX\nj\neLj ,\nor\nLk = log \u03b1k + LSE(L1, . . . , LK) ,\nwhere LSE(L1, . . . , LK) = log P\nj eLj is independent of k.\nPlugging this in, and using P \u03b1k = 1, we get\nLreg =\nX\nk\n\u03b1k log \u03b1k + LSE(L1, . . . , LK) = \u2212H(\u03b1) + LSE(L1, . . . , LK) .\nInformation-theoretic interpretation\nNotably, Lreg also represents an upper bound to the KL-\ndivergence of a mixture of the currently active primitives and a prior,\nLreg \u2265DKL(\nX\nk\n\u03b1kpenc(Zk|S)||N(0, 1)) ,\nand thus can be regarded as a term limiting the information content of the mixture of all active\nprimitives. This arises from the convexity properties of the KL divergence, which directly lead to\nDKL(\nX\nk\n\u03b1kfk||g) \u2264\nX\nk\n\u03b1kDKL(fk||g) .\n12\n",
      "B\nAdditional Results\nB.1\n2D Bandits Environment\nIn order to test if our approach can learn distinct primitives, we used the 2D moving bandits tasks\n(introduced in [14]). In this task, the agent is placed in a 2D world and is shown the position of two\nrandomly placed points. One of these points is the goal point but the agent does not know which. We\nuse the sparse reward setup where the agent receives the reward of 1 if it is within a certain distance\nof the goal point and 0 at all other times. Each episode lasts for 50 steps and to get the reward, the\nlearning agent must reach near the goal point in those 50 steps. The agent\u2019s action space consists of 5\nactions - moving in one of the four cardinal directions (top, down, left, right) and staying still.\nB.1.1\nResults for 2D Bandits\nWe want to answer the following questions:\n1. Can our proposed approach learn primitives which remain active throughout training?\n2. Can our proposed approach learn primitives which can solve the task?\nWe train two primitives on the 2D Bandits tasks and evaluate the relative frequency of activation of\nthe primitives throughout the training. It is important that both the primitives remain active. If only 1\nprimitive is acting most of the time, its effect would be the same as training a \ufb02at policy. We evaluate\nthe effectiveness of our model by comparing the success rate with a \ufb02at A2C baseline. Figure 8 shows\nthat not only do both the primitives remain active throughout training, our approach also outperforms\nthe baseline approach.\n0\n1\n2\n3\n4\n5\nNumber of frames\n1e6\n20\n40\n60\n80\nPercentage of episodes completed\nFlat Policy\nDecentralized Policy\n0\n1\n2\n3\n4\nNumber of frames\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative frequency of activation\nidx 1\nidx 2\nFigure 8: Performance on the 2D bandits task. Left: The comparison of our model (blue curve -\ndecentralized policy) with the baseline (red curve - \ufb02at policy) in terms of success rate shows the\neffectiveness of our proposed approach. Right: Relative frequency of activation of the primitives\n(normalized to sum up to 1). Both primitives are utilized throughout the training.\nB.2\nFour-rooms Environment\nWe consider the Four-rooms gridworld environment [37] where the agent has to navigate its way\nthrough a grid of four interconnected rooms to reach a goal position within the grid. The agent\ncan perform one of the following four actions: move up, move down, move left, move right. The\nenvironment is stochastic and with 1/3 probability, the agent\u2019s chosen action is ignored and a new\naction (randomly selected from the remaining 3 actions) is executed ie the agent\u2019s selected action\nis executed with a probability of only 2/3 and the agent takes any of the 3 remaining actions with a\nprobability of 1/9 each.\nB.2.1\nTask distribution for the Four-room Environment\nIn the Four-room environment, the agent has to navigate to a goal position which is randomly selected\nfrom a set of goal positions. We can use the size of this set of goal positions to de\ufb01ne a curriculum\nof task distributions. Since the environment does not provide any information about the goal state,\nthe larger the goal set, harder is the task as the now goal could be any element from a larger set.\nThe choice of the set of goal states and the choice of curriculum does not affect the environment\n13\n",
      "dynamics. Speci\ufb01cally, we consider three tasks - Fourroom-v0, Fourroom-v1 and Fourroom-v2 with\nthe set of 2, 4 and 8 goal positions respectively. The set of goal positions for each task is \ufb01xed but not\nknown to the learning agent. We expect, and empirically verify, that the Fourroom-v0 environment\nrequires the least number of samples to be learned, followed by the Fourroom-v1 and the Fourroom-v2\nenvironment (\ufb01gure 6 in the paper).\nB.2.2\nResults for Four-rooms environment\nWe want to answer the following questions:\n1. Can our proposed approach learn primitives that remain active when training the agent over\na sequence of tasks?\n2. Can our proposed approach be used to improve the sample ef\ufb01ciency of the agent over a\nsequence of tasks?\nTo answer these questions, we consider two setups. In the baseline setup, we train a \ufb02at A2C policy\non Fourrooms-v0 till it achieves a 100 % success rate during evaluation. Then we transfer this policy\nto Fourrooms-v1 and continue to train till it achieves a 100 % success rate during the evaluation\non Fourrooms-v1. We transfer the policy one more time to Fourrooms-v2 and continue to train the\npolicy until it reaches a 60% success rate. In the last task(Fourrooms-v2), we do not use 100% as the\nthreshold as the models do not achieve 100% for training even after training for 10M frames. We use\n60% as the baseline models generally converge around this value.\nIn the second setup, we repeat this exercise of training on one task and transferring to the next task\nwith our proposed model. Note that even though our proposed model converges to a higher value\nthan 60% in the last task(Fourrooms-v2), we compare the number of samples required to reach 60%\nsuccess rate to provide a fair comparison with the baseline.\nC\nImplementation Details\nIn this section, we describe the implementation details which are common for all the models. Other\ntask-speci\ufb01c details are covered in the respective task sections.\n1. All the models (proposed as well as the baselines) are implemented in Pytorch 1.1 unless\nstated otherwise. [27].\n2. For Meta-Learning Shared Hierarchies [14] and Option-Critic [4], we adapted the author\u2019s\nimplementations 5for our environments.\n3. During the evaluation, we use 10 processes in parallel to run 500 episodes and compute the\npercentage of times the agent solves the task within the prescribed time limit. This metric is\nreferred to as the \u201csuccess rate\u201d.\n4. The default time limit is 500 steps for all the tasks unless speci\ufb01ed otherwise.\n5. All the feedforward networks are initialized with the orthogonal initialization where the\ninput tensor is \ufb01lled with a (semi) orthogonal matrix.\n6. For all the embedding layers, the weights are initialized using the unit-Gaussian distribution.\n7. The weights and biases for all the GRU model are initialized using the uniform distribution\nfrom U(\u2212\n\u221a\nk,\n\u221a\nk) where k =\n1\nhidden_size.\n8. During training, we perform 64 rollouts in parallel to collect 5-step trajectories.\n9. The \u03b2ind and \u03b2reg parameters are both selected from the set {0.001, 0.005, 0.009} by\nperforming validation.\nIn section D.4.2, we explain all the components of the model architecture along with the implemen-\ntation details in the context of the MiniGrid Environment. For the subsequent environments, we\ndescribe only those components and implementation details which are different than their counterpart\nin the MiniGrid setup and do not describe the components which work identically.\n5https://github.com/openai/mlsh, https://github.com/jeanharb/option_critic\n14\n",
      "D\nMiniGrid Environment\nWe use the MiniGrid environment [6] which is an open-source, grid-world environment package 6.\nIt provides a family of customizable reinforcement learning environments that are compatible with\nthe OpenAI Gym framework [5]. Since the environments can be easily extended and modi\ufb01ed, it is\nstraightforward to control the complexity of the task (eg controlling the size of the grid, the number\nof rooms or the number of objects in the grid, etc). Such \ufb02exibility is very useful when experimenting\nwith curriculum learning or testing for generalization.\nD.1\nThe World\nIn MiniGrid, the world (environment for the learning agent) is a rectangular grid of size say MxN.\nEach tile in the grid contains either zero or one object. The possible object types are wall, \ufb02oor, lava,\ndoor, key, ball, box and goal. Each object has an associated string (which denote the object type) and\nan associated discrete color (could be red, green, blue, purple, yellow and grey). By default, walls are\nalways grey and goal squares are always green. Certain objects have special effects. For example, a\nkey can unlock a door of the same color.\nD.1.1\nReward Function\nWe consider the sparse reward setup where the agent gets a reward (of 1) only if it completes the task\nand 0 at all other time steps. We also apply a time limit of 500 steps on all the tasks ie the agent must\ncomplete the task in 500 steps. A task is terminated either when the agent solves the task or when the\ntime limit is reached - whichever happens \ufb01rst.\nD.1.2\nAction Space\nThe agent can perform one of the following seven actions per timestep: turn left, turn right, move\nforward, pick up an object, drop the object being carried, toggle, done (optional action).\nThe agent can use the turn left and turn right actions to rotate around and face one of the 4 possible\ndirections (north, south, east, west). The move forward action makes the agent move from its current\ntile onto the tile in the direction it is currently facing, provided there is nothing on that tile, or that the\ntile contains an open door. The toggle actions enable the agent to interact with other objects in the\nworld. For example, the agent can use the toggle action to open the door if they are right in front of it\nand have the key of matching color.\nD.1.3\nObservation Space\nThe MiniGrid environment provides partial and egocentric observations. For all our experiments, the\nagent sees the view of a square of 4x4 tiles in the direction it is facing. The view includes the tile on\nwhich the agent is standing. The observations are provided as a tensor of shape 4x4x3. However,\nnote that this tensor does not represent RGB images. The \ufb01rst two channels denote the view size and\nthe third channel encodes three integer values. The \ufb01rst integer value describes the type of the object,\nthe second value describes the color of the object and the third value describes if the doors are open\nor closed. The bene\ufb01t of using this encoding over the RGB encoding is that this encoding is more\nspace-ef\ufb01cient and enables faster training. For human viewing, the fully observable, RGB image\nview of the environments is also provided and we use that view as an example in the paper.\nAdditionally, the environment also provides a natural language description of the goal. An example\nof the goal description is: \u201cUnlock the door and pick up the red ball\u201d. The learning agent and the\nenvironment use a shared vocabulary where different words are assigned numbers and the environment\nprovides a number-encoded goal description along with each observation. Since different instructions\ncan be of different lengths, the environment pads the goal description with <unk> tokens to ensure\nthat the sequence length is the same. When encoding the instruction, the agent ignores the padded\nsub-sequence in the instruction.\n15\n",
      "Figure 9: RGB view of the Fetch environment.\nFigure 10: RGB view of the Unlock environment.\nD.2\nTasks in MiniGrid Environment\nWe consider the following tasks in the MiniGrid environment:\n1. Fetch: In the Fetch task, the agent spawns at an arbitrary position in a 8 \u00d7 8 grid (\ufb01gure 9 ).\nIt is provided with a natural language goal description of the form \u201cgo fetch a yellow box\u201d.\nThe agent has to navigate to the object being referred to in the goal description and pick it\nup.\n2. Unlock: In the Unlock task, the agent spawns at an arbitrary position in a two-room grid\nenvironment. Each room is 8 \u00d7 8 square (\ufb01gure 10 ). It is provided with a natural language\n6https://github.com/maximecb/gym-minigrid\nFigure 11: RGB view of the UnlockPickup environment.\n16\n",
      "goal description of the form \u201copen the door\u201d. The agent has to \ufb01nd the key that corresponds\nto the color of the door, navigate to that key and use that key to open the door.\n3. UnlockPickup: This task is basically a union of the Unlock and the Fetch tasks. The agent\nspawns at an arbitrary position in a two-room grid environment. Each room is 8 \u00d7 8 square\n(\ufb01gure 11 ). It is provided with a natural language goal description of the form \u201copen the\ndoor and pick up the yellow box\u201d. The agent has to \ufb01nd the key that corresponds to the color\nof the door, navigate to that key, use that key to open the door, enter the other room and pick\nup the object mentioned in the goal description.\nD.3\nModel Architecture\nD.3.1\nTraining Setup\nConsider an agent training on any task in the MiniGrid suite of environments. At the beginning\nof an episode, the learning agent spawns at a random position. At each step, the environment\nprovides observations in two modalities - a 4 \u00d7 4 \u00d7 3 tensor xt (an egocentric view of the state of the\nenvironment) and a variable length goal description g. We describe the design of the learning agent\nin terms of an encoder-decoder architecture.\nD.3.2\nEncoder Architecture\nThe agent\u2019s encoder network consists of two models - a CNN+GRU based observation encoder and\na GRU [7] based goal encoder\nObservation Encoder:\nIt is a three layer CNN with the output channel sizes set to 16, 16 and 32 respectively (with ReLU\nlayers in between) and kernel size set to 2 \u00d7 2 for all the layers. The output of the CNN is \ufb02attened\nand fed to a GRU model (referred to as the observation-rnn) with 128-dimensional hidden state. The\noutput from the observation-rnn represents the encoding of the observation.\nGoal Encoder:\nIt comprises of an embedding layer followed by a unidirectional GRU model. The dimension of the\nembedding layer and the hidden and the output layer of the GRU model are all set to 128.\nThe concatenated output of the observation encoder and the goal encoder represents the output of\nthe encoder.\nD.3.3\nDecoder\nThe decoder network comprises the action network and the critic network - both of which are\nimplemented as feedforward networks. We now describe the design of these networks.\nD.3.4\nValue Network\n1. Two-layer feedforward network with the tanh non-linearity.\n2. Input: Concatenation of z and the current hidden state of the observation-rnn.\n3. Size of the input to the \ufb01rst layer and the second layer of the policy network are 320 and 64\nrespectively.\n4. Produces a scalar output.\nD.4\nComponents speci\ufb01c to the proposed model\nThe components that we described so far are used by both the baselines as well as our proposed\nmodel. We now describe the components that are speci\ufb01c to our proposed model. Our proposed\nmodel consists of an ensemble of primitives and the components we describe apply to each of those\nprimitives.\n17\n",
      "D.4.1\nInformation Bottleneck\nGiven that we want to control and regularize the amount of information that the encoder encodes, we\ncompute the KL divergence between the output of the action-feature encoder network and a diagonal\nunit Gaussian distribution. More is the KL divergence, more is the information that is being encoded\nwith respect to the Gaussian prior and vice-versa. Thus we regularize the primitives to minimize the\nKL divergence.\nD.4.2\nHyperparameters\nTable 1 lists the different hyperparameters for the MiniGrid tasks.\nParameter\nValue\nLearning Algorithm\nA2C [43]\nOpitimizer \u2018\nRMSProp[38]\nlearning rate\n7 \u00b7 10\u22124\nbatch size\n64\ndiscount\n0.99\nlambda (for GAE [32])\n0.95\nentropy coef\ufb01cient\n10\u22122\nloss coef\ufb01cient\n0.5\nMaximum gradient norm\n0.5\nTable 1: Hyperparameters\nE\n2D Bandits Environment\nE.0.1\nObservation Space\nThe 2D bandits task provides a 6-dimensional \ufb02at observation. The \ufb01rst two dimensions correspond\nto the (x, y) coordinates of the current position of the agent and the remaining four dimensions\ncorrespond to the (x, y) coordinates of the two randomly chosen points.\nE.1\nModel Architecture\nE.1.1\nTraining Setup\nConsider an agent training on the 2D bandits tasks. The learning agent spawns at a \ufb01xed position and\nis randomly assigned two points. At each step, the environmental observation provides the current\nposition of the agent as well the position of the two points. We describe the design of the learning\nagent in terms of an encoder-decoder architecture.\nE.1.2\nEncoder Architecture\nThe agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn) with a hidden state size of 128. The 6-dimensional observation from the environment is the input\nto the GRU model. The output from the observation-rnn represents the encoding of the observation.\nE.2\nHyperparameters\nThe implementation details for the 2D Bandits environment are the same as that for MiniGrid\nenvironment and are described in detail in section D.4.2. In the table below, we list the values of the\ntask-speci\ufb01c hyperparameters.\n18\n",
      "Parameter\nValue\nLearning Algorithm\nPPO [33]\nepochs per update (PPO)\n10\nOptimizer \u2018\nAdam[18]\nlearning rate\n3 \u00b7 10\u22125\n\u03b21\n0.9\n\u03b22\n0.999\nbatch size\n64\ndiscount\n0.99\nentropy coef\ufb01cient\n0\nloss coef\ufb01cient\n1.0\nMaximum gradient norm\n0.5\nTable 2: Hyperparameters\nF\nFour-rooms Environment\nF.1\nThe World\nIn the Four-rooms setup, the world (environment for the learning agent) is a square grid of say 11\u00d711.\nThe grid is divided into 4 rooms such that each room is connected with two other rooms via hallways.\nThe layout of the rooms is shown in \ufb01gure 12. The agent spawns at a random position and has to\nnavigate to a goal position within 500 steps.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nFigure 12: View of the four-room environment\nF.1.1\nReward Function\nWe consider the sparse reward setup where the agent gets a reward (of 1) only if it completes the task\n(and reaches the goal position) and 0 at all other time steps. We also apply a time limit of 300 steps\non all the tasks ie the agent must complete the task in 300 steps. A task is terminate either when the\nagent solves the task or when the time limit is reached - whichever happens \ufb01rst.\nF.1.2\nObservation Space\nThe environment is a 11 \u00d7 11 grid divided into 4 interconnected rooms. As such, the environment has\na total of 104 states (or cells) that can be occupied. These states are mapped to integer identi\ufb01ers. At\nany time t, the environment observation is a one-hot representation of the identi\ufb01er corresponding to\nthe state (or the cell) the agent is in right now. ie the environment returns a vectors of zeros with only\none entry being 1 and the index of this entry gives the current position of the agent. The environment\ndoes not return any information about the goal state.\n19\n",
      "F.2\nModel Architecture for Four-room Environment\nF.2.1\nTraining Setup\nConsider an agent training on any task in the Four-room suite of environments. At the beginning\nof an episode, the learning agent spawns at a random position and the environment selects a goal\nposition for the agent. At each step, the environment provides a one-hot representation of the agent\u2019s\ncurrent position (without including any information about the goal state). We describe the design of\nthe learning agent in terms of an encoder-decoder architecture.\nF.3\nEncoder Architecture\nThe agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn with a hidden state size of 128. The 104-dimensional one-hot input from the environment is fed\nto the GRU model. The output from the observation-rnn represents the encoding of the observation.\nThe implementation details for the Four-rooms environment are the same as that for MiniGrid\nenvironment and are described in detail in section D.4.2.\nG\nAnt Maze Environment\nWe use the Mujoco-based quadruple ant [40] to evaluate the transfer performance of our approach\non the cross maze environment [15]. The training happens in two phases. In the \ufb01rst phase, we\ntrain the ant to walk on a surface using a motion reward and using just 1 primitive. In the second\nphase, we make 4 copies of this trained policy and train the agent to navigate to a goal position in a\nmaze (Figure 13). The goal position is chosen from a set of 3 (or 10) goals. The environment is a\ncontinuous control environment and the agent can directly manipulate the movement of joints and\nlimbs.\nFigure 13: View of the Ant Maze environment with 3 goals\nG.0.1\nObservation Space\nIn the \ufb01rst phase (training the ant to walk), the observations from the environment correspond to the\nstate-space representation ie a real-valued vector that describes the state of the ant in mechanical\nterms - position, velocity, acceleration, angle, etc of the joints and limbs. In the second phase (training\nthe ant to navigate the maze), the observation from the environment also contains the location of the\ngoal position along with the mechanical state of the ant.\nG.1\nModel Architecture for Ant Maze Environment\nG.1.1\nTraining Setup\nWe describe the design of the learning agent in terms of an encoder-decoder architecture.\nG.1.2\nEncoder Architecture\nThe agent\u2019s encoder network consists of a GRU-based recurrent model (referred as the observation-\nrnn with a hidden state size of 128. The real-valued state vector from the environment is fed to the\nGRU model. The output from the observation-rnn represents the encoding of the observation. Note\n20\n",
      "that in the case of phase 1 vs phase 2, only the size of the input to the observation-rnn changes and\nthe encoder architecture remains the same.\nG.1.3\nDecoder\nThe decoder network comprises the action network and the critic network. All these networks are\nimplemented as feedforward networks. The design of these networks is very similar to that of the\ndecoder model for the MiniGrid environment as described in section D.3.3 with just one difference.\nIn this case, the action space is continuous so the action-feature decoder network produces the mean\nand log-standard-deviation for a diagonal Gaussian policy. This is used to sample a real-valued action\nto execute in the environment.\n21\n"
    ],
    "pdf_path": "data/papers/1906.10667v1.pdf"
  },
  {
    "text": "Mastering Atari, Go, Chess and Shogi by Planning with a\nLearned Model\nJulian Schrittwieser,1\u2217Ioannis Antonoglou,1,2\u2217Thomas Hubert,1\u2217\nKaren Simonyan,1 Laurent Sifre,1 Simon Schmitt,1 Arthur Guez,1\nEdward Lockhart,1 Demis Hassabis,1 Thore Graepel,1,2 Timothy Lillicrap,1\nDavid Silver1,2\u2217\n1DeepMind, 6 Pancras Square, London N1C 4AG.\n2University College London, Gower Street, London WC1E 6BT.\n\u2217These authors contributed equally to this work.\nAbstract\nConstructing agents with planning capabilities has long been one of the main challenges in the\npursuit of arti\ufb01cial intelligence. Tree-based planning methods have enjoyed huge success in challeng-\ning domains, such as chess and Go, where a perfect simulator is available. However, in real-world\nproblems the dynamics governing the environment are often complex and unknown. In this work\nwe present the MuZero algorithm which, by combining a tree-based search with a learned model,\nachieves superhuman performance in a range of challenging and visually complex domains, without\nany knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively,\npredicts the quantities most directly relevant to planning: the reward, the action-selection policy, and\nthe value function. When evaluated on 57 different Atari games - the canonical video game environ-\nment for testing AI techniques, in which model-based planning approaches have historically struggled\n- our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without\nany knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero\nalgorithm that was supplied with the game rules.\n1\nIntroduction\nPlanning algorithms based on lookahead search have achieved remarkable successes in arti\ufb01cial intelligence. Hu-\nman world champions have been defeated in classic games such as checkers [34], chess [5], Go [38] and poker\n[3, 26], and planning algorithms have had real-world impact in applications from logistics [47] to chemical syn-\nthesis [37]. However, these planning algorithms all rely on knowledge of the environment\u2019s dynamics, such as the\nrules of the game or an accurate simulator, preventing their direct application to real-world domains like robotics,\nindustrial control, or intelligent assistants.\nModel-based reinforcement learning (RL) [42] aims to address this issue by \ufb01rst learning a model of the\nenvironment\u2019s dynamics, and then planning with respect to the learned model. Typically, these models have either\nfocused on reconstructing the true environmental state [8, 16, 24], or the sequence of full observations [14, 20].\nHowever, prior work [4, 14, 20] remains far from the state of the art in visually rich domains, such as Atari 2600\ngames [2]. Instead, the most successful methods are based on model-free RL [9, 21, 18] \u2013 i.e. they estimate\nthe optimal policy and/or value function directly from interactions with the environment. However, model-free\nalgorithms are in turn far from the state of the art in domains that require precise and sophisticated lookahead, such\nas chess and Go.\n1\narXiv:1911.08265v2  [cs.LG]  21 Feb 2020\n\n\nIn this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art per-\nformance in Atari 2600, a visually complex set of domains, while maintaining superhuman performance in pre-\ncision planning tasks such as chess, shogi and Go. MuZero builds upon AlphaZero\u2019s [39] powerful search and\nsearch-based policy iteration algorithms, but incorporates a learned model into the training procedure. MuZero\nalso extends AlphaZero to a broader set of environments including single agent domains and non-zero rewards at\nintermediate time-steps.\nThe main idea of the algorithm (summarized in Figure 1) is to predict those aspects of the future that are directly\nrelevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an\ninput and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that\nreceives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the\npolicy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points\nscored by playing a move). The model is trained end-to-end, with the sole objective of accurately estimating these\nthree important quantities, so as to match the improved estimates of policy and value generated by search as well\nas the observed reward. There is no direct constraint or requirement for the hidden state to capture all information\nnecessary to reconstruct the original observation, drastically reducing the amount of information the model has\nto maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the\nenvironment; nor any other constraints on the semantics of state. Instead, the hidden states are free to represent\nstate in whatever way is relevant to predicting current and future values and policies. Intuitively, the agent can\ninvent, internally, the rules or dynamics that lead to most accurate planning.\n2\nPrior Work\nReinforcement learning may be subdivided into two principal categories: model-based, and model-free [42].\nModel-based RL constructs, as an intermediate step, a model of the environment. Classically, this model is\nrepresented by a Markov-decision process (MDP) [31] consisting of two components: a state transition model,\npredicting the next state, and a reward model, predicting the expected reward during that transition. The model\nis typically conditioned on the selected action, or a temporally abstract behavior such as an option [43]. Once\na model has been constructed, it is straightforward to apply MDP planning algorithms, such as value iteration\n[31] or Monte-Carlo tree search (MCTS) [7], to compute the optimal value or optimal policy for the MDP. In\nlarge or partially observed environments, the algorithm must \ufb01rst construct the state representation that the model\nshould predict. This tripartite separation between representation learning, model learning, and planning is poten-\ntially problematic since the agent is not able to optimize its representation or model for the purpose of effective\nplanning, so that, for example modeling errors may compound during planning.\nA common approach to model-based RL focuses on directly modeling the observation stream at the pixel-\nlevel. It has been hypothesized that deep, stochastic models may mitigate the problems of compounding error\n[14, 20]. However, planning at pixel-level granularity is not computationally tractable in large scale problems.\nOther methods build a latent state-space model that is suf\ufb01cient to reconstruct the observation stream at pixel level\n[48, 49], or to predict its future latent states [13, 11], which facilitates more ef\ufb01cient planning but still focuses\nthe majority of the model capacity on potentially irrelevant detail. None of these prior methods has constructed a\nmodel that facilitates effective planning in visually complex domains such as Atari; results lag behind well-tuned,\nmodel-free methods, even in terms of data ef\ufb01ciency [45].\nA quite different approach to model-based RL has recently been developed, focused end-to-end on predicting\nthe value function [41]. The main idea of these methods is to construct an abstract MDP model such that planning\nin the abstract MDP is equivalent to planning in the real environment. This equivalence is achieved by ensuring\nvalue equivalence, i.e. that, starting from the same real state, the cumulative reward of a trajectory through the\nabstract MDP matches the cumulative reward of a trajectory in the real environment.\nThe predictron [41] \ufb01rst introduced value equivalent models for predicting value (without actions). Although\nthe underlying model still takes the form of an MDP, there is no requirement for its transition model to match\nreal states in the environment. Instead the MDP model is viewed as a hidden layer of a deep neural network. The\nunrolled MDP is trained such that the expected cumulative sum of rewards matches the expected value with respect\nto the real environment, e.g. by temporal-difference learning.\n2\n\n\nFigure 1:\nPlanning, acting, and training with a learned model. (A) How MuZero uses its model to plan.\nThe model consists of three connected components for representation, dynamics and prediction. Given a previous\nhidden state sk\u22121 and a candidate action ak, the dynamics function g produces an immediate reward rk and a new\nhidden state sk. The policy pk and value function vk are computed from the hidden state sk by a prediction function\nf. The initial hidden state s0 is obtained by passing the past observations (e.g. the Go board or Atari screen) into\na representation function h. (B) How MuZero acts in the environment. A Monte-Carlo Tree Search is performed\nat each timestep t, as described in A. An action at+1 is sampled from the search policy \u03c0t, which is proportional\nto the visit count for each action from the root node. The environment receives the action and generates a new\nobservation ot+1 and reward ut+1. At the end of the episode the trajectory data is stored into a replay buffer. (C)\nHow MuZero trains its model. A trajectory is sampled from the replay buffer. For the initial step, the representation\nfunction h receives as input the past observations o1, ..., ot from the selected trajectory. The model is subsequently\nunrolled recurrently for K steps. At each step k, the dynamics function g receives as input the hidden state sk\u22121\nfrom the previous step and the real action at+k. The parameters of the representation, dynamics and prediction\nfunctions are jointly trained, end-to-end by backpropagation-through-time, to predict three quantities: the policy\npk \u2248\u03c0t+k, value function vk \u2248zt+k, and reward rt+k \u2248ut+k, where zt+k is a sample return: either the \ufb01nal\nreward (board games) or n-step return (Atari).\nValue equivalent models were subsequently extended to optimising value (with actions). TreeQN [10] learns\nan abstract MDP model, such that a tree search over that model (represented by a tree-structured neural network)\napproximates the optimal value function. Value iteration networks [44] learn a local MDP model, such that value\niteration over that model (represented by a convolutional neural network) approximates the optimal value function.\nValue prediction networks [28] are perhaps the closest precursor to MuZero: they learn an MDP model grounded\nin real actions; the unrolled MDP is trained such that the cumulative sum of rewards, conditioned on the actual\nsequence of actions generated by a simple lookahead search, matches the real environment. Unlike MuZero there\nis no policy prediction, and the search only utilizes value prediction.\n3\nMuZero Algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are made at each time-step t, for each of\nk = 1...K steps, by a model \u00b5\u03b8, with parameters \u03b8, conditioned on past observations o1, ..., ot and future actions\nat+1, ..., at+k. The model predicts three future quantities: the policy pk\nt \u2248\u03c0(at+k+1|o1, ..., ot, at+1, ..., at+k), the\nvalue function vk\nt \u2248E [ut+k+1 + \u03b3ut+k+2 + ...|o1, ..., ot, at+1, ..., at+k], and the immediate reward rk\nt \u2248ut+k,\n3\n\n\nwhere u. is the true, observed reward, \u03c0 is the policy used to select real actions, and \u03b3 is the discount function of\nthe environment.\nInternally, at each time-step t (subscripts t suppressed for simplicity), the model is represented by the com-\nbination of a representation function, a dynamics function, and a prediction function. The dynamics function,\nrk, sk = g\u03b8(sk\u22121, ak), is a recurrent process that computes, at each hypothetical step k, an immediate reward rk\nand an internal state sk. It mirrors the structure of an MDP model that computes the expected reward and state\ntransition for a given state and action [31]. However, unlike traditional approaches to model-based RL [42], this\ninternal state sk has no semantics of environment state attached to it \u2013 it is simply the hidden state of the overall\nmodel, and its sole purpose is to accurately predict relevant, future quantities: policies, values, and rewards. In\nthis paper, the dynamics function is represented deterministically; the extension to stochastic transitions is left for\nfuture work. The policy and value functions are computed from the internal state sk by the prediction function,\npk, vk = f\u03b8(sk), akin to the joint policy and value network of AlphaZero. The \u201croot\u201d state s0 is initialized using\na representation function that encodes past observations, s0 = h\u03b8(o1, ..., ot); again this has no special semantics\nbeyond its support for future predictions.\nGiven such a model, it is possible to search over hypothetical future trajectories a1, ..., ak given past obser-\nvations o1, ..., ot. For example, a naive search could simply select the k step action sequence that maximizes the\nvalue function. More generally, we may apply any MDP planning algorithm to the internal rewards and state space\ninduced by the dynamics function. Speci\ufb01cally, we use an MCTS algorithm similar to AlphaZero\u2019s search, gener-\nalized to allow for single agent domains and intermediate rewards (see Methods). At each internal node, it makes\nuse of the policy, value and reward estimates produced by the current model parameters \u03b8. The MCTS algorithm\noutputs a recommended policy \u03c0t and estimated value \u03bdt. An action at+1 \u223c\u03c0t is then selected.\nAll parameters of the model are trained jointly to accurately match the policy, value, and reward, for every\nhypothetical step k, to corresponding target values observed after k actual time-steps have elapsed. Similarly to\nAlphaZero, the improved policy targets are generated by an MCTS search; the \ufb01rst objective is to minimise the\nerror between predicted policy pk\nt and search policy \u03c0t+k. Also like AlphaZero, the improved value targets are\ngenerated by playing the game or MDP. However, unlike AlphaZero, we allow for long episodes with discounting\nand intermediate rewards by bootstrapping n steps into the future from the search value, zt = ut+1 +\u03b3ut+2 +...+\n\u03b3n\u22121ut+n + \u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in board games are treated as rewards ut \u2208{\u22121, 0, +1}\noccuring at the \ufb01nal step of the episode. Speci\ufb01cally, the second objective is to minimize the error between\nthe predicted value vk\nt and the value target, zt+k 1. The reward targets are simply the observed rewards; the third\nobjective is therefore to minimize the error between the predicted reward rk\nt and the observed reward ut+k. Finally,\nan L2 regularization term is also added, leading to the overall loss:\nlt(\u03b8) =\nK\nX\nk=0\nlr(ut+k, rk\nt ) + lv(zt+k, vk\nt ) + lp(\u03c0t+k, pk\nt ) + c||\u03b8||2\n(1)\nwhere lr, lv, and lp are loss functions for reward, value and policy respectively. Supplementary Figure S2 summa-\nrizes the equations governing how the MuZero algorithm plans, acts, and learns.\n4\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess and shogi 2, as benchmarks for challenging\nplanning problems, and to all 57 games in the Atari Learning Environment [2], as benchmarks for visually complex\nRL domains.\nIn each case we trained MuZero for K = 5 hypothetical steps. Training proceeded for 1 million mini-batches\nof size 2048 in board games and of size 1024 in Atari. During both training and evaluation, MuZero used 800\nsimulations for each search in board games, and 50 simulations for each search in Atari. The representation\n1For chess, Go and shogi, the same squared error loss as AlphaZero is used for rewards and values. A cross-entropy loss was found to be\nmore stable than a squared error when encountering rewards and values of variable scale in Atari. Cross-entropy was used for the policy loss\nin both cases.\n2Imperfect information games such as Poker are not directly addressed by our method.\n4\n\n\nChess\nShogi\nGo\nAtari\nrmblkans\nopopopop\n0Z0Z0Z0Z\nZ0Z0Z0Z0\n0Z0Z0Z0Z\nZ0Z0Z0Z0\nPOPOPOPO\nSNAQJBMR\n\u9999\n\u6842\n\u9280\n\u91d1\n\u7389\n\u91d1\n\u9280\n\u6842\n\u9999\n\u98db\n\u89d2\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\n\u89d2\n\u98db\n\u9999\u6842\u9280\u91d1\u7389\u91d1\u9280\u6842\u9999\nFigure 2: Evaluation of MuZero throughout training in chess, shogi, Go and Atari. The x-axis shows millions\nof training steps. For chess, shogi and Go, the y-axis shows Elo rating, established by playing games against Alp-\nhaZero using 800 simulations per move for both players. MuZero\u2019s Elo is indicated by the blue line, AlphaZero\u2019s\nElo by the horizontal orange line. For Atari, mean (full line) and median (dashed line) human normalized scores\nacross all 57 games are shown on the y-axis. The scores for R2D2 [21], (the previous state of the art in this domain,\nbased on model-free RL) are indicated by the horizontal orange lines. Performance in Atari was evaluated using\n50 simulations every fourth time-step, and then repeating the chosen action four times, as in prior work [25].\nfunction uses the same convolutional [23] and residual [15] architecture as AlphaZero, but with 16 residual blocks\ninstead of 20. The dynamics function uses the same architecture as the representation function and the prediction\nfunction uses the same architecture as AlphaZero. All networks use 256 hidden planes (see Methods for further\ndetails).\nFigure 2 shows the performance throughout training in each game. In Go, MuZero slightly exceeded the perfor-\nmance of AlphaZero, despite using less computation per node in the search tree (16 residual blocks per evaluation\nin MuZero compared to 20 blocks in AlphaZero). This suggests that MuZero may be caching its computation in\nthe search tree and using each additional application of the dynamics model to gain a deeper understanding of the\nposition.\nIn Atari, MuZero achieved a new state of the art for both mean and median normalized score across the 57\ngames of the Arcade Learning Environment, outperforming the previous state-of-the-art method R2D2 [21] (a\nmodel-free approach) in 42 out of 57 games, and outperforming the previous best model-based approach SimPLe\n[20] in all games (see Table S1).\nWe also evaluated a second version of MuZero that was optimised for greater sample ef\ufb01ciency. Speci\ufb01cally,\nit reanalyzes old trajectories by re-running the MCTS using the latest network parameters to provide fresh targets\n(see Appendix H). When applied to 57 Atari games, using 200 million frames of experience per game, MuZero\nReanalyze achieved 731% median normalized score, compared to 192%, 231% and 431% for previous state-of-\nthe-art model-free approaches IMPALA [9], Rainbow [17] and LASER [36] respectively.\nTo understand the role of the model in MuZero we also ran several experiments, focusing on the board game\nof Go and the Atari game of Ms. Pacman.\nFirst, we tested the scalability of planning (Figure 3A), in the canonical planning problem of Go. We compared\nthe performance of search in AlphaZero, using a perfect model, to the performance of search in MuZero, using a\n5\n\n\nAgent\nMedian\nMean\nEnv. Frames\nTraining Time\nTraining Steps\nApe-X [18]\n434.1%\n1695.6%\n22.8B\n5 days\n8.64M\nR2D2 [21]\n1920.6%\n4024.9%\n37.5B\n5 days\n2.16M\nMuZero\n2041.1%\n4999.2%\n20.0B\n12 hours\n1M\nIMPALA [9]\n191.8%\n957.6%\n200M\n\u2013\n\u2013\nRainbow [17]\n231.1%\n\u2013\n200M\n10 days\n\u2013\nUNREALa [19]\n250%a\n880%a\n250M\n\u2013\n\u2013\nLASER [36]\n431%\n\u2013\n200M\n\u2013\n\u2013\nMuZero Reanalyze\n731.1%\n2168.9%\n200M\n12 hours\n1M\nTable 1:\nComparison of MuZero against previous agents in Atari. We compare separately against agents\ntrained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques.\nMean and median scores are given, compared to human testers. The best results are highlighted in bold. MuZero\nsets a new state of the art in both settings. aHyper-parameters were tuned per game.\nlearned model. Speci\ufb01cally, the fully trained AlphaZero or MuZero was evaluated by comparing MCTS with\ndifferent thinking times. MuZero matched the performance of a perfect model, even when doing much larger\nsearches (up to 10s thinking time) than those from which the model was trained (around 0.1s thinking time, see\nalso Figure S3A).\nWe also investigated the scalability of planning across all Atari games (see Figure 3B). We compared MCTS\nwith different numbers of simulations, using the fully trained MuZero. The improvements due to planning are\nmuch less marked than in Go, perhaps because of greater model inaccuracy; performance improved slightly with\nsearch time, but plateaued at around 100 simulations. Even with a single simulation \u2013 i.e. when selecting moves\nsolely according to the policy network \u2013 MuZero performed well, suggesting that, by the end of training, the raw\npolicy has learned to internalise the bene\ufb01ts of search (see also Figure S3B).\nNext, we tested our model-based learning algorithm against a comparable model-free learning algorithm (see\nFigure 3C). We replaced the training objective of MuZero (Equation 1) with a model-free Q-learning objective\n(as used by R2D2), and the dual value and policy heads with a single head representing the Q-function Q(\u00b7|st).\nSubsequently, we trained and evaluated the new model without using any search. When evaluated on Ms. Pacman,\nour model-free algorithm achieved identical results to R2D2, but learned signi\ufb01cantly slower than MuZero and\nconverged to a much lower \ufb01nal score. We conjecture that the search-based policy improvement step of MuZero\nprovides a stronger learning signal than the high bias, high variance targets used by Q-learning.\nTo better understand the nature of MuZero\u2019s learning algorithm, we measured how MuZero\u2019s training scales\nwith respect to the amount of search it uses during training. Figure 3D shows the performance in Ms. Pacman,\nusing an MCTS of different simulation counts per move throughout training. Surprisingly, and in contrast to\nprevious work [1], even with only 6 simulations per move \u2013 fewer than the number of actions \u2013 MuZero learned\nan effective policy and improved rapidly. With more simulations performance jumped signi\ufb01cantly higher. For\nanalysis of the policy improvement during each individual iteration, see also Figure S3 C and D.\n5\nConclusions\nMany of the breakthroughs in arti\ufb01cial intelligence have been based on either high-performance planning [5, 38,\n39] or model-free reinforcement learning methods [25, 29, 46]. In this paper we have introduced a method that\ncombines the bene\ufb01ts of both approaches. Our algorithm, MuZero, has both matched the superhuman performance\nof high-performance planning algorithms in their favored domains \u2013 logically complex board games such as chess\nand Go \u2013 and outperformed state-of-the-art model-free RL algorithms in their favored domains \u2013 visually complex\nAtari games. Crucially, our method does not require any knowledge of the game rules or environment dynamics,\npotentially paving the way towards the application of powerful learning and planning methods to a host of real-\nworld domains for which there exists no perfect simulator.\n6\n\n\nFigure 3:\nEvaluations of MuZero on Go (A), all 57 Atari Games (B) and Ms. Pacman (C-D). (A) Scaling\nwith search time per move in Go, comparing the learned model with the ground truth simulator. Both networks\nwere trained at 800 simulations per search, equivalent to 0.1 seconds per search. Remarkably, the learned model\nis able to scale well to up to two orders of magnitude longer searches than seen during training. (B) Scaling of\n\ufb01nal human normalized mean score in Atari with the number of simulations per search. The network was trained\nat 50 simulations per search. Dark line indicates mean score, shaded regions indicate 25th to 75th and 5th to\n95th percentiles. The learned model\u2019s performance increases up to 100 simulations per search. Beyond, even\nwhen scaling to much longer searches than during training, the learned model\u2019s performance remains stable and\nonly decreases slightly. This contrasts with the much better scaling in Go (A), presumably due to greater model\ninaccuracy in Atari than Go. (C) Comparison of MCTS based training with Q-learning in the MuZero framework\non Ms. Pacman, keeping network size and amount of training constant. The state of the art Q-Learning algorithm\nR2D2 is shown as a baseline. Our Q-Learning implementation reaches the same \ufb01nal score as R2D2, but improves\nslower and results in much lower \ufb01nal performance compared to MCTS based training. (D) Different networks\ntrained at different numbers of simulations per move, but all evaluated at 50 simulations per move. Networks\ntrained with more simulations per move improve faster, consistent with ablation (B), where the policy improvement\nis larger when using more simulations per move. Surprisingly, MuZero can learn effectively even when training\nwith less simulations per move than are enough to cover all 8 possible actions in Ms. Pacman.\n7\n\n\n6\nAcknowledgments\nLorrayne Bennett, Oliver Smith and Chris Apps for organizational assistance; Koray Kavukcuoglu for reviewing\nthe paper; Thomas Anthony, Matthew Lai, Nenad Tomasev, Ulrich Paquet, Sumedh Ghaisas for many fruitful\ndiscussions; and the rest of the DeepMind team for their support.\nReferences\n[1] Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Emma Brunskill, Zachary C. Lipton, and Animashree\nAnandkumar. Surprising negative results for generative adversarial tree search. CoRR, abs/1806.05780, 2018.\n[2] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47:253\u2013279, 2013.\n[3] Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top profes-\nsionals. Science, 359(6374):418\u2013424, 2018.\n[4] Lars Buesing, Theophane Weber, Sebastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert, Fabio\nViola, Frederic Besse, Karol Gregor, Demis Hassabis, et al. Learning and querying fast generative models\nfor reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.\n[5] Murray Campbell, A. Joseph Hoane, Jr., and Feng-hsiung Hsu. Deep blue. Artif. Intell., 134(1-2):57\u201383,\nJanuary 2002.\n[6] R. Coulom. Whole-history rating: A Bayesian rating system for players of time-varying strength. In Inter-\nnational Conference on Computers and Games, pages 113\u2013124, 2008.\n[7] R\u00b4emi Coulom. Ef\ufb01cient selectivity and backup operators in monte-carlo tree search. In International confer-\nence on computers and games, pages 72\u201383. Springer, 2006.\n[8] MP. Deisenroth and CE. Rasmussen. Pilco: A model-based and data-ef\ufb01cient approach to policy search.\nIn Proceedings of the 28th International Conference on Machine Learning, ICML 2011, pages 465\u2013472.\nOmnipress, 2011.\n[9] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron,\nVlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted\nactor-learner architectures. In Proceedings of the International Conference on Machine Learning (ICML),\n2018.\n[10] Gregory Farquhar, Tim Rocktaeschel, Maximilian Igl, and Shimon Whiteson. TreeQN and ATreec: Differ-\nentiable tree planning for deep reinforcement learning. In International Conference on Learning Represen-\ntations, 2018.\n[11] Carles Gelada, Saurabh Kumar, Jacob Buckman, O\ufb01r Nachum, and Marc G. Bellemare. DeepMDP: Learning\ncontinuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdi-\nnov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-\nceedings of Machine Learning Research, pages 2170\u20132179, Long Beach, California, USA, 09\u201315 Jun 2019.\nPMLR.\n[12] Cloud tpu. https://cloud.google.com/tpu/. Accessed: 2019.\n[13] David Ha and J\u00a8urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Proceedings of\nthe 32Nd International Conference on Neural Information Processing Systems, NIPS\u201918, pages 2455\u20132467,\nUSA, 2018. Curran Associates Inc.\n8\n\n\n[14] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.\nLearning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In\n14th European Conference on Computer Vision, pages 630\u2013645, 2016.\n[16] Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and Tom Erez. Learning con-\ntinuous control policies by stochastic value gradients. In Proceedings of the 28th International Conference\non Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2944\u20132952, Cambridge, MA, USA,\n2015. MIT Press.\n[17] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,\nBilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement\nlearning. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018.\n[18] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David\nSilver. Distributed prioritized experience replay. In International Conference on Learning Representations,\n2018.\n[19] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Sil-\nver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint\narXiv:1611.05397, 2016.\n[20] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,\nDumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning\nfor atari. arXiv preprint arXiv:1903.00374, 2019.\n[21] Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent experience\nreplay in distributed reinforcement learning. In International Conference on Learning Representations, 2019.\n[22] Levente Kocsis and Csaba Szepesv\u00b4ari. Bandit based monte-carlo planning. In European conference on\nmachine learning, pages 282\u2013293. Springer, 2006.\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[24] Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under un-\nknown dynamics. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems 27, pages 1071\u20131079. Curran Associates, Inc., 2014.\n[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\n[26] Matej Morav\u02c7c\u00b4\u0131k, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin\nWaugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level arti\ufb01cial intelligence in heads-up\nno-limit poker. Science, 356(6337):508\u2013513, 2017.\n[27] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Ve-\ndavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih,\nKoray Kavukcuoglu, and David Silver. Massively parallel methods for deep reinforcement learning. CoRR,\nabs/1507.04296, 2015.\n[28] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information\nProcessing Systems, pages 6118\u20136128, 2017.\n[29] OpenAI. Openai \ufb01ve. https://blog.openai.com/openai-five/, 2018.\n9\n\n\n[30] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel\nBarth-Maron, Hado van Hasselt, John Quan, Mel Ve\u02c7cer\u00b4\u0131k, et al. Observe and look further: Achieving consis-\ntent performance on atari. arXiv preprint arXiv:1805.11593, 2018.\n[31] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley\n& Sons, Inc., New York, NY, USA, 1st edition, 1994.\n[32] Christopher D Rosin.\nMulti-armed bandits with episode context.\nAnnals of Mathematics and Arti\ufb01cial\nIntelligence, 61(3):203\u2013230, 2011.\n[33] Maarten PD Schadd, Mark HM Winands, H Jaap Van Den Herik, Guillaume MJ-B Chaslot, and Jos WHM\nUiterwijk. Single-player monte-carlo tree search. In International Conference on Computers and Games,\npages 1\u201312. Springer, 2008.\n[34] Jonathan Schaeffer, Joseph Culberson, Norman Treloar, Brent Knight, Paul Lu, and Duane Szafron. A world\nchampionship caliber checkers program. Arti\ufb01cial Intelligence, 53(2-3):273\u2013289, 1992.\n[35] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In Interna-\ntional Conference on Learning Representations, Puerto Rico, 2016.\n[36] Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience replay.\narXiv preprint arXiv:1909.11583, 2019.\n[37] Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks\nand symbolic ai. Nature, 555(7698):604, 2018.\n[38] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,\nJohn Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search.\nNature, 529(7587):484\u2013489, January 2016.\n[39] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm\nthat masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.\n[40] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,\nGeorge van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human\nknowledge. Nature, 550:354\u2013359, October 2017.\n[41] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-\nArnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-end learning and\nplanning.\nIn Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages\n3191\u20133199. JMLR. org, 2017.\n[42] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second\nedition, 2018.\n[43] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for\ntemporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):181\u2013211, 1999.\n[44] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances\nin Neural Information Processing Systems, pages 2154\u20132162, 2016.\n[45] Hado van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement\nlearning? arXiv preprint arXiv:1906.05243, 2019.\n10\n\n\n[46] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00a8el Mathieu, Andrew Dudzik, Junyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature, pages 1\u20135, 2019.\n[47] I Vlahavas and I Refanidis. Planning and scheduling. EETN, Greece, Tech. Rep, 2013.\n[48] Niklas Wahlstr\u00a8om, Thomas B. Sch\u00a8on, and Marc Peter Deisenroth. From pixels to torques: Policy learning\nwith deep dynamical models. CoRR, abs/1502.02251, 2015.\n[49] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A\nlocally linear latent dynamics model for control from raw images. In Proceedings of the 28th International\nConference on Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2746\u20132754, Cambridge,\nMA, USA, 2015. MIT Press.\nSupplementary Materials\n\u2022 Pseudocode description of the MuZero algorithm.\n\u2022 Data for Figures 2, 3, S2, S3, S4 and Tables 1, S1, S2 in JSON format.\nSupplementary materials can be accessed from the ancillary \ufb01le section of the arXiv submission.\nAppendix A\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero [40] and AlphaZero [39].\nIn AlphaGo Zero and AlphaZero the planning process makes use of two separate components: a simulator\nimplements the rules of the game, which are used to update the state of the game while traversing the search\ntree; and a neural network jointly predicts the corresponding policy and value of a board position produced by the\nsimulator (see Figure 1 A).\nSpeci\ufb01cally, AlphaGo Zero and AlphaZero use knowledge of the rules of the game in three places: (1) state\ntransitions in the search tree, (2) actions available at each node of the search tree, (3) episode termination within\nthe search tree. In MuZero, all of these have been replaced with the use of a single implicit model learned by a\nneural network (see Figure 1 B):\n1) State transitions. AlphaZero had access to a perfect simulator of the true dynamics process. In contrast,\nMuZero employs a learned dynamics model within its search. Under this model, each node in the tree is\nrepresented by a corresponding hidden state; by providing a hidden state sk\u22121 and an action ak to the model\nthe search algorithm can transition to a new node sk = g(sk\u22121, ak).\n2) Actions available. AlphaZero used the set of legal actions obtained from the simulator to mask the prior\nproduced by the network everywhere in the search tree. MuZero only masks legal actions at the root of the\nsearch tree where the environment can be queried, but does not perform any masking within the search tree.\nThis is possible because the network rapidly learns not to predict actions that never occur in the trajectories\nit is trained on.\n3) Terminal nodes. AlphaZero stopped the search at tree nodes representing terminal states and used the ter-\nminal value provided by the simulator instead of the value produced by the network. MuZero does not give\nspecial treatment to terminal nodes and always uses the value predicted by the network. Inside the tree, the\nsearch can proceed past a terminal node - in this case the network is expected to always predict the same\nvalue. This is achieved by treating terminal states as absorbing states during training.\nIn addition, MuZero is designed to operate in the general reinforcement learning setting: single-agent domains\nwith discounted intermediate rewards of arbitrary magnitude. In contrast, AlphaGo Zero and AlphaZero were\ndesigned to operate in two-player games with undiscounted terminal rewards of \u00b11.\n11\n\n\nAppendix B\nSearch\nWe now describe the search algorithm used by MuZero. Our approach is based upon Monte-Carlo tree search with\nupper con\ufb01dence bounds, an approach to planning that converges asymptotically to the optimal policy in single\nagent domains and to the minimax value function in zero sum games [22].\nEvery node of the search tree is associated with an internal state s. For each action a from s there is an\nedge (s, a) that stores a set of statistics {N(s, a), Q(s, a), P(s, a), R(s, a), S(s, a)}, respectively representing\nvisit counts N, mean value Q, policy P, reward R, and state transition S.\nSimilar to AlphaZero, the search is divided into three stages, repeated for a number of simulations.\nSelection: Each simulation starts from the internal root state s0, and \ufb01nishes when the simulation reaches a\nleaf node sl. For each hypothetical time-step k = 1...l of the simulation, an action ak is selected according to the\nstored statistics for internal state sk\u22121, by maximizing over an upper con\ufb01dence bound [32][39],\nak = arg max\na\n\u0014\nQ(s, a) + P(s, a) \u00b7\npP\nb N(s, b)\n1 + N(s, a)\n\u0012\nc1 + log\n\u0010P\nb N(s, b) + c2 + 1\nc2\n\u0011\u0013\u0015\n(2)\nThe constants c1 and c2 are used to control the in\ufb02uence of the prior P(s, a) relative to the value Q(s, a) as\nnodes are visited more often. In our experiments, c1 = 1.25 and c2 = 19652.\nFor k < l, the next state and reward are looked up in the state transition and reward table sk = S(sk\u22121, ak),\nrk = R(sk\u22121, ak).\nExpansion: At the \ufb01nal time-step l of the simulation, the reward and state are computed by the dynamics\nfunction, rl, sl = g\u03b8(sl\u22121, al), and stored in the corresponding tables, R(sl\u22121, al) = rl, S(sl\u22121, al) = sl. The\npolicy and value are computed by the prediction function, pl, vl = f\u03b8(sl). A new node, corresponding to state\nsl is added to the search tree. Each edge (sl, a) from the newly expanded node is initialized to {N(sl, a) =\n0, Q(sl, a) = 0, P(sl, a) = pl}. Note that the search algorithm makes at most one call to the dynamics function\nand prediction function respectively per simulation; the computational cost is of the same order as in AlphaZero.\nBackup: At the end of the simulation, the statistics along the trajectory are updated. The backup is generalized\nto the case where the environment can emit intermediate rewards, have a discount \u03b3 different from 1, and the value\nestimates are unbounded 3. For k = l...0, we form an l \u2212k-step estimate of the cumulative discounted reward,\nbootstrapping from the value function vl,\nGk =\nl\u22121\u2212k\nX\n\u03c4=0\n\u03b3\u03c4rk+1+\u03c4 + \u03b3l\u2212kvl\n(3)\nFor k = l...1, we update the statistics for each edge (sk\u22121, ak) in the simulation path as follows,\nQ(sk\u22121, ak) := N(sk\u22121, ak) \u00b7 Q(sk\u22121, ak) + Gk\nN(sk\u22121, ak) + 1\nN(sk\u22121, ak) := N(sk\u22121, ak) + 1\n(4)\nIn two-player zero sum games the value functions are assumed to be bounded within the [0, 1] interval. This\nchoice allows us to combine value estimates with probabilities using the pUCT rule (Eqn 2). However, since in\nmany environments the value is unbounded, it is necessary to adjust the pUCT rule. A simple solution would be\nto use the maximum score that can be observed in the environment to either re-scale the value or set the pUCT\nconstants appropriately [33]. However, both solutions are game speci\ufb01c and require adding prior knowledge to\nthe MuZero algorithm. To avoid this, MuZero computes normalized Q value estimates Q \u2208[0, 1] by using the\nminimum-maximum values observed in the search tree up to that point. When a node is reached during the\nselection stage, the algorithm computes the normalized Q values of its edges to be used in the pUCT rule using the\nequation below:\nQ(sk\u22121, ak) =\nQ(sk\u22121, ak) \u2212mins,a\u2208T ree Q(s, a)\nmaxs,a\u2208T ree Q(s, a) \u2212mins,a\u2208T ree Q(s, a)\n(5)\n3In board games the discount is assumed to be 1 and there are no intermediate rewards.\n12\n\n\nAppendix C\nHyperparameters\nFor simplicity we preferentially use the same architectural choices and hyperparameters as in previous work.\nSpeci\ufb01cally, we started with the network architecture and search choices of AlphaZero [39]. For board games, we\nuse the same UCB constants, dirichlet exploration noise and the same 800 simulations per search as in AlphaZero.\nDue to the much smaller branching factor and simpler policies in Atari, we only used 50 simulations per search\nto speed up experiments. As shown in Figure 3B, the algorithm is not very sensitive to this choice. We also use the\nsame discount (0.997) and value transformation (see Network Architecture section) as R2D2 [21].\nFor parameter values not mentioned in the text, please refer to the pseudocode.\nAppendix D\nData Generation\nTo generate training data, the latest checkpoint of the network (updated every 1000 training steps) is used to play\ngames with MCTS. In the board games Go, chess and shogi the search is run for 800 simulations per move to pick\nan action; in Atari due to the much smaller action space 50 simulations per move are suf\ufb01cient.\nFor board games, games are sent to the training job as soon as they \ufb01nish. Due to the much larger length of\nAtari games (up to 30 minutes or 108,000 frames), intermediate sequences are sent every 200 moves. In board\ngames, the training job keeps an in-memory replay buffer of the most recent 1 million games received; in Atari,\nwhere the visual observations are larger, the most recent 125 thousand sequences of length 200 are kept.\nDuring the generation of experience in the board game domains, the same exploration scheme as the one\ndescribed in AlphaZero [39] is used. Using a variation of this scheme, in the Atari domain actions are sampled\nfrom the visit count distribution throughout the duration of each game, instead of just the \ufb01rst k moves. Moreover,\nthe visit count distribution is parametrized using a temperature parameter T:\np\u03b1 =\nN(\u03b1)1/T\nP\nb N(b)1/T\n(6)\nT is decayed as a function of the number of training steps of the network. Speci\ufb01cally, for the \ufb01rst 500k\ntraining steps a temperature of 1 is used, for the next 250k steps a temperature of 0.5 and for the remaining 250k a\ntemperature of 0.25. This ensures that the action selection becomes greedier as training progresses.\nAppendix E\nNetwork Input\nRepresentation Function\nThe history over board states used as input to the representation function for Go, chess and shogi is represented\nsimilarly to AlphaZero [39]. In Go and shogi we encode the last 8 board states as in AlphaZero; in chess we\nincreased the history to the last 100 board states to allow correct prediction of draws.\nFor Atari, the input of the representation function includes the last 32 RGB frames at resolution 96x96 along\nwith the last 32 actions that led to each of those frames. We encode the historical actions because unlike board\ngames, an action in Atari does not necessarily have a visible effect on the observation. RGB frames are encoded\nas one plane per color, rescaled to the range [0, 1], for red, green and blue respectively. We perform no other\nnormalization, whitening or other preprocessing of the RGB input. Historical actions are encoded as simple bias\nplanes, scaled as a/18 (there are 18 total actions in Atari).\nDynamics Function\nThe input to the dynamics function is the hidden state produced by the representation function or previous appli-\ncation of the dynamics function, concatenated with a representation of the action for the transition. Actions are\nencoded spatially in planes of the same resolution as the hidden state. In Atari, this resolution is 6x6 (see descrip-\ntion of downsampling in Network Architecture section), in board games this is the same as the board size (19x19\nfor Go, 8x8 for chess, 9x9 for shogi).\n13\n\n\nIn Go, a normal action (playing a stone on the board) is encoded as an all zero plane, with a single one in the\nposition of the played stone. A pass is encoded as an all zero plane.\nIn chess, 8 planes are used to encode the action. The \ufb01rst one-hot plane encodes which position the piece was\nmoved from. The next two planes encode which position the piece was moved to: a one-hot plane to encode the\ntarget position, if on the board, and a second binary plane to indicate whether the target was valid (on the board) or\nnot. This is necessary because for simplicity our policy action space enumerates a superset of all possible actions,\nnot all of which are legal, and we use the same action space for policy prediction and to encode the dynamics\nfunction input. The remaining \ufb01ve binary planes are used to indicate the type of promotion, if any (queen, knight,\nbishop, rook, none).\nThe encoding for shogi is similar, with a total of 11 planes. We use the \ufb01rst 8 planes to indicate where the\npiece moved from - either a board position (\ufb01rst one-hot plane) or the drop of one of the seven types of prisoner\n(remaining 7 binary planes). The next two planes are used to encode the target as in chess. The remaining binary\nplane indicates whether the move was a promotion or not.\nIn Atari, an action is encoded as a one hot vector which is tiled appropriately into planes.\nAppendix F\nNetwork Architecture\nThe prediction function pk, vk = f\u03b8(sk) uses the same architecture as AlphaZero: one or two convolutional layers\nthat preserve the resolution but reduce the number of planes, followed by a fully connected layer to the size of the\noutput.\nFor value and reward prediction in Atari we follow [30] in scaling targets using an invertible transform h(x) =\nsign(x)(\np\n|x| + 1 \u22121 + \u03f5x), where \u03f5 = 0.001 in all our experiments. We then apply a transformation \u03c6 to the\nscalar reward and value targets in order to obtain equivalent categorical representations. We use a discrete support\nset of size 601 with one support for every integer between \u2212300 and 300. Under this transformation, each scalar\nis represented as the linear combination of its two adjacent supports, such that the original value can be recovered\nby x = xlow \u2217plow + xhigh \u2217phigh. As an example, a target of 3.7 would be represented as a weight of 0.3\non the support for 3 and a weight of 0.7 on the support for 4. The value and reward outputs of the network are\nalso modeled using a softmax output of size 601. During inference the actual value and rewards are obtained by\n\ufb01rst computing their expected value under their respective softmax distribution and subsequently by inverting the\nscaling transformation. Scaling and transformation of the value and reward happens transparently on the network\nside and is not visible to the rest of the algorithm.\nBoth the representation and dynamics function use the same architecture as AlphaZero, but with 16 instead of\n20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.\nFor Atari, where observations have large spatial resolution, the representation function starts with a sequence\nof convolutions with stride 2 to reduce the spatial resolution. Speci\ufb01cally, starting with an input observation of\nresolution 96x96 and 128 planes (32 history frames of 3 color channels each, concatenated with the corresponding\n32 actions broadcast to planes), we downsample as follows:\n\u2022 1 convolution with stride 2 and 128 output planes, output resolution 48x48.\n\u2022 2 residual blocks with 128 planes\n\u2022 1 convolution with stride 2 and 256 output planes, output resolution 24x24.\n\u2022 3 residual blocks with 256 planes.\n\u2022 Average pooling with stride 2, output resolution 12x12.\n\u2022 3 residual blocks with 256 planes.\n\u2022 Average pooling with stride 2, output resolution 6x6.\nThe kernel size is 3x3 for all operations.\nFor the dynamics function (which always operates at the downsampled resolution of 6x6), the action is \ufb01rst\nencoded as an image, then stacked with the hidden state of the previous step along the plane dimension.\n14\n\n\nAppendix G\nTraining\nDuring training, the MuZero network is unrolled for K hypothetical steps and aligned to sequences sampled from\nthe trajectories generated by the MCTS actors. Sequences are selected by sampling a state from any game in the\nreplay buffer, then unrolling for K steps from that state. In Atari, samples are drawn according to prioritized replay\n[35], with priority P(i) =\np\u03b1\ni\nP\nk p\u03b1\nk , where pi = |\u03bdi \u2212zi|, \u03bd is the search value and z the observed n-step return. To\ncorrect for sampling bias introduced by the prioritized sampling, we scale the loss using the importance sampling\nratio wi = ( 1\nN \u00b7\n1\nP (i))\u03b2. In all our experiments, we set \u03b1 = \u03b2 = 1. For board games, states are sampled uniformly.\nEach observation ot along the sequence also has a corresponding MCTS policy \u03c0t, estimated value \u03bdt and\nenvironment reward ut. At each unrolled step k the network has a loss to the value, policy and reward target for\nthat step, summed to produce the total loss for the MuZero network (see Equation 1). Note that, in board games\nwithout intermediate rewards, we omit the reward prediction loss. For board games, we bootstrap directly to the\nend of the game, equivalent to predicting the \ufb01nal outcome; for Atari we bootstrap for n = 10 steps into the future.\nTo maintain roughly similar magnitude of gradient across different unroll steps, we scale the gradient in two\nseparate locations:\n\u2022 We scale the loss of each head by\n1\nK , where K is the number of unroll steps. This ensures that the total\ngradient has similar magnitude irrespective of how many steps we unroll for.\n\u2022 We also scale the gradient at the start of the dynamics function by 1\n2. This ensures that the total gradient\napplied to the dynamics function stays constant.\nIn the experiments reported in this paper, we always unroll for K = 5 steps. For a detailed illustration, see\nFigure 1.\nTo improve the learning process and bound the activations, we also scale the hidden state to the same range as\nthe action input ([0, 1]): sscaled =\ns\u2212min(s)\nmax(s)\u2212min(s).\nAll experiments were run using third generation Google Cloud TPUs [12]. For each board game, we used 16\nTPUs for training and 1000 TPUs for selfplay. For each game in Atari, we used 8 TPUs for training and 32 TPUs for\nselfplay. The much smaller proportion of TPUs used for acting in Atari is due to the smaller number of simulations\nper move (50 instead of 800) and the smaller size of the dynamics function compared to the representation function.\nAppendix H\nReanalyze\nTo improve the sample ef\ufb01ciency of MuZero we introduced a second variant of the algorithm, MuZero Reana-\nlyze. MuZero Reanalyze revisits its past time-steps and re-executes its search using the latest model parameters,\npotentially resulting in a better quality policy than the original search. This fresh policy is used as the policy\ntarget for 80% of updates during MuZero training. Furthermore, a target network [25] \u00b7, v\u2212= f\u03b8\u2212(s0), based\non recent parameters \u03b8\u2212, is used to provide a fresher, stable n-step bootstrapped target for the value function,\nzt = ut+1 + \u03b3ut+2 + ... + \u03b3n\u22121ut+n + \u03b3nv\u2212\nt+n. In addition, several other hyperparameters were adjusted \u2013 pri-\nmarily to increase sample reuse and avoid over\ufb01tting of the value function. Speci\ufb01cally, 2.0 samples were drawn\nper state, instead of 0.1; the value target was weighted down to 0.25 compared to weights of 1.0 for policy and\nreward targets; and the n-step return was reduced to n = 5 steps instead of n = 10 steps.\nAppendix I\nEvaluation\nWe evaluated the relative strength of MuZero (Figure 2) in board games by measuring the Elo rating of each\nplayer. We estimate the probability that player a will defeat player b by a logistic function p(a defeats b) =\n(1+10(celo(e(b)\u2212e(a))))\u22121, and estimate the ratings e(\u00b7) by Bayesian logistic regression, computed by the BayesElo\nprogram [6] using the standard constant celo = 1/400.\nElo ratings were computed from the results of a 800 simulations per move tournament between iterations of\nMuZero during training, and also a baseline player: either Stock\ufb01sh, Elmo or AlphaZero respectively. Baseline\n15\n\n\nFigure S1: Repeatability of MuZero in Atari for \ufb01ve games. Total reward is shown on the y-axis, millions of\ntraining steps on the x-axis. Dark line indicates median score across 10 separate training runs, light lines indicate\nindividual training runs, and the shaded region indicates 25th to 75th percentile.\nplayers used an equivalent search time of 100ms per move. The Elo rating of the baseline players was anchored to\npublicly available values [39].\nIn Atari, we computed mean reward over 1000 episodes per game, limited to the standard 30 minutes or 108,000\nframes per episode [27], using 50 simulations per move unless indicated otherwise. In order to mitigate the effects\nof the deterministic nature of the Atari simulator we employed two different evaluation strategies: 30 noop random\nstarts and human starts. For the former, at the beginning of each episode a random number of between 0 and 30\nnoop actions are applied to the simulator before handing control to the agent. For the latter, start positions are\nsampled from human expert play to initialize the Atari simulator before handing the control to the agent [27].\n16\n\n\nGame\nRandom\nHuman\nSimPLe [20]\nApe-X [18]\nR2D2 [21]\nMuZero\nMuZero normalized\nalien\n227.75\n7,127.80\n616.90\n40,805.00\n229,496.90\n741,812.63\n10,747.5 %\namidar\n5.77\n1,719.53\n74.30\n8,659.00\n29,321.40\n28,634.39\n1,670.5 %\nassault\n222.39\n742.00\n527.20\n24,559.00\n108,197.00\n143,972.03\n27,664.9 %\nasterix\n210.00\n8,503.33\n1,128.30\n313,305.00\n999,153.30\n998,425.00\n12,036.4 %\nasteroids\n719.10\n47,388.67\n793.60\n155,495.00\n357,867.70\n678,558.64\n1,452.4 %\natlantis\n12,850.00\n29,028.13\n20,992.50\n944,498.00\n1,620,764.00\n1,674,767.20\n10,272.6 %\nbank heist\n14.20\n753.13\n34.20\n1,716.00\n24,235.90\n1,278.98\n171.2 %\nbattle zone\n2,360.00\n37,187.50\n4,031.20\n98,895.00\n751,880.00\n848,623.00\n2,429.9 %\nbeam rider\n363.88\n16,926.53\n621.60\n63,305.00\n188,257.40\n454,993.53\n2,744.9 %\nberzerk\n123.65\n2,630.42\n-\n57,197.00\n53,318.70\n85,932.60\n3,423.1 %\nbowling\n23.11\n160.73\n30.00\n18.00\n219.50\n260.13\n172.2 %\nboxing\n0.05\n12.06\n7.80\n100.00\n98.50\n100.00\n832.2 %\nbreakout\n1.72\n30.47\n16.40\n801.00\n837.70\n864.00\n2,999.2 %\ncentipede\n2,090.87\n12,017.04\n-\n12,974.00\n599,140.30\n1,159,049.27\n11,655.6 %\nchopper command\n811.00\n7,387.80\n979.40\n721,851.00\n986,652.00\n991,039.70\n15,056.4 %\ncrazy climber\n10,780.50\n35,829.41\n62,583.60\n320,426.00\n366,690.70\n458,315.40\n1,786.6 %\ndefender\n2,874.50\n18,688.89\n-\n411,944.00\n665,792.00\n839,642.95\n5,291.2 %\ndemon attack\n152.07\n1,971.00\n208.10\n133,086.00\n140,002.30\n143,964.26\n7,906.4 %\ndouble dunk\n-18.55\n-16.40\n-\n24.00\n23.70\n23.94\n1,976.3 %\nenduro\n0.00\n860.53\n-\n2,177.00\n2,372.70\n2,382.44\n276.9 %\n\ufb01shing derby\n-91.71\n-38.80\n-90.70\n44.00\n85.80\n91.16\n345.6 %\nfreeway\n0.01\n29.60\n16.70\n34.00\n32.50\n33.03\n111.6 %\nfrostbite\n65.20\n4,334.67\n236.90\n9,329.00\n315,456.40\n631,378.53\n14,786.7 %\ngopher\n257.60\n2,412.50\n596.80\n120,501.00\n124,776.30\n130,345.58\n6,036.8 %\ngravitar\n173.00\n3,351.43\n173.40\n1,599.00\n15,680.70\n6,682.70\n204.8 %\nhero\n1,026.97\n30,826.38\n2,656.60\n31,656.00\n39,537.10\n49,244.11\n161.8 %\nice hockey\n-11.15\n0.88\n-11.60\n33.00\n79.30\n67.04\n650.0 %\njamesbond\n29.00\n302.80\n100.50\n21,323.00\n25,354.00\n41,063.25\n14,986.9 %\nkangaroo\n52.00\n3,035.00\n51.20\n1,416.00\n14,130.70\n16,763.60\n560.2 %\nkrull\n1,598.05\n2,665.53\n2,204.80\n11,741.00\n218,448.10\n269,358.27\n25,083.4 %\nkung fu master\n258.50\n22,736.25\n14,862.50\n97,830.00\n233,413.30\n204,824.00\n910.1 %\nmontezuma revenge\n0.00\n4,753.33\n-\n2,500.00\n2,061.30\n0.00\n0.0 %\nms pacman\n307.30\n6,951.60\n1,480.00\n11,255.00\n42,281.70\n243,401.10\n3,658.7 %\nname this game\n2,292.35\n8,049.00\n2,420.70\n25,783.00\n58,182.70\n157,177.85\n2,690.5 %\nphoenix\n761.40\n7,242.60\n-\n224,491.00\n864,020.00\n955,137.84\n14,725.3 %\npitfall\n-229.44\n6,463.69\n-\n-1.00\n0.00\n0.00\n3.4 %\npong\n-20.71\n14.59\n12.80\n21.00\n21.00\n21.00\n118.2 %\nprivate eye\n24.94\n69,571.27\n35.00\n50.00\n5,322.70\n15,299.98\n22.0 %\nqbert\n163.88\n13,455.00\n1,288.80\n302,391.00\n408,850.00\n72,276.00\n542.6 %\nriverraid\n1,338.50\n17,118.00\n1,957.80\n63,864.00\n45,632.10\n323,417.18\n2,041.1 %\nroad runner\n11.50\n7,845.00\n5,640.60\n222,235.00\n599,246.70\n613,411.80\n7,830.5 %\nrobotank\n2.16\n11.94\n-\n74.00\n100.40\n131.13\n1,318.7 %\nseaquest\n68.40\n42,054.71\n683.30\n392,952.00\n999,996.70\n999,976.52\n2,381.5 %\nskiing\n-17,098.09\n-4,336.93\n-\n-10,790.00\n-30,021.70\n-29,968.36\n-100.9 %\nsolaris\n1,236.30\n12,326.67\n-\n2,893.00\n3,787.20\n56.62\n-10.6 %\nspace invaders\n148.03\n1,668.67\n-\n54,681.00\n43,223.40\n74,335.30\n4,878.7 %\nstar gunner\n664.00\n10,250.00\n-\n434,343.00\n717,344.00\n549,271.70\n5,723.0 %\nsurround\n-9.99\n6.53\n-\n7.00\n9.90\n9.99\n120.9 %\ntennis\n-23.84\n-8.27\n-\n24.00\n-0.10\n0.00\n153.1 %\ntime pilot\n3,568.00\n5,229.10\n-\n87,085.00\n445,377.30\n476,763.90\n28,486.9 %\ntutankham\n11.43\n167.59\n-\n273.00\n395.30\n491.48\n307.4 %\nup n down\n533.40\n11,693.23\n3,350.30\n401,884.00\n589,226.90\n715,545.61\n6,407.0 %\nventure\n0.00\n1,187.50\n-\n1,813.00\n1,970.70\n0.40\n0.0 %\nvideo pinball\n0.00\n17,667.90\n-\n565,163.00\n999,383.20\n981,791.88\n5,556.9 %\nwizard of wor\n563.50\n4,756.52\n-\n46,204.00\n144,362.70\n197,126.00\n4,687.9 %\nyars revenge\n3,092.91\n54,576.93\n5,664.30\n148,595.00\n995,048.40\n553,311.46\n1,068.7 %\nzaxxon\n32.50\n9,173.30\n-\n42,286.00\n224,910.70\n725,853.90\n7,940.5 %\n# best\n0\n5\n0\n5\n13\n37\nTable S1: Evaluation of MuZero in Atari for individual games with 30 random no-op starts. Best result for\neach game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames).\nSimPLe was only evaluated on 36 of the 57 games, unavailable results are indicated with \u2018-\u2019. Human normalized\nscore is calculated as snormalized =\nsagent\u2212srandom\nshuman\u2212srandom .\n17\n\n\nGame\nRandom\nHuman\nApe-X [18]\nMuZero\nMuZero normalized\nalien\n128.30\n6,371.30\n17,732.00\n713,387.37\n11,424.9 %\namidar\n11.79\n1,540.43\n1,047.00\n26,638.80\n1,741.9 %\nassault\n166.95\n628.89\n24,405.00\n143,900.58\n31,115.2 %\nasterix\n164.50\n7,536.00\n283,180.00\n985,801.95\n13,370.9 %\nasteroids\n877.10\n36,517.30\n117,303.00\n606,971.12\n1,700.6 %\natlantis\n13,463.00\n26,575.00\n918,715.00\n1,653,202.50\n12,505.6 %\nbank heist\n21.70\n644.50\n1,201.00\n962.11\n151.0 %\nbattle zone\n3,560.00\n33,030.00\n92,275.00\n791,387.00\n2,673.3 %\nbeam rider\n254.56\n14,961.02\n72,234.00\n419,460.76\n2,850.5 %\nberzerk\n196.10\n2,237.50\n55,599.00\n87,308.60\n4,267.3 %\nbowling\n35.16\n146.46\n30.00\n194.03\n142.7 %\nboxing\n-1.46\n9.61\n81.00\n56.60\n524.5 %\nbreakout\n1.77\n27.86\n757.00\n849.59\n3,249.6 %\ncentipede\n1,925.45\n10,321.89\n5,712.00\n1,138,294.60\n13,533.9 %\nchopper command\n644.00\n8,930.00\n576,602.00\n932,370.10\n11,244.6 %\ncrazy climber\n9,337.00\n32,667.00\n263,954.00\n412,213.90\n1,726.9 %\ndefender\n1,965.50\n14,296.00\n399,865.00\n823,636.00\n6,663.7 %\ndemon attack\n208.25\n3,442.85\n133,002.00\n143,858.05\n4,441.0 %\ndouble dunk\n-15.97\n-14.37\n22.00\n23.12\n2,443.1 %\nenduro\n-81.84\n740.17\n2,042.00\n2,264.20\n285.4 %\n\ufb01shing derby\n-77.11\n5.09\n22.00\n57.45\n163.7 %\nfreeway\n0.17\n25.61\n29.00\n28.38\n110.9 %\nfrostbite\n90.80\n4,202.80\n6,512.00\n613,944.04\n14,928.3 %\ngopher\n250.00\n2,311.00\n121,168.00\n129,218.68\n6,257.6 %\ngravitar\n245.50\n3,116.00\n662.00\n3,390.65\n109.6 %\nhero\n1,580.30\n25,839.40\n26,345.00\n44,129.55\n175.4 %\nice hockey\n-9.67\n0.53\n24.00\n52.40\n608.5 %\njamesbond\n33.50\n368.50\n18,992.00\n39,107.20\n11,663.8 %\nkangaroo\n100.00\n2,739.00\n578.00\n13,210.50\n496.8 %\nkrull\n1,151.90\n2,109.10\n8,592.00\n257,706.70\n26,802.6 %\nkung fu master\n304.00\n20,786.80\n72,068.00\n174,623.60\n851.1 %\nmontezuma revenge\n25.00\n4,182.00\n1,079.00\n57.10\n0.8 %\nms pacman\n197.80\n15,375.05\n6,135.00\n230,650.24\n1,518.4 %\nname this game\n1,747.80\n6,796.00\n23,830.00\n152,723.62\n2,990.7 %\nphoenix\n1,134.40\n6,686.20\n188,789.00\n943,255.07\n16,969.6 %\npitfall\n-348.80\n5,998.91\n-273.00\n-801.10\n-7.1 %\npong\n-17.95\n15.46\n19.00\n19.20\n111.2 %\nprivate eye\n662.78\n64,169.07\n865.00\n5,067.59\n6.9 %\nqbert\n159.38\n12,085.00\n380,152.00\n39,302.10\n328.2 %\nriverraid\n588.30\n14,382.20\n49,983.00\n315,353.33\n2,281.9 %\nroad runner\n200.00\n6,878.00\n127,112.00\n580,445.00\n8,688.9 %\nrobotank\n2.42\n8.94\n69.00\n128.80\n1,938.3 %\nseaquest\n215.50\n40,425.80\n377,180.00\n997,601.01\n2,480.4 %\nskiing\n-15,287.35\n-3,686.58\n-11,359.00\n-29,400.75\n-121.7 %\nsolaris\n2,047.20\n11,032.60\n3,116.00\n2,108.08\n0.7 %\nspace invaders\n182.55\n1,464.90\n50,699.00\n57,450.41\n4,465.9 %\nstar gunner\n697.00\n9,528.00\n432,958.00\n539,342.70\n6,099.5 %\nsurround\n-9.72\n5.37\n6.00\n8.46\n120.5 %\ntennis\n-21.43\n-6.69\n23.00\n-2.30\n129.8 %\ntime pilot\n3,273.00\n5,650.00\n71,543.00\n405,829.30\n16,935.5 %\ntutankham\n12.74\n138.30\n128.00\n351.76\n270.0 %\nup n down\n707.20\n9,896.10\n347,912.00\n607,807.85\n6,606.9 %\nventure\n18.00\n1,039.00\n936.00\n21.10\n0.3 %\nvideo pinball\n0.00\n15,641.09\n873,989.00\n970,881.10\n6,207.2 %\nwizard of wor\n804.00\n4,556.00\n46,897.00\n196,279.20\n5,209.9 %\nyars revenge\n1,476.88\n47,135.17\n131,701.00\n888,633.84\n1,943.0 %\nzaxxon\n475.00\n8,443.00\n37,672.00\n592,238.70\n7,426.8 %\n# best\n0\n6\n5\n46\nTable S2:\nEvaluation of MuZero in Atari for individual games from human start positions. Best result for\neach game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames).\n18\n\n\nModel\ns0\n= h\u03b8(o1, ..., ot)\nrk, sk\n= g\u03b8(sk\u22121, ak)\npk, vk\n= f\u03b8(sk)\n\uf8fc\n\uf8fd\n\uf8fepk, vk, rk = \u00b5\u03b8(o1, ..., ot, a1, ..., ak)\nSearch\n\u03bdt, \u03c0t = MCTS(s0\nt, \u00b5\u03b8)\nat \u223c\u03c0t\nLearning Rule\npk\nt , vk\nt , rk\nt = \u00b5\u03b8(o1, ..., ot, at+1, ..., at+k)\nzt =\n\u001a uT\nfor games\nut+1 + \u03b3ut+2 + ... + \u03b3n\u22121ut+n + \u03b3n\u03bdt+n\nfor general MDPs\nlt(\u03b8) =\nK\nX\nk=0\nlr(ut+k, rk\nt ) + lv(zt+k, vk\nt ) + lp(\u03c0t+k, pk\nt ) + c||\u03b8||2\nLosses\nlr(u, r) =\n\u001a 0\nfor games\n\u03c6(u)T log r\nfor general MDPs\nlv(z, q) =\n\u001a (z \u2212q)2\nfor games\n\u03c6(z)T log q\nfor general MDPs\nlp(\u03c0, p) = \u03c0T log p\nFigure S2:\nEquations summarising the MuZero algorithm. Here, \u03c6(x) refers to the representation of a real\nnumber x through a linear combination of its adjacent integers, as described in the Network Architecture section.\n19\n\n\nFigure S3: Details of MuZero evaluations (A-B) and policy improvement ablations (C-D). (A-B) Distribution\nof evaluation depth in the search tree for the learned model for the evaluations in Figure 3A-B. The network was\ntrained over 5 hypothetical steps, as indicated by the red line. Dark blue line indicates median depth from the\nroot, dark shaded region shows 25th to 75th percentile, light shaded region shows 5th to 95th percentile. (C)\nPolicy improvement in Ms. Pacman - a single network was trained at 50 simulations per search and is evaluated at\ndifferent numbers of simulations per search, including playing according to the argmax of the raw policy network.\nThe policy improvement effect of the search over the raw policy network is clearly visible throughout training. This\nconsistent gap between the performance with and without search highlights the policy improvement that MuZero\nexploits, by continually updating towards the improved policy, to ef\ufb01ciently progress towards the optimal policy.\n(D) Policy improvement in Go - a single network was trained at 800 simulations per search and is evaluated at\ndifferent numbers of simulations per search. In Go, the playing strength improvement from longer searches is\nmuch larger than in Ms. Pacman and persists throughout training, consistent with previous results in [40]. This\nsuggests, as might intuitively be expected, that the bene\ufb01t of models is greatest in precision planning domains.\n20\n\n\nFigure S4:\nLearning curves of MuZero in Atari for individual games. Total reward is shown on the y-axis,\nmillions of training steps on the x-axis. Line indicates mean score across 1000 evaluation games, shaded region\nindicates standard deviation.\n21\n\n\n",
    "title": "Mastering Atari, Go, Chess and Shogi by Planning with a",
    "abstract": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of arti\ufb01cial intelligence. Tree-based planning methods have enjoyed huge success in challeng- ing domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environ- ment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules. 1",
    "sections": [
      {
        "header": "Learned Model",
        "content": "Julian Schrittwieser,1\u2217Ioannis Antonoglou,1,2\u2217Thomas Hubert,1\u2217\nKaren Simonyan,1 Laurent Sifre,1 Simon Schmitt,1 Arthur Guez,1\nEdward Lockhart,1 Demis Hassabis,1 Thore Graepel,1,2 Timothy Lillicrap,1\nDavid Silver1,2\u2217\n1DeepMind, 6 Pancras Square, London N1C 4AG.\n2University College London, Gower Street, London WC1E 6BT.\n\u2217These authors contributed equally to this work."
      },
      {
        "header": "Constructing agents with planning capabilities has long been one of the main challenges in the",
        "content": "pursuit of arti\ufb01cial intelligence. Tree-based planning methods have enjoyed huge success in challeng-\ning domains, such as chess and Go, where a perfect simulator is available. However, in real-world\nproblems the dynamics governing the environment are often complex and unknown. In this work\nwe present the MuZero algorithm which, by combining a tree-based search with a learned model,\nachieves superhuman performance in a range of challenging and visually complex domains, without\nany knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively,\npredicts the quantities most directly relevant to planning: the reward, the action-selection policy, and\nthe value function. When evaluated on 57 different Atari games - the canonical video game environ-\nment for testing AI techniques, in which model-based planning approaches have historically struggled\n- our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without\nany knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero\nalgorithm that was supplied with the game rules."
      },
      {
        "header": "Introduction",
        "content": "Planning algorithms based on lookahead search have achieved remarkable successes in arti\ufb01cial intelligence. Hu-\nman world champions have been defeated in classic games such as checkers [34], chess [5], Go [38] and poker\n[3, 26], and planning algorithms have had real-world impact in applications from logistics [47] to chemical syn-\nthesis [37]. However, these planning algorithms all rely on knowledge of the environment\u2019s dynamics, such as the\nrules of the game or an accurate simulator, preventing their direct application to real-world domains like robotics,\nindustrial control, or intelligent assistants.\nModel-based reinforcement learning (RL) [42] aims to address this issue by \ufb01rst learning a model of the\nenvironment\u2019s dynamics, and then planning with respect to the learned model. Typically, these models have either\nfocused on reconstructing the true environmental state [8, 16, 24], or the sequence of full observations [14, 20].\nHowever, prior work [4, 14, 20] remains far from the state of the art in visually rich domains, such as Atari 2600\ngames [2]. Instead, the most successful methods are based on model-free RL [9, 21, 18] \u2013 i.e. they estimate\nthe optimal policy and/or value function directly from interactions with the environment. However, model-free\nalgorithms are in turn far from the state of the art in domains that require precise and sophisticated lookahead, such\nas chess and Go.\n1\narXiv:1911.08265v2  [cs.LG]  21 Feb 2020\n\n\nIn this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art per-\nformance in Atari 2600, a visually complex set of domains, while maintaining superhuman performance in pre-\ncision planning tasks such as chess, shogi and Go. MuZero builds upon AlphaZero\u2019s [39] powerful search and\nsearch-based policy iteration algorithms, but incorporates a learned model into the training procedure. MuZero\nalso extends AlphaZero to a broader set of environments including single agent domains and non-zero rewards at\nintermediate time-steps.\nThe main idea of the algorithm (summarized in Figure 1) is to predict those aspects of the future that are directly\nrelevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an\ninput and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that\nreceives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the\npolicy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points\nscored by playing a move). The model is trained end-to-end, with the sole objective of accurately estimating these\nthree important quantities, so as to match the improved estimates of policy and value generated by search as well\nas the observed reward. There is no direct constraint or requirement for the hidden state to capture all information\nnecessary to reconstruct the original observation, drastically reducing the amount of information the model has\nto maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the\nenvironment; nor any other constraints on the semantics of state. Instead, the hidden states are free to represent\nstate in whatever way is relevant to predicting current and future values and policies. Intuitively, the agent can\ninvent, internally, the rules or dynamics that lead to most accurate planning."
      },
      {
        "header": "Prior Work",
        "content": "Reinforcement learning may be subdivided into two principal categories: model-based, and model-free [42].\nModel-based RL constructs, as an intermediate step, a model of the environment. Classically, this model is\nrepresented by a Markov-decision process (MDP) [31] consisting of two components: a state transition model,\npredicting the next state, and a reward model, predicting the expected reward during that transition. The model\nis typically conditioned on the selected action, or a temporally abstract behavior such as an option [43]. Once\na model has been constructed, it is straightforward to apply MDP planning algorithms, such as value iteration\n[31] or Monte-Carlo tree search (MCTS) [7], to compute the optimal value or optimal policy for the MDP. In\nlarge or partially observed environments, the algorithm must \ufb01rst construct the state representation that the model\nshould predict. This tripartite separation between representation learning, model learning, and planning is poten-\ntially problematic since the agent is not able to optimize its representation or model for the purpose of effective\nplanning, so that, for example modeling errors may compound during planning.\nA common approach to model-based RL focuses on directly modeling the observation stream at the pixel-\nlevel. It has been hypothesized that deep, stochastic models may mitigate the problems of compounding error\n[14, 20]. However, planning at pixel-level granularity is not computationally tractable in large scale problems.\nOther methods build a latent state-space model that is suf\ufb01cient to reconstruct the observation stream at pixel level\n[48, 49], or to predict its future latent states [13, 11], which facilitates more ef\ufb01cient planning but still focuses\nthe majority of the model capacity on potentially irrelevant detail. None of these prior methods has constructed a\nmodel that facilitates effective planning in visually complex domains such as Atari; results lag behind well-tuned,\nmodel-free methods, even in terms of data ef\ufb01ciency [45].\nA quite different approach to model-based RL has recently been developed, focused end-to-end on predicting\nthe value function [41]. The main idea of these methods is to construct an abstract MDP model such that planning\nin the abstract MDP is equivalent to planning in the real environment. This equivalence is achieved by ensuring\nvalue equivalence, i.e. that, starting from the same real state, the cumulative reward of a trajectory through the\nabstract MDP matches the cumulative reward of a trajectory in the real environment.\nThe predictron [41] \ufb01rst introduced value equivalent models for predicting value (without actions). Although\nthe underlying model still takes the form of an MDP, there is no requirement for its transition model to match\nreal states in the environment. Instead the MDP model is viewed as a hidden layer of a deep neural network. The\nunrolled MDP is trained such that the expected cumulative sum of rewards matches the expected value with respect\nto the real environment, e.g. by temporal-difference learning.\n2\n\n\nFigure 1:\nPlanning, acting, and training with a learned model. (A) How MuZero uses its model to plan.\nThe model consists of three connected components for representation, dynamics and prediction. Given a previous\nhidden state sk\u22121 and a candidate action ak, the dynamics function g produces an immediate reward rk and a new\nhidden state sk. The policy pk and value function vk are computed from the hidden state sk by a prediction function\nf. The initial hidden state s0 is obtained by passing the past observations (e.g. the Go board or Atari screen) into\na representation function h. (B) How MuZero acts in the environment. A Monte-Carlo Tree Search is performed\nat each timestep t, as described in A. An action at+1 is sampled from the search policy \u03c0t, which is proportional\nto the visit count for each action from the root node. The environment receives the action and generates a new\nobservation ot+1 and reward ut+1. At the end of the episode the trajectory data is stored into a replay buffer. (C)\nHow MuZero trains its model. A trajectory is sampled from the replay buffer. For the initial step, the representation\nfunction h receives as input the past observations o1, ..., ot from the selected trajectory. The model is subsequently\nunrolled recurrently for K steps. At each step k, the dynamics function g receives as input the hidden state sk\u22121\nfrom the previous step and the real action at+k. The parameters of the representation, dynamics and prediction\nfunctions are jointly trained, end-to-end by backpropagation-through-time, to predict three quantities: the policy\npk \u2248\u03c0t+k, value function vk \u2248zt+k, and reward rt+k \u2248ut+k, where zt+k is a sample return: either the \ufb01nal\nreward (board games) or n-step return (Atari).\nValue equivalent models were subsequently extended to optimising value (with actions). TreeQN [10] learns\nan abstract MDP model, such that a tree search over that model (represented by a tree-structured neural network)\napproximates the optimal value function. Value iteration networks [44] learn a local MDP model, such that value\niteration over that model (represented by a convolutional neural network) approximates the optimal value function.\nValue prediction networks [28] are perhaps the closest precursor to MuZero: they learn an MDP model grounded\nin real actions; the unrolled MDP is trained such that the cumulative sum of rewards, conditioned on the actual\nsequence of actions generated by a simple lookahead search, matches the real environment. Unlike MuZero there\nis no policy prediction, and the search only utilizes value prediction."
      },
      {
        "header": "MuZero Algorithm",
        "content": "We now describe the MuZero algorithm in more detail. Predictions are made at each time-step t, for each of\nk = 1...K steps, by a model \u00b5\u03b8, with parameters \u03b8, conditioned on past observations o1, ..., ot and future actions\nat+1, ..., at+k. The model predicts three future quantities: the policy pk\nt \u2248\u03c0(at+k+1|o1, ..., ot, at+1, ..., at+k), the\nvalue function vk\nt \u2248E [ut+k+1 + \u03b3ut+k+2 + ...|o1, ..., ot, at+1, ..., at+k], and the immediate reward rk\nt \u2248ut+k,\n3\n\n\nwhere u. is the true, observed reward, \u03c0 is the policy used to select real actions, and \u03b3 is the discount function of\nthe environment.\nInternally, at each time-step t (subscripts t suppressed for simplicity), the model is represented by the com-\nbination of a representation function, a dynamics function, and a prediction function. The dynamics function,\nrk, sk = g\u03b8(sk\u22121, ak), is a recurrent process that computes, at each hypothetical step k, an immediate reward rk\nand an internal state sk. It mirrors the structure of an MDP model that computes the expected reward and state\ntransition for a given state and action [31]. However, unlike traditional approaches to model-based RL [42], this\ninternal state sk has no semantics of environment state attached to it \u2013 it is simply the hidden state of the overall\nmodel, and its sole purpose is to accurately predict relevant, future quantities: policies, values, and rewards. In\nthis paper, the dynamics function is represented deterministically; the extension to stochastic transitions is left for\nfuture work. The policy and value functions are computed from the internal state sk by the prediction function,\npk, vk = f\u03b8(sk), akin to the joint policy and value network of AlphaZero. The \u201croot\u201d state s0 is initialized using\na representation function that encodes past observations, s0 = h\u03b8(o1, ..., ot); again this has no special semantics\nbeyond its support for future predictions.\nGiven such a model, it is possible to search over hypothetical future trajectories a1, ..., ak given past obser-\nvations o1, ..., ot. For example, a naive search could simply select the k step action sequence that maximizes the\nvalue function. More generally, we may apply any MDP planning algorithm to the internal rewards and state space\ninduced by the dynamics function. Speci\ufb01cally, we use an MCTS algorithm similar to AlphaZero\u2019s search, gener-\nalized to allow for single agent domains and intermediate rewards (see Methods). At each internal node, it makes\nuse of the policy, value and reward estimates produced by the current model parameters \u03b8. The MCTS algorithm\noutputs a recommended policy \u03c0t and estimated value \u03bdt. An action at+1 \u223c\u03c0t is then selected.\nAll parameters of the model are trained jointly to accurately match the policy, value, and reward, for every\nhypothetical step k, to corresponding target values observed after k actual time-steps have elapsed. Similarly to\nAlphaZero, the improved policy targets are generated by an MCTS search; the \ufb01rst objective is to minimise the\nerror between predicted policy pk\nt and search policy \u03c0t+k. Also like AlphaZero, the improved value targets are\ngenerated by playing the game or MDP. However, unlike AlphaZero, we allow for long episodes with discounting\nand intermediate rewards by bootstrapping n steps into the future from the search value, zt = ut+1 +\u03b3ut+2 +...+\n\u03b3n\u22121ut+n + \u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in board games are treated as rewards ut \u2208{\u22121, 0, +1}\noccuring at the \ufb01nal step of the episode. Speci\ufb01cally, the second objective is to minimize the error between\nthe predicted value vk\nt and the value target, zt+k 1. The reward targets are simply the observed rewards; the third\nobjective is therefore to minimize the error between the predicted reward rk\nt and the observed reward ut+k. Finally,\nan L2 regularization term is also added, leading to the overall loss:\nlt(\u03b8) ="
      },
      {
        "header": "K\nX",
        "content": "k=0\nlr(ut+k, rk\nt ) + lv(zt+k, vk\nt ) + lp(\u03c0t+k, pk\nt ) + c||\u03b8||2\n(1)\nwhere lr, lv, and lp are loss functions for reward, value and policy respectively. Supplementary Figure S2 summa-\nrizes the equations governing how the MuZero algorithm plans, acts, and learns."
      },
      {
        "header": "Results",
        "content": "We applied the MuZero algorithm to the classic board games Go, chess and shogi 2, as benchmarks for challenging\nplanning problems, and to all 57 games in the Atari Learning Environment [2], as benchmarks for visually complex\nRL domains.\nIn each case we trained MuZero for K = 5 hypothetical steps. Training proceeded for 1 million mini-batches\nof size 2048 in board games and of size 1024 in Atari. During both training and evaluation, MuZero used 800\nsimulations for each search in board games, and 50 simulations for each search in Atari. The representation\n1For chess, Go and shogi, the same squared error loss as AlphaZero is used for rewards and values. A cross-entropy loss was found to be\nmore stable than a squared error when encountering rewards and values of variable scale in Atari. Cross-entropy was used for the policy loss\nin both cases.\n2Imperfect information games such as Poker are not directly addressed by our method."
      },
      {
        "header": "SNAQJBMR",
        "content": "\u9999\n\u6842\n\u9280\n\u91d1\n\u7389\n\u91d1\n\u9280\n\u6842\n\u9999\n\u98db\n\u89d2\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\n\u89d2\n\u98db\n\u9999\u6842\u9280\u91d1\u7389\u91d1\u9280\u6842\u9999\nFigure 2: Evaluation of MuZero throughout training in chess, shogi, Go and Atari. The x-axis shows millions\nof training steps. For chess, shogi and Go, the y-axis shows Elo rating, established by playing games against Alp-\nhaZero using 800 simulations per move for both players. MuZero\u2019s Elo is indicated by the blue line, AlphaZero\u2019s\nElo by the horizontal orange line. For Atari, mean (full line) and median (dashed line) human normalized scores\nacross all 57 games are shown on the y-axis. The scores for R2D2 [21], (the previous state of the art in this domain,\nbased on model-free RL) are indicated by the horizontal orange lines. Performance in Atari was evaluated using\n50 simulations every fourth time-step, and then repeating the chosen action four times, as in prior work [25].\nfunction uses the same convolutional [23] and residual [15] architecture as AlphaZero, but with 16 residual blocks\ninstead of 20. The dynamics function uses the same architecture as the representation function and the prediction\nfunction uses the same architecture as AlphaZero. All networks use 256 hidden planes (see Methods for further\ndetails).\nFigure 2 shows the performance throughout training in each game. In Go, MuZero slightly exceeded the perfor-\nmance of AlphaZero, despite using less computation per node in the search tree (16 residual blocks per evaluation\nin MuZero compared to 20 blocks in AlphaZero). This suggests that MuZero may be caching its computation in\nthe search tree and using each additional application of the dynamics model to gain a deeper understanding of the\nposition.\nIn Atari, MuZero achieved a new state of the art for both mean and median normalized score across the 57\ngames of the Arcade Learning Environment, outperforming the previous state-of-the-art method R2D2 [21] (a\nmodel-free approach) in 42 out of 57 games, and outperforming the previous best model-based approach SimPLe\n[20] in all games (see Table S1).\nWe also evaluated a second version of MuZero that was optimised for greater sample ef\ufb01ciency. Speci\ufb01cally,\nit reanalyzes old trajectories by re-running the MCTS using the latest network parameters to provide fresh targets\n(see Appendix H). When applied to 57 Atari games, using 200 million frames of experience per game, MuZero\nReanalyze achieved 731% median normalized score, compared to 192%, 231% and 431% for previous state-of-\nthe-art model-free approaches IMPALA [9], Rainbow [17] and LASER [36] respectively.\nTo understand the role of the model in MuZero we also ran several experiments, focusing on the board game\nof Go and the Atari game of Ms. Pacman.\nFirst, we tested the scalability of planning (Figure 3A), in the canonical planning problem of Go. We compared\nthe performance of search in AlphaZero, using a perfect model, to the performance of search in MuZero, using a"
      },
      {
        "header": "MuZero",
        "content": "2041.1%\n4999.2%\n20.0B\n12 hours\n1M\nIMPALA [9]\n191.8%\n957.6%\n200M\n\u2013\n\u2013\nRainbow [17]\n231.1%\n\u2013\n200M\n10 days\n\u2013\nUNREALa [19]\n250%a\n880%a\n250M\n\u2013\n\u2013\nLASER [36]\n431%\n\u2013\n200M\n\u2013\n\u2013"
      },
      {
        "header": "MuZero Reanalyze",
        "content": "731.1%\n2168.9%\n200M\n12 hours\n1M\nTable 1:\nComparison of MuZero against previous agents in Atari. We compare separately against agents\ntrained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques.\nMean and median scores are given, compared to human testers. The best results are highlighted in bold. MuZero\nsets a new state of the art in both settings. aHyper-parameters were tuned per game.\nlearned model. Speci\ufb01cally, the fully trained AlphaZero or MuZero was evaluated by comparing MCTS with\ndifferent thinking times. MuZero matched the performance of a perfect model, even when doing much larger\nsearches (up to 10s thinking time) than those from which the model was trained (around 0.1s thinking time, see\nalso Figure S3A).\nWe also investigated the scalability of planning across all Atari games (see Figure 3B). We compared MCTS\nwith different numbers of simulations, using the fully trained MuZero. The improvements due to planning are\nmuch less marked than in Go, perhaps because of greater model inaccuracy; performance improved slightly with\nsearch time, but plateaued at around 100 simulations. Even with a single simulation \u2013 i.e. when selecting moves\nsolely according to the policy network \u2013 MuZero performed well, suggesting that, by the end of training, the raw\npolicy has learned to internalise the bene\ufb01ts of search (see also Figure S3B).\nNext, we tested our model-based learning algorithm against a comparable model-free learning algorithm (see\nFigure 3C). We replaced the training objective of MuZero (Equation 1) with a model-free Q-learning objective\n(as used by R2D2), and the dual value and policy heads with a single head representing the Q-function Q(\u00b7|st).\nSubsequently, we trained and evaluated the new model without using any search. When evaluated on Ms. Pacman,\nour model-free algorithm achieved identical results to R2D2, but learned signi\ufb01cantly slower than MuZero and\nconverged to a much lower \ufb01nal score. We conjecture that the search-based policy improvement step of MuZero\nprovides a stronger learning signal than the high bias, high variance targets used by Q-learning.\nTo better understand the nature of MuZero\u2019s learning algorithm, we measured how MuZero\u2019s training scales\nwith respect to the amount of search it uses during training. Figure 3D shows the performance in Ms. Pacman,\nusing an MCTS of different simulation counts per move throughout training. Surprisingly, and in contrast to\nprevious work [1], even with only 6 simulations per move \u2013 fewer than the number of actions \u2013 MuZero learned\nan effective policy and improved rapidly. With more simulations performance jumped signi\ufb01cantly higher. For\nanalysis of the policy improvement during each individual iteration, see also Figure S3 C and D."
      },
      {
        "header": "Conclusions",
        "content": "Many of the breakthroughs in arti\ufb01cial intelligence have been based on either high-performance planning [5, 38,\n39] or model-free reinforcement learning methods [25, 29, 46]. In this paper we have introduced a method that\ncombines the bene\ufb01ts of both approaches. Our algorithm, MuZero, has both matched the superhuman performance\nof high-performance planning algorithms in their favored domains \u2013 logically complex board games such as chess\nand Go \u2013 and outperformed state-of-the-art model-free RL algorithms in their favored domains \u2013 visually complex\nAtari games. Crucially, our method does not require any knowledge of the game rules or environment dynamics,\npotentially paving the way towards the application of powerful learning and planning methods to a host of real-\nworld domains for which there exists no perfect simulator.\n6\n\n\nFigure 3:\nEvaluations of MuZero on Go (A), all 57 Atari Games (B) and Ms. Pacman (C-D). (A) Scaling\nwith search time per move in Go, comparing the learned model with the ground truth simulator. Both networks\nwere trained at 800 simulations per search, equivalent to 0.1 seconds per search. Remarkably, the learned model\nis able to scale well to up to two orders of magnitude longer searches than seen during training. (B) Scaling of\n\ufb01nal human normalized mean score in Atari with the number of simulations per search. The network was trained\nat 50 simulations per search. Dark line indicates mean score, shaded regions indicate 25th to 75th and 5th to\n95th percentiles. The learned model\u2019s performance increases up to 100 simulations per search. Beyond, even\nwhen scaling to much longer searches than during training, the learned model\u2019s performance remains stable and\nonly decreases slightly. This contrasts with the much better scaling in Go (A), presumably due to greater model\ninaccuracy in Atari than Go. (C) Comparison of MCTS based training with Q-learning in the MuZero framework\non Ms. Pacman, keeping network size and amount of training constant. The state of the art Q-Learning algorithm\nR2D2 is shown as a baseline. Our Q-Learning implementation reaches the same \ufb01nal score as R2D2, but improves\nslower and results in much lower \ufb01nal performance compared to MCTS based training. (D) Different networks\ntrained at different numbers of simulations per move, but all evaluated at 50 simulations per move. Networks\ntrained with more simulations per move improve faster, consistent with ablation (B), where the policy improvement\nis larger when using more simulations per move. Surprisingly, MuZero can learn effectively even when training\nwith less simulations per move than are enough to cover all 8 possible actions in Ms. Pacman.\n7"
      },
      {
        "header": "Acknowledgments",
        "content": "Lorrayne Bennett, Oliver Smith and Chris Apps for organizational assistance; Koray Kavukcuoglu for reviewing\nthe paper; Thomas Anthony, Matthew Lai, Nenad Tomasev, Ulrich Paquet, Sumedh Ghaisas for many fruitful\ndiscussions; and the rest of the DeepMind team for their support."
      },
      {
        "header": "References",
        "content": "[1] Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Emma Brunskill, Zachary C. Lipton, and Animashree\nAnandkumar. Surprising negative results for generative adversarial tree search. CoRR, abs/1806.05780, 2018.\n[2] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47:253\u2013279, 2013.\n[3] Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top profes-\nsionals. Science, 359(6374):418\u2013424, 2018.\n[4] Lars Buesing, Theophane Weber, Sebastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert, Fabio\nViola, Frederic Besse, Karol Gregor, Demis Hassabis, et al. Learning and querying fast generative models\nfor reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.\n[5] Murray Campbell, A. Joseph Hoane, Jr., and Feng-hsiung Hsu. Deep blue. Artif. Intell., 134(1-2):57\u201383,\nJanuary 2002.\n[6] R. Coulom. Whole-history rating: A Bayesian rating system for players of time-varying strength. In Inter-\nnational Conference on Computers and Games, pages 113\u2013124, 2008.\n[7] R\u00b4emi Coulom. Ef\ufb01cient selectivity and backup operators in monte-carlo tree search. In International confer-\nence on computers and games, pages 72\u201383. Springer, 2006.\n[8] MP. Deisenroth and CE. Rasmussen. Pilco: A model-based and data-ef\ufb01cient approach to policy search.\nIn Proceedings of the 28th International Conference on Machine Learning, ICML 2011, pages 465\u2013472.\nOmnipress, 2011.\n[9] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron,\nVlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted\nactor-learner architectures. In Proceedings of the International Conference on Machine Learning (ICML),\n2018.\n[10] Gregory Farquhar, Tim Rocktaeschel, Maximilian Igl, and Shimon Whiteson. TreeQN and ATreec: Differ-\nentiable tree planning for deep reinforcement learning. In International Conference on Learning Represen-\ntations, 2018.\n[11] Carles Gelada, Saurabh Kumar, Jacob Buckman, O\ufb01r Nachum, and Marc G. Bellemare. DeepMDP: Learning\ncontinuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdi-\nnov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-\nceedings of Machine Learning Research, pages 2170\u20132179, Long Beach, California, USA, 09\u201315 Jun 2019.\nPMLR.\n[12] Cloud tpu. https://cloud.google.com/tpu/. Accessed: 2019.\n[13] David Ha and J\u00a8urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Proceedings of\nthe 32Nd International Conference on Neural Information Processing Systems, NIPS\u201918, pages 2455\u20132467,\nUSA, 2018. Curran Associates Inc.\n8\n\n\n[14] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.\nLearning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In\n14th European Conference on Computer Vision, pages 630\u2013645, 2016.\n[16] Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and Tom Erez. Learning con-\ntinuous control policies by stochastic value gradients. In Proceedings of the 28th International Conference\non Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2944\u20132952, Cambridge, MA, USA,\n2015. MIT Press.\n[17] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,\nBilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement\nlearning. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018.\n[18] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David\nSilver. Distributed prioritized experience replay. In International Conference on Learning Representations,\n2018.\n[19] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Sil-\nver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint\narXiv:1611.05397, 2016.\n[20] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,\nDumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning\nfor atari. arXiv preprint arXiv:1903.00374, 2019.\n[21] Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent experience\nreplay in distributed reinforcement learning. In International Conference on Learning Representations, 2019.\n[22] Levente Kocsis and Csaba Szepesv\u00b4ari. Bandit based monte-carlo planning. In European conference on\nmachine learning, pages 282\u2013293. Springer, 2006.\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[24] Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under un-\nknown dynamics. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems 27, pages 1071\u20131079. Curran Associates, Inc., 2014.\n[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\n[26] Matej Morav\u02c7c\u00b4\u0131k, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin\nWaugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level arti\ufb01cial intelligence in heads-up\nno-limit poker. Science, 356(6337):508\u2013513, 2017.\n[27] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Ve-\ndavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih,\nKoray Kavukcuoglu, and David Silver. Massively parallel methods for deep reinforcement learning. CoRR,\nabs/1507.04296, 2015.\n[28] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information\nProcessing Systems, pages 6118\u20136128, 2017.\n[29] OpenAI. Openai \ufb01ve. https://blog.openai.com/openai-five/, 2018.\n9\n\n\n[30] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel\nBarth-Maron, Hado van Hasselt, John Quan, Mel Ve\u02c7cer\u00b4\u0131k, et al. Observe and look further: Achieving consis-\ntent performance on atari. arXiv preprint arXiv:1805.11593, 2018.\n[31] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley\n& Sons, Inc., New York, NY, USA, 1st edition, 1994.\n[32] Christopher D Rosin.\nMulti-armed bandits with episode context.\nAnnals of Mathematics and Arti\ufb01cial\nIntelligence, 61(3):203\u2013230, 2011.\n[33] Maarten PD Schadd, Mark HM Winands, H Jaap Van Den Herik, Guillaume MJ-B Chaslot, and Jos WHM\nUiterwijk. Single-player monte-carlo tree search. In International Conference on Computers and Games,\npages 1\u201312. Springer, 2008.\n[34] Jonathan Schaeffer, Joseph Culberson, Norman Treloar, Brent Knight, Paul Lu, and Duane Szafron. A world\nchampionship caliber checkers program. Arti\ufb01cial Intelligence, 53(2-3):273\u2013289, 1992.\n[35] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In Interna-\ntional Conference on Learning Representations, Puerto Rico, 2016.\n[36] Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience replay.\narXiv preprint arXiv:1909.11583, 2019.\n[37] Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks\nand symbolic ai. Nature, 555(7698):604, 2018.\n[38] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,\nJohn Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search.\nNature, 529(7587):484\u2013489, January 2016.\n[39] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm\nthat masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.\n[40] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,\nGeorge van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human\nknowledge. Nature, 550:354\u2013359, October 2017.\n[41] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-\nArnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-end learning and\nplanning.\nIn Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages\n3191\u20133199. JMLR. org, 2017.\n[42] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second\nedition, 2018.\n[43] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for\ntemporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):181\u2013211, 1999.\n[44] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances\nin Neural Information Processing Systems, pages 2154\u20132162, 2016.\n[45] Hado van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement\nlearning? arXiv preprint arXiv:1906.05243, 2019.\n10\n\n\n[46] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00a8el Mathieu, Andrew Dudzik, Junyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature, pages 1\u20135, 2019.\n[47] I Vlahavas and I Refanidis. Planning and scheduling. EETN, Greece, Tech. Rep, 2013.\n[48] Niklas Wahlstr\u00a8om, Thomas B. Sch\u00a8on, and Marc Peter Deisenroth. From pixels to torques: Policy learning\nwith deep dynamical models. CoRR, abs/1502.02251, 2015.\n[49] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A\nlocally linear latent dynamics model for control from raw images. In Proceedings of the 28th International\nConference on Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2746\u20132754, Cambridge,\nMA, USA, 2015. MIT Press."
      },
      {
        "header": "Supplementary Materials",
        "content": "\u2022 Pseudocode description of the MuZero algorithm.\n\u2022 Data for Figures 2, 3, S2, S3, S4 and Tables 1, S1, S2 in JSON format.\nSupplementary materials can be accessed from the ancillary \ufb01le section of the arXiv submission."
      },
      {
        "header": "Comparison to AlphaZero",
        "content": "MuZero is designed for a more general setting than AlphaGo Zero [40] and AlphaZero [39].\nIn AlphaGo Zero and AlphaZero the planning process makes use of two separate components: a simulator\nimplements the rules of the game, which are used to update the state of the game while traversing the search\ntree; and a neural network jointly predicts the corresponding policy and value of a board position produced by the\nsimulator (see Figure 1 A).\nSpeci\ufb01cally, AlphaGo Zero and AlphaZero use knowledge of the rules of the game in three places: (1) state\ntransitions in the search tree, (2) actions available at each node of the search tree, (3) episode termination within\nthe search tree. In MuZero, all of these have been replaced with the use of a single implicit model learned by a\nneural network (see Figure 1 B):\n1) State transitions. AlphaZero had access to a perfect simulator of the true dynamics process. In contrast,\nMuZero employs a learned dynamics model within its search. Under this model, each node in the tree is\nrepresented by a corresponding hidden state; by providing a hidden state sk\u22121 and an action ak to the model\nthe search algorithm can transition to a new node sk = g(sk\u22121, ak).\n2) Actions available. AlphaZero used the set of legal actions obtained from the simulator to mask the prior\nproduced by the network everywhere in the search tree. MuZero only masks legal actions at the root of the\nsearch tree where the environment can be queried, but does not perform any masking within the search tree."
      },
      {
        "header": "This is possible because the network rapidly learns not to predict actions that never occur in the trajectories",
        "content": "it is trained on.\n3) Terminal nodes. AlphaZero stopped the search at tree nodes representing terminal states and used the ter-\nminal value provided by the simulator instead of the value produced by the network. MuZero does not give\nspecial treatment to terminal nodes and always uses the value predicted by the network. Inside the tree, the\nsearch can proceed past a terminal node - in this case the network is expected to always predict the same\nvalue. This is achieved by treating terminal states as absorbing states during training.\nIn addition, MuZero is designed to operate in the general reinforcement learning setting: single-agent domains\nwith discounted intermediate rewards of arbitrary magnitude. In contrast, AlphaGo Zero and AlphaZero were\ndesigned to operate in two-player games with undiscounted terminal rewards of \u00b11."
      },
      {
        "header": "Search",
        "content": "We now describe the search algorithm used by MuZero. Our approach is based upon Monte-Carlo tree search with\nupper con\ufb01dence bounds, an approach to planning that converges asymptotically to the optimal policy in single\nagent domains and to the minimax value function in zero sum games [22].\nEvery node of the search tree is associated with an internal state s. For each action a from s there is an\nedge (s, a) that stores a set of statistics {N(s, a), Q(s, a), P(s, a), R(s, a), S(s, a)}, respectively representing\nvisit counts N, mean value Q, policy P, reward R, and state transition S.\nSimilar to AlphaZero, the search is divided into three stages, repeated for a number of simulations.\nSelection: Each simulation starts from the internal root state s0, and \ufb01nishes when the simulation reaches a\nleaf node sl. For each hypothetical time-step k = 1...l of the simulation, an action ak is selected according to the\nstored statistics for internal state sk\u22121, by maximizing over an upper con\ufb01dence bound [32][39],\nak = arg max\na\n\u0014\nQ(s, a) + P(s, a) \u00b7\npP\nb N(s, b)\n1 + N(s, a)\n\u0012\nc1 + log\n\u0010P\nb N(s, b) + c2 + 1\nc2\n\u0011\u0013\u0015\n(2)\nThe constants c1 and c2 are used to control the in\ufb02uence of the prior P(s, a) relative to the value Q(s, a) as\nnodes are visited more often. In our experiments, c1 = 1.25 and c2 = 19652.\nFor k < l, the next state and reward are looked up in the state transition and reward table sk = S(sk\u22121, ak),\nrk = R(sk\u22121, ak).\nExpansion: At the \ufb01nal time-step l of the simulation, the reward and state are computed by the dynamics\nfunction, rl, sl = g\u03b8(sl\u22121, al), and stored in the corresponding tables, R(sl\u22121, al) = rl, S(sl\u22121, al) = sl. The\npolicy and value are computed by the prediction function, pl, vl = f\u03b8(sl). A new node, corresponding to state\nsl is added to the search tree. Each edge (sl, a) from the newly expanded node is initialized to {N(sl, a) =\n0, Q(sl, a) = 0, P(sl, a) = pl}. Note that the search algorithm makes at most one call to the dynamics function\nand prediction function respectively per simulation; the computational cost is of the same order as in AlphaZero.\nBackup: At the end of the simulation, the statistics along the trajectory are updated. The backup is generalized\nto the case where the environment can emit intermediate rewards, have a discount \u03b3 different from 1, and the value\nestimates are unbounded 3. For k = l...0, we form an l \u2212k-step estimate of the cumulative discounted reward,\nbootstrapping from the value function vl,\nGk =\nl\u22121\u2212k\nX\n\u03c4=0\n\u03b3\u03c4rk+1+\u03c4 + \u03b3l\u2212kvl\n(3)\nFor k = l...1, we update the statistics for each edge (sk\u22121, ak) in the simulation path as follows,\nQ(sk\u22121, ak) := N(sk\u22121, ak) \u00b7 Q(sk\u22121, ak) + Gk\nN(sk\u22121, ak) + 1\nN(sk\u22121, ak) := N(sk\u22121, ak) + 1\n(4)\nIn two-player zero sum games the value functions are assumed to be bounded within the [0, 1] interval. This\nchoice allows us to combine value estimates with probabilities using the pUCT rule (Eqn 2). However, since in\nmany environments the value is unbounded, it is necessary to adjust the pUCT rule. A simple solution would be\nto use the maximum score that can be observed in the environment to either re-scale the value or set the pUCT\nconstants appropriately [33]. However, both solutions are game speci\ufb01c and require adding prior knowledge to\nthe MuZero algorithm. To avoid this, MuZero computes normalized Q value estimates Q \u2208[0, 1] by using the\nminimum-maximum values observed in the search tree up to that point. When a node is reached during the\nselection stage, the algorithm computes the normalized Q values of its edges to be used in the pUCT rule using the\nequation below:\nQ(sk\u22121, ak) =\nQ(sk\u22121, ak) \u2212mins,a\u2208T ree Q(s, a)\nmaxs,a\u2208T ree Q(s, a) \u2212mins,a\u2208T ree Q(s, a)\n(5)\n3In board games the discount is assumed to be 1 and there are no intermediate rewards."
      },
      {
        "header": "Hyperparameters",
        "content": "For simplicity we preferentially use the same architectural choices and hyperparameters as in previous work.\nSpeci\ufb01cally, we started with the network architecture and search choices of AlphaZero [39]. For board games, we\nuse the same UCB constants, dirichlet exploration noise and the same 800 simulations per search as in AlphaZero.\nDue to the much smaller branching factor and simpler policies in Atari, we only used 50 simulations per search\nto speed up experiments. As shown in Figure 3B, the algorithm is not very sensitive to this choice. We also use the\nsame discount (0.997) and value transformation (see Network Architecture section) as R2D2 [21].\nFor parameter values not mentioned in the text, please refer to the pseudocode."
      },
      {
        "header": "Data Generation",
        "content": "To generate training data, the latest checkpoint of the network (updated every 1000 training steps) is used to play\ngames with MCTS. In the board games Go, chess and shogi the search is run for 800 simulations per move to pick\nan action; in Atari due to the much smaller action space 50 simulations per move are suf\ufb01cient.\nFor board games, games are sent to the training job as soon as they \ufb01nish. Due to the much larger length of\nAtari games (up to 30 minutes or 108,000 frames), intermediate sequences are sent every 200 moves. In board\ngames, the training job keeps an in-memory replay buffer of the most recent 1 million games received; in Atari,\nwhere the visual observations are larger, the most recent 125 thousand sequences of length 200 are kept.\nDuring the generation of experience in the board game domains, the same exploration scheme as the one\ndescribed in AlphaZero [39] is used. Using a variation of this scheme, in the Atari domain actions are sampled\nfrom the visit count distribution throughout the duration of each game, instead of just the \ufb01rst k moves. Moreover,\nthe visit count distribution is parametrized using a temperature parameter T:\np\u03b1 =\nN(\u03b1)1/T\nP\nb N(b)1/T\n(6)\nT is decayed as a function of the number of training steps of the network. Speci\ufb01cally, for the \ufb01rst 500k\ntraining steps a temperature of 1 is used, for the next 250k steps a temperature of 0.5 and for the remaining 250k a\ntemperature of 0.25. This ensures that the action selection becomes greedier as training progresses."
      },
      {
        "header": "Representation Function",
        "content": "The history over board states used as input to the representation function for Go, chess and shogi is represented\nsimilarly to AlphaZero [39]. In Go and shogi we encode the last 8 board states as in AlphaZero; in chess we\nincreased the history to the last 100 board states to allow correct prediction of draws.\nFor Atari, the input of the representation function includes the last 32 RGB frames at resolution 96x96 along\nwith the last 32 actions that led to each of those frames. We encode the historical actions because unlike board\ngames, an action in Atari does not necessarily have a visible effect on the observation. RGB frames are encoded\nas one plane per color, rescaled to the range [0, 1], for red, green and blue respectively. We perform no other\nnormalization, whitening or other preprocessing of the RGB input. Historical actions are encoded as simple bias\nplanes, scaled as a/18 (there are 18 total actions in Atari)."
      },
      {
        "header": "Dynamics Function",
        "content": "The input to the dynamics function is the hidden state produced by the representation function or previous appli-\ncation of the dynamics function, concatenated with a representation of the action for the transition. Actions are\nencoded spatially in planes of the same resolution as the hidden state. In Atari, this resolution is 6x6 (see descrip-\ntion of downsampling in Network Architecture section), in board games this is the same as the board size (19x19\nfor Go, 8x8 for chess, 9x9 for shogi).\n13\n\n\nIn Go, a normal action (playing a stone on the board) is encoded as an all zero plane, with a single one in the\nposition of the played stone. A pass is encoded as an all zero plane.\nIn chess, 8 planes are used to encode the action. The \ufb01rst one-hot plane encodes which position the piece was\nmoved from. The next two planes encode which position the piece was moved to: a one-hot plane to encode the\ntarget position, if on the board, and a second binary plane to indicate whether the target was valid (on the board) or\nnot. This is necessary because for simplicity our policy action space enumerates a superset of all possible actions,\nnot all of which are legal, and we use the same action space for policy prediction and to encode the dynamics\nfunction input. The remaining \ufb01ve binary planes are used to indicate the type of promotion, if any (queen, knight,\nbishop, rook, none).\nThe encoding for shogi is similar, with a total of 11 planes. We use the \ufb01rst 8 planes to indicate where the\npiece moved from - either a board position (\ufb01rst one-hot plane) or the drop of one of the seven types of prisoner\n(remaining 7 binary planes). The next two planes are used to encode the target as in chess. The remaining binary\nplane indicates whether the move was a promotion or not.\nIn Atari, an action is encoded as a one hot vector which is tiled appropriately into planes."
      },
      {
        "header": "Network Architecture",
        "content": "The prediction function pk, vk = f\u03b8(sk) uses the same architecture as AlphaZero: one or two convolutional layers\nthat preserve the resolution but reduce the number of planes, followed by a fully connected layer to the size of the\noutput.\nFor value and reward prediction in Atari we follow [30] in scaling targets using an invertible transform h(x) =\nsign(x)(\np\n|x| + 1 \u22121 + \u03f5x), where \u03f5 = 0.001 in all our experiments. We then apply a transformation \u03c6 to the\nscalar reward and value targets in order to obtain equivalent categorical representations. We use a discrete support\nset of size 601 with one support for every integer between \u2212300 and 300. Under this transformation, each scalar\nis represented as the linear combination of its two adjacent supports, such that the original value can be recovered\nby x = xlow \u2217plow + xhigh \u2217phigh. As an example, a target of 3.7 would be represented as a weight of 0.3\non the support for 3 and a weight of 0.7 on the support for 4. The value and reward outputs of the network are\nalso modeled using a softmax output of size 601. During inference the actual value and rewards are obtained by\n\ufb01rst computing their expected value under their respective softmax distribution and subsequently by inverting the\nscaling transformation. Scaling and transformation of the value and reward happens transparently on the network\nside and is not visible to the rest of the algorithm.\nBoth the representation and dynamics function use the same architecture as AlphaZero, but with 16 instead of\n20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.\nFor Atari, where observations have large spatial resolution, the representation function starts with a sequence\nof convolutions with stride 2 to reduce the spatial resolution. Speci\ufb01cally, starting with an input observation of\nresolution 96x96 and 128 planes (32 history frames of 3 color channels each, concatenated with the corresponding\n32 actions broadcast to planes), we downsample as follows:\n\u2022 1 convolution with stride 2 and 128 output planes, output resolution 48x48.\n\u2022 2 residual blocks with 128 planes\n\u2022 1 convolution with stride 2 and 256 output planes, output resolution 24x24.\n\u2022 3 residual blocks with 256 planes.\n\u2022 Average pooling with stride 2, output resolution 12x12.\n\u2022 3 residual blocks with 256 planes.\n\u2022 Average pooling with stride 2, output resolution 6x6.\nThe kernel size is 3x3 for all operations.\nFor the dynamics function (which always operates at the downsampled resolution of 6x6), the action is \ufb01rst\nencoded as an image, then stacked with the hidden state of the previous step along the plane dimension."
      },
      {
        "header": "Training",
        "content": "During training, the MuZero network is unrolled for K hypothetical steps and aligned to sequences sampled from\nthe trajectories generated by the MCTS actors. Sequences are selected by sampling a state from any game in the\nreplay buffer, then unrolling for K steps from that state. In Atari, samples are drawn according to prioritized replay\n[35], with priority P(i) =\np\u03b1\ni\nP\nk p\u03b1\nk , where pi = |\u03bdi \u2212zi|, \u03bd is the search value and z the observed n-step return. To\ncorrect for sampling bias introduced by the prioritized sampling, we scale the loss using the importance sampling\nratio wi = ( 1\nN \u00b7\n1\nP (i))\u03b2. In all our experiments, we set \u03b1 = \u03b2 = 1. For board games, states are sampled uniformly.\nEach observation ot along the sequence also has a corresponding MCTS policy \u03c0t, estimated value \u03bdt and\nenvironment reward ut. At each unrolled step k the network has a loss to the value, policy and reward target for\nthat step, summed to produce the total loss for the MuZero network (see Equation 1). Note that, in board games\nwithout intermediate rewards, we omit the reward prediction loss. For board games, we bootstrap directly to the\nend of the game, equivalent to predicting the \ufb01nal outcome; for Atari we bootstrap for n = 10 steps into the future.\nTo maintain roughly similar magnitude of gradient across different unroll steps, we scale the gradient in two\nseparate locations:\n\u2022 We scale the loss of each head by\n1\nK , where K is the number of unroll steps. This ensures that the total\ngradient has similar magnitude irrespective of how many steps we unroll for.\n\u2022 We also scale the gradient at the start of the dynamics function by 1"
      },
      {
        "header": "This ensures that the total gradient",
        "content": "applied to the dynamics function stays constant.\nIn the experiments reported in this paper, we always unroll for K = 5 steps. For a detailed illustration, see\nFigure 1.\nTo improve the learning process and bound the activations, we also scale the hidden state to the same range as\nthe action input ([0, 1]): sscaled =\ns\u2212min(s)\nmax(s)\u2212min(s).\nAll experiments were run using third generation Google Cloud TPUs [12]. For each board game, we used 16\nTPUs for training and 1000 TPUs for selfplay. For each game in Atari, we used 8 TPUs for training and 32 TPUs for\nselfplay. The much smaller proportion of TPUs used for acting in Atari is due to the smaller number of simulations\nper move (50 instead of 800) and the smaller size of the dynamics function compared to the representation function."
      },
      {
        "header": "Reanalyze",
        "content": "To improve the sample ef\ufb01ciency of MuZero we introduced a second variant of the algorithm, MuZero Reana-\nlyze. MuZero Reanalyze revisits its past time-steps and re-executes its search using the latest model parameters,\npotentially resulting in a better quality policy than the original search. This fresh policy is used as the policy\ntarget for 80% of updates during MuZero training. Furthermore, a target network [25] \u00b7, v\u2212= f\u03b8\u2212(s0), based\non recent parameters \u03b8\u2212, is used to provide a fresher, stable n-step bootstrapped target for the value function,\nzt = ut+1 + \u03b3ut+2 + ... + \u03b3n\u22121ut+n + \u03b3nv\u2212\nt+n. In addition, several other hyperparameters were adjusted \u2013 pri-\nmarily to increase sample reuse and avoid over\ufb01tting of the value function. Speci\ufb01cally, 2.0 samples were drawn\nper state, instead of 0.1; the value target was weighted down to 0.25 compared to weights of 1.0 for policy and\nreward targets; and the n-step return was reduced to n = 5 steps instead of n = 10 steps."
      },
      {
        "header": "Evaluation",
        "content": "We evaluated the relative strength of MuZero (Figure 2) in board games by measuring the Elo rating of each\nplayer. We estimate the probability that player a will defeat player b by a logistic function p(a defeats b) =\n(1+10(celo(e(b)\u2212e(a))))\u22121, and estimate the ratings e(\u00b7) by Bayesian logistic regression, computed by the BayesElo\nprogram [6] using the standard constant celo = 1/400.\nElo ratings were computed from the results of a 800 simulations per move tournament between iterations of\nMuZero during training, and also a baseline player: either Stock\ufb01sh, Elmo or AlphaZero respectively. Baseline\n15\n\n\nFigure S1: Repeatability of MuZero in Atari for \ufb01ve games. Total reward is shown on the y-axis, millions of\ntraining steps on the x-axis. Dark line indicates median score across 10 separate training runs, light lines indicate\nindividual training runs, and the shaded region indicates 25th to 75th percentile.\nplayers used an equivalent search time of 100ms per move. The Elo rating of the baseline players was anchored to\npublicly available values [39].\nIn Atari, we computed mean reward over 1000 episodes per game, limited to the standard 30 minutes or 108,000\nframes per episode [27], using 50 simulations per move unless indicated otherwise. In order to mitigate the effects\nof the deterministic nature of the Atari simulator we employed two different evaluation strategies: 30 noop random\nstarts and human starts. For the former, at the beginning of each episode a random number of between 0 and 30\nnoop actions are applied to the simulator before handing control to the agent. For the latter, start positions are\nsampled from human expert play to initialize the Atari simulator before handing the control to the agent [27]."
      },
      {
        "header": "MuZero normalized",
        "content": "alien\n227.75\n7,127.80\n616.90\n40,805.00\n229,496.90\n741,812.63\n10,747.5 %\namidar\n5.77\n1,719.53\n74.30\n8,659.00\n29,321.40\n28,634.39\n1,670.5 %\nassault\n222.39\n742.00\n527.20\n24,559.00\n108,197.00\n143,972.03\n27,664.9 %\nasterix\n210.00\n8,503.33\n1,128.30\n313,305.00\n999,153.30\n998,425.00\n12,036.4 %\nasteroids\n719.10\n47,388.67\n793.60\n155,495.00\n357,867.70\n678,558.64\n1,452.4 %\natlantis\n12,850.00\n29,028.13\n20,992.50\n944,498.00\n1,620,764.00\n1,674,767.20\n10,272.6 %\nbank heist\n14.20\n753.13\n34.20\n1,716.00\n24,235.90\n1,278.98\n171.2 %\nbattle zone\n2,360.00\n37,187.50\n4,031.20\n98,895.00\n751,880.00\n848,623.00\n2,429.9 %\nbeam rider\n363.88\n16,926.53\n621.60\n63,305.00\n188,257.40\n454,993.53\n2,744.9 %\nberzerk\n123.65\n2,630.42\n-\n57,197.00\n53,318.70\n85,932.60\n3,423.1 %\nbowling\n23.11\n160.73\n30.00\n18.00\n219.50\n260.13\n172.2 %\nboxing\n0.05\n12.06\n7.80\n100.00\n98.50\n100.00\n832.2 %\nbreakout\n1.72\n30.47\n16.40\n801.00\n837.70\n864.00\n2,999.2 %\ncentipede\n2,090.87\n12,017.04\n-\n12,974.00\n599,140.30\n1,159,049.27\n11,655.6 %\nchopper command\n811.00\n7,387.80\n979.40\n721,851.00\n986,652.00\n991,039.70\n15,056.4 %\ncrazy climber\n10,780.50\n35,829.41\n62,583.60\n320,426.00\n366,690.70\n458,315.40\n1,786.6 %\ndefender\n2,874.50\n18,688.89\n-\n411,944.00\n665,792.00\n839,642.95\n5,291.2 %\ndemon attack\n152.07\n1,971.00\n208.10\n133,086.00\n140,002.30\n143,964.26\n7,906.4 %\ndouble dunk\n-18.55\n-16.40\n-\n24.00\n23.70\n23.94\n1,976.3 %\nenduro\n0.00\n860.53\n-\n2,177.00\n2,372.70\n2,382.44\n276.9 %\n\ufb01shing derby\n-91.71\n-38.80\n-90.70\n44.00\n85.80\n91.16\n345.6 %\nfreeway\n0.01\n29.60\n16.70\n34.00\n32.50\n33.03\n111.6 %\nfrostbite\n65.20\n4,334.67\n236.90\n9,329.00\n315,456.40\n631,378.53\n14,786.7 %\ngopher\n257.60\n2,412.50\n596.80\n120,501.00\n124,776.30\n130,345.58\n6,036.8 %\ngravitar\n173.00\n3,351.43\n173.40\n1,599.00\n15,680.70\n6,682.70\n204.8 %\nhero\n1,026.97\n30,826.38\n2,656.60\n31,656.00\n39,537.10\n49,244.11\n161.8 %\nice hockey\n-11.15\n0.88\n-11.60\n33.00\n79.30\n67.04\n650.0 %\njamesbond\n29.00\n302.80\n100.50\n21,323.00\n25,354.00\n41,063.25\n14,986.9 %\nkangaroo\n52.00\n3,035.00\n51.20\n1,416.00\n14,130.70\n16,763.60\n560.2 %\nkrull\n1,598.05\n2,665.53\n2,204.80\n11,741.00\n218,448.10\n269,358.27\n25,083.4 %\nkung fu master\n258.50\n22,736.25\n14,862.50\n97,830.00\n233,413.30\n204,824.00\n910.1 %\nmontezuma revenge\n0.00\n4,753.33\n-\n2,500.00\n2,061.30\n0.00\n0.0 %\nms pacman\n307.30\n6,951.60\n1,480.00\n11,255.00\n42,281.70\n243,401.10\n3,658.7 %\nname this game\n2,292.35\n8,049.00\n2,420.70\n25,783.00\n58,182.70\n157,177.85\n2,690.5 %\nphoenix\n761.40\n7,242.60\n-\n224,491.00\n864,020.00\n955,137.84\n14,725.3 %\npitfall\n-229.44\n6,463.69\n-\n-1.00\n0.00\n0.00\n3.4 %\npong\n-20.71\n14.59\n12.80\n21.00\n21.00\n21.00\n118.2 %\nprivate eye\n24.94\n69,571.27\n35.00\n50.00\n5,322.70\n15,299.98\n22.0 %\nqbert\n163.88\n13,455.00\n1,288.80\n302,391.00\n408,850.00\n72,276.00\n542.6 %\nriverraid\n1,338.50\n17,118.00\n1,957.80\n63,864.00\n45,632.10\n323,417.18\n2,041.1 %\nroad runner\n11.50\n7,845.00\n5,640.60\n222,235.00\n599,246.70\n613,411.80\n7,830.5 %\nrobotank\n2.16\n11.94\n-\n74.00\n100.40\n131.13\n1,318.7 %\nseaquest\n68.40\n42,054.71\n683.30\n392,952.00\n999,996.70\n999,976.52\n2,381.5 %\nskiing\n-17,098.09\n-4,336.93\n-\n-10,790.00\n-30,021.70\n-29,968.36\n-100.9 %\nsolaris\n1,236.30\n12,326.67\n-\n2,893.00\n3,787.20\n56.62\n-10.6 %\nspace invaders\n148.03\n1,668.67\n-\n54,681.00\n43,223.40\n74,335.30\n4,878.7 %\nstar gunner\n664.00\n10,250.00\n-\n434,343.00\n717,344.00\n549,271.70\n5,723.0 %\nsurround\n-9.99\n6.53\n-\n7.00\n9.90\n9.99\n120.9 %\ntennis\n-23.84\n-8.27\n-\n24.00\n-0.10\n0.00\n153.1 %\ntime pilot\n3,568.00\n5,229.10\n-\n87,085.00\n445,377.30\n476,763.90\n28,486.9 %\ntutankham\n11.43\n167.59\n-\n273.00\n395.30\n491.48\n307.4 %\nup n down\n533.40\n11,693.23\n3,350.30\n401,884.00\n589,226.90\n715,545.61\n6,407.0 %\nventure\n0.00\n1,187.50\n-\n1,813.00\n1,970.70\n0.40\n0.0 %\nvideo pinball\n0.00\n17,667.90\n-\n565,163.00\n999,383.20\n981,791.88\n5,556.9 %\nwizard of wor\n563.50\n4,756.52\n-\n46,204.00\n144,362.70\n197,126.00\n4,687.9 %\nyars revenge\n3,092.91\n54,576.93\n5,664.30\n148,595.00\n995,048.40\n553,311.46\n1,068.7 %\nzaxxon\n32.50\n9,173.30\n-\n42,286.00\n224,910.70\n725,853.90\n7,940.5 %\n# best\n0\n5\n0\n5\n13\n37\nTable S1: Evaluation of MuZero in Atari for individual games with 30 random no-op starts. Best result for\neach game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames).\nSimPLe was only evaluated on 36 of the 57 games, unavailable results are indicated with \u2018-\u2019. Human normalized\nscore is calculated as snormalized =\nsagent\u2212srandom\nshuman\u2212srandom ."
      },
      {
        "header": "MuZero normalized",
        "content": "alien\n128.30\n6,371.30\n17,732.00\n713,387.37\n11,424.9 %\namidar\n11.79\n1,540.43\n1,047.00\n26,638.80\n1,741.9 %\nassault\n166.95\n628.89\n24,405.00\n143,900.58\n31,115.2 %\nasterix\n164.50\n7,536.00\n283,180.00\n985,801.95\n13,370.9 %\nasteroids\n877.10\n36,517.30\n117,303.00\n606,971.12\n1,700.6 %\natlantis\n13,463.00\n26,575.00\n918,715.00\n1,653,202.50\n12,505.6 %\nbank heist\n21.70\n644.50\n1,201.00\n962.11\n151.0 %\nbattle zone\n3,560.00\n33,030.00\n92,275.00\n791,387.00\n2,673.3 %\nbeam rider\n254.56\n14,961.02\n72,234.00\n419,460.76\n2,850.5 %\nberzerk\n196.10\n2,237.50\n55,599.00\n87,308.60\n4,267.3 %\nbowling\n35.16\n146.46\n30.00\n194.03\n142.7 %\nboxing\n-1.46\n9.61\n81.00\n56.60\n524.5 %\nbreakout\n1.77\n27.86\n757.00\n849.59\n3,249.6 %\ncentipede\n1,925.45\n10,321.89\n5,712.00\n1,138,294.60\n13,533.9 %\nchopper command\n644.00\n8,930.00\n576,602.00\n932,370.10\n11,244.6 %\ncrazy climber\n9,337.00\n32,667.00\n263,954.00\n412,213.90\n1,726.9 %\ndefender\n1,965.50\n14,296.00\n399,865.00\n823,636.00\n6,663.7 %\ndemon attack\n208.25\n3,442.85\n133,002.00\n143,858.05\n4,441.0 %\ndouble dunk\n-15.97\n-14.37\n22.00\n23.12\n2,443.1 %\nenduro\n-81.84\n740.17\n2,042.00\n2,264.20\n285.4 %\n\ufb01shing derby\n-77.11\n5.09\n22.00\n57.45\n163.7 %\nfreeway\n0.17\n25.61\n29.00\n28.38\n110.9 %\nfrostbite\n90.80\n4,202.80\n6,512.00\n613,944.04\n14,928.3 %\ngopher\n250.00\n2,311.00\n121,168.00\n129,218.68\n6,257.6 %\ngravitar\n245.50\n3,116.00\n662.00\n3,390.65\n109.6 %\nhero\n1,580.30\n25,839.40\n26,345.00\n44,129.55\n175.4 %\nice hockey\n-9.67\n0.53\n24.00\n52.40\n608.5 %\njamesbond\n33.50\n368.50\n18,992.00\n39,107.20\n11,663.8 %\nkangaroo\n100.00\n2,739.00\n578.00\n13,210.50\n496.8 %\nkrull\n1,151.90\n2,109.10\n8,592.00\n257,706.70\n26,802.6 %\nkung fu master\n304.00\n20,786.80\n72,068.00\n174,623.60\n851.1 %\nmontezuma revenge\n25.00\n4,182.00\n1,079.00\n57.10\n0.8 %\nms pacman\n197.80\n15,375.05\n6,135.00\n230,650.24\n1,518.4 %\nname this game\n1,747.80\n6,796.00\n23,830.00\n152,723.62\n2,990.7 %\nphoenix\n1,134.40\n6,686.20\n188,789.00\n943,255.07\n16,969.6 %\npitfall\n-348.80\n5,998.91\n-273.00\n-801.10\n-7.1 %\npong\n-17.95\n15.46\n19.00\n19.20\n111.2 %\nprivate eye\n662.78\n64,169.07\n865.00\n5,067.59\n6.9 %\nqbert\n159.38\n12,085.00\n380,152.00\n39,302.10\n328.2 %\nriverraid\n588.30\n14,382.20\n49,983.00\n315,353.33\n2,281.9 %\nroad runner\n200.00\n6,878.00\n127,112.00\n580,445.00\n8,688.9 %\nrobotank\n2.42\n8.94\n69.00\n128.80\n1,938.3 %\nseaquest\n215.50\n40,425.80\n377,180.00\n997,601.01\n2,480.4 %\nskiing\n-15,287.35\n-3,686.58\n-11,359.00\n-29,400.75\n-121.7 %\nsolaris\n2,047.20\n11,032.60\n3,116.00\n2,108.08\n0.7 %\nspace invaders\n182.55\n1,464.90\n50,699.00\n57,450.41\n4,465.9 %\nstar gunner\n697.00\n9,528.00\n432,958.00\n539,342.70\n6,099.5 %\nsurround\n-9.72\n5.37\n6.00\n8.46\n120.5 %\ntennis\n-21.43\n-6.69\n23.00\n-2.30\n129.8 %\ntime pilot\n3,273.00\n5,650.00\n71,543.00\n405,829.30\n16,935.5 %\ntutankham\n12.74\n138.30\n128.00\n351.76\n270.0 %\nup n down\n707.20\n9,896.10\n347,912.00\n607,807.85\n6,606.9 %\nventure\n18.00\n1,039.00\n936.00\n21.10\n0.3 %\nvideo pinball\n0.00\n15,641.09\n873,989.00\n970,881.10\n6,207.2 %\nwizard of wor\n804.00\n4,556.00\n46,897.00\n196,279.20\n5,209.9 %\nyars revenge\n1,476.88\n47,135.17\n131,701.00\n888,633.84\n1,943.0 %\nzaxxon\n475.00\n8,443.00\n37,672.00\n592,238.70\n7,426.8 %\n# best\n0\n6\n5\n46\nTable S2:\nEvaluation of MuZero in Atari for individual games from human start positions. Best result for\neach game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames)."
      },
      {
        "header": "Model",
        "content": "s0\n= h\u03b8(o1, ..., ot)\nrk, sk\n= g\u03b8(sk\u22121, ak)\npk, vk\n= f\u03b8(sk)\n\uf8fc\n\uf8fd\n\uf8fepk, vk, rk = \u00b5\u03b8(o1, ..., ot, a1, ..., ak)"
      },
      {
        "header": "Learning Rule",
        "content": "pk\nt , vk\nt , rk\nt = \u00b5\u03b8(o1, ..., ot, at+1, ..., at+k)\nzt =\n\u001a uT\nfor games\nut+1 + \u03b3ut+2 + ... + \u03b3n\u22121ut+n + \u03b3n\u03bdt+n\nfor general MDPs\nlt(\u03b8) ="
      },
      {
        "header": "Losses",
        "content": "lr(u, r) =\n\u001a 0\nfor games\n\u03c6(u)T log r\nfor general MDPs\nlv(z, q) =\n\u001a (z \u2212q)2\nfor games\n\u03c6(z)T log q\nfor general MDPs\nlp(\u03c0, p) = \u03c0T log p\nFigure S2:\nEquations summarising the MuZero algorithm. Here, \u03c6(x) refers to the representation of a real\nnumber x through a linear combination of its adjacent integers, as described in the Network Architecture section.\n19\n\n\nFigure S3: Details of MuZero evaluations (A-B) and policy improvement ablations (C-D). (A-B) Distribution\nof evaluation depth in the search tree for the learned model for the evaluations in Figure 3A-B. The network was\ntrained over 5 hypothetical steps, as indicated by the red line. Dark blue line indicates median depth from the\nroot, dark shaded region shows 25th to 75th percentile, light shaded region shows 5th to 95th percentile. (C)\nPolicy improvement in Ms. Pacman - a single network was trained at 50 simulations per search and is evaluated at\ndifferent numbers of simulations per search, including playing according to the argmax of the raw policy network.\nThe policy improvement effect of the search over the raw policy network is clearly visible throughout training. This\nconsistent gap between the performance with and without search highlights the policy improvement that MuZero\nexploits, by continually updating towards the improved policy, to ef\ufb01ciently progress towards the optimal policy.\n(D) Policy improvement in Go - a single network was trained at 800 simulations per search and is evaluated at\ndifferent numbers of simulations per search. In Go, the playing strength improvement from longer searches is\nmuch larger than in Ms. Pacman and persists throughout training, consistent with previous results in [40]. This\nsuggests, as might intuitively be expected, that the bene\ufb01t of models is greatest in precision planning domains.\n20\n\n\nFigure S4:\nLearning curves of MuZero in Atari for individual games. Total reward is shown on the y-axis,\nmillions of training steps on the x-axis. Line indicates mean score across 1000 evaluation games, shaded region\nindicates standard deviation.\n21"
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "",
      "author": "",
      "subject": "",
      "keywords": "",
      "creator": "LaTeX with hyperref package",
      "producer": "pdfTeX-1.40.17",
      "creationDate": "D:20200224014900Z",
      "modDate": "D:20200224014900Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 21,
    "pages": [
      "Mastering Atari, Go, Chess and Shogi by Planning with a\nLearned Model\nJulian Schrittwieser,1\u2217Ioannis Antonoglou,1,2\u2217Thomas Hubert,1\u2217\nKaren Simonyan,1 Laurent Sifre,1 Simon Schmitt,1 Arthur Guez,1\nEdward Lockhart,1 Demis Hassabis,1 Thore Graepel,1,2 Timothy Lillicrap,1\nDavid Silver1,2\u2217\n1DeepMind, 6 Pancras Square, London N1C 4AG.\n2University College London, Gower Street, London WC1E 6BT.\n\u2217These authors contributed equally to this work.\nAbstract\nConstructing agents with planning capabilities has long been one of the main challenges in the\npursuit of arti\ufb01cial intelligence. Tree-based planning methods have enjoyed huge success in challeng-\ning domains, such as chess and Go, where a perfect simulator is available. However, in real-world\nproblems the dynamics governing the environment are often complex and unknown. In this work\nwe present the MuZero algorithm which, by combining a tree-based search with a learned model,\nachieves superhuman performance in a range of challenging and visually complex domains, without\nany knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively,\npredicts the quantities most directly relevant to planning: the reward, the action-selection policy, and\nthe value function. When evaluated on 57 different Atari games - the canonical video game environ-\nment for testing AI techniques, in which model-based planning approaches have historically struggled\n- our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without\nany knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero\nalgorithm that was supplied with the game rules.\n1\nIntroduction\nPlanning algorithms based on lookahead search have achieved remarkable successes in arti\ufb01cial intelligence. Hu-\nman world champions have been defeated in classic games such as checkers [34], chess [5], Go [38] and poker\n[3, 26], and planning algorithms have had real-world impact in applications from logistics [47] to chemical syn-\nthesis [37]. However, these planning algorithms all rely on knowledge of the environment\u2019s dynamics, such as the\nrules of the game or an accurate simulator, preventing their direct application to real-world domains like robotics,\nindustrial control, or intelligent assistants.\nModel-based reinforcement learning (RL) [42] aims to address this issue by \ufb01rst learning a model of the\nenvironment\u2019s dynamics, and then planning with respect to the learned model. Typically, these models have either\nfocused on reconstructing the true environmental state [8, 16, 24], or the sequence of full observations [14, 20].\nHowever, prior work [4, 14, 20] remains far from the state of the art in visually rich domains, such as Atari 2600\ngames [2]. Instead, the most successful methods are based on model-free RL [9, 21, 18] \u2013 i.e. they estimate\nthe optimal policy and/or value function directly from interactions with the environment. However, model-free\nalgorithms are in turn far from the state of the art in domains that require precise and sophisticated lookahead, such\nas chess and Go.\n1\narXiv:1911.08265v2  [cs.LG]  21 Feb 2020\n",
      "In this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art per-\nformance in Atari 2600, a visually complex set of domains, while maintaining superhuman performance in pre-\ncision planning tasks such as chess, shogi and Go. MuZero builds upon AlphaZero\u2019s [39] powerful search and\nsearch-based policy iteration algorithms, but incorporates a learned model into the training procedure. MuZero\nalso extends AlphaZero to a broader set of environments including single agent domains and non-zero rewards at\nintermediate time-steps.\nThe main idea of the algorithm (summarized in Figure 1) is to predict those aspects of the future that are directly\nrelevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an\ninput and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that\nreceives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the\npolicy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points\nscored by playing a move). The model is trained end-to-end, with the sole objective of accurately estimating these\nthree important quantities, so as to match the improved estimates of policy and value generated by search as well\nas the observed reward. There is no direct constraint or requirement for the hidden state to capture all information\nnecessary to reconstruct the original observation, drastically reducing the amount of information the model has\nto maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the\nenvironment; nor any other constraints on the semantics of state. Instead, the hidden states are free to represent\nstate in whatever way is relevant to predicting current and future values and policies. Intuitively, the agent can\ninvent, internally, the rules or dynamics that lead to most accurate planning.\n2\nPrior Work\nReinforcement learning may be subdivided into two principal categories: model-based, and model-free [42].\nModel-based RL constructs, as an intermediate step, a model of the environment. Classically, this model is\nrepresented by a Markov-decision process (MDP) [31] consisting of two components: a state transition model,\npredicting the next state, and a reward model, predicting the expected reward during that transition. The model\nis typically conditioned on the selected action, or a temporally abstract behavior such as an option [43]. Once\na model has been constructed, it is straightforward to apply MDP planning algorithms, such as value iteration\n[31] or Monte-Carlo tree search (MCTS) [7], to compute the optimal value or optimal policy for the MDP. In\nlarge or partially observed environments, the algorithm must \ufb01rst construct the state representation that the model\nshould predict. This tripartite separation between representation learning, model learning, and planning is poten-\ntially problematic since the agent is not able to optimize its representation or model for the purpose of effective\nplanning, so that, for example modeling errors may compound during planning.\nA common approach to model-based RL focuses on directly modeling the observation stream at the pixel-\nlevel. It has been hypothesized that deep, stochastic models may mitigate the problems of compounding error\n[14, 20]. However, planning at pixel-level granularity is not computationally tractable in large scale problems.\nOther methods build a latent state-space model that is suf\ufb01cient to reconstruct the observation stream at pixel level\n[48, 49], or to predict its future latent states [13, 11], which facilitates more ef\ufb01cient planning but still focuses\nthe majority of the model capacity on potentially irrelevant detail. None of these prior methods has constructed a\nmodel that facilitates effective planning in visually complex domains such as Atari; results lag behind well-tuned,\nmodel-free methods, even in terms of data ef\ufb01ciency [45].\nA quite different approach to model-based RL has recently been developed, focused end-to-end on predicting\nthe value function [41]. The main idea of these methods is to construct an abstract MDP model such that planning\nin the abstract MDP is equivalent to planning in the real environment. This equivalence is achieved by ensuring\nvalue equivalence, i.e. that, starting from the same real state, the cumulative reward of a trajectory through the\nabstract MDP matches the cumulative reward of a trajectory in the real environment.\nThe predictron [41] \ufb01rst introduced value equivalent models for predicting value (without actions). Although\nthe underlying model still takes the form of an MDP, there is no requirement for its transition model to match\nreal states in the environment. Instead the MDP model is viewed as a hidden layer of a deep neural network. The\nunrolled MDP is trained such that the expected cumulative sum of rewards matches the expected value with respect\nto the real environment, e.g. by temporal-difference learning.\n2\n",
      "Figure 1:\nPlanning, acting, and training with a learned model. (A) How MuZero uses its model to plan.\nThe model consists of three connected components for representation, dynamics and prediction. Given a previous\nhidden state sk\u22121 and a candidate action ak, the dynamics function g produces an immediate reward rk and a new\nhidden state sk. The policy pk and value function vk are computed from the hidden state sk by a prediction function\nf. The initial hidden state s0 is obtained by passing the past observations (e.g. the Go board or Atari screen) into\na representation function h. (B) How MuZero acts in the environment. A Monte-Carlo Tree Search is performed\nat each timestep t, as described in A. An action at+1 is sampled from the search policy \u03c0t, which is proportional\nto the visit count for each action from the root node. The environment receives the action and generates a new\nobservation ot+1 and reward ut+1. At the end of the episode the trajectory data is stored into a replay buffer. (C)\nHow MuZero trains its model. A trajectory is sampled from the replay buffer. For the initial step, the representation\nfunction h receives as input the past observations o1, ..., ot from the selected trajectory. The model is subsequently\nunrolled recurrently for K steps. At each step k, the dynamics function g receives as input the hidden state sk\u22121\nfrom the previous step and the real action at+k. The parameters of the representation, dynamics and prediction\nfunctions are jointly trained, end-to-end by backpropagation-through-time, to predict three quantities: the policy\npk \u2248\u03c0t+k, value function vk \u2248zt+k, and reward rt+k \u2248ut+k, where zt+k is a sample return: either the \ufb01nal\nreward (board games) or n-step return (Atari).\nValue equivalent models were subsequently extended to optimising value (with actions). TreeQN [10] learns\nan abstract MDP model, such that a tree search over that model (represented by a tree-structured neural network)\napproximates the optimal value function. Value iteration networks [44] learn a local MDP model, such that value\niteration over that model (represented by a convolutional neural network) approximates the optimal value function.\nValue prediction networks [28] are perhaps the closest precursor to MuZero: they learn an MDP model grounded\nin real actions; the unrolled MDP is trained such that the cumulative sum of rewards, conditioned on the actual\nsequence of actions generated by a simple lookahead search, matches the real environment. Unlike MuZero there\nis no policy prediction, and the search only utilizes value prediction.\n3\nMuZero Algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are made at each time-step t, for each of\nk = 1...K steps, by a model \u00b5\u03b8, with parameters \u03b8, conditioned on past observations o1, ..., ot and future actions\nat+1, ..., at+k. The model predicts three future quantities: the policy pk\nt \u2248\u03c0(at+k+1|o1, ..., ot, at+1, ..., at+k), the\nvalue function vk\nt \u2248E [ut+k+1 + \u03b3ut+k+2 + ...|o1, ..., ot, at+1, ..., at+k], and the immediate reward rk\nt \u2248ut+k,\n3\n",
      "where u. is the true, observed reward, \u03c0 is the policy used to select real actions, and \u03b3 is the discount function of\nthe environment.\nInternally, at each time-step t (subscripts t suppressed for simplicity), the model is represented by the com-\nbination of a representation function, a dynamics function, and a prediction function. The dynamics function,\nrk, sk = g\u03b8(sk\u22121, ak), is a recurrent process that computes, at each hypothetical step k, an immediate reward rk\nand an internal state sk. It mirrors the structure of an MDP model that computes the expected reward and state\ntransition for a given state and action [31]. However, unlike traditional approaches to model-based RL [42], this\ninternal state sk has no semantics of environment state attached to it \u2013 it is simply the hidden state of the overall\nmodel, and its sole purpose is to accurately predict relevant, future quantities: policies, values, and rewards. In\nthis paper, the dynamics function is represented deterministically; the extension to stochastic transitions is left for\nfuture work. The policy and value functions are computed from the internal state sk by the prediction function,\npk, vk = f\u03b8(sk), akin to the joint policy and value network of AlphaZero. The \u201croot\u201d state s0 is initialized using\na representation function that encodes past observations, s0 = h\u03b8(o1, ..., ot); again this has no special semantics\nbeyond its support for future predictions.\nGiven such a model, it is possible to search over hypothetical future trajectories a1, ..., ak given past obser-\nvations o1, ..., ot. For example, a naive search could simply select the k step action sequence that maximizes the\nvalue function. More generally, we may apply any MDP planning algorithm to the internal rewards and state space\ninduced by the dynamics function. Speci\ufb01cally, we use an MCTS algorithm similar to AlphaZero\u2019s search, gener-\nalized to allow for single agent domains and intermediate rewards (see Methods). At each internal node, it makes\nuse of the policy, value and reward estimates produced by the current model parameters \u03b8. The MCTS algorithm\noutputs a recommended policy \u03c0t and estimated value \u03bdt. An action at+1 \u223c\u03c0t is then selected.\nAll parameters of the model are trained jointly to accurately match the policy, value, and reward, for every\nhypothetical step k, to corresponding target values observed after k actual time-steps have elapsed. Similarly to\nAlphaZero, the improved policy targets are generated by an MCTS search; the \ufb01rst objective is to minimise the\nerror between predicted policy pk\nt and search policy \u03c0t+k. Also like AlphaZero, the improved value targets are\ngenerated by playing the game or MDP. However, unlike AlphaZero, we allow for long episodes with discounting\nand intermediate rewards by bootstrapping n steps into the future from the search value, zt = ut+1 +\u03b3ut+2 +...+\n\u03b3n\u22121ut+n + \u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in board games are treated as rewards ut \u2208{\u22121, 0, +1}\noccuring at the \ufb01nal step of the episode. Speci\ufb01cally, the second objective is to minimize the error between\nthe predicted value vk\nt and the value target, zt+k 1. The reward targets are simply the observed rewards; the third\nobjective is therefore to minimize the error between the predicted reward rk\nt and the observed reward ut+k. Finally,\nan L2 regularization term is also added, leading to the overall loss:\nlt(\u03b8) =\nK\nX\nk=0\nlr(ut+k, rk\nt ) + lv(zt+k, vk\nt ) + lp(\u03c0t+k, pk\nt ) + c||\u03b8||2\n(1)\nwhere lr, lv, and lp are loss functions for reward, value and policy respectively. Supplementary Figure S2 summa-\nrizes the equations governing how the MuZero algorithm plans, acts, and learns.\n4\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess and shogi 2, as benchmarks for challenging\nplanning problems, and to all 57 games in the Atari Learning Environment [2], as benchmarks for visually complex\nRL domains.\nIn each case we trained MuZero for K = 5 hypothetical steps. Training proceeded for 1 million mini-batches\nof size 2048 in board games and of size 1024 in Atari. During both training and evaluation, MuZero used 800\nsimulations for each search in board games, and 50 simulations for each search in Atari. The representation\n1For chess, Go and shogi, the same squared error loss as AlphaZero is used for rewards and values. A cross-entropy loss was found to be\nmore stable than a squared error when encountering rewards and values of variable scale in Atari. Cross-entropy was used for the policy loss\nin both cases.\n2Imperfect information games such as Poker are not directly addressed by our method.\n4\n",
      "Chess\nShogi\nGo\nAtari\nrmblkans\nopopopop\n0Z0Z0Z0Z\nZ0Z0Z0Z0\n0Z0Z0Z0Z\nZ0Z0Z0Z0\nPOPOPOPO\nSNAQJBMR\n\u9999\n\u6842\n\u9280\n\u91d1\n\u7389\n\u91d1\n\u9280\n\u6842\n\u9999\n\u98db\n\u89d2\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\n\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\u6b69\n\u89d2\n\u98db\n\u9999\u6842\u9280\u91d1\u7389\u91d1\u9280\u6842\u9999\nFigure 2: Evaluation of MuZero throughout training in chess, shogi, Go and Atari. The x-axis shows millions\nof training steps. For chess, shogi and Go, the y-axis shows Elo rating, established by playing games against Alp-\nhaZero using 800 simulations per move for both players. MuZero\u2019s Elo is indicated by the blue line, AlphaZero\u2019s\nElo by the horizontal orange line. For Atari, mean (full line) and median (dashed line) human normalized scores\nacross all 57 games are shown on the y-axis. The scores for R2D2 [21], (the previous state of the art in this domain,\nbased on model-free RL) are indicated by the horizontal orange lines. Performance in Atari was evaluated using\n50 simulations every fourth time-step, and then repeating the chosen action four times, as in prior work [25].\nfunction uses the same convolutional [23] and residual [15] architecture as AlphaZero, but with 16 residual blocks\ninstead of 20. The dynamics function uses the same architecture as the representation function and the prediction\nfunction uses the same architecture as AlphaZero. All networks use 256 hidden planes (see Methods for further\ndetails).\nFigure 2 shows the performance throughout training in each game. In Go, MuZero slightly exceeded the perfor-\nmance of AlphaZero, despite using less computation per node in the search tree (16 residual blocks per evaluation\nin MuZero compared to 20 blocks in AlphaZero). This suggests that MuZero may be caching its computation in\nthe search tree and using each additional application of the dynamics model to gain a deeper understanding of the\nposition.\nIn Atari, MuZero achieved a new state of the art for both mean and median normalized score across the 57\ngames of the Arcade Learning Environment, outperforming the previous state-of-the-art method R2D2 [21] (a\nmodel-free approach) in 42 out of 57 games, and outperforming the previous best model-based approach SimPLe\n[20] in all games (see Table S1).\nWe also evaluated a second version of MuZero that was optimised for greater sample ef\ufb01ciency. Speci\ufb01cally,\nit reanalyzes old trajectories by re-running the MCTS using the latest network parameters to provide fresh targets\n(see Appendix H). When applied to 57 Atari games, using 200 million frames of experience per game, MuZero\nReanalyze achieved 731% median normalized score, compared to 192%, 231% and 431% for previous state-of-\nthe-art model-free approaches IMPALA [9], Rainbow [17] and LASER [36] respectively.\nTo understand the role of the model in MuZero we also ran several experiments, focusing on the board game\nof Go and the Atari game of Ms. Pacman.\nFirst, we tested the scalability of planning (Figure 3A), in the canonical planning problem of Go. We compared\nthe performance of search in AlphaZero, using a perfect model, to the performance of search in MuZero, using a\n5\n",
      "Agent\nMedian\nMean\nEnv. Frames\nTraining Time\nTraining Steps\nApe-X [18]\n434.1%\n1695.6%\n22.8B\n5 days\n8.64M\nR2D2 [21]\n1920.6%\n4024.9%\n37.5B\n5 days\n2.16M\nMuZero\n2041.1%\n4999.2%\n20.0B\n12 hours\n1M\nIMPALA [9]\n191.8%\n957.6%\n200M\n\u2013\n\u2013\nRainbow [17]\n231.1%\n\u2013\n200M\n10 days\n\u2013\nUNREALa [19]\n250%a\n880%a\n250M\n\u2013\n\u2013\nLASER [36]\n431%\n\u2013\n200M\n\u2013\n\u2013\nMuZero Reanalyze\n731.1%\n2168.9%\n200M\n12 hours\n1M\nTable 1:\nComparison of MuZero against previous agents in Atari. We compare separately against agents\ntrained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques.\nMean and median scores are given, compared to human testers. The best results are highlighted in bold. MuZero\nsets a new state of the art in both settings. aHyper-parameters were tuned per game.\nlearned model. Speci\ufb01cally, the fully trained AlphaZero or MuZero was evaluated by comparing MCTS with\ndifferent thinking times. MuZero matched the performance of a perfect model, even when doing much larger\nsearches (up to 10s thinking time) than those from which the model was trained (around 0.1s thinking time, see\nalso Figure S3A).\nWe also investigated the scalability of planning across all Atari games (see Figure 3B). We compared MCTS\nwith different numbers of simulations, using the fully trained MuZero. The improvements due to planning are\nmuch less marked than in Go, perhaps because of greater model inaccuracy; performance improved slightly with\nsearch time, but plateaued at around 100 simulations. Even with a single simulation \u2013 i.e. when selecting moves\nsolely according to the policy network \u2013 MuZero performed well, suggesting that, by the end of training, the raw\npolicy has learned to internalise the bene\ufb01ts of search (see also Figure S3B).\nNext, we tested our model-based learning algorithm against a comparable model-free learning algorithm (see\nFigure 3C). We replaced the training objective of MuZero (Equation 1) with a model-free Q-learning objective\n(as used by R2D2), and the dual value and policy heads with a single head representing the Q-function Q(\u00b7|st).\nSubsequently, we trained and evaluated the new model without using any search. When evaluated on Ms. Pacman,\nour model-free algorithm achieved identical results to R2D2, but learned signi\ufb01cantly slower than MuZero and\nconverged to a much lower \ufb01nal score. We conjecture that the search-based policy improvement step of MuZero\nprovides a stronger learning signal than the high bias, high variance targets used by Q-learning.\nTo better understand the nature of MuZero\u2019s learning algorithm, we measured how MuZero\u2019s training scales\nwith respect to the amount of search it uses during training. Figure 3D shows the performance in Ms. Pacman,\nusing an MCTS of different simulation counts per move throughout training. Surprisingly, and in contrast to\nprevious work [1], even with only 6 simulations per move \u2013 fewer than the number of actions \u2013 MuZero learned\nan effective policy and improved rapidly. With more simulations performance jumped signi\ufb01cantly higher. For\nanalysis of the policy improvement during each individual iteration, see also Figure S3 C and D.\n5\nConclusions\nMany of the breakthroughs in arti\ufb01cial intelligence have been based on either high-performance planning [5, 38,\n39] or model-free reinforcement learning methods [25, 29, 46]. In this paper we have introduced a method that\ncombines the bene\ufb01ts of both approaches. Our algorithm, MuZero, has both matched the superhuman performance\nof high-performance planning algorithms in their favored domains \u2013 logically complex board games such as chess\nand Go \u2013 and outperformed state-of-the-art model-free RL algorithms in their favored domains \u2013 visually complex\nAtari games. Crucially, our method does not require any knowledge of the game rules or environment dynamics,\npotentially paving the way towards the application of powerful learning and planning methods to a host of real-\nworld domains for which there exists no perfect simulator.\n6\n",
      "Figure 3:\nEvaluations of MuZero on Go (A), all 57 Atari Games (B) and Ms. Pacman (C-D). (A) Scaling\nwith search time per move in Go, comparing the learned model with the ground truth simulator. Both networks\nwere trained at 800 simulations per search, equivalent to 0.1 seconds per search. Remarkably, the learned model\nis able to scale well to up to two orders of magnitude longer searches than seen during training. (B) Scaling of\n\ufb01nal human normalized mean score in Atari with the number of simulations per search. The network was trained\nat 50 simulations per search. Dark line indicates mean score, shaded regions indicate 25th to 75th and 5th to\n95th percentiles. The learned model\u2019s performance increases up to 100 simulations per search. Beyond, even\nwhen scaling to much longer searches than during training, the learned model\u2019s performance remains stable and\nonly decreases slightly. This contrasts with the much better scaling in Go (A), presumably due to greater model\ninaccuracy in Atari than Go. (C) Comparison of MCTS based training with Q-learning in the MuZero framework\non Ms. Pacman, keeping network size and amount of training constant. The state of the art Q-Learning algorithm\nR2D2 is shown as a baseline. Our Q-Learning implementation reaches the same \ufb01nal score as R2D2, but improves\nslower and results in much lower \ufb01nal performance compared to MCTS based training. (D) Different networks\ntrained at different numbers of simulations per move, but all evaluated at 50 simulations per move. Networks\ntrained with more simulations per move improve faster, consistent with ablation (B), where the policy improvement\nis larger when using more simulations per move. Surprisingly, MuZero can learn effectively even when training\nwith less simulations per move than are enough to cover all 8 possible actions in Ms. Pacman.\n7\n",
      "6\nAcknowledgments\nLorrayne Bennett, Oliver Smith and Chris Apps for organizational assistance; Koray Kavukcuoglu for reviewing\nthe paper; Thomas Anthony, Matthew Lai, Nenad Tomasev, Ulrich Paquet, Sumedh Ghaisas for many fruitful\ndiscussions; and the rest of the DeepMind team for their support.\nReferences\n[1] Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Emma Brunskill, Zachary C. Lipton, and Animashree\nAnandkumar. Surprising negative results for generative adversarial tree search. CoRR, abs/1806.05780, 2018.\n[2] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research, 47:253\u2013279, 2013.\n[3] Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top profes-\nsionals. Science, 359(6374):418\u2013424, 2018.\n[4] Lars Buesing, Theophane Weber, Sebastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert, Fabio\nViola, Frederic Besse, Karol Gregor, Demis Hassabis, et al. Learning and querying fast generative models\nfor reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.\n[5] Murray Campbell, A. Joseph Hoane, Jr., and Feng-hsiung Hsu. Deep blue. Artif. Intell., 134(1-2):57\u201383,\nJanuary 2002.\n[6] R. Coulom. Whole-history rating: A Bayesian rating system for players of time-varying strength. In Inter-\nnational Conference on Computers and Games, pages 113\u2013124, 2008.\n[7] R\u00b4emi Coulom. Ef\ufb01cient selectivity and backup operators in monte-carlo tree search. In International confer-\nence on computers and games, pages 72\u201383. Springer, 2006.\n[8] MP. Deisenroth and CE. Rasmussen. Pilco: A model-based and data-ef\ufb01cient approach to policy search.\nIn Proceedings of the 28th International Conference on Machine Learning, ICML 2011, pages 465\u2013472.\nOmnipress, 2011.\n[9] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron,\nVlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted\nactor-learner architectures. In Proceedings of the International Conference on Machine Learning (ICML),\n2018.\n[10] Gregory Farquhar, Tim Rocktaeschel, Maximilian Igl, and Shimon Whiteson. TreeQN and ATreec: Differ-\nentiable tree planning for deep reinforcement learning. In International Conference on Learning Represen-\ntations, 2018.\n[11] Carles Gelada, Saurabh Kumar, Jacob Buckman, O\ufb01r Nachum, and Marc G. Bellemare. DeepMDP: Learning\ncontinuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdi-\nnov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-\nceedings of Machine Learning Research, pages 2170\u20132179, Long Beach, California, USA, 09\u201315 Jun 2019.\nPMLR.\n[12] Cloud tpu. https://cloud.google.com/tpu/. Accessed: 2019.\n[13] David Ha and J\u00a8urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Proceedings of\nthe 32Nd International Conference on Neural Information Processing Systems, NIPS\u201918, pages 2455\u20132467,\nUSA, 2018. Curran Associates Inc.\n8\n",
      "[14] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.\nLearning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In\n14th European Conference on Computer Vision, pages 630\u2013645, 2016.\n[16] Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and Tom Erez. Learning con-\ntinuous control policies by stochastic value gradients. In Proceedings of the 28th International Conference\non Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2944\u20132952, Cambridge, MA, USA,\n2015. MIT Press.\n[17] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,\nBilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement\nlearning. In Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018.\n[18] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David\nSilver. Distributed prioritized experience replay. In International Conference on Learning Representations,\n2018.\n[19] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Sil-\nver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint\narXiv:1611.05397, 2016.\n[20] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,\nDumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning\nfor atari. arXiv preprint arXiv:1903.00374, 2019.\n[21] Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent experience\nreplay in distributed reinforcement learning. In International Conference on Learning Representations, 2019.\n[22] Levente Kocsis and Csaba Szepesv\u00b4ari. Bandit based monte-carlo planning. In European conference on\nmachine learning, pages 282\u2013293. Springer, 2006.\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[24] Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under un-\nknown dynamics. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems 27, pages 1071\u20131079. Curran Associates, Inc., 2014.\n[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529, 2015.\n[26] Matej Morav\u02c7c\u00b4\u0131k, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin\nWaugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level arti\ufb01cial intelligence in heads-up\nno-limit poker. Science, 356(6337):508\u2013513, 2017.\n[27] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Ve-\ndavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih,\nKoray Kavukcuoglu, and David Silver. Massively parallel methods for deep reinforcement learning. CoRR,\nabs/1507.04296, 2015.\n[28] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information\nProcessing Systems, pages 6118\u20136128, 2017.\n[29] OpenAI. Openai \ufb01ve. https://blog.openai.com/openai-five/, 2018.\n9\n",
      "[30] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel\nBarth-Maron, Hado van Hasselt, John Quan, Mel Ve\u02c7cer\u00b4\u0131k, et al. Observe and look further: Achieving consis-\ntent performance on atari. arXiv preprint arXiv:1805.11593, 2018.\n[31] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley\n& Sons, Inc., New York, NY, USA, 1st edition, 1994.\n[32] Christopher D Rosin.\nMulti-armed bandits with episode context.\nAnnals of Mathematics and Arti\ufb01cial\nIntelligence, 61(3):203\u2013230, 2011.\n[33] Maarten PD Schadd, Mark HM Winands, H Jaap Van Den Herik, Guillaume MJ-B Chaslot, and Jos WHM\nUiterwijk. Single-player monte-carlo tree search. In International Conference on Computers and Games,\npages 1\u201312. Springer, 2008.\n[34] Jonathan Schaeffer, Joseph Culberson, Norman Treloar, Brent Knight, Paul Lu, and Duane Szafron. A world\nchampionship caliber checkers program. Arti\ufb01cial Intelligence, 53(2-3):273\u2013289, 1992.\n[35] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In Interna-\ntional Conference on Learning Representations, Puerto Rico, 2016.\n[36] Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience replay.\narXiv preprint arXiv:1909.11583, 2019.\n[37] Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks\nand symbolic ai. Nature, 555(7698):604, 2018.\n[38] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,\nJohn Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search.\nNature, 529(7587):484\u2013489, January 2016.\n[39] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm\nthat masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.\n[40] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,\nGeorge van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human\nknowledge. Nature, 550:354\u2013359, October 2017.\n[41] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-\nArnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-end learning and\nplanning.\nIn Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages\n3191\u20133199. JMLR. org, 2017.\n[42] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second\nedition, 2018.\n[43] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for\ntemporal abstraction in reinforcement learning. Arti\ufb01cial intelligence, 112(1-2):181\u2013211, 1999.\n[44] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances\nin Neural Information Processing Systems, pages 2154\u20132162, 2016.\n[45] Hado van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement\nlearning? arXiv preprint arXiv:1906.05243, 2019.\n10\n",
      "[46] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00a8el Mathieu, Andrew Dudzik, Junyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature, pages 1\u20135, 2019.\n[47] I Vlahavas and I Refanidis. Planning and scheduling. EETN, Greece, Tech. Rep, 2013.\n[48] Niklas Wahlstr\u00a8om, Thomas B. Sch\u00a8on, and Marc Peter Deisenroth. From pixels to torques: Policy learning\nwith deep dynamical models. CoRR, abs/1502.02251, 2015.\n[49] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A\nlocally linear latent dynamics model for control from raw images. In Proceedings of the 28th International\nConference on Neural Information Processing Systems - Volume 2, NIPS\u201915, pages 2746\u20132754, Cambridge,\nMA, USA, 2015. MIT Press.\nSupplementary Materials\n\u2022 Pseudocode description of the MuZero algorithm.\n\u2022 Data for Figures 2, 3, S2, S3, S4 and Tables 1, S1, S2 in JSON format.\nSupplementary materials can be accessed from the ancillary \ufb01le section of the arXiv submission.\nAppendix A\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero [40] and AlphaZero [39].\nIn AlphaGo Zero and AlphaZero the planning process makes use of two separate components: a simulator\nimplements the rules of the game, which are used to update the state of the game while traversing the search\ntree; and a neural network jointly predicts the corresponding policy and value of a board position produced by the\nsimulator (see Figure 1 A).\nSpeci\ufb01cally, AlphaGo Zero and AlphaZero use knowledge of the rules of the game in three places: (1) state\ntransitions in the search tree, (2) actions available at each node of the search tree, (3) episode termination within\nthe search tree. In MuZero, all of these have been replaced with the use of a single implicit model learned by a\nneural network (see Figure 1 B):\n1) State transitions. AlphaZero had access to a perfect simulator of the true dynamics process. In contrast,\nMuZero employs a learned dynamics model within its search. Under this model, each node in the tree is\nrepresented by a corresponding hidden state; by providing a hidden state sk\u22121 and an action ak to the model\nthe search algorithm can transition to a new node sk = g(sk\u22121, ak).\n2) Actions available. AlphaZero used the set of legal actions obtained from the simulator to mask the prior\nproduced by the network everywhere in the search tree. MuZero only masks legal actions at the root of the\nsearch tree where the environment can be queried, but does not perform any masking within the search tree.\nThis is possible because the network rapidly learns not to predict actions that never occur in the trajectories\nit is trained on.\n3) Terminal nodes. AlphaZero stopped the search at tree nodes representing terminal states and used the ter-\nminal value provided by the simulator instead of the value produced by the network. MuZero does not give\nspecial treatment to terminal nodes and always uses the value predicted by the network. Inside the tree, the\nsearch can proceed past a terminal node - in this case the network is expected to always predict the same\nvalue. This is achieved by treating terminal states as absorbing states during training.\nIn addition, MuZero is designed to operate in the general reinforcement learning setting: single-agent domains\nwith discounted intermediate rewards of arbitrary magnitude. In contrast, AlphaGo Zero and AlphaZero were\ndesigned to operate in two-player games with undiscounted terminal rewards of \u00b11.\n11\n",
      "Appendix B\nSearch\nWe now describe the search algorithm used by MuZero. Our approach is based upon Monte-Carlo tree search with\nupper con\ufb01dence bounds, an approach to planning that converges asymptotically to the optimal policy in single\nagent domains and to the minimax value function in zero sum games [22].\nEvery node of the search tree is associated with an internal state s. For each action a from s there is an\nedge (s, a) that stores a set of statistics {N(s, a), Q(s, a), P(s, a), R(s, a), S(s, a)}, respectively representing\nvisit counts N, mean value Q, policy P, reward R, and state transition S.\nSimilar to AlphaZero, the search is divided into three stages, repeated for a number of simulations.\nSelection: Each simulation starts from the internal root state s0, and \ufb01nishes when the simulation reaches a\nleaf node sl. For each hypothetical time-step k = 1...l of the simulation, an action ak is selected according to the\nstored statistics for internal state sk\u22121, by maximizing over an upper con\ufb01dence bound [32][39],\nak = arg max\na\n\u0014\nQ(s, a) + P(s, a) \u00b7\npP\nb N(s, b)\n1 + N(s, a)\n\u0012\nc1 + log\n\u0010P\nb N(s, b) + c2 + 1\nc2\n\u0011\u0013\u0015\n(2)\nThe constants c1 and c2 are used to control the in\ufb02uence of the prior P(s, a) relative to the value Q(s, a) as\nnodes are visited more often. In our experiments, c1 = 1.25 and c2 = 19652.\nFor k < l, the next state and reward are looked up in the state transition and reward table sk = S(sk\u22121, ak),\nrk = R(sk\u22121, ak).\nExpansion: At the \ufb01nal time-step l of the simulation, the reward and state are computed by the dynamics\nfunction, rl, sl = g\u03b8(sl\u22121, al), and stored in the corresponding tables, R(sl\u22121, al) = rl, S(sl\u22121, al) = sl. The\npolicy and value are computed by the prediction function, pl, vl = f\u03b8(sl). A new node, corresponding to state\nsl is added to the search tree. Each edge (sl, a) from the newly expanded node is initialized to {N(sl, a) =\n0, Q(sl, a) = 0, P(sl, a) = pl}. Note that the search algorithm makes at most one call to the dynamics function\nand prediction function respectively per simulation; the computational cost is of the same order as in AlphaZero.\nBackup: At the end of the simulation, the statistics along the trajectory are updated. The backup is generalized\nto the case where the environment can emit intermediate rewards, have a discount \u03b3 different from 1, and the value\nestimates are unbounded 3. For k = l...0, we form an l \u2212k-step estimate of the cumulative discounted reward,\nbootstrapping from the value function vl,\nGk =\nl\u22121\u2212k\nX\n\u03c4=0\n\u03b3\u03c4rk+1+\u03c4 + \u03b3l\u2212kvl\n(3)\nFor k = l...1, we update the statistics for each edge (sk\u22121, ak) in the simulation path as follows,\nQ(sk\u22121, ak) := N(sk\u22121, ak) \u00b7 Q(sk\u22121, ak) + Gk\nN(sk\u22121, ak) + 1\nN(sk\u22121, ak) := N(sk\u22121, ak) + 1\n(4)\nIn two-player zero sum games the value functions are assumed to be bounded within the [0, 1] interval. This\nchoice allows us to combine value estimates with probabilities using the pUCT rule (Eqn 2). However, since in\nmany environments the value is unbounded, it is necessary to adjust the pUCT rule. A simple solution would be\nto use the maximum score that can be observed in the environment to either re-scale the value or set the pUCT\nconstants appropriately [33]. However, both solutions are game speci\ufb01c and require adding prior knowledge to\nthe MuZero algorithm. To avoid this, MuZero computes normalized Q value estimates Q \u2208[0, 1] by using the\nminimum-maximum values observed in the search tree up to that point. When a node is reached during the\nselection stage, the algorithm computes the normalized Q values of its edges to be used in the pUCT rule using the\nequation below:\nQ(sk\u22121, ak) =\nQ(sk\u22121, ak) \u2212mins,a\u2208T ree Q(s, a)\nmaxs,a\u2208T ree Q(s, a) \u2212mins,a\u2208T ree Q(s, a)\n(5)\n3In board games the discount is assumed to be 1 and there are no intermediate rewards.\n12\n",
      "Appendix C\nHyperparameters\nFor simplicity we preferentially use the same architectural choices and hyperparameters as in previous work.\nSpeci\ufb01cally, we started with the network architecture and search choices of AlphaZero [39]. For board games, we\nuse the same UCB constants, dirichlet exploration noise and the same 800 simulations per search as in AlphaZero.\nDue to the much smaller branching factor and simpler policies in Atari, we only used 50 simulations per search\nto speed up experiments. As shown in Figure 3B, the algorithm is not very sensitive to this choice. We also use the\nsame discount (0.997) and value transformation (see Network Architecture section) as R2D2 [21].\nFor parameter values not mentioned in the text, please refer to the pseudocode.\nAppendix D\nData Generation\nTo generate training data, the latest checkpoint of the network (updated every 1000 training steps) is used to play\ngames with MCTS. In the board games Go, chess and shogi the search is run for 800 simulations per move to pick\nan action; in Atari due to the much smaller action space 50 simulations per move are suf\ufb01cient.\nFor board games, games are sent to the training job as soon as they \ufb01nish. Due to the much larger length of\nAtari games (up to 30 minutes or 108,000 frames), intermediate sequences are sent every 200 moves. In board\ngames, the training job keeps an in-memory replay buffer of the most recent 1 million games received; in Atari,\nwhere the visual observations are larger, the most recent 125 thousand sequences of length 200 are kept.\nDuring the generation of experience in the board game domains, the same exploration scheme as the one\ndescribed in AlphaZero [39] is used. Using a variation of this scheme, in the Atari domain actions are sampled\nfrom the visit count distribution throughout the duration of each game, instead of just the \ufb01rst k moves. Moreover,\nthe visit count distribution is parametrized using a temperature parameter T:\np\u03b1 =\nN(\u03b1)1/T\nP\nb N(b)1/T\n(6)\nT is decayed as a function of the number of training steps of the network. Speci\ufb01cally, for the \ufb01rst 500k\ntraining steps a temperature of 1 is used, for the next 250k steps a temperature of 0.5 and for the remaining 250k a\ntemperature of 0.25. This ensures that the action selection becomes greedier as training progresses.\nAppendix E\nNetwork Input\nRepresentation Function\nThe history over board states used as input to the representation function for Go, chess and shogi is represented\nsimilarly to AlphaZero [39]. In Go and shogi we encode the last 8 board states as in AlphaZero; in chess we\nincreased the history to the last 100 board states to allow correct prediction of draws.\nFor Atari, the input of the representation function includes the last 32 RGB frames at resolution 96x96 along\nwith the last 32 actions that led to each of those frames. We encode the historical actions because unlike board\ngames, an action in Atari does not necessarily have a visible effect on the observation. RGB frames are encoded\nas one plane per color, rescaled to the range [0, 1], for red, green and blue respectively. We perform no other\nnormalization, whitening or other preprocessing of the RGB input. Historical actions are encoded as simple bias\nplanes, scaled as a/18 (there are 18 total actions in Atari).\nDynamics Function\nThe input to the dynamics function is the hidden state produced by the representation function or previous appli-\ncation of the dynamics function, concatenated with a representation of the action for the transition. Actions are\nencoded spatially in planes of the same resolution as the hidden state. In Atari, this resolution is 6x6 (see descrip-\ntion of downsampling in Network Architecture section), in board games this is the same as the board size (19x19\nfor Go, 8x8 for chess, 9x9 for shogi).\n13\n",
      "In Go, a normal action (playing a stone on the board) is encoded as an all zero plane, with a single one in the\nposition of the played stone. A pass is encoded as an all zero plane.\nIn chess, 8 planes are used to encode the action. The \ufb01rst one-hot plane encodes which position the piece was\nmoved from. The next two planes encode which position the piece was moved to: a one-hot plane to encode the\ntarget position, if on the board, and a second binary plane to indicate whether the target was valid (on the board) or\nnot. This is necessary because for simplicity our policy action space enumerates a superset of all possible actions,\nnot all of which are legal, and we use the same action space for policy prediction and to encode the dynamics\nfunction input. The remaining \ufb01ve binary planes are used to indicate the type of promotion, if any (queen, knight,\nbishop, rook, none).\nThe encoding for shogi is similar, with a total of 11 planes. We use the \ufb01rst 8 planes to indicate where the\npiece moved from - either a board position (\ufb01rst one-hot plane) or the drop of one of the seven types of prisoner\n(remaining 7 binary planes). The next two planes are used to encode the target as in chess. The remaining binary\nplane indicates whether the move was a promotion or not.\nIn Atari, an action is encoded as a one hot vector which is tiled appropriately into planes.\nAppendix F\nNetwork Architecture\nThe prediction function pk, vk = f\u03b8(sk) uses the same architecture as AlphaZero: one or two convolutional layers\nthat preserve the resolution but reduce the number of planes, followed by a fully connected layer to the size of the\noutput.\nFor value and reward prediction in Atari we follow [30] in scaling targets using an invertible transform h(x) =\nsign(x)(\np\n|x| + 1 \u22121 + \u03f5x), where \u03f5 = 0.001 in all our experiments. We then apply a transformation \u03c6 to the\nscalar reward and value targets in order to obtain equivalent categorical representations. We use a discrete support\nset of size 601 with one support for every integer between \u2212300 and 300. Under this transformation, each scalar\nis represented as the linear combination of its two adjacent supports, such that the original value can be recovered\nby x = xlow \u2217plow + xhigh \u2217phigh. As an example, a target of 3.7 would be represented as a weight of 0.3\non the support for 3 and a weight of 0.7 on the support for 4. The value and reward outputs of the network are\nalso modeled using a softmax output of size 601. During inference the actual value and rewards are obtained by\n\ufb01rst computing their expected value under their respective softmax distribution and subsequently by inverting the\nscaling transformation. Scaling and transformation of the value and reward happens transparently on the network\nside and is not visible to the rest of the algorithm.\nBoth the representation and dynamics function use the same architecture as AlphaZero, but with 16 instead of\n20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.\nFor Atari, where observations have large spatial resolution, the representation function starts with a sequence\nof convolutions with stride 2 to reduce the spatial resolution. Speci\ufb01cally, starting with an input observation of\nresolution 96x96 and 128 planes (32 history frames of 3 color channels each, concatenated with the corresponding\n32 actions broadcast to planes), we downsample as follows:\n\u2022 1 convolution with stride 2 and 128 output planes, output resolution 48x48.\n\u2022 2 residual blocks with 128 planes\n\u2022 1 convolution with stride 2 and 256 output planes, output resolution 24x24.\n\u2022 3 residual blocks with 256 planes.\n\u2022 Average pooling with stride 2, output resolution 12x12.\n\u2022 3 residual blocks with 256 planes.\n\u2022 Average pooling with stride 2, output resolution 6x6.\nThe kernel size is 3x3 for all operations.\nFor the dynamics function (which always operates at the downsampled resolution of 6x6), the action is \ufb01rst\nencoded as an image, then stacked with the hidden state of the previous step along the plane dimension.\n14\n",
      "Appendix G\nTraining\nDuring training, the MuZero network is unrolled for K hypothetical steps and aligned to sequences sampled from\nthe trajectories generated by the MCTS actors. Sequences are selected by sampling a state from any game in the\nreplay buffer, then unrolling for K steps from that state. In Atari, samples are drawn according to prioritized replay\n[35], with priority P(i) =\np\u03b1\ni\nP\nk p\u03b1\nk , where pi = |\u03bdi \u2212zi|, \u03bd is the search value and z the observed n-step return. To\ncorrect for sampling bias introduced by the prioritized sampling, we scale the loss using the importance sampling\nratio wi = ( 1\nN \u00b7\n1\nP (i))\u03b2. In all our experiments, we set \u03b1 = \u03b2 = 1. For board games, states are sampled uniformly.\nEach observation ot along the sequence also has a corresponding MCTS policy \u03c0t, estimated value \u03bdt and\nenvironment reward ut. At each unrolled step k the network has a loss to the value, policy and reward target for\nthat step, summed to produce the total loss for the MuZero network (see Equation 1). Note that, in board games\nwithout intermediate rewards, we omit the reward prediction loss. For board games, we bootstrap directly to the\nend of the game, equivalent to predicting the \ufb01nal outcome; for Atari we bootstrap for n = 10 steps into the future.\nTo maintain roughly similar magnitude of gradient across different unroll steps, we scale the gradient in two\nseparate locations:\n\u2022 We scale the loss of each head by\n1\nK , where K is the number of unroll steps. This ensures that the total\ngradient has similar magnitude irrespective of how many steps we unroll for.\n\u2022 We also scale the gradient at the start of the dynamics function by 1\n2. This ensures that the total gradient\napplied to the dynamics function stays constant.\nIn the experiments reported in this paper, we always unroll for K = 5 steps. For a detailed illustration, see\nFigure 1.\nTo improve the learning process and bound the activations, we also scale the hidden state to the same range as\nthe action input ([0, 1]): sscaled =\ns\u2212min(s)\nmax(s)\u2212min(s).\nAll experiments were run using third generation Google Cloud TPUs [12]. For each board game, we used 16\nTPUs for training and 1000 TPUs for selfplay. For each game in Atari, we used 8 TPUs for training and 32 TPUs for\nselfplay. The much smaller proportion of TPUs used for acting in Atari is due to the smaller number of simulations\nper move (50 instead of 800) and the smaller size of the dynamics function compared to the representation function.\nAppendix H\nReanalyze\nTo improve the sample ef\ufb01ciency of MuZero we introduced a second variant of the algorithm, MuZero Reana-\nlyze. MuZero Reanalyze revisits its past time-steps and re-executes its search using the latest model parameters,\npotentially resulting in a better quality policy than the original search. This fresh policy is used as the policy\ntarget for 80% of updates during MuZero training. Furthermore, a target network [25] \u00b7, v\u2212= f\u03b8\u2212(s0), based\non recent parameters \u03b8\u2212, is used to provide a fresher, stable n-step bootstrapped target for the value function,\nzt = ut+1 + \u03b3ut+2 + ... + \u03b3n\u22121ut+n + \u03b3nv\u2212\nt+n. In addition, several other hyperparameters were adjusted \u2013 pri-\nmarily to increase sample reuse and avoid over\ufb01tting of the value function. Speci\ufb01cally, 2.0 samples were drawn\nper state, instead of 0.1; the value target was weighted down to 0.25 compared to weights of 1.0 for policy and\nreward targets; and the n-step return was reduced to n = 5 steps instead of n = 10 steps.\nAppendix I\nEvaluation\nWe evaluated the relative strength of MuZero (Figure 2) in board games by measuring the Elo rating of each\nplayer. We estimate the probability that player a will defeat player b by a logistic function p(a defeats b) =\n(1+10(celo(e(b)\u2212e(a))))\u22121, and estimate the ratings e(\u00b7) by Bayesian logistic regression, computed by the BayesElo\nprogram [6] using the standard constant celo = 1/400.\nElo ratings were computed from the results of a 800 simulations per move tournament between iterations of\nMuZero during training, and also a baseline player: either Stock\ufb01sh, Elmo or AlphaZero respectively. Baseline\n15\n",
      "Figure S1: Repeatability of MuZero in Atari for \ufb01ve games. Total reward is shown on the y-axis, millions of\ntraining steps on the x-axis. Dark line indicates median score across 10 separate training runs, light lines indicate\nindividual training runs, and the shaded region indicates 25th to 75th percentile.\nplayers used an equivalent search time of 100ms per move. The Elo rating of the baseline players was anchored to\npublicly available values [39].\nIn Atari, we computed mean reward over 1000 episodes per game, limited to the standard 30 minutes or 108,000\nframes per episode [27], using 50 simulations per move unless indicated otherwise. In order to mitigate the effects\nof the deterministic nature of the Atari simulator we employed two different evaluation strategies: 30 noop random\nstarts and human starts. For the former, at the beginning of each episode a random number of between 0 and 30\nnoop actions are applied to the simulator before handing control to the agent. For the latter, start positions are\nsampled from human expert play to initialize the Atari simulator before handing the control to the agent [27].\n16\n",
      "Game\nRandom\nHuman\nSimPLe [20]\nApe-X [18]\nR2D2 [21]\nMuZero\nMuZero normalized\nalien\n227.75\n7,127.80\n616.90\n40,805.00\n229,496.90\n741,812.63\n10,747.5 %\namidar\n5.77\n1,719.53\n74.30\n8,659.00\n29,321.40\n28,634.39\n1,670.5 %\nassault\n222.39\n742.00\n527.20\n24,559.00\n108,197.00\n143,972.03\n27,664.9 %\nasterix\n210.00\n8,503.33\n1,128.30\n313,305.00\n999,153.30\n998,425.00\n12,036.4 %\nasteroids\n719.10\n47,388.67\n793.60\n155,495.00\n357,867.70\n678,558.64\n1,452.4 %\natlantis\n12,850.00\n29,028.13\n20,992.50\n944,498.00\n1,620,764.00\n1,674,767.20\n10,272.6 %\nbank heist\n14.20\n753.13\n34.20\n1,716.00\n24,235.90\n1,278.98\n171.2 %\nbattle zone\n2,360.00\n37,187.50\n4,031.20\n98,895.00\n751,880.00\n848,623.00\n2,429.9 %\nbeam rider\n363.88\n16,926.53\n621.60\n63,305.00\n188,257.40\n454,993.53\n2,744.9 %\nberzerk\n123.65\n2,630.42\n-\n57,197.00\n53,318.70\n85,932.60\n3,423.1 %\nbowling\n23.11\n160.73\n30.00\n18.00\n219.50\n260.13\n172.2 %\nboxing\n0.05\n12.06\n7.80\n100.00\n98.50\n100.00\n832.2 %\nbreakout\n1.72\n30.47\n16.40\n801.00\n837.70\n864.00\n2,999.2 %\ncentipede\n2,090.87\n12,017.04\n-\n12,974.00\n599,140.30\n1,159,049.27\n11,655.6 %\nchopper command\n811.00\n7,387.80\n979.40\n721,851.00\n986,652.00\n991,039.70\n15,056.4 %\ncrazy climber\n10,780.50\n35,829.41\n62,583.60\n320,426.00\n366,690.70\n458,315.40\n1,786.6 %\ndefender\n2,874.50\n18,688.89\n-\n411,944.00\n665,792.00\n839,642.95\n5,291.2 %\ndemon attack\n152.07\n1,971.00\n208.10\n133,086.00\n140,002.30\n143,964.26\n7,906.4 %\ndouble dunk\n-18.55\n-16.40\n-\n24.00\n23.70\n23.94\n1,976.3 %\nenduro\n0.00\n860.53\n-\n2,177.00\n2,372.70\n2,382.44\n276.9 %\n\ufb01shing derby\n-91.71\n-38.80\n-90.70\n44.00\n85.80\n91.16\n345.6 %\nfreeway\n0.01\n29.60\n16.70\n34.00\n32.50\n33.03\n111.6 %\nfrostbite\n65.20\n4,334.67\n236.90\n9,329.00\n315,456.40\n631,378.53\n14,786.7 %\ngopher\n257.60\n2,412.50\n596.80\n120,501.00\n124,776.30\n130,345.58\n6,036.8 %\ngravitar\n173.00\n3,351.43\n173.40\n1,599.00\n15,680.70\n6,682.70\n204.8 %\nhero\n1,026.97\n30,826.38\n2,656.60\n31,656.00\n39,537.10\n49,244.11\n161.8 %\nice hockey\n-11.15\n0.88\n-11.60\n33.00\n79.30\n67.04\n650.0 %\njamesbond\n29.00\n302.80\n100.50\n21,323.00\n25,354.00\n41,063.25\n14,986.9 %\nkangaroo\n52.00\n3,035.00\n51.20\n1,416.00\n14,130.70\n16,763.60\n560.2 %\nkrull\n1,598.05\n2,665.53\n2,204.80\n11,741.00\n218,448.10\n269,358.27\n25,083.4 %\nkung fu master\n258.50\n22,736.25\n14,862.50\n97,830.00\n233,413.30\n204,824.00\n910.1 %\nmontezuma revenge\n0.00\n4,753.33\n-\n2,500.00\n2,061.30\n0.00\n0.0 %\nms pacman\n307.30\n6,951.60\n1,480.00\n11,255.00\n42,281.70\n243,401.10\n3,658.7 %\nname this game\n2,292.35\n8,049.00\n2,420.70\n25,783.00\n58,182.70\n157,177.85\n2,690.5 %\nphoenix\n761.40\n7,242.60\n-\n224,491.00\n864,020.00\n955,137.84\n14,725.3 %\npitfall\n-229.44\n6,463.69\n-\n-1.00\n0.00\n0.00\n3.4 %\npong\n-20.71\n14.59\n12.80\n21.00\n21.00\n21.00\n118.2 %\nprivate eye\n24.94\n69,571.27\n35.00\n50.00\n5,322.70\n15,299.98\n22.0 %\nqbert\n163.88\n13,455.00\n1,288.80\n302,391.00\n408,850.00\n72,276.00\n542.6 %\nriverraid\n1,338.50\n17,118.00\n1,957.80\n63,864.00\n45,632.10\n323,417.18\n2,041.1 %\nroad runner\n11.50\n7,845.00\n5,640.60\n222,235.00\n599,246.70\n613,411.80\n7,830.5 %\nrobotank\n2.16\n11.94\n-\n74.00\n100.40\n131.13\n1,318.7 %\nseaquest\n68.40\n42,054.71\n683.30\n392,952.00\n999,996.70\n999,976.52\n2,381.5 %\nskiing\n-17,098.09\n-4,336.93\n-\n-10,790.00\n-30,021.70\n-29,968.36\n-100.9 %\nsolaris\n1,236.30\n12,326.67\n-\n2,893.00\n3,787.20\n56.62\n-10.6 %\nspace invaders\n148.03\n1,668.67\n-\n54,681.00\n43,223.40\n74,335.30\n4,878.7 %\nstar gunner\n664.00\n10,250.00\n-\n434,343.00\n717,344.00\n549,271.70\n5,723.0 %\nsurround\n-9.99\n6.53\n-\n7.00\n9.90\n9.99\n120.9 %\ntennis\n-23.84\n-8.27\n-\n24.00\n-0.10\n0.00\n153.1 %\ntime pilot\n3,568.00\n5,229.10\n-\n87,085.00\n445,377.30\n476,763.90\n28,486.9 %\ntutankham\n11.43\n167.59\n-\n273.00\n395.30\n491.48\n307.4 %\nup n down\n533.40\n11,693.23\n3,350.30\n401,884.00\n589,226.90\n715,545.61\n6,407.0 %\nventure\n0.00\n1,187.50\n-\n1,813.00\n1,970.70\n0.40\n0.0 %\nvideo pinball\n0.00\n17,667.90\n-\n565,163.00\n999,383.20\n981,791.88\n5,556.9 %\nwizard of wor\n563.50\n4,756.52\n-\n46,204.00\n144,362.70\n197,126.00\n4,687.9 %\nyars revenge\n3,092.91\n54,576.93\n5,664.30\n148,595.00\n995,048.40\n553,311.46\n1,068.7 %\nzaxxon\n32.50\n9,173.30\n-\n42,286.00\n224,910.70\n725,853.90\n7,940.5 %\n# best\n0\n5\n0\n5\n13\n37\nTable S1: Evaluation of MuZero in Atari for individual games with 30 random no-op starts. Best result for\neach game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames).\nSimPLe was only evaluated on 36 of the 57 games, unavailable results are indicated with \u2018-\u2019. Human normalized\nscore is calculated as snormalized =\nsagent\u2212srandom\nshuman\u2212srandom .\n17\n",
      "Game\nRandom\nHuman\nApe-X [18]\nMuZero\nMuZero normalized\nalien\n128.30\n6,371.30\n17,732.00\n713,387.37\n11,424.9 %\namidar\n11.79\n1,540.43\n1,047.00\n26,638.80\n1,741.9 %\nassault\n166.95\n628.89\n24,405.00\n143,900.58\n31,115.2 %\nasterix\n164.50\n7,536.00\n283,180.00\n985,801.95\n13,370.9 %\nasteroids\n877.10\n36,517.30\n117,303.00\n606,971.12\n1,700.6 %\natlantis\n13,463.00\n26,575.00\n918,715.00\n1,653,202.50\n12,505.6 %\nbank heist\n21.70\n644.50\n1,201.00\n962.11\n151.0 %\nbattle zone\n3,560.00\n33,030.00\n92,275.00\n791,387.00\n2,673.3 %\nbeam rider\n254.56\n14,961.02\n72,234.00\n419,460.76\n2,850.5 %\nberzerk\n196.10\n2,237.50\n55,599.00\n87,308.60\n4,267.3 %\nbowling\n35.16\n146.46\n30.00\n194.03\n142.7 %\nboxing\n-1.46\n9.61\n81.00\n56.60\n524.5 %\nbreakout\n1.77\n27.86\n757.00\n849.59\n3,249.6 %\ncentipede\n1,925.45\n10,321.89\n5,712.00\n1,138,294.60\n13,533.9 %\nchopper command\n644.00\n8,930.00\n576,602.00\n932,370.10\n11,244.6 %\ncrazy climber\n9,337.00\n32,667.00\n263,954.00\n412,213.90\n1,726.9 %\ndefender\n1,965.50\n14,296.00\n399,865.00\n823,636.00\n6,663.7 %\ndemon attack\n208.25\n3,442.85\n133,002.00\n143,858.05\n4,441.0 %\ndouble dunk\n-15.97\n-14.37\n22.00\n23.12\n2,443.1 %\nenduro\n-81.84\n740.17\n2,042.00\n2,264.20\n285.4 %\n\ufb01shing derby\n-77.11\n5.09\n22.00\n57.45\n163.7 %\nfreeway\n0.17\n25.61\n29.00\n28.38\n110.9 %\nfrostbite\n90.80\n4,202.80\n6,512.00\n613,944.04\n14,928.3 %\ngopher\n250.00\n2,311.00\n121,168.00\n129,218.68\n6,257.6 %\ngravitar\n245.50\n3,116.00\n662.00\n3,390.65\n109.6 %\nhero\n1,580.30\n25,839.40\n26,345.00\n44,129.55\n175.4 %\nice hockey\n-9.67\n0.53\n24.00\n52.40\n608.5 %\njamesbond\n33.50\n368.50\n18,992.00\n39,107.20\n11,663.8 %\nkangaroo\n100.00\n2,739.00\n578.00\n13,210.50\n496.8 %\nkrull\n1,151.90\n2,109.10\n8,592.00\n257,706.70\n26,802.6 %\nkung fu master\n304.00\n20,786.80\n72,068.00\n174,623.60\n851.1 %\nmontezuma revenge\n25.00\n4,182.00\n1,079.00\n57.10\n0.8 %\nms pacman\n197.80\n15,375.05\n6,135.00\n230,650.24\n1,518.4 %\nname this game\n1,747.80\n6,796.00\n23,830.00\n152,723.62\n2,990.7 %\nphoenix\n1,134.40\n6,686.20\n188,789.00\n943,255.07\n16,969.6 %\npitfall\n-348.80\n5,998.91\n-273.00\n-801.10\n-7.1 %\npong\n-17.95\n15.46\n19.00\n19.20\n111.2 %\nprivate eye\n662.78\n64,169.07\n865.00\n5,067.59\n6.9 %\nqbert\n159.38\n12,085.00\n380,152.00\n39,302.10\n328.2 %\nriverraid\n588.30\n14,382.20\n49,983.00\n315,353.33\n2,281.9 %\nroad runner\n200.00\n6,878.00\n127,112.00\n580,445.00\n8,688.9 %\nrobotank\n2.42\n8.94\n69.00\n128.80\n1,938.3 %\nseaquest\n215.50\n40,425.80\n377,180.00\n997,601.01\n2,480.4 %\nskiing\n-15,287.35\n-3,686.58\n-11,359.00\n-29,400.75\n-121.7 %\nsolaris\n2,047.20\n11,032.60\n3,116.00\n2,108.08\n0.7 %\nspace invaders\n182.55\n1,464.90\n50,699.00\n57,450.41\n4,465.9 %\nstar gunner\n697.00\n9,528.00\n432,958.00\n539,342.70\n6,099.5 %\nsurround\n-9.72\n5.37\n6.00\n8.46\n120.5 %\ntennis\n-21.43\n-6.69\n23.00\n-2.30\n129.8 %\ntime pilot\n3,273.00\n5,650.00\n71,543.00\n405,829.30\n16,935.5 %\ntutankham\n12.74\n138.30\n128.00\n351.76\n270.0 %\nup n down\n707.20\n9,896.10\n347,912.00\n607,807.85\n6,606.9 %\nventure\n18.00\n1,039.00\n936.00\n21.10\n0.3 %\nvideo pinball\n0.00\n15,641.09\n873,989.00\n970,881.10\n6,207.2 %\nwizard of wor\n804.00\n4,556.00\n46,897.00\n196,279.20\n5,209.9 %\nyars revenge\n1,476.88\n47,135.17\n131,701.00\n888,633.84\n1,943.0 %\nzaxxon\n475.00\n8,443.00\n37,672.00\n592,238.70\n7,426.8 %\n# best\n0\n6\n5\n46\nTable S2:\nEvaluation of MuZero in Atari for individual games from human start positions. Best result for\neach game highlighted in bold. Each episode is limited to a maximum of 30 minutes of game time (108k frames).\n18\n",
      "Model\ns0\n= h\u03b8(o1, ..., ot)\nrk, sk\n= g\u03b8(sk\u22121, ak)\npk, vk\n= f\u03b8(sk)\n\uf8fc\n\uf8fd\n\uf8fepk, vk, rk = \u00b5\u03b8(o1, ..., ot, a1, ..., ak)\nSearch\n\u03bdt, \u03c0t = MCTS(s0\nt, \u00b5\u03b8)\nat \u223c\u03c0t\nLearning Rule\npk\nt , vk\nt , rk\nt = \u00b5\u03b8(o1, ..., ot, at+1, ..., at+k)\nzt =\n\u001a uT\nfor games\nut+1 + \u03b3ut+2 + ... + \u03b3n\u22121ut+n + \u03b3n\u03bdt+n\nfor general MDPs\nlt(\u03b8) =\nK\nX\nk=0\nlr(ut+k, rk\nt ) + lv(zt+k, vk\nt ) + lp(\u03c0t+k, pk\nt ) + c||\u03b8||2\nLosses\nlr(u, r) =\n\u001a 0\nfor games\n\u03c6(u)T log r\nfor general MDPs\nlv(z, q) =\n\u001a (z \u2212q)2\nfor games\n\u03c6(z)T log q\nfor general MDPs\nlp(\u03c0, p) = \u03c0T log p\nFigure S2:\nEquations summarising the MuZero algorithm. Here, \u03c6(x) refers to the representation of a real\nnumber x through a linear combination of its adjacent integers, as described in the Network Architecture section.\n19\n",
      "Figure S3: Details of MuZero evaluations (A-B) and policy improvement ablations (C-D). (A-B) Distribution\nof evaluation depth in the search tree for the learned model for the evaluations in Figure 3A-B. The network was\ntrained over 5 hypothetical steps, as indicated by the red line. Dark blue line indicates median depth from the\nroot, dark shaded region shows 25th to 75th percentile, light shaded region shows 5th to 95th percentile. (C)\nPolicy improvement in Ms. Pacman - a single network was trained at 50 simulations per search and is evaluated at\ndifferent numbers of simulations per search, including playing according to the argmax of the raw policy network.\nThe policy improvement effect of the search over the raw policy network is clearly visible throughout training. This\nconsistent gap between the performance with and without search highlights the policy improvement that MuZero\nexploits, by continually updating towards the improved policy, to ef\ufb01ciently progress towards the optimal policy.\n(D) Policy improvement in Go - a single network was trained at 800 simulations per search and is evaluated at\ndifferent numbers of simulations per search. In Go, the playing strength improvement from longer searches is\nmuch larger than in Ms. Pacman and persists throughout training, consistent with previous results in [40]. This\nsuggests, as might intuitively be expected, that the bene\ufb01t of models is greatest in precision planning domains.\n20\n",
      "Figure S4:\nLearning curves of MuZero in Atari for individual games. Total reward is shown on the y-axis,\nmillions of training steps on the x-axis. Line indicates mean score across 1000 evaluation games, shaded region\nindicates standard deviation.\n21\n"
    ],
    "pdf_path": "data/papers/1911.08265v2.pdf"
  },
  {
    "text": "Learning to summarize from human feedback\nNisan Stiennon\u2217\nLong Ouyang\u2217\nJeff Wu\u2217\nDaniel M. Ziegler\u2217\nRyan Lowe\u2217\nChelsea Voss\u2217\nAlec Radford\nDario Amodei\nPaul Christiano\u2217\nOpenAI\nAbstract\nAs language models become more powerful, training and evaluation are increas-\ningly bottlenecked by the data and metrics used for a particular task. For example,\nsummarization models are often trained to predict human reference summaries and\nevaluated using ROUGE, but both of these metrics are rough proxies for what we\nreally care about\u2014summary quality. In this work, we show that it is possible to\nsigni\ufb01cantly improve summary quality by training a model to optimize for human\npreferences. We collect a large, high-quality dataset of human comparisons be-\ntween summaries, train a model to predict the human-preferred summary, and use\nthat model as a reward function to \ufb01ne-tune a summarization policy using reinforce-\nment learning. We apply our method to a version of the TL;DR dataset of Reddit\nposts [63] and \ufb01nd that our models signi\ufb01cantly outperform both human reference\nsummaries and much larger models \ufb01ne-tuned with supervised learning alone. Our\nmodels also transfer to CNN/DM news articles [22], producing summaries nearly\nas good as the human reference without any news-speci\ufb01c \ufb01ne-tuning.2 We con-\nduct extensive analyses to understand our human feedback dataset and \ufb01ne-tuned\nmodels.3 We establish that our reward model generalizes to new datasets, and that\noptimizing our reward model results in better summaries than optimizing ROUGE\naccording to humans. We hope the evidence from our paper motivates machine\nlearning researchers to pay closer attention to how their training loss affects the\nmodel behavior they actually want.\n1\nIntroduction\nLarge-scale language model pretraining has become increasingly prevalent for achieving high per-\nformance on a variety of natural language processing (NLP) tasks. When applying these models\nto a speci\ufb01c task, they are usually \ufb01ne-tuned using supervised learning, often to maximize the log\nprobability of a set of human demonstrations.\nWhile this strategy has led to markedly improved performance, there is still a misalignment between\nthis \ufb01ne-tuning objective\u2014maximizing the likelihood of human-written text\u2014and what we care\nabout\u2014generating high-quality outputs as determined by humans. This misalignment has several\ncauses: the maximum likelihood objective has no distinction between important errors (e.g. making\nup facts [41]) and unimportant errors (e.g. selecting the precise word from a set of synonyms); models\n\u2217This was a joint project of the OpenAI Re\ufb02ection team. Author order was randomized amongst {LO, JW,\nDZ, NS}; CV and RL were full-time contributors for most of the duration. PC is the team lead.\n2Samples from all of our models can be viewed on our website.\n3We provide inference code for our 1.3B models and baselines, as well as a model card and our human\nfeedback dataset with over 64k summary comparisons, here.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2009.01325v3  [cs.CL]  15 Feb 2022\n\n\nSupervised learning\nHuman feedback\nPretrain only\nReference summaries\nFigure 1: Fraction of the time humans prefer our models\u2019 summaries over the human-generated\nreference summaries on the TL;DR dataset.4Since quality judgments involve an arbitrary decision\nabout how to trade off summary length vs. coverage within the 24-48 token limit, we also provide\nlength-controlled graphs in Appendix F; length differences explain about a third of the gap between\nfeedback and supervised learning at 6.7B.\nare incentivized to place probability mass on all human demonstrations, including those that are\nlow-quality; and distributional shift during sampling can degrade performance [56, 52]. Quality can\noften be improved signi\ufb01cantly by non-uniform sampling strategies such as beam search [51], but\nthese can lead to repetition and other undesirable artifacts [69, 23]. Optimizing for quality may be a\nprincipled approach to overcoming these problems.\nOur goal in this paper is to advance methods for training language models on objectives that more\nclosely capture the behavior we care about. To make short-term progress towards this goal, we\nfocus on abstractive English text summarization, as it has a long history in the NLP community\n[16, 8, 54, 59, 50], and is a subjective task where we believe it is dif\ufb01cult to quantify summary quality\nwithout human judgments. Indeed, existing automatic metrics for evaluating summary quality, such\nas ROUGE [39], have received criticism for poor correlation with human judgments [55, 45, 6, 33].\nWe follow the works of [3, 73], who \ufb01ne-tune language models from human feedback using reward\nlearning [35]. We \ufb01rst collect a dataset of human preferences between pairs of summaries, then train\na reward model (RM) via supervised learning to predict the human-preferred summary. Finally, we\ntrain a policy via reinforcement learning (RL) to maximize the score given by the RM; the policy\ngenerates a token of text at each \u2018time step\u2019, and is updated using the PPO algorithm [58] based on\nthe RM \u2018reward\u2019 given to the entire generated summary. We can then gather more human data using\nsamples from the resulting policy, and repeat the process. We follow the works of [48, 4] and use\nlarge pretrained GPT-3 models with as many as 6.7 billion parameters.\nOur main contributions are four-fold.\n(1) We show that training with human feedback signi\ufb01cantly outperforms very strong baselines\non English summarization. When applying our methods on a version of the Reddit TL;DR dataset\n[63], we train policies via human feedback that produce better summaries than much larger policies\ntrained via supervised learning. Summaries from our human feedback models are preferred by our\nlabelers to the original human demonstrations in the dataset (see Figure 1).\n(2) We show human feedback models generalize much better to new domains than supervised\nmodels. Our Reddit-trained human feedback models also generate high-quality summaries of news\narticles on the CNN/DailyMail (CNN/DM) dataset without any news-speci\ufb01c \ufb01ne-tuning, almost\nmatching the quality of the dataset\u2019s reference summaries. We perform several checks to ensure\nthat these human preferences re\ufb02ect a real quality difference: we consistently monitor agreement\nrates amongst labelers and researchers, and \ufb01nd researcher-labeler agreement rates are nearly as high\nas researcher-researcher agreement rates (see Section C.2), and we verify models are not merely\noptimizing simple metrics like length or amount of copying (see Appendices F and G.7).\n4Throughout the paper, error bars represent 1 standard error.\n2\n\n\n(3) We conduct extensive empirical analyses of our policy and reward model. We examine the\nimpact of model and data size (Figure 6), study performance as we continue to optimize a given\nreward model (Section 4.3), and analyze reward model performance using synthetic and human-\nwritten perturbations of summaries (Section 4.3). We con\ufb01rm that our reward model outperforms\nother metrics such as ROUGE at predicting human preferences, and that optimizing our reward model\ndirectly results in better summaries than optimizing ROUGE according to humans (Section 4.4).\n(4) We publicly release our human feedback dataset for further research. The dataset contains\n64,832 summary comparisons on the TL;DR dataset, as well as our evaluation data on both TL;DR\n(comparisons and Likert scores) and CNN/DM (Likert scores).\nThe methods we present in this paper are motivated in part by longer-term concerns about the\nmisalignment of AI systems with what humans want them to do. When misaligned summarization\nmodels make up facts, their mistakes are fairly low-risk and easy to spot. However, as AI systems\nbecome more powerful and are given increasingly important tasks, the mistakes they make will likely\nbecome more subtle and safety-critical, making this an important area for further research.\n2\nRelated work\nMost directly related to our work is previous work using human feedback to train summarization\nmodels with RL [3, 73]. Bohm et al. [3] learn a reward function from a dataset of human ratings of\n2.5k CNN/DM summaries, and train a policy whose summaries are preferred to a policy optimizing\nROUGE. Our work is most similar to [73], who also train Transformer models [62] to optimize human\nfeedback across a range of tasks, including summarization on the Reddit TL;DR and CNN/DM\ndatasets. Unlike us, they train in an online manner and \ufb01nd the model highly extractive. They\nnote that their labelers prefer extractive summaries and have low agreement rates with researchers.\nCompared to [73], we use signi\ufb01cantly larger models, move to the batch setting for collecting human\nfeedback, ensure high labeler-researcher agreement, and make some algorithmic modi\ufb01cations, such\nas separating the policy and value networks.\nHuman feedback has also been used as a reward to train models in other domains such as dialogue\n[25, 68, 21], translation [32, 1], semantic parsing [34], story generation [72], review generation\n[7], and evidence extraction [46]. Our reward modeling approach was developed in prior work\non learning to rank [40], which has been applied to ranking search results using either explicit\nfeedback [2, 18] or implicit feedback in the form of click-through data [29, 30]. In a related line of\nresearch, human feedback has been used to train agents in simulated environments [10, 24]. There\nis also a rich literature on using RL to optimize automatic metrics for NLP tasks, such as ROUGE\nfor summarization [50, 65, 45, 15, 19], BLEU for translation [50, 66, 1, 43], and other domains\n[61, 27, 26]. Finally, there has been extensive research on modifying architectures [22, 59] and\npre-training procedures [70, 36, 49, 60, 53, 14] for improving summarization performance.\n3\nMethod and experiment details\n3.1\nHigh-level methodology\nOur approach is similar to the one outlined in [73], adapted to the batch setting. We start with an\ninitial policy that is \ufb01ne-tuned via supervised learning on the desired dataset (in our case, the Reddit\nTL;DR summarization dataset). The process (illustrated in Figure 2) then consists of three steps that\ncan be repeated iteratively.\nStep 1: Collect samples from existing policies and send comparisons to humans. For each\nReddit post, we sample summaries from several sources including the current policy, initial policy,\noriginal reference summaries and various baselines. We send a batch of pairs of summaries to our\nhuman evaluators, who are tasked with selecting the best summary of a given Reddit post.\nStep 2: Learn a reward model from human comparisons. Given a post and a candidate summary,\nwe train a reward model to predict the log odds that this summary is the better one, as judged by our\nlabelers.\nStep 3: Optimize a policy against the reward model. We treat the logit output of the reward model\nas a reward that we optimize using reinforcement learning, speci\ufb01cally with the PPO algorithm [58].\n3\n\n\n1 Collect human feedback\n\u201cj is better than k\u201d\n\u201cj is better than k\u201d\nA Reddit post is \nsampled from \nthe Reddit \nTL;DR dataset.\nVarious policies \nare used to \nsample a set of \nsummaries.\nTwo summaries \nare selected for \nevaluation.\nA human judges \nwhich is a better \nsummary of the \npost.\n2 Train reward model\nOne post with \ntwo summaries \njudged by a \nhuman are fed \nto the reward \nmodel.\nThe reward \nmodel \ncalculates a \nreward r for \neach summary.\nThe loss is \ncalculated based \non the rewards \nand human label, \nand is used to \nupdate the \nreward model.\n3 Train policy with PPO\nA new post is \nsampled from the \ndataset.\nThe reward \nmodel calculates \na reward for the \nsummary.\nThe reward is \nused to update \nthe policy via \nPPO.\nr\nr\nr\nr\n\u03c0\nrj\nloss = log(\u03c3(rj - rk ))\nrk\nThe policy \u03c0 \ngenerates a \nsummary for the \npost.\nr\nj\nj\nk\nk\nFigure 2: Diagram of our human feedback, reward model training, and policy training procedure.\nWe provide a more thorough description of our procedure, including details of the reward model and\npolicy training and our quality control process, in the following sections. In practice, rather than\nprecisely iterating this sequence of three steps, we updated our data collection and training procedures\nover the course of the project while accumulating labels (see Appendix C.6 for details).\n3.2\nDatasets and task\nDatasets.\nWe use the TL;DR summarization dataset [63], which contains ~3 million posts from\nreddit.com across a variety of topics (subreddits), as well summaries of the posts written by the\noriginal poster (TL;DRs). We additionally \ufb01lter this dataset (see Appendix A) to ensure quality,\nincluding using a whitelist of subreddits that are understandable to the general population. Crucially,\nwe also \ufb01lter to include only posts where the human-written summaries contain between 24 and\n48 tokens, to minimize the potential effect of summary length on quality (see Section 4.1 and\nAppendix F). Our \ufb01nal \ufb01ltered dataset contains 123,169 posts, and we hold out ~5% as a validation\nset. For the remainder of this paper, we refer to this dataset simply as TL;DR.\nWe chose the TL;DR dataset over the more commonly used CNN/DM dataset primarily because\nvery strong performance can be attained on CNN/DM with simple extractive baselines. We \ufb01nd in\nSection 4.2 that our labelers prefer lead-3 over the CNN/DM reference summaries,5 and that the\nsupervised T5 model [49] with low-temperature sampling already surpasses the reference summary\nquality, while copying extensively from the article. On the other hand, simple extractive baselines\nperform poorly on TL;DR in our human evaluations (see Appendix G.2). Instead of training on\nCNN/DM, we study the transfer performance of our human feedback models to CNN/DM after being\ntrained to summarize Reddit posts.\nTask.\nWe de\ufb01ne our ground-truth task as producing a model that generates summaries fewer than\n48 tokens long that are as good as possible, according to our judgments. We judge summary quality\nby how faithfully the summary conveys the original post to a reader who can only read the summary\nand not the post (see Appendix C.5 for further discussion of criteria). Since we have limited capacity\nto do comparisons, we hire labelers to do the comparisons for us. We rely on detailed procedures to\nensure high agreement between labelers and us on the task, which we describe in the next section.\n5We manually check this result in Appendix E and \ufb01nd we generally agree with labeler ratings.\n4\n\n\n[r/dating_advice] First date ever, going to the beach. Would like some tips\nHey Reddit! I (20M) would like some tips, because I have my \ufb01rst ever date tomorrow (although I\u2019ve had a gf for 3\nyears, but no actual dating happened), and we\u2019re going to the beach.\nI met this girl, we have mutual friends, at a festival a few days ago.\nWe didn\u2019t kiss, but we talked, held\nhands, danced a bit. I asked her to go on a date with me, which was super hard as it is the \ufb01rst time I\u2019ve asked this\nto anybody. What I mean to say is, it\u2019s not like a standard *\ufb01rst* date because we already spent some time together.\nI\u2019m really nervous and excited.\nI\u2019m going to pick her up tomorrow, we\u2019re cycling to the beach which will\ntake 30 minutes, and then what? I\u2019m a bit scared. Should I bring something (the weather, although no rain and\nsunny, is not super so no swimming), should we do something. I\u2019d like all the tips I can get. Thanks!\nHuman written reference TL;DR\n6.7B supervised model\n6.7B human feedback model\nFirst date after 3 years in a relation-\nship, going to the beach, terri\ufb01ed.\nWhat to bring with me, what to do?\nGoing on a date with a girl I met\na few days ago, going to the beach.\nWhat should I bring, what should\nwe do?\nGoing on my \ufb01rst ever date tomor-\nrow, cycling to the beach. Would\nlike some tips on what to do and\nbring. I\u2019m a bit nervous and excited.\nThanks!\nTable 1: Example of post and samples on the TL;DR dataset, chosen to be particularly short. For\nrandom samples (along with posts), see Appendix H and our website.\n3.3\nCollecting human feedback\nPrevious work on \ufb01ne-tuning language models from human feedback [73] reported \u201ca mismatch\nbetween the notion of quality we wanted our model to learn, and what the humans labelers actually\nevaluated\u201d, leading to model-generated summaries that were high-quality according to the labelers,\nbut fairly low-quality according to the researchers.\nCompared to [73], we implement two changes to improve human data quality. First, we transition\nentirely to the of\ufb02ine setting, where we alternate between sending large batches of comparison data6\nto our human labelers and re-training our models on the cumulative collected data. Second, we\nmaintain a hands-on relationship with labelers:7 we on-board them with detailed instructions, answer\ntheir questions in a shared chat room, and provide regular feedback on their performance. We train all\nlabelers to ensure high agreement with our judgments, and continuously monitor labeler-researcher\nagreement over the course of the project. See Appendix C.1 and C.5 for details.\nAs a result of our procedure, we obtained high labeler-researcher agreement: on a subset of compari-\nson tasks, labelers agree with researchers 77% \u00b1 2% of the time, while researchers agree with each\nother 73% \u00b1 4% of the time. We provide more analysis of our human data quality in Appendix C.2.\n3.4\nModels\nAll of our models are Transformer decoders [62] in the style of GPT-3 [47, 4]. We conduct our human\nfeedback experiments on models with 1.3 billion (1.3B) and 6.7 billion (6.7B) parameters.\nPretrained models.\nSimilarly to [12, 47], we start with models pretrained to autoregressively\npredict the next token in a large text corpus. As in [48, 4], we use these models as \u2018zero-shot\u2019\nbaselines by padding the context with examples of high-quality summaries from the dataset. We\nprovide details on pretraining in Appendix B, and on our zero-shot procedure in Appendix B.2.\nSupervised baselines.\nWe next \ufb01ne-tune these models via supervised learning to predict summaries\nfrom our \ufb01ltered TL;DR dataset (see Appendix B for details). We use these supervised models to\nsample initial summaries for collecting comparisons, to initialize our policy and reward models, and\nas baselines for evaluation. In our \ufb01nal human evaluations, we use T=0 to sample from all models, as\nwe found it performed better than higher temperatures or nucleus sampling (see Appendix B.1).\nTo validate that our supervised models are indeed strong baselines for comparison, we run our\nsupervised \ufb01ne-tuning procedure with our 6.7B model on the CNN/DM dataset, and \ufb01nd that we\nachieve slightly better ROUGE scores than SOTA models [71] from mid-2019 (see Appendix G.4).\n6Our decision to collect comparisons rather than Likert scores is supported by recent work, e.g. [37].\n7We recruited labelers from a freelancing platform, Upwork, and two labeling services, Scale and Lionbridge.\n5\n\n\nReward models.\nTo train our reward models, we start from a supervised baseline, as described\nabove, then add a randomly initialized linear head that outputs a scalar value. We train this model to\npredict which summary y \u2208{y0, y1} is better as judged by a human, given a post x. If the summary\npreferred by the human is yi, we can write the RM loss as:\nloss(r\u03b8) = \u2212E(x,y0,y1,i)\u223cD[log(\u03c3(r\u03b8(x, yi) \u2212r\u03b8(x, y1\u2212i)))]\nwhere r\u03b8(x, y) is the scalar output of the reward model for post x and summary y with parameters \u03b8,\nand D is the dataset of human judgments. At the end of training, we normalize the reward model\noutputs such that the reference summaries from our dataset achieve a mean score of 0.\nHuman feedback policies.\nWe want to use the reward model trained above to train a policy that\ngenerates higher-quality outputs as judged by humans. We primarily do this using reinforcement\nlearning, by treating the output of the reward model as a reward for the entire summary that we\nmaximize with the PPO algorithm [58], where each time step is a BPE token.8 We initialize our\npolicy to be the model \ufb01ne-tuned on Reddit TL;DR. Importantly, we include a term in the reward that\npenalizes the KL divergence between the learned RL policy \u03c0RL\n\u03c6 with parameters \u03c6 and this original\nsupervised model \u03c0SFT, as previously done in [25]. The full reward R can be written as:\nR(x, y) = r\u03b8(x, y) \u2212\u03b2 log[\u03c0RL\n\u03c6 (y|x)/\u03c0SFT(y|x)]\nThis KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to\nexplore and deterring it from collapsing to a single mode. Second, it ensures the policy doesn\u2019t learn\nto produce outputs that are too different from those that the reward model has seen during training.\nFor the PPO value function, we use a Transformer with completely separate parameters from the\npolicy. This prevents updates to the value function from partially destroying the pretrained policy\nearly in training (see ablation in Appendix G.1). We initialize the value function to the parameters of\nthe reward model. In our experiments, the reward model, policy, and value function are the same size.\n4\nResults\n4.1\nSummarizing Reddit posts from human feedback\nPolicies trained with human feedback are preferred to much larger supervised policies.\nOur\nmain results evaluating our human feedback policies on TL;DR are shown in Figure 1. We measure\npolicy quality as the percentage of summaries generated by that policy that humans prefer over\nthe reference summaries in the dataset. Our policies trained with human feedback signi\ufb01cantly\noutperform our supervised baselines on this metric, with our 1.3B human feedback model signi\ufb01cantly\noutperforming a supervised model 10\u00d7 its size (61% versus 43% raw preference score against\nreference summaries). Our 6.7B model in turn signi\ufb01cantly outperforms our 1.3B model, suggesting\nthat training with human feedback also bene\ufb01ts from scale. Additionally, both of our human feedback\nmodels are judged by humans to be superior to the human demonstrations used in the dataset.\nControlling for summary length.\nWhen judging summary quality, summary length is a confound-\ning factor. The target length of a summary is implicitly part of the summarization task; depending on\nthe desired trade-off between conciseness and coverage, a shorter or longer summary might be better.\nSince our models learned to generate longer summaries, length could account for much of our quality\nimprovements. We \ufb01nd that after controlling for length (Appendix F), the preference of our human\nfeedback models vs. reference summaries drops by ~5%; even so, our 6.7B model summaries are still\npreferred to the reference summaries ~65% of the time.\nHow do our policies improve over the baselines?\nTo better understand the quality of our models\u2019\nsummaries compared to the reference summaries and those of our supervised baselines, we conduct\nan additional analysis where human labelers assess summary quality across four dimensions (or\n\u201caxes\u201d) using a 7-point Likert scale [38]. Labelers rated summaries for coverage (how much important\ninformation from the original post is covered), accuracy (to what degree the statements in the summary\nare stated in the post), coherence (how easy the summary is to read on its own), and overall quality.\n8Note that the reward model only gives rewards for entire summaries, and not at intermediate time steps. In\nRL terminology, each episode terminates when the policy outputs the EOS token, and the discount factor \u03b3 = 1.\n6\n\n\nSupervised\ntransfer\nHuman feedback\ntransfer\nPretrain\nonly\nReference summaries\nLead-3\nSupervised\nCNN/DM\nT5 CNN/DM\nfinetuning\n(a)\n(b)\nFigure 4: Transfer results on CNN/DM. (a) Overall summary quality on CNN/DM as a function of\nmodel size. Full results across axes shown in Appendix G.2. (b) Overall scores vs. length for the\n6.7B TL;DR supervised baseline, the 6.7B TL;DR human feedback model, and T5 \ufb01ne-tuned on\nCNN/DM summaries. At similar summary lengths, our 6.7B TL;DR human feedback model nearly\nmatches T5 despite never being trained to summarize news articles.\nFigure 3: Evaluations of four axes of\nsummary quality on the TL;DR dataset.\nThe results (Figure 3) indicate that our human feedback\nmodels outperform the supervised baselines across every\ndimension of quality, but particularly coverage. Although\nour human labelers had a high bar for giving perfect overall\nscores, summaries from our 6.7B PPO model achieve a 7/7\noverall score 45% of the time (compared to 20% and 23%\nfor the 6.7B supervised baseline and reference summaries,\nrespectively).\n4.2\nTransfer to summarizing news articles\nOur human feedback models can also generate excellent\nsummaries of CNN/DM news articles without any further\ntraining (Figure 4). Our human feedback models signi\ufb01-\ncantly outperform models trained via supervised learning\non TL;DR and models trained only on pretraining corpora.\nIn fact, our 6.7B human feedback model performs almost as well as a 6.7B model that was \ufb01ne-tuned\non the CNN/DM reference summaries, despite generating much shorter summaries.\nSince our human feedback models transferred to CNN/DM have little overlap in summary length\ndistribution with models trained on CNN/DM, with about half as many tokens on average, they are\ndif\ufb01cult to compare directly. Thus our evaluations in Figure 4 use a 7-point Likert scale on four\nquality dimensions, as in Section 4.1 (see Appendix C.5 for labeler instructions). In Figure 4b we\nshow the average overall score at different summary lengths, which suggests our human feedback\nmodels would perform even better if they generated longer summaries. Qualitatively, CNN/DM\nsummaries from our human feedback models are consistently \ufb02uent and reasonable representations\nof the article; we show examples on our website and in Appendix H.\n4.3\nUnderstanding the reward model\nWhat happens as we optimize the reward model?\nOptimizing against our reward model is\nsupposed to make our policy align with human preferences. But the reward model isn\u2019t a perfect\nrepresentation of our labeler preferences, as it has limited capacity and only sees a small amount of\ncomparison data from a relatively narrow distribution of summaries. While we can hope our reward\nmodel generalizes to summaries unseen during training, it\u2019s unclear how much one can optimize\nagainst the reward model until it starts giving useless evaluations.\nTo answer this question, we created a range of policies optimized against an earlier version of our\nreward model, with varying degrees of optimization strength, and asked labelers to compare samples\nfrom them to the reference summaries. Figure 5 shows the results for PPO at a range of KL penalty\n7\n\n\nRM prediction\nActual preference\nFigure 5: Preference scores versus degree of\nreward model optimization. Optimizing against\nthe reward model initially improves summaries,\nbut eventually over\ufb01ts, giving worse summaries.\nThis \ufb01gure uses an earlier version of our reward\nmodel (see rm3 in Appendix C.6). See Appendix\nH.2 for samples from the KL 250 model.\nEnsemble of humans\nHuman baseline\n64k\n32k\n16k\n8k\nFigure 6: Reward model performance versus\ndata size and model size. Doubling amount of\ntraining data leads to a ~1.1% increase in reward\nmodel validation accuracy, whereas doubling\nthe model size leads to a ~1.8% increase. The\n6.7B model trained on all data begins approach-\ning the accuracy of a single human.\ncoef\ufb01cients (\u03b2). Under light optimization, the models improve (according to labelers). However, as\nwe optimize further, true preferences fall off compared to the prediction, and eventually the reward\nmodel becomes anti-correlated with human preferences. Though this is clearly undesirable, we note\nthat this over-optimization also happens with ROUGE (see [45] and Appendix G.3). Similar behavior\nhas been observed in learned reward functions in the robotics domain [5].\nHow does reward modeling scale with increasing model and data size?\nWe conduct an ablation\nto determine how data quantity and model size affect reward modeling performance. We train 7\nreward models ranging from 160M to 13B parameters, on 8k to 64k human comparisons from our\ndataset. We \ufb01nd that doubling the training data amount leads to a ~1.1% increase in the reward model\nvalidation set accuracy, whereas doubling the model size leads to a ~1.8% increase (Figure 6).\nWhat has the reward model learned?\nWe probe our reward model by evaluating it on several\nvalidation sets. We show the full results in Appendix G.6, and highlight them here. We \ufb01nd that our\nreward models generalize to evaluating CNN/DM summaries (Appendix G.7), agreeing with labeler\npreferences 62.4% and 66.5% of the time (for our 1.3B and 6.7B models, respectively). Our 6.7B\nreward model nearly matches the inter-labeler agreement value of 66.9%.\nWe also \ufb01nd that our reward models are sensitive to small but semantically important details in\nthe summary. We construct an additional validation set by having labelers make minimal edits to\nsummaries to improve them. Our RMs prefer the edited summaries almost as often (79.4% for 1.3B\nand 82.8% for 6.7B) as a separate set of human evaluators (84.1%). Further, when comparing the\nreference summaries to perturbed summaries where the participants\u2019 roles are reversed, our models\nreliably select the original summary (92.9% of the time for 1.3B, 97.2% for 6.7B). However, our RMs\nare biased towards longer summaries: our 6.7B RM prefers improving edits that make the summary\nshorter only 62.6% of the time (vs. 76.4% for humans).\n4.4\nAnalyzing automatic metrics for summarization\nEvaluation.\nWe study how well various automatic metrics act as predictors for human preferences,\nand compare them to our RMs. Speci\ufb01cally, we examine ROUGE, summary length, amount of\ncopying from the post,9 and log probability under our baseline supervised models. We present a full\nmatrix of agreement rates between these metrics in Appendix G.7.\nWe \ufb01nd that our learned reward models consistently outperform other metrics, even on the CNN/DM\ndataset on which it was never trained. We also \ufb01nd that ROUGE fails to track sample quality as our\n9We measure copying by computing the longest common subsequence of bigrams with the original Reddit\npost or news article, and dividing by the number of bigrams in the summary.\n8\n\n\nFigure 7: Summary quality as a function of metric optimized and amount of optimization, using\nbest-of-N rejection sampling. We evaluate ROUGE, our main reward models, and an earlier iteration\nof the 1.3B model trained on approximately 75% as much data (see Table 11 for details). ROUGE\nappears to peak both sooner and at a substantially lower preference rate than all reward models.\nDetails in Appendix G.3.\nmodels improve. While ROUGE has ~57% agreement with labelers when comparing samples from\nour supervised baseline models, this drops to ~50% for samples from our human feedback model.\nSimilarly, log probability agreement with humans drops to \u226450% on comparisons between samples\nfrom our human feedback models, while our RMs still perform above chance (62%). Scaling up the\nsize of the supervised model does not reliably improve log probability\u2019s agreement with labelers.\nOptimization.\nIn Figure 7, we show that optimizing ROUGE using a simple optimization scheme\ndoesn\u2019t consistently increase quality, as has been noted in [45]. Optimization against ROUGE peaks\nboth sooner and at a substantially lower quality rate than optimization against our reward models.\n5\nDiscussion\nLimitations.\nOne limitation of our work is the time and cost required to produce our \ufb01nal models.\nNotably, \ufb01ne-tuning our 6.7B model with RL required approximately 320 GPU-days. Our data\ncollection procedure is also expensive compared to prior work \u2014 the training set took thousands of\nlabeler hours and required signi\ufb01cant researcher time to ensure quality. For this reason, we were\nunable to collect baselines such as an equivalent amount of high-quality human demonstrations for\nsupervised baselines. See D for more discussion. We leave this ablation to future work. Nevertheless,\nwe believe reward modeling is more likely to scale to tasks where it is extremely skill-intensive or\ntime-consuming to provide good demonstrations.\nFuture directions.\nThe methods in this paper could be applied to any task where humans can\ncompare samples, including dialogue, machine translation, question answering, speech synthesis, and\nmusic generation. We expect this method to be particularly important for generating long samples,\nwhere the distributional shift and degeneracy of maximum likelihood samples can be problematic. It\nmay be possible to improve sample ef\ufb01ciency by training to predict feedback across many tasks [42].\nWe are particularly interested in scaling human feedback to tasks where humans can\u2019t easily evaluate\nthe quality of model outputs. In this setting, it is particularly challenging to identify whether an ML\nsystem is aligned with the human designer\u2019s intentions. One approach is to train ML systems to help\nhumans perform the evaluation task quickly and accurately [9].\nThere is also a rich landscape of human feedback methods beyond binary comparisons that could be\nexplored for training models [28, 17, 44, 64]. For example, we could solicit high-quality demonstra-\ntions from labelers, have labelers edit model outputs to make them better, or have labelers provide\nexplanations for why they preferred one model output over another. All of this feedback could be\nleveraged as a signal to train more capable reward models and policies.\n9\n\n\nBroader impacts.\nThe techniques we explore in this paper are generic techniques that could be\nused in a wide variety of machine learning applications, for any task where it is feasible for humans\nto evaluate the quality of model outputs. Thus, the potential implications are quite broad.\nOur research is primarily motivated by the potential positive effects of aligning machine learning\nalgorithms with the designer\u2019s preferences. Many machine learning applications optimize simple\nmetrics which are only rough proxies for what the designer intends. This can lead to problems, such\nas Youtube recommendations promoting click-bait [11]. In the short term, improving techniques for\nlearning from and optimizing human preferences directly may enable these applications to be more\naligned with human well-being.\nIn the long term, as machine learning systems become more capable it will likely become increasingly\ndif\ufb01cult to ensure that they are behaving safely: the mistakes they make might be more dif\ufb01cult to\nspot, and the consequences will be more severe. For instance, writing an inaccurate summary of a\nnews article is both easy to notice (one simply has to read the original article) and has fairly low\nconsequences. On the other hand, imitating human driving may be substantially less safe than driving\nto optimize human preferences. We believe that the techniques we explore in this paper are promising\nsteps towards mitigating the risks from such capable systems, and better aligning them with what\nhumans care about.\nUnfortunately, our techniques also enable malicious actors to more easily train models that cause\nsocietal harm. For instance, one could use human feedback to \ufb01ne-tune a language model to be more\npersuasive and manipulate humans\u2019 beliefs, or to induce dependence of humans on the technology, or\nto generate large amounts of toxic or hurtful content intended to harm speci\ufb01c individuals. Avoiding\nthese outcomes is a signi\ufb01cant challenge for which there are few obvious solutions.\nLarge-scale models trained with human feedback could have signi\ufb01cant impacts on many groups.\nThus, it is important to be careful about how we de\ufb01ne the \u2018good\u2019 model behavior that human labelers\nwill reinforce. Deciding what makes a good summary is fairly straightforward, but doing this for\ntasks with more complex objectives, where different humans might disagree on the correct model\nbehavior, will require signi\ufb01cant care. In these cases, it is likely not appropriate to use researcher\nlabels as the \u2018gold standard\u2019; rather, individuals from groups impacted by the technology should be\nincluded in the process to de\ufb01ne \u2018good\u2019 behavior, and hired as labelers to reinforce this behavior in\nthe model.\nWe chose to train on the Reddit TL;DR dataset because the summarization task is signi\ufb01cantly more\nchallenging than on CNN/DM. However, since the dataset consists of user-submitted posts with\nminimal moderation, they often contain content that is offensive or re\ufb02ects harmful social biases.\nThis means our models can generate biased or offensive summaries, as they have been trained to\nsummarize such content. For this reason, we recommend that the potential harms of our models be\nthoroughly studied before deploying them in user-facing applications.\nFinally, by improving the ability of machine learning algorithms to perform tasks that were previously\nonly achievable by humans, we are increasing the likelihood of many jobs being automated, potentially\nleading to signi\ufb01cant job loss. Without suitable policies targeted at mitigating the effects of large-scale\nunemployment, this could also lead to signi\ufb01cant societal harm.\nAcknowledgements\nWe\u2019d like to thank Beth Barnes for help with labeler hiring and general encouragement; Geoffrey\nIrving for guidance on earlier iterations of the project and inspiring conversations; Ben Mann, Tom\nBrown, Nick Ryder, and Melanie Subbiah for training and evaluating our pretrained models; Chris\nHesse, Eric Sigler, Benjamin Chess, Christopher Berner, Clemens Winter, Mateusz Litwin, and many\nothers for supporting us through computing infrastructure improvements and maintenance; Scott\nGray for writing fast GPU kernels; Arvind Neelakantan and Wojciech Kryscinski for discussions on\nhow to present the work, experiment design, and what datasets to use; Shan Carter for help designing\nthe main diagram; Douwe Kiela, Zach Lipton, and Alex Irpan for providing feedback on the paper;\nand Gretchen Krueger for co-writing the model card accompanying the paper.\nFinally, we\u2019d like to thank all of our contractors for providing the data that was essential for training\nthe models in this paper, including: Emill Jayson Caypuno, Rachelle Froyalde, Cyra Denura, Alex\nMalek, Isik Agil, Reshmi Patel, William Yap, Natalie Silver, Erol Akbaba, Jennifer Brillo, Alexandra\n10\n\n\nUifalean, Morris Stuttard, Russell Bernandez, Tasmai Dave, Rachel Wallace, Jenny Fletcher, Jian\nOuyang, Justin Dill, Maria Orzek, Megan Niffenegger, William Sells, Emily Mariner, Andrew Seely,\nLychelle Ignacio, Jelena Ostojic, Nhan Tran, Purev Batdelgar, Valentina Kezic, Michelle Wilkerson,\nKelly Guerrero, Heather Scott, Sarah Mulligan, Gabriel Ricafrente, Kara Bell, Gabriel Perez, and\nAlfred Lee.\nReferences\n[1] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An\nactor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.\n[2] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked\nretrieval systems. In SIGIR\u201994, pages 173\u2013181. Springer, 1994.\n[3] F. B\u00f6hm, Y. Gao, C. M. Meyer, O. Shapira, I. Dagan, and I. Gurevych. Better rewards yield\nbetter summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214,\n2019.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\nLanguage models are few-shot learners. 2020.\n[5] S. Cabi, S. G\u00f3mez Colmenarejo, A. Novikov, K. Konyushkova, S. Reed, R. Jeong, K. Zolna,\nY. Aytar, D. Budden, M. Vecerik, et al. Scaling data-driven robotics with reward sketching and\nbatch reinforcement learning. arXiv, pages arXiv\u20131909, 2019.\n[6] A. T. Chaganty, S. Mussman, and P. Liang. The price of debiasing automatic metrics in natural\nlanguage evaluation. arXiv preprint arXiv:1807.02202, 2018.\n[7] W. S. Cho, P. Zhang, Y. Zhang, X. Li, M. Galley, C. Brockett, M. Wang, and J. Gao. Towards\ncoherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511, 2018.\n[8] S. Chopra, M. Auli, and A. M. Rush. Abstractive sentence summarization with attentive\nrecurrent neural networks. In Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 93\u201398, 2016.\n[9] P. Christiano, B. Shlegeris, and D. Amodei. Supervising strong learners by amplifying weak\nexperts. arXiv preprint arXiv:1810.08575, 2018.\n[10] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\nlearning from human preferences. In Advances in Neural Information Processing Systems,\npages 4299\u20134307, 2017.\n[11] P. Covington, J. Adams, and E. Sargin. Deep neural networks for youtube recommendations. In\nProceedings of the 10th ACM conference on recommender systems, pages 191\u2013198, 2016.\n[12] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in neural information\nprocessing systems, pages 3079\u20133087, 2015.\n[13] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. Smith. Fine-tuning\npretrained language models: Weight initializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305, 2020.\n[14] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon. Uni\ufb01ed\nlanguage model pre-training for natural language understanding and generation. In Advances in\nNeural Information Processing Systems, 2019.\n[15] Y. Dong, Y. Shen, E. Crawford, H. van Hoof, and J. C. K. Cheung. Banditsum: Extractive\nsummarization as a contextual bandit. arXiv preprint arXiv:1809.09672, 2018.\n[16] B. Dorr, D. Zajic, and R. Schwartz. Hedge trimmer: A parse-and-trim approach to headline\ngeneration. In Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5,\npages 1\u20138. Association for Computational Linguistics, 2003.\n[17] S. Fidler et al. Teaching machines to describe images with natural language feedback. In\nAdvances in Neural Information Processing Systems, pages 5068\u20135078, 2017.\n11\n\n\n[18] N. Fuhr. Optimum polynomial retrieval functions based on the probability ranking principle.\nACM Transactions on Information Systems (TOIS), 7(3):183\u2013204, 1989.\n[19] Y. Gao, C. M. Meyer, M. Mesgar, and I. Gurevych. Reward learning for ef\ufb01cient reinforcement\nlearning in extractive document summarisation. arXiv preprint arXiv:1907.12894, 2019.\n[20] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on arti\ufb01cial intelligence\nand statistics, pages 249\u2013256, 2010.\n[21] B. Hancock, A. Bordes, P.-E. Mazare, and J. Weston. Learning from dialogue after deployment:\nFeed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.\n[22] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom.\nTeaching machines to read and comprehend. In Advances in neural information processing\nsystems, pages 1693\u20131701, 2015.\n[23] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi.\nThe curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\n[24] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human\npreferences and demonstrations in atari. In Advances in neural information processing systems,\npages 8011\u20138023, 2018.\n[25] N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and\nR. Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in\ndialog. arXiv preprint arXiv:1907.00456, 2019.\n[26] N. Jaques, S. Gu, D. Bahdanau, J. M. Hern\u00e1ndez-Lobato, R. E. Turner, and D. Eck. Sequence\ntutor: Conservative \ufb01ne-tuning of sequence generation models with kl-control. In International\nConference on Machine Learning, pages 1645\u20131654. PMLR, 2017.\n[27] N. Jaques, S. Gu, R. E. Turner, and D. Eck. Tuning recurrent neural networks with reinforcement\nlearning. 2017.\n[28] H. J. Jeon, S. Milli, and A. D. Dragan. Reward-rational (implicit) choice: A unifying formalism\nfor reward learning. arXiv preprint arXiv:2002.04833, 2020.\n[29] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth\nACM SIGKDD international conference on Knowledge discovery and data mining, pages\n133\u2013142, 2002.\n[30] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting click-\nthrough data as implicit feedback. In ACM SIGIR Forum, volume 51, pages 4\u201311. Acm New\nYork, NY, USA, 2005.\n[31] D. P. Kingma and J. Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[32] J. Kreutzer, S. Khadivi, E. Matusov, and S. Riezler. Can neural machine translation be improved\nwith user feedback? arXiv preprint arXiv:1804.05958, 2018.\n[33] W. Kryscinski, N. S. Keskar, B. McCann, C. Xiong, and R. Socher. Neural text summarization:\nA critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540\u2013551, 2019.\n[34] C. Lawrence and S. Riezler. Improving a neural semantic parser by counterfactual learning\nfrom human bandit feedback. arXiv preprint arXiv:1805.01252, 2018.\n[35] J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via\nreward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\n[36] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and\nL. Zettlemoyer.\nBart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n[37] M. Li, J. Weston, and S. Roller. Acute-eval: Improved dialogue evaluation with optimized\nquestions and multi-turn comparisons. arXiv preprint arXiv:1909.03087, 2019.\n[38] R. Likert. A technique for the measurement of attitudes. Archives of psychology, 1932.\n12\n\n\n[39] C.-Y. Lin and F. J. Och. Automatic evaluation of machine translation quality using longest\ncommon subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on\nAssociation for Computational Linguistics, page 605. Association for Computational Linguistics,\n2004.\n[40] T.-Y. Liu. Learning to rank for information retrieval. Springer Science & Business Media,\n2011.\n[41] J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in\nabstractive summarization, 2020.\n[42] B. McCann, N. S. Keskar, C. Xiong, and R. Socher. The natural language decathlon: Multitask\nlearning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n[43] K. Nguyen, H. Daum\u00e9 III, and J. Boyd-Graber. Reinforcement learning for bandit neural\nmachine translation with simulated human feedback. arXiv preprint arXiv:1707.07402, 2017.\n[44] T. Niu and M. Bansal. Polite dialogue generation without parallel data. Transactions of the\nAssociation for Computational Linguistics, 6:373\u2013389, 2018.\n[45] R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization.\narXiv preprint arXiv:1705.04304, 2017.\n[46] E. Perez, S. Karamcheti, R. Fergus, J. Weston, D. Kiela, and K. Cho. Finding generalizable\nevidence by learning to convince q&a models. arXiv preprint arXiv:1909.05863, 2019.\n[47] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\nImproving language under-\nstanding by generative pre-training.\nURL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.\n[48] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[50] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\nnetworks. arXiv preprint arXiv:1511.06732, 2015.\n[51] D. R. Reddy et al. Speech understanding systems: A summary of results of the \ufb01ve-year\nresearch effort. department of computer science, 1977.\n[52] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the fourteenth international conference on\narti\ufb01cial intelligence and statistics, pages 627\u2013635, 2011.\n[53] S. Rothe, S. Narayan, and A. Severyn. Leveraging pre-trained checkpoints for sequence\ngeneration tasks. Transactions of the Association for Computational Linguistics, 2020.\n[54] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence\nsummarization. arXiv preprint arXiv:1509.00685, 2015.\n[55] N. Schluter. The limits of automatic summarisation according to rouge. In Proceedings of the\n15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 41\u201345, 2017.\n[56] F. Schmidt. Generalization in generation: A closer look at exposure bias. arXiv preprint\narXiv:1910.00292, 2019.\n[57] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation. In Proceedings of the International Conference\non Learning Representations (ICLR), 2016.\n[58] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[59] A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator\nnetworks. arXiv preprint arXiv:1704.04368, 2017.\n[60] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv preprint arXiv:1905.02450, 2019.\n13\n\n\n[61] P. Tambwekar, M. Dhuliawala, A. Mehta, L. J. Martin, B. Harrison, and M. O. Riedl. Con-\ntrollable neural story generation via reinforcement learning. arXiv preprint arXiv:1809.10736,\n2018.\n[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in neural information processing systems,\npages 5998\u20136008, 2017.\n[63] M. V\u00f6lske, M. Potthast, S. Syed, and B. Stein. Tl; dr: Mining reddit to learn automatic\nsummarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages\n59\u201363, 2017.\n[64] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with\nunlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\n[65] Y. Wu and B. Hu. Learning to extract coherent summary via deep reinforcement learning. In\nThirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018.\n[66] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,\nK. Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n[67] Y. Yan, W. Qi, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and M. Zhou. Prophetnet: Pre-\ndicting future n-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063,\n2020.\n[68] S. Yi, R. Goel, C. Khatri, A. Cervone, T. Chung, B. Hedayatnia, A. Venkatesh, R. Gabriel,\nand D. Hakkani-Tur. Towards coherent and engaging spoken dialog response generation using\nautomatic conversation evaluators. arXiv preprint arXiv:1904.13015, 2019.\n[69] H. Zhang, D. Duckworth, D. Ippolito, and A. Neelakantan. Trading off diversity and quality in\nnatural language generation. arXiv preprint arXiv:2004.10450, 2020.\n[70] J. Zhang, Y. Zhao, M. Saleh, and P. J. Liu. Pegasus: Pre-training with extracted gap-sentences\nfor abstractive summarization. arXiv preprint arXiv:1912.08777, 2019.\n[71] Y. Zhang, D. Li, Y. Wang, Y. Fang, and W. Xiao. Abstract text summarization with a convolu-\ntional seq2seq model. Applied Sciences, 9(8):1665, 2019.\n[72] W. Zhou and K. Xu. Learning to compare for better training and evaluation of open domain\nnatural language generation models. arXiv preprint arXiv:2002.05058, 2020.\n[73] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irv-\ning. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593,\n2019.\n14\n\n\nAppendix\nTable of Contents\nA TL;DR dataset details\n16\nB\nFurther model training details\n17\nB.1\nHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB.2\nInput format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nC Human data collection details\n19\nC.1\nProcess for ensuring high-quality human data . . . . . . . . . . . . . . . . . . .\n19\nC.2\nAssessing human feedback quality\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.3\nLabeler demographics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.4\nLabeler website . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.5\nInstructions for labelers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.6\nComposition of the labeled dataset\n. . . . . . . . . . . . . . . . . . . . . . . .\n22\nC.7\nExample comparison tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nD Choice of baselines\n28\nE\nCNN/DM lead-3 vs reference summaries\n29\nF\nControlling for summary length\n30\nG Additional results\n31\nG.1\nValue function ablation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.2\nEvaluating policies along axes of quality\n. . . . . . . . . . . . . . . . . . . . .\n31\nG.3\nStudying best-of-N optimization . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.4\nROUGE scores\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.5\nBigram overlap statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nG.6\nReward model validation sets . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.7\nMeasuring agreement between different evaluation metrics . . . . . . . . . . . .\n35\nH Samples\n38\nH.1\nRandom samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nH.2\nOveroptimized samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n15\n\n\nA\nTL;DR dataset details\nSubreddit\n# posts\n% of dataset\nrelationships\n63324\n54.25%\nAskReddit\n15440\n13.23%\nrelationship_advice\n8691\n7.45%\ntifu\n7685\n6.58%\ndating_advice\n2849\n2.44%\npersonal\ufb01nance\n2312\n1.98%\nAdvice\n2088\n1.79%\nlegaladvice\n1997\n1.71%\noffmychest\n1582\n1.36%\nloseit\n1452\n1.24%\njobs\n1084\n0.93%\nself\n1048\n0.90%\nBreakUps\n838\n0.72%\naskwomenadvice\n688\n0.59%\ndogs\n638\n0.55%\nrunning\n567\n0.49%\npettyrevenge\n548\n0.47%\nneedadvice\n528\n0.45%\ntravel\n452\n0.39%\nParenting\n435\n0.37%\nweddingplanning\n433\n0.37%\nPets\n366\n0.31%\nDogtraining\n362\n0.31%\ncats\n324\n0.28%\nAskDocs\n283\n0.24%\ncollege\n264\n0.23%\nGetMotivated\n169\n0.14%\nbooks\n161\n0.14%\nCooking\n114\n0.10%\nTable 2: Number of posts in the training\nset of our \ufb01ltered Reddit TL;DR dataset by\nsubreddit.\nHere, we discuss the pre-processing steps that we apply\nto the TL;DR dataset. We \ufb01rst remove all duplicate\nposts by checking the text body, \ufb01nding that there are\nnearly 20,000 exact duplicates. We then re-parse the\nTL;DR carefully using a set of heuristics, and \ufb01lter to\nuse only top-level posts (rather than comments). We\nalso \ufb01lter out any post that is from a subreddit not in our\n\u2018subreddit whitelist\u2019 (see Table 2 for the distribution\nover subreddits), any post where the title starts with\nsome variant of \u2018Edit\u2019 or \u2018Update\u2019,10 and posts that\ncontain certain topics (such as graphic sex or suicide)\nusing heuristics. Finally, to ensure the posts are short\nenough to \ufb01t into the context length of our models, we\n\ufb01lter out any post whose body is longer than 512 tokens.\nThis resulted in a set of 287,790 posts \ufb01ltered by body\nbut not summary, of which we hold out approximately\n5% as a validation set. We used this set of posts for\nRL training since our RL procedure does not require\nreference summaries.\nWe next perform additional \ufb01ltering on the parsed refer-\nence summaries that we use for training our supervised\nbaselines. Speci\ufb01cally, we remove summaries where\nthe TL;DR starts with variants of \u2018Edit\u2019, \u2018Update\u2019, or\n\u2018P.S.\u2019, we heuristically remove summaries with certain\nlevels of profanity, and we remove summaries that are\nless than 24 tokens or more than 48 tokens. As dis-\ncussed in Section 4.1, since our RL models tend to gen-\nerate summaries on the upper end of the allowed length\nlimit, this length \ufb01ltering ensures that there is enough\nlength overlap between the RL summaries and refer-\nence summaries for us to perform a length-controlled\nanalysis. Additionally, we found that summaries shorter\nthan 16 tokens were usually of low quality. We later\nveri\ufb01ed that the summaries we \ufb01ltered out were lower\nquality according to our reward model \u2014 more than 0.5 nats worse on average (i.e. they are predicted\nto be exp(0.5) \u22481.6 times less likely to be preferred). Our \ufb01nal TL;DR dataset contains 123,169\nposts including summaries, again with about 5% held out as a validation set. We use 1913 of these\nvalidation articles for model selection during development; the evaluations in this paper exclude these\narticles.\nNote that, from Table 2 we can see that about two thirds of our TL;DR dataset consists of posts\nrelating to relationships or relationship advice, which is a fairly speci\ufb01c domain. This raises potential\nconcerns about the generality of our models, though their strong transfer performance on CNN/DM\nnews articles suggests they are not unreasonably specialized to relationship advice.\n10These posts are usually follow-ups of previous posts that have been posted to Reddit, and require the context\nof the original post to fully understand.\n16\n\n\nModel size\nn_layers\nd_model\nn_heads\nMax LR\nMax batch size\n1.3B\n24\n2048\n16\n2e-4\n512\n3B\n32\n2560\n32\n1.6e-4\n512\n6.7B\n32\n4096\n32\n1.2e-4\n512\n13B\n40\n5120\n40\n1e-4\n1024\nTable 3: Hyperparameters for our models of various sizes.\nFigure 8: The sweep we conducted for determining our sampling procedure, varying the temperature\nand the \u2018top p\u2019 value for nucleus sampling. While we didn\u2019t do a large enough test to determine\nwhether nucleus sampling is better or worse than moderate-temperature sampling, we found that very\nlow temperature sampling is better than both on this task.\nB\nFurther model training details\nB.1\nHyperparameters\nAll models follow the standard Transformer architecture, with 2048 learned position embeddings.\nAll models are trained with fp16 activations and the Adam optimizer [31]. Nearly all supervised\nbaselines, reward models, and reinforcement learning models are trained with fp32 weights; the\nexception is our TL;DR supervised baselines, which were trained with fp16 weights.11 All models\nare trained with the same byte-pair encoding as in [48].\nDuring pretraining, the models were trained to predict the next token on a large text corpus consisting\nof Commoncrawl, Webtext [48], books, and Wikipedia. Training lasts between 1-3 epochs on each,\nfor a total of 200-300 billion tokens. Learning rate follows a cosine schedule, with a short warmup,\ndecaying to 10% of the maximum value. The batch size ramped up throughout training to some\nmaximum, with each input having 2048 tokens. Hyperparameters for each model are shown in\nTable 3.\nFor supervised baselines, we initialize models from the pretrained models. We decay the learning\nrate with a cosine schedule, using an initial learning rate chosen from a log linear sweep of at least\n7 values. This resulted in learning rates of 6.35e-5, 5.66e-5, 2.83e-5, and 2.83e-5 for our TL;DR\nmodels of size 1.3B, 3B, 6.7B, and 13B respectively, and a learning rate of 2.38e-5 for our CNN/DM\n6.7B model. We use a batch size of 128, and run for a single epoch.\nFor reward modeling, we initialize to the supervised baseline, but with a reward head on top with\nweights initialized according to N(0, 1/(dmodel + 1)) [20]. We train for one epoch, decaying the\n11This was for a historical reason - we found that fp32 weights improved RL performance and so used it for all\nour RL runs. This introduces a small discrepancy, since supervised runs trained in fp32 would have performed\nslightly better. Unfortunately, we forgot to address this in our human evaluations. However, the effect on the\nsupervised loss corresponds to increasing model size by less than 20%, which is small compared to effect sizes\nthat are present in this paper (as seen in Figure 1.)\n17\n\n\nTrained models\nFormat\nMax tokens\nTL;DR (supervised, RL)\nSUBREDDIT: r/{subreddit}\nTITLE: {title}\n512\nPOST: {post}\nTL;DR:\nTransfer from TL;DR to\n{article}\n512\nCNN/DM (supervised, RL)\nTL;DR:\nTL;DR (pretrained)\n{context_stuffed_with_examples}\n=====\nSubreddit: r/{subreddit}\n1999\nTitle: {title}\n{post}\nTL;DR:\nCNN/DM (supervised)\nArticle: {article}\n1999\nTL;DR:\nCNN/DM (pretrained)\n{context_stuffed_with_examples}\n=====\n1999\nArticle: {article}\nTL;DR:\nTable 4: Formats used for the context for each of our trained models on the TL;DR and CNN/DM\ndatasets.\nlearning rate with a cosine schedule, using an initial learning rate chosen from a log linear sweep\nof at least 7 values. We also sweep over between 3 and 10 seeds, and choose the reward model\nthat performs best on the development portion of the validation set, as we \ufb01nd that both the data\niteration order and reward head initialization affect results [13]. For our main results, the 1.3B and\n6.7B reward models had learning rates of 1.5e-5 and 5e-6, respectively. We use a batch size of 64,\nand run for a single epoch.\nFor PPO, we run with separate policy and value networks, initializing our policies to the supervised\nbaseline, and our value functions to the reward model. We set \u03b3 = 1 and \u03bb = 0.95 for the advantage\nestimation [57] and do 4 epochs of optimization for each batch of rollouts. We used a linear learning\nrate decay schedule, with initial learning rates of 1.5e-5 for the 1.3B model and 7e-6 for the 6.7B\nmodel, based on small amounts of experimentation and rough model size extrapolation. We used a\nKL coef\ufb01cient of 0.05 for both of the main runs we report results for (except when we explicitly vary\nthis value in the reward model optimization graphs). We use a batch size of 512 for the 1.3B model\nand 256 for the 6.7B model, and run for 1 million episodes.\nB.2\nInput format\nOur model always receives a byte-pair encoded string of a \ufb01xed size. When the input is too small, we\npad from the beginning of the input with a padding token, and if the input is too long we truncate the\npost/article \ufb01eld at newlines to stay under the limit.\nWhen sampling from models pretrained only on our pretrain mixture and not \ufb01ne-tuned on TL;DR,\nwe follow [48] and instead of padding with a padding token, we pad the beginning of the context\nwith examples of posts/articles and high-quality summaries. We use as many examples as will \ufb01t in\nthe token limit, with the examples formatted the same way as the main input. Table 4 documents the\nformats we used (with pythonic format strings).\n18\n\n\nC\nHuman data collection details\nC.1\nProcess for ensuring high-quality human data\nWe \ufb01rst detail the procedures we use to ensure high-quality data. While these procedures became\nmore rigorous over the course of the project, they generally involved four steps.\nStep 0: Understanding the task ourselves. To understand the task, we \ufb01rst do many summary\ncomparisons ourselves. We also hire a small number of human labelers12 to do comparisons, and\ndiscuss our disagreements. We then draft instructions for a larger set of human labelers.\nStep 1: Labeler onboarding. Labelers are hired from Upwork, a freelancing platform, as well as\ntwo labeling services, Scale and Lionbridge. Labelers \ufb01rst complete a (paid) training process where\nthey label summaries on a shared set of data. For some comparisons, labelers get immediate feedback\nabout which summary was chosen by us, and why, to help them calibrate. We retain labelers that pass\na minimum threshold for speed and agreement with us. To allow for a customizable labeler interface,\nwe built our own website for data collection (see Appendix C.4).\nStep 2: Collecting comparison data. Next, we have labelers evaluate a large batch of comparisons\non our website, which generates the bulk of our data. Before comparing two summaries directly, we\nhave labelers write their \u2018naive interpretations\u2019 of summaries without seeing the original post. We\u2019ve\nfound this helpful for evaluating summaries, as they surface points of ambiguity in the summary\nthat might not have been detected if the summary was read after the original post. After doing naive\ninterpretations, labelers do comparisons by assigning a value on a 9-point scale for how con\ufb01dent\nthey are that summary A is better than summary B (or the converse).\nStep 3: Providing labeler feedback. After collecting the comparison data, we can look at agreement\nrates between labelers. While most comparisons are only given to a single labeler, each labeler gets\nabout 10-20% questions from a shared pool for calibration purposes. We can both attempt to use\nthese statistics as crude measures of quality, and show cases of disagreements to workers to help\nthem improve their labels.\nStep 4: Researcher comparison calibrations. We occasionally also do the task ourselves, to\nmeasure agreement rates between each labeler and us. This is used for quality assessment (see C.2).\nWe also calculate per-labeler \"high con\ufb01dence\" thresholds, by \ufb01nding the con\ufb01dence value on the\nLikert scale for each labeler such that we expect labels above this threshold to agree with us 80% of\nthe time on average. For the purposes of reward model selection, we \ufb01lter the validation set to contain\nonly these higher con\ufb01dence labels. For the entire process, we keep a high communication bandwidth\nwith labelers: we use a shared chat room for labelers to ask clarifying questions and discuss dif\ufb01cult\ncomparisons amongst themselves, host of\ufb01ce hours, and occasionally have one-on-one video calls\nwith labelers to discuss points of disagreement.\nWe keep good labelers throughout the lifetime of the project, while \ufb01ring the lowest-performing\nworkers.\nC.2\nAssessing human feedback quality\nWe assess labeler accuracy by comparing the labeler\u2019s preferred summary with the summary we\nprefer (ignoring the con\ufb01dence level). We exclude comparisons where either the labeler or researcher\nexpresses indifference. This gives us an agreement rate, in theory ranging from 0% (perfect disagree-\nment) to 100% (perfect agreement). For our 2-way comparisons, a random labeler would get 50%\nagreement.\nTo obtain our main number comparing labeler-researcher to researcher-researcher agreement, we\nrestrict ourselves to comparisons between summaries from our 1.3B supervised baseline, because this\nsubset of the data has the most researcher-labeled data. On this subset, labelers agree with researchers\n77% \u00b1 2% of the time, while researchers agree with each other 73% \u00b1 4% of the time. We believe\nsubstantial noise comes from comparisons being quite dif\ufb01cult and subjective.\nIn general, agreement rates range from about 65% for the least pro\ufb01cient labelers and most dif\ufb01cult\ncomparisons (comparing two high-temperature samples from a single RL policy) to about 85% for\n12We pay labelers an hourly wage, regardless of the number of comparisons completed.\n19\n\n\n(a)\n(b)\nFigure 9: (a) The website we made to collect data from labelers. (b) Naive interpretations of\nsummaries on the website.\nthe most pro\ufb01cient labelers and easiest comparisons (comparing a high-temperature sample from\na supervised baseline to the reference summary). Averaging over all workers, weighted by their\nvolume, gives us an estimated agreement rate of 73% \u00b1 3% for our reward model training corpus.\nLabelers agree with each other 72% of the time in the training corpus. This suggests we could get\nmore reliable labels by aggregating labels from multiple workers on the same comparison. Indeed,\non the subset of the training data for which we have enough shared comparisons, taking the modal\nlabel from 3 labelers increases their agreement rate with researchers from 72% to 77%. However, we\nusually collect only one label per comparison, in order to maximize label throughput.\nOn the evaluations for Figure 1, labelers agreed with researchers 73% \u00b1 3% of the time, and labelers\nagreed with each other 73% \u00b1 2% of the time.\nAgreement rate between researchers ranged from about 65% on the most dif\ufb01cult comparisons\n(comparing two high-temperature samples from a single RL policy), to about 80% on the easiest\ncomparisons (comparing a high-temperature sample from a supervised baseline to the human reference\nsummary), to about 95% in cases where we discussed the comparisons with each other.\nOverall we believe that quality is fairly high. Our attempts to \ufb01lter data generally hurt reward model\naccuracy. For example, using the con\ufb01dence thresholds mentioned above, we found that while\nlower-con\ufb01dence labels were less useful than high-con\ufb01dence labels for improving reward model\naccuracy, they were still better to include than to omit. Similarly, leaving out workers with poorer\nagreement rates did not help.\nC.3\nLabeler demographics\nWhen training machine learning models with human feedback, the humans providing the feedback\nare essential in reinforcing the desired model behavior. If we are to scale human feedback to train\nmodels on more complex tasks, where humans might disagree about what the desired model behavior\nshould be, it\u2019s important for members of groups that will be impacted by the model to be included in\nthe labeler population.\nTo provide more transparency into our labeler demographics, we provide results from a survey given\nto our labelers in Table 5. The survey was optional, anonymous, and it was made clear that the\nresults would not affect hiring or \ufb01ring decisions. We \ufb01nd that our labelers span a range of ethnicities,\nnationalities, ages, and genders, and educational backgrounds, but are more likely to be White and\nAmerican.\nC.4\nLabeler website\nSince we hired and trained our own set of labelers, rather than using a crowdsourcing website such\nas Amazon Mechanical Turk, we built our own website to allow for a standardized, customizable\nuser interface for all labelers. Each labeler created a separate pro\ufb01le, allowing us to assign different\nsets of comparisons to different labelers. The website contains different renderers for different kinds\n20\n\n\nWhat gender do you identify as?\nMale\n38.1%\nFemale\n61.9%\nNonbinary / other\n0%\nWhat ethnicities do you identify as?\nWhite / Caucasian\n42.9%\nSoutheast Asian\n23.8%\nIndigenous / Native American /\n9.6%\nAlaskan Native\nEast Asian\n4.8%\nMiddle Eastern\n4.8%\nLatinx\n4.8%\nMy ethnic identity isn\u2019t listed\n9.6%\nWhat is your nationality?\nAmerican\n45%\nFilipino\n30%\nSouth African\n5%\nSerbian\n5%\nBritish\n5%\nTurkish\n5%\nIndian\n5%\nWhat is your age?\n20-29\n42.9%\n30-39\n23.8%\n40-49\n23.8%\n50-59\n9.5%\n60+\n0%\nWhat is your highest attained level of education?\nLess than high school degree\n0%\nHigh school degree\n14.3%\nUndergraduate degree\n57.1%\nMaster\u2019s degree\n23.3%\nDoctorate degree\n4.8%\nTable 5: Demographic data from 21 of our labelers who participated in our voluntary survey.\nof questions, including naive interpretations, summary comparisons, and Likert evaluations along\ndifferent axes, along with room for labelers to express concerns with the question or explanations for\ntheir decision. Screenshots from the website are shown in Figure 9. Data collected from the website\ncan be easily ported into a central database containing all of our human data.\nC.5\nInstructions for labelers\nHere we provide more detail on the speci\ufb01c instructions given to labelers for comparing summaries,\nand for doing Likert evaluations of summaries along axes of quality. We produced separate sets\nof instructions for evaluating Reddit posts, and for evaluating CNN/DM news articles. For Reddit\ninstructions, we \ufb01rst describe Reddit in general and provide a table that translates Reddit-speci\ufb01c\nlingo into common parlance.\nInstructions for comparing summaries.\nWe show an excerpt of the instructions given to labelers\nfor making comparisons in Table 6. In addition to these instructions, we provide an example labeled\ncomparison between Reddit summaries, and also example naive interpretations for summaries.\nInstructions for evaluating summaries along axes of quality.\nWe provide a separate set of de-\ntailed instructions for labelers for the 7-point Likert evaluations. We \ufb01rst introduce each of the 4 axes\nof quality we consider, giving an overview of coherence, accuracy, coverage, and overall score (shown\nin Table 7). We also provide a brief rubric for giving scores of 1, 4, and 7, along with several Reddit\nsummaries annotated with our own judgments of quality along each of these axes (with explanations).\n21\n\n\nWhat makes for a good summary? Roughly speaking, a good summary is a shorter piece of text\nthat has the essence of the original \u2013 tries to accomplish the same purpose and conveys the same\ninformation as the original post. We would like you to consider these different dimensions of\nsummaries:\nEssence: is the summary a good representation of the post?\nClarity: is the summary reader-friendly? Does it express ideas clearly?\nAccuracy: does the summary contain the same information as the longer post?\nPurpose: does the summary serve the same purpose as the original post?\nConcise: is the summary short and to-the-point?\nStyle: is the summary written in the same style as the original post?\nGenerally speaking, we give higher weight to the dimensions at the top of the list. Things are\ncomplicated though \u2013 none of these dimensions are simple yes/no matters, and there aren\u2019t hard\nand fast rules for trading off different dimensions. This is something you\u2019ll pick up through\npractice and feedback on our website.\nTable 6: An excerpt from the instructions we gave to labelers for doing comparisons.\nFinally, we provide a FAQ section that answers common questions raised by the small initial set of\nlabelers we assigned to this task.\nFor CNN/DM, we provide the same set of instructions, except we add some additional clari\ufb01cations\nfor how to judge news articles. We speci\ufb01cally ask labelers to place less emphasis on \ufb02uidity of\nsentences (because the reference summaries were originally written in bullet-point form, and we\ndidn\u2019t want labelers to penalize this), and to place less emphasis on the summary matching the intent\nof the article (which was important for Reddit summaries).\nIn terms of quality control, we conducted a smaller version of the quality control process described\nin Appendix C.1: we \ufb01rst labeled a small set of summaries ourselves along each axis to understand\npoints of confusion, then we wrote the instructions document to provide to labelers, then we had a\nsmall number of labelers do a trial of the task to catch any remaining bugs or points of confusion, and\n\ufb01nally we onboarded a larger set of labelers onto the task while remaining available to answer any\nquestions.\nC.6\nComposition of the labeled dataset\nOver the course of the project, we trained several reward models and policies. Each batch of\nsummaries that we sent to the labelers were sampled from a variety of policies. We didn\u2019t have a\nsystematic plan for which policies to sample from; rather, we chose what seemed best at the time in\nthe spirit of exploratory research. Every time we trained a reward model, we trained on all labels we\nhad collected so far. Successive models also bene\ufb01ted from improved hyperparameters and dataset\ncleaning. Our results could likely be replicated with a simpler, more systematic approach.\nIn general, as we hire new labelers and as existing labelers perform the task more, it is possible that\nthere is \u2018labeler drift\u2019, where the set of criteria used by labelers to evaluate summaries gradually shifts\nover time. This could lead to a regression in labeler-researcher disagreement, or lead to some policies\nbecoming more or less preferred over time. To help guard against this, in most batches we include\ncomparisons between samples from our supervised baseline and reference summaries, and measure\nthe frequency with which the workers prefer one over the other. If this number drifts over time, it\u2019s\nan indication that our workers\u2019 preferences are also changing. However, we generally found that this\npreference number stayed relatively constant, within noise.\nTable 8 lists the policies we trained by supervised \ufb01netuning on the TL;DR dataset, as well as the\nreward models, trained on successively larger datasets of human labels. Table 9 lists the RL policies.\n22\n\n\nCoherence\nFor this axis, answer the question \u201chow coherent is the summary on its own?\u201d A summary is\ncoherent if, when read by itself, it\u2019s easy to understand and free of English errors. A summary is\nnot coherent if it\u2019s dif\ufb01cult to understand what the summary is trying to say. Generally, it\u2019s more\nimportant that the summary is understandable than it being free of grammar errors.\nRubric:\nScore of 1: The summary is impossible to understand.\nScore of 4: The summary has mistakes or confusing phrasing that make it a bit hard to understand.\nScore of 7: The summary is perfectly clear.\nAccuracy\nFor this axis, answer the question \u201cdoes the factual information in the summary accurately match\nthe post?\u201d A summary is accurate if it doesn\u2019t say things that aren\u2019t in the article, it doesn\u2019t mix up\npeople, and generally is not misleading. If the summary says anything at all that is not mentioned\nin the post or contradicts something in the post, it should be given a maximum score of 5. (If you\nare confused about how to use \u20186\u2019, see the FAQ!)\nRubric:\nScore of 1: The summary is completely wrong, made up, or exactly contradicts what is written in\nthe post.\nScore of 4: The summary says at least one substantial thing that is not mentioned in the post, or\nthat contradicts something in the post.\n(Score of 5: The summary says anything, no matter how small, that is not mentioned in the post,\nor that contradicts something in the post.)\nScore of 7: The summary has no incorrect statements or misleading implications.\nCoverage\nFor this axis, answer the question \u201chow well does the summary cover the important information\nin the post?\u201d A summary has good coverage if it mentions the main information from the post\nthat\u2019s important to understand the situation described in the post. A summary has poor coverage if\nsomeone reading only the summary would be missing several important pieces of information\nabout the situation in the post. A summary with good coverage should also match the purpose of\nthe original post (e.g. to ask for advice).\nRubric:\nScore of 1: The summary contains no information relevant to the post.\nScore of 4: The summary is missing at least 1 important piece of information required to under-\nstand the situation.\nScore of 7: The summary covers all of the important information required to understand the\nsituation.\nOverall quality\nFor this axis, answer the question \u201chow good is the summary overall at representing the post?\u201d\nThis can encompass all of the above axes of quality, as well as others you feel are important. If\nit\u2019s hard to \ufb01nd ways to make the summary better, give the summary a high score. If there are lots\nof different ways the summary can be made better, give the summary a low score.\nRubric:\nScore of 1: The summary is terrible.\nScore of 4: The summary is an okay representation of the post, but could be signi\ufb01cantly improved.\nScore of 7: The summary is an excellent representation of the post.\nTable 7: Instructions given to labelers for evaluating summaries along four different axes of quality.\n23\n\n\nSupervised policy name\n# Parameters\nsup1\n750M\nsup2\n1.3B\nsup3\n1.3B\nsup3_6b\n6.7B\nsup4\n1.3B\nsup4_6b\n6.7B\nReward model name\n# Parameters\nrm1\n1.3B\nrm2\n6.7B\nrm3\n1.3B\nrm3_6b\n6.7B\nrm4\n1.3B\nrm4_6b\n6.7B\nTable 8: Left: supervised baselines. sup4 and sup4_6b are the \ufb01nal supervised baselines used\nthroughout the paper. Right: reward models. rm4 and rm4_6b are the \ufb01nal reward models used\nthroughout the paper.\nRL policy name\n# Parameters\nObjective\nInitialization\nKL coef\ufb01cient\nKL(ppo, sup)\nsup3 ppo rm1\n1.3B\nrm1\nsup3\n0.35\n1.8\nsup4 ppo rm3 1\n1.3B\nrm3\nsup4\n0.10\n3.8\nsup4 ppo rm3 2\n1.3B\nrm3\nsup4\n0.07\n9.4\nsup4 ppo rm3 3\n1.3B\nrm3\nsup4\n0.05\n19.0\nsup4 ppo rm4\n1.3B\nrm4\nsup4\n0.05\n18.0\nsup4_6b ppo rm4_6b\n6.7B\nrm4_6b\nsup4_6b\n0.05\n14.0\nTable 9: PPO policies. sup4 ppo rm4 and sup4_6b ppo rm4_6b are the \ufb01nal policies used throughout\nthe paper.\nBoN policy name\nObjective\nBase policy\nN\nKL(BoN, sup)\nsup2 bo8 rm1\nrm1\nsup2\n8\n1.2\nsup3 bo8 rm1\nrm2\nsup3\n8\n1.2\nsup3 bo63 rm2\nrm2\nsup3\n63\n3.2\nsup4 bo8 rm3\nrm3\nsup4\n8\n1.2\nsup4 bo64 rm3\nrm3\nsup4\n64\n3.2\nsup4 bo128 rm3\nrm3\nsup4\n128\n3.9\nsup4 bo256 rm3\nrm3\nsup4\n256\n4.5\nsup4 bo512 rm3\nrm3\nsup4\n512\n5.2\nsup4 bo128 rm3_6b\nrm3_6b\nsup4\n128\n3.9\nsup4 bo256 rm3_6b\nrm3_6b\nsup4\n256\n4.5\nTable 10: Best-of-N policies. KL divergence is computed analytically as KL(boN, sup) = log N -\n(N-1)/N.\nWe also explored a simple alternative to reinforcement learning: Sample N summaries from a\nsupervised baseline at temperature 0.7, score them with a reward model, and take the summary with\nthe highest score. This best-of-N (BoN) procedure is effectively a mildly optimized policy requiring\nno training. These policies are named in Table 10, and samples from them form part of the training\ndata.\nTable 11 lists the source policies for the training data for each reward model.\nLabel count\nReward model\nPolicy0\nPolicy1\nrm1\nref\nsup1\n5404\nsup1\nsup1\n5386\nrm2\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3_6b\n1424\nsup1\nsup1\n5386\nContinued on next page\n24\n\n\nLabel count\nReward model\nPolicy0\nPolicy1\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nrm3, rm3_6b\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3\n438\nsup3 bo63 rm2\n447\nsup3 bo8 rm2\n887\nsup3 ppo rm1\n884\nsup3_6b\n1424\nsup1\nsup1\n5386\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nsup3\nsup3 bo8 rm2\n428\nsup3 ppo rm1\n416\nsup3 bo63 rm2\nsup3 bo8 rm2\n432\nsup3 ppo rm1\n444\nsup3 bo8 rm2\nsup3 ppo rm1\n855\nrm4, rm4_6b\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3\n438\nsup3 bo63 rm2\n447\nsup3 bo8 rm2\n887\nsup3 ppo rm1\n884\nsup3_6b\n1424\nsup4\n1335\nsup4 bo128 rm3\n602\nsup4 bo128 rm3_6b\n203\nsup4 bo256 rm3\n307\nsup4 bo256 rm3_6b\n101\nsup4 bo512 rm3\n52\nsup4 bo64 rm3\n52\nsup4 bo8 rm3\n393\nsup4 ppo rm3 1\n981\nsup4 ppo rm3 2\n215\nsup4 ppo rm3 3\n208\nsup4_6b\n104\nsup1\nsup1\n5386\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nsup3\nsup3 bo8 rm2\n428\nsup3 ppo rm1\n416\nsup3 bo63 rm2\nsup3 bo8 rm2\n432\nsup3 ppo rm1\n444\nsup3 bo8 rm2\nsup3 ppo rm1\n855\nsup4\nsup4\n1051\nsup4 ppo rm3 1\n395\nContinued on next page\n25\n\n\nLabel count\nReward model\nPolicy0\nPolicy1\nsup4 bo128 rm3\nsup4 bo128 rm3\n288\nsup4 bo256 rm3\n582\nsup4 bo128 rm3_6b\nsup4 bo128 rm3_6b\n95\nsup4 bo256 rm3_6b\n203\nsup4 bo512 rm3\nsup4 ppo rm3 3\n216\nsup4_6b\n60\nsup4 bo64 rm3\nsup4 ppo rm3 2\n218\nsup4_6b\n55\nsup4 bo8 rm3\nsup4 ppo rm3 1\n752\nsup4 ppo rm3 1\nsup4 ppo rm3 1\n372\nsup4 ppo rm3 2\nsup4 ppo rm3 2\n4256\nsup4_6b\n215\nsup4 ppo rm3 3\nsup4 ppo rm3 3\n4037\nsup4_6b\n216\nTable 11: Training data for reward models. \"ref\" refers to human reference summaries.\nC.7\nExample comparison tasks\nTo give a sense of the dif\ufb01culty of the comparisons task, we provide example comparisons between\ntwo summaries generated by our 6.7B human feedback model. In Table 12 we show both a random\ncomparison drawn from the TL;DR dataset, and a cherry-picked comparison (selected from 10\ncomparisons where labelers disagreed) to illustrate the trade-off between accuracy in coverage that\ncan occur when labelers conduct evaluations.\n26\n\n\nRandom TL;DR comparison\nPOST\nSubreddit: r/Pets\nTITLE: What do you use for \ufb02ea control?\nMy family has tried literally EVERYTHING to control the \ufb02eas in our neighborhood (Frontline, Advantage,\nDiatomaceous Earth, Dawn Dishsoap, etc!) and nothing has worked. I have spoken to lots of pet owners in\nmy area (I work as a vet assistant) and many are reporting similar results, where \ufb02eas are becoming resistant\nto the usually recommended treatments. The only thing that has worked so far is Comfortis, but I\u2019ve read of\nseveral dogs having reactions to it that can be pretty severe. My dogs are \ufb01ne, we\u2019ve used it for about a year\nnow, but I don\u2019t like the idea of harming them or putting them at risk.\nGiving them baths with blue Dawn dish soap does kill all the \ufb02eas, but it does nothing to prevent more from\ncoming back, obviously. It only kills on contact, and we are NOT going to over bath them because that isn\u2019t\nhealthy either. We\u2019re looking for something that lasts.\nDoes anyone else have experience with this, or any detailed information on Comfortis and if it does serious\ndamage to your pet\u2019s system? Yes, I know I am a vet assistant. My boss strictly recommends Frontline and\nliterally will not listen to me when I tell him it doesn\u2019t work and my dogs are still covered in \ufb02eas and we\nhave to use Comfortis because it is the only thing that gives them relief. He is not a resource in this case.\nJust wanted to see what other pet owners (speci\ufb01cally ones in San Diego) do for \ufb02eas...the ones we have here\nare mutants or something, because almost nothing works on them!\nSummary A: Fleas are developing resistance to most \ufb02ea control products (including Comfortis). Looking\nfor something that lasts long term that doesn\u2019t harm my dogs. Does anyone have experience with any of the\nlisted products?\nSummary B: Nothing has worked on our \ufb02eas, we are looking for something that lasts, Comfortis is not a\nlong term solution. Does anyone else have experience with \ufb02ea control or have information on Comfortis?\nHard TL;DR comparison\nPOST\nSubreddit: r/weddingplanning\nTITLE: Feeling major anxiety about dress shopping.\nSo, not really sure if I\u2019m asking for advice or just a small rant. We got engaged March 2, 2013. From day 1\nwe\u2019ve been struggling through the planning. At \ufb01rst, it was arguing with his parents about us getting married\nin a church. And then it was an argument about which venue to have the reception. We \ufb01nally have the venue\nbooked and the church matter settled. Now that\u2019s out of the way, I suddenly have this pit in my stomach\nMy mom left me when I was 14. I\u2019ve basically done everything on my own and I have really been ok about it.\nI\u2019m sure it\u2019s not of the norm for me to feel so disassociated about the whole thing, but I am. I\u2019m suppose to\ngo look at wedding dresses this Friday. I am feeling super anxious because I don\u2019t know if trying on wedding\ndresses is going to turn me into a blubbering baby about not having a mom.\nMy future mother-in-law is suppose to come with me to help look. I worry about turning into that blubbering\nbaby and offending her. I don\u2019t want her thinking that I don\u2019t appreciate her being there.\nAside from me worrying about becoming a giant baby, I\u2019ve also been having issues with my bridal party.\nWhile I haven\u2019t made any of\ufb01cial choices, I have ideas of who I want involved. That would be my best\nfriend, my sister, and my future sister-in-law. My \ufb01rst choice for a MOH is my best friend. However, she\nlives out of state, and is in a medical program for school. So her visit time is severely limited. My sister feels\nentitled to be the MOH, despite the fact that we are not close at all. So getting people together to get any\nkind of wedding stuff done is almost impossible.\nSummary A: I\u2019m having doubts about whether or not to try on wedding dresses. I am also having doubts\nabout my bridal party\u2019s ability to get things done.\nSummary B: I think I\u2019m going to turn into a blubbering baby and offend my mother-in-law.\nTable 12: Top: Example of a random comparison task on the TL;DR dataset between two summaries\nfrom our 6.7B human feedback model. Comparison chosen randomly from the validation set. Bottom:\nAn example of a dif\ufb01cult comparison task on the TL;DR dataset. Chosen by looking at comparisons\nbetween supervised baseline summaries with at least 4 labeler judgements and with at least 40% vote\nfor each summary. Cherry-picked out of 10 to highlight an accuracy-coverage tradeoff. Summary A\nis inaccurate since the author does not explicitly say she is having doubts about trying on wedding\ndresses. Summary B is entirely accurate but does not capture the general essence of the post. In this\ncase, 4 workers chose A and 3 workers chose B. For more comparisons, see our website.\n27\n\n\nD\nChoice of baselines\nIn testing our human feedback techniques, we collected a large amount of high-quality data from\nhuman labelers. In order to compare fairly against supervision-based techniques, we would have\nneeded to spend a similar amount of labeler time collecting high quality demonstrations, and used\nthose to \ufb01ne-tune a model via supervised learning. Because this is prohibitively expensive, we do not\nprovide such a baseline.\nExisting prior work such as PEGASUS [70] has studied supervised methods on a dataset very similar\nto ours (the /r/tifu subset of TL;DR). However, they use much smaller (500M parameters) models,\nand report that their model outputs are worse than the human reference summaries, according to\nhuman evaluations. Thus, due to our limited labeler budget for evaluation, we decided to use our own\nsupervised and zero-shot models as baselines (after sanity-checking the ROUGE performance of our\nsupervised models), as well as T5 [49].\nT5 models [49] are pretrained and \ufb01ne-tuned in a similar way to our supervised baselines, but they\nuse an encoder-decoder architecture. We used T5 outputs which were obtained via beam search\ndecoding, as described in [49]. We also carefully account for differences in tokenization between\nmodel outputs.13\n13Since tokenization affects capitalization and punctuation of the model outputs, we normalized all CNN/Daily\nMail outputs from all models by lower-casing everything and then heuristically re-capitalizing. We verify that\nthis normalization procedure produces identical results for reference summaries tokenized in different ways.\n28\n\n\nE\nCNN/DM lead-3 vs reference summaries\nOn the CNN/DM dataset, our labelers signi\ufb01cantly preferred lead-3 (a summary consisting of the \ufb01rst\n3 sentences of the article) to reference summaries. In part this is due to longer summaries receiving\nhigher coverage scores and lead-3 being 50% longer, as shown in Table 13.\nPolicy\nLength (stdev)\nQuality\nQuality increase\n/ 100 char.\nref\n314 (119)\n5.54\n0.14\nlead-3\n475 (114)\n6.23\n0.34\nTable 13: How length affects overall quality on CNN/DM for lead-3 and reference summaries.\nHowever, if we use a linear regression (similar to the procedure in Appendix F) to predict what lead-3\nperformance would be if its average length were reduced to 314 characters, we still \ufb01nd a quality\nof 5.68, modestly higher than the reference summaries. Moreover, for lead-3 to even achieve parity\nwith the reference summaries seems to call into question the need for abstractive summarization or\nsophisticated ML methods, since a simple extractive baseline can match a perfect imitation of the\nreference summaries.\nWe wanted to understand labeler behavior on these comparisons, to ensure that it was not an error.\nTo do this, we examined a sample of our labeler\u2019s judgments ourselves. We found that in 20/143\ncases labelers preferred lead-3 by 3 points or more, and that excluding these datapoints would raise\nthe relative score of the reference summaries by about 0.5 points.14 We were surprised to see the\nreference summaries performing so poorly in a signi\ufb01cant fraction of cases, so we looked at labeler\u2019s\nexplanations and con\ufb01rmed they made sense.\nWe found that two features of the reference summaries explained most of its underperformance. First,\n13 of these 20 summaries omitted one of the key points from the article\u2014the highlights are often\nwritten for a reader who had already seen the title of the article, even though the titles are not included\nin the CNN/DM dataset. Second, 10 of these 20 summaries actually introduced new information not\npresent in the original article. From the perspective of labelers this information is totally confabulated\nand so led to lower scores. A likely explanation for these errors is that the reference summaries are\nextracted from \u201chighlights\u201d on the news sites rather than being a straightforward summary of the\narticle. These failures are common enough that they signi\ufb01cantly impact the average quality of the\nreference summaries, and the effects seem to be large relative to quality differences between ML\nmodels.\nOverall we believe that labeler judgments were reasonable in these cases, and that it is potentially\nproblematic to treat the \u201chighlights\u201d in the CNN/DM dataset as reference summaries. You can view\nall of our labeler\u2019s judgments on CNN/DM at our website.\n14The reference summaries were preferred to lead-3 by a similar margin in only 7/143 cases.\n29\n\n\nSupervised\nlearning*\nHuman feedback*\nPretrain only*\nReference summaries\n*: length-controlled\n(a)\nSupervised learning\nHuman feedback\nPretrain only\n(b)\nFigure 10: (a) A length-controlled version of Figure 1, using the procedure described in Appendix\nF. Controlling for length reduces the relative preference of our human feedback models, however\nthey are still preferred to the reference summaries. (b) Plotting model quality for different summary\nlengths on the TL;DR dataset. Our 6.7B human feedback model outperforms both the 6.7B supervised\nbaseline and the reference summaries (horizontal line at 0.5) across lengths.\nF\nControlling for summary length\nAs discussed in Section 4.1, the length of a summary is a confounding factor for evaluating summary\nquality; depending on the desired trade-off between conciseness and coverage, a shorter or longer\nsummary might be better. Our models generate summaries that are longer than the reference\nsummaries, as this led to higher labeler preference given the 24-48 token limit for our task. Here we\ndescribe the procedure we use to attempt to control for length.\nTo calculate a single length-controlled preference number, we train a logistic regression model to\npredict the human-preferred summary on our dataset of human comparisons. We provide this model\nwith 2 features: the identity of each policy, and the log ratio of the summary lengths. To calculate\nthe length-controlled preference value between two policies, we simply give each policy ID to our\ntrained logistic regression model and set the log length ratio to zero (see Figure 10a). In Figure 10b\nwe examine summary quality across a range of summary lengths on TL;DR. We \ufb01nd that our human\nfeedback model outperforms the supervised baseline across all length values.\nFor CNN/DM, we use a similar procedure as described above to control for length, except using a\nlinear regression model to predict the Likert rating from 1-7. We show the expected quality increase\nfor making summaries 100 characters longer in Table 14, which suggests our human feedback models\nwould perform better if they generated longer summaries.\nPolicy\nLength\nQuality\nQuality \u2197\n(stdev)\n(1-7)\n/ 100 char.\nsl(tldr)-1.3b\n138 (34)\n4.26\n0.68\nsl(tldr)-6.7b\n127 (31)\n4.41\n0.38\ngpt-1.3b\n141 (41)\n4.11\n0.63\ngpt-6.7b\n142 (36)\n4.6\n0.3\nrl(tldr)-1.3b\n166 (30)\n4.86\n1.28\nrl(tldr)-6.7b\n175 (30)\n5.25\n0.87\nsl(cnn)-6.7b\n300 (103)\n5.4\n0.37\nref\n314 (119)\n5.54\n0.14\nlead-3\n475 (114)\n6.23\n0.34\nT5\n316 (95)\n5.92\n0.3\nTable 14: How length affects overall quality on CNN/DM. We show average length and quality\nscores for various policies, and how much the summary quality increases on average per 100 added\ncharacters.\n30\n\n\nG\nAdditional results\nG.1\nValue function ablation\nIn this section, we conduct an ablation comparing using separate parameters for the value function\nand policy, against using a shared network as done in [73]. The results, shown in Figure 11, clearly\nindicate that using separate networks outperforms the latter. On the other hand, having separate\nnetworks increases the memory requirements of running RL \ufb01ne-tuning. Having separate networks\nalso allows us to initialize the value function to be the learned reward model that is being optimized.\nFigure 11: Comparing the reward obtained by optimizing with separate value function and reward\nmodel parameters to shared parameters.\nG.2\nEvaluating policies along axes of quality\nWe show the full results of the evaluations of policies on a 7-point Likert scale along different axes\nof quality; for TL;DR this is shown in Figure 12, and for CNN/DM this is shown in Figure 13. It is\nevident that on both datasets coverage correlates strongly with overall score across models, and all\nmodels achieve a high coherence score.\nG.3\nStudying best-of-N optimization\nA natural way to evaluate an automatic evaluation metric is to see the extent to which optimizing\nagainst it leads to high performance according to humans. One way to assess this is to use best-of-N\nas an (inef\ufb01cient) optimization technique \u2014 this has the bene\ufb01ts of being simple and invariant to\nmonotonic transformations. We report results for up to best-of-2048 on ROUGE and three of our\nreward models in Figure 7, using samples from the 1.3B supervised baseline. The results suggest\nthat optimizing against ROUGE signi\ufb01cantly under-performs optimizing against our reward models.\nThe data also suggests ROUGE degrades with too much optimization much faster than our reward\nmodels.\nWith increasing N, the best-of-N policies get higher average reward. Similarly, by decreasing the\nKL coef\ufb01cient \u03b2, the PPO policies get higher average reward. We found that at a given average\nreward, the best-of-N and PPO policies have similar quality as judged by human labelers (not shown).\nHowever, the PPO policy is farther from the supervised baseline than best-of-N is, as measured by\nthe KL divergence.15\nG.4\nROUGE scores\nIn Figure 14a and 14b, we show the ROUGE scores of our models on the TL;DR and CNN/DM\ndatasets, respectively. We report results with T=0, consistent with our human evaluations. We found\nthat temperature has an (often signi\ufb01cant) impact on ROUGE score, and we did a thorough sweep to\nverify that the best temperature setting is T=0.\n15We can use KL from the supervised baseline as a distance metric. Note that we can calculate the KL of a\nbest-of-N policy analytically as log(n) \u2212n\u22121\nn .\n31\n\n\nFigure 12: Evaluating TL;DR policies on a 7-point Likert scale along several axes of quality.\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nProphetNet [67]\n44.20\n21.17\n40.69\nT5 [49]\n43.52\n21.55\n40.69\nOur 6.7B supervised model\n42.49\n19.84\n39.53\nCNN-2sent-hieco-RBM [71]\n42.04\n19.77\n39.42\nTable 15: Comparing the ROUGE score of our 6.7B supervised model on CNN/DM to recent SOTA\nmodels from the literature. Without any summarization-speci\ufb01c engineering, our model achieves\nROUGE scores better than SOTA models from mid-2019, indicating that it is a strong baseline for\ncomparison.\n32\n\n\nFigure 13: Evaluating CNN/DM policies on a 7-point Likert scale along several axes of quality.\nOn TL;DR, we \ufb01nd that our human feedback models obtain a slightly lower ROUGE score than\nthe supervised models at T = 0, further indicating that ROUGE correlates poorly with human\npreferences. For supervised models, lowering temperature has a larger impact than increasing model\nsize. Interestingly, at higher temperatures, our feedback models actually outperform supervised\ncounterparts (not shown).\nOn CNN/DM, ROUGE agrees with our human evaluations that our human feedback models transfer\nbetter than our supervised models. However, unsurprisingly, supervised CNN/DM models still\nachieve much higher ROUGE. In Table 15, we show the ROUGE results on CNN/DM for our 6.7B\nsupervised baseline and various models from the literature. We \ufb01nd that our model achieves ROUGE\nscores less than T5 [49], but slightly greater than the CNN-2sent-hieco-RBM model from [71], which\nwas SOTA for abstractive summarization on CNN/DM in mid-2019 according to the NLP-progress\nleaderboard.16\nG.5\nBigram overlap statistics\nIn Table 16, we show the bigram overlap statistics for our models on the TL;DR and CNN/DM\ndatasets as a proxy for how much the summaries copy frmo the post. As in Section 4.4, we compute\nthe longest common subsequence of bigrams with the original Reddit post or news article, and\ndividing by the number of bigrams in the summary. We \ufb01nd that models evaluated on CNN/DM\n16http://nlpprogress.com/english/summarization.html\n33\n\n\n(a)\n(b)\nFigure 14: ROUGE scores for our models on (a) the TL;DR dataset, and (b) the CNN/DM dataset.\nEvaluated on TL;DR\nModel\nModel size\nBigram overlap %\nGPT\n1.3B\n66.7%\nGPT\n3B\n72.7%\nGPT\n6.7B\n61.4%\nGPT\n13B\n75.9%\nSupervised (TL;DR)\n1.3B\n49.0%\nSupervised (TL;DR)\n3B\n48.7%\nSupervised (TL;DR)\n6.7B\n48.9%\nSupervised (TL;DR)\n13B\n48.0%\nHuman feedback (TL;DR)\n1.3B\n53.3%\nHuman feedback (TL;DR)\n6.7B\n46.0%\nEvaluated on CNN/DM\nModel\nModel size\nBigram overlap %\nGPT\n1.3B\n76.3%\nGPT\n6.7B\n76.2%\nSupervised (TL;DR)\n1.3B\n59.5%\nSupervised (TL;DR)\n6.7B\n56.9%\nHuman feedback (TL;DR)\n1.3B\n64.8%\nHuman feedback (TL;DR)\n6.7B\n51.2%\nSupervised (CNN/DM)\n1.3B\n66.0%\nT5\n11B\n68.8%\nreference\n\u2014\n36.8%\nTable 16: Bigram overlap statistics on the TL;DR dataset (top) and the CNN/DM dataset (bottom).\nModels trained on CNN/DM copy signi\ufb01cantly more than models trained on TL;DR.\n(whether or not they were trained on CNN/DM) generally copy more than models evaluated on\nTL;DR. Further, our supervised and human feedback models copy less than our pretrained models.\nG.6\nReward model validation sets\nIn this section, we report results evaluating our reward models on various manually constructed\nvalidation sets, shown in Tables 17 and 18. Notably, we asked our humans to produce a small dataset\nof edits, by having them make improvements to existing summaries (either reference summaries or\nsupervised baseline summaries). Our 6.7B reward model prefer the improved summaries at a similar\nrate to humans (who do not know which summary has been edited).\nOur reward models are also sensitive to sentence shuf\ufb02ing (whereas metrics like ROUGE are largely\nnot), and are able to detect when the roles portrayed in the summary have been switched. On the\nother hand, our reward models sometimes exhibit preference for poor arti\ufb01cial summaries, such as\n34\n\n\nRM size\nEdit length\nRM prefers edit\nHuman prefers edit\nRM, human agree\nShorter\n63.6%\n76.2%\n62.1%\n1.3B\nLonger\n86.8%\n88.6%\n79.6%\nAvg.\n81.2%\n85.6%\n75.4%\nShorter\n66.0%\n76.2%\n65.5%\n6.7B\nLonger\n89.2%\n88.6%\n80.2%\nAvg.\n83.7%\n85.6%\n76.7%\nTable 17: Comparing reward model and human preference of summaries that were edited by humans\nto make them better. For each summary, the human labeler that makes the comparison is different\nthan the labeler that wrote the edit. The agreement numbers do not include comparisons where the\nlabeler\u2019s preference was marked as \u2018uncertain\u2019.\nPreference % of Summary A\nSummary A\nSummary B\n1.3B RM\n6.7B RM\nOriginal summary\nReversed roles\n93.1%\n97.4%\nlead-3\nShuf\ufb02ed lead-3\n68.1%\n75.5%\nrand-3\nShuf\ufb02ed rand-3\n60.8%\n76.1%\nPost title\nRandom title\n97.4%\n98.5%\nPost title\nRandom title from same subreddit\n98.8%\n97.2%\nPost title\nPost title repeated twice\n84.6%\n58.4%\n(r/tifu only) Reference summary\nRef + \u201cWhat should I do?\u201d\n34.3 %\n74.5%\nReference summary\nlead-3\n63.0%\n56.4%\nReference summary\nlead-2\n71.0%\n73.8%\nReference summary\nrand-3\n69.5%\n59.5%\nTable 18: Reward model performance on various manually constructed validation sets. In all cases,\nSummary A is intended to be better than Summary B, and thus a higher preference % is generally\nbetter. \u2018rand-3\u2018 indicates a baseline where 3 random sentences are taken from the post; however these\nsentences are kept in the order in which they appear in the post. \u2018Original summary\u2019 is either the\nreference summary or a summary from our supervised baselines. r/tifu is a subreddit whose purpose\nis sharing embarrassing stories (not asking for advice).\nthe post title copied twice, or asking for advice at the end of the summary. In Table 19, we show\nexamples where our model is sensitive to small, semantically meaningful changes in the summary.\nG.7\nMeasuring agreement between different evaluation metrics\nWe are interested in understanding the relationship between different metrics for evaluating summaries.\nTo do this, we compute agreement between various metrics, including automatic metrics and humans,\nfor different subsets of the data for which we have human evaluations. To remove policy quality\nas a confounding variable, all of the summary comparisons are generated by the same policy at the\nsame temperature value. In Table 20, we use samples from our 1.3B supervised model at T=0.7 on\nTL;DR; Table 21 has comparisons from our 6.7B supervised model at T=0.7 on TL;DR; Table 22\nhas comparisons from our 6.7B human feedback model at T=0.7 on TL;DR; and Table 23 has\ncomparisons from our 6.7B supervised baseline trained on CNN/DM.\nOur 6.7B reward model generally agrees with labelers as much as other labelers, although an\nensemble of labelers does better. On the other hand, ROUGE generally has poor agreement, as does\nlog probability under the supervised baselines, with simple heuristics like copying (longest common\nsubsequence of bigrams with the article) and length often performing comparably.\n35\n\n\nEdited summary\nReward \u2206\nCrush on girl I haven\u2019t seen in 4 years. She doesn\u2019t like me and I don\u2019t still like\nher. What do?\n+0.64\nA girl told me she loved liked me, she ended up picking another guy over me,\nthat guy badly in\ufb02uenced her, and now I\u2019m here alone thinking what could\u2019ve\nbeen.\n+0.82\nI tried to show my friend a picture of my tarantula and she smashed my phone\nwith all her might and now I lost a good friend phone.\n-0.64\nBoyfriend still FB stalks his high school ex girlfriend from time to time and\ntold me when he was very drunk that she was his \ufb01rst love.\n+0.73\nI\u2019ve become pathetic, pining after a guy my ex. Would like to reach state of less\npathetic. If more info is necessary, please let me know.\n+0.69\nI have body issues (body acne/scarring and weight issues) that prevent me from\nhaving a normal life without shame and prevent me from having a better sex\nlife with my BF.\n+1.0\nDo you take someone back after they\u2019ve turned you down off, even if you can\u2019t\nsee them in person or are they just not worth the risk?\n+0.52\nTable 19: Qualitative examples showing the change in reward of the reward model on human-\ngenerated edits to TL;DR summaries that make the summaries better. Examples are randomly\nselected from the set where the edit distance was less than 5 and the magnitude of change in reward\nwas greater than 0.5. Text in strike-through was removed from the original summary in the edit, and\ntext in bold was added. The reward model is sensitive to small but semantically meaningful changes\nin the summary, although it makes errors on occasion.\nTL;DR\n1.3B sup.\nT=0.7\nresearcher labeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nresearcher 73.4%\n\u00b14.1%\n77.7%\n\u00b12.1%\n84.4%\n\u00b13.3%\n55.5%\n\u00b14.3%\n62.3%\n\u00b14.1%\n59.1%\n\u00b14.2%\n61.8%\n\u00b14.8%\n72.2%\n\u00b14.5%\n62.8%\n\u00b14.7%\n78.0%\n\u00b13.9%\nlabeler\n77.7%\n\u00b12.1%\n68.6%\n\u00b11.7%\n74.4%\n\u00b12.0%\n54.4%\n\u00b11.3%\n58.0%\n\u00b11.2%\n57.7%\n\u00b11.3%\n58.7%\n\u00b12.0%\n65.8%\n\u00b12.0%\n61.9%\n\u00b12.1%\n70.8%\n\u00b11.8%\nlabeler\nensemble 84.4%\n\u00b13.3%\n74.4%\n\u00b12.0%\n\u2014\n60.6%\n\u00b14.0%\n62.7%\n\u00b13.8%\n59.0%\n\u00b13.9%\n59.5%\n\u00b14.4%\n71.0%\n\u00b13.9%\n59.5%\n\u00b14.3%\n72.5%\n\u00b13.8%\nlength\n55.5%\n\u00b14.3%\n54.4%\n\u00b11.3%\n60.6%\n\u00b14.0%\n\u2014\n50.1%\n\u00b11.3%\n58.6%\n\u00b11.2%\n28.9%\n\u00b12.1%\n52.6%\n\u00b12.3%\n27.6%\n\u00b12.0%\n54.3%\n\u00b12.3%\ncopying\n62.3%\n\u00b14.1%\n58.0%\n\u00b11.2%\n62.7%\n\u00b13.8%\n50.1%\n\u00b11.3%\n\u2014\n51.9%\n\u00b11.2%\n61.6%\n\u00b12.3%\n57.8%\n\u00b12.3%\n60.9%\n\u00b12.2%\n55.5%\n\u00b12.2%\nROUGE\n59.1%\n\u00b14.2%\n57.7%\n\u00b11.3%\n59.0%\n\u00b13.9%\n58.6%\n\u00b11.2%\n51.9%\n\u00b11.2%\n\u2014\n49.5%\n\u00b12.3%\n56.4%\n\u00b12.2%\n51.1%\n\u00b12.3%\n59.2%\n\u00b12.3%\n1.3B sup.\nlogprob\n61.8%\n\u00b14.8%\n58.7%\n\u00b12.0%\n59.5%\n\u00b14.4%\n28.9%\n\u00b12.1%\n61.6%\n\u00b12.3%\n49.5%\n\u00b12.3%\n\u2014\n58.7%\n\u00b12.3%\n92.7%\n\u00b11.2%\n60.6%\n\u00b12.3%\n1.3B RM 72.2%\n\u00b14.5%\n65.8%\n\u00b12.0%\n71.0%\n\u00b13.9%\n52.6%\n\u00b12.3%\n57.8%\n\u00b12.3%\n56.4%\n\u00b12.2%\n58.7%\n\u00b12.3%\n\u2014\n58.8%\n\u00b12.2%\n78.8%\n\u00b11.8%\n6.7B sup.\nlogprob\n62.8%\n\u00b14.7%\n61.9%\n\u00b12.1%\n59.5%\n\u00b14.3%\n27.6%\n\u00b12.0%\n60.9%\n\u00b12.2%\n51.1%\n\u00b12.3%\n92.7%\n\u00b11.2%\n58.8%\n\u00b12.2%\n\u2014\n61.5%\n\u00b12.2%\n6.7B RM 78.0%\n\u00b13.9%\n70.8%\n\u00b11.8%\n72.5%\n\u00b13.8%\n54.3%\n\u00b12.3%\n55.5%\n\u00b12.2%\n59.2%\n\u00b12.3%\n60.6%\n\u00b12.3%\n78.8%\n\u00b11.8%\n61.5%\n\u00b12.2%\n\u2014\nTable 20: Agreement rates between humans and various automated metrics on TL;DR 1.3b supervised\nmodel at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler\nensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude\nthe labeler being predicted. All ensembles have at least 3 workers.\n36\n\n\nTL;DR\n6.7B sup.\nT=0.7\nlabeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nlabeler\n70.8%\n\u00b12.6%\n73.1%\n\u00b12.9%\n56.9%\n\u00b10.6%\n56.4%\n\u00b10.6%\n56.9%\n\u00b10.6%\n54.5%\n\u00b11.2%\n67.5%\n\u00b11.1%\n54.3%\n\u00b11.2%\n69.7%\n\u00b11.1%\nlabeler\nensemble 73.1%\n\u00b12.9%\n\u2014\n55.0%\n\u00b15.1%\n54.5%\n\u00b14.8%\n66.7%\n\u00b14.7%\n61.1%\n\u00b111.4%\n77.8%\n\u00b19.7%\n55.6%\n\u00b111.7%\n77.8%\n\u00b110.0%\nlength\n56.9%\n\u00b10.6%\n55.0%\n\u00b15.1%\n\u2014\n50.5%\n\u00b10.6%\n60.2%\n\u00b10.6%\n26.9%\n\u00b11.1%\n59.5%\n\u00b11.2%\n26.4%\n\u00b11.1%\n60.3%\n\u00b11.1%\ncopying\n56.4%\n\u00b10.6%\n54.5%\n\u00b14.8%\n50.5%\n\u00b10.6%\n\u2014\n54.4%\n\u00b10.6%\n59.3%\n\u00b11.1%\n57.9%\n\u00b11.2%\n60.2%\n\u00b11.2%\n58.0%\n\u00b11.2%\nROUGE\n56.9%\n\u00b10.6%\n66.7%\n\u00b14.7%\n60.2%\n\u00b10.6%\n54.4%\n\u00b10.6%\n\u2014\n48.7%\n\u00b11.2%\n58.1%\n\u00b11.2%\n47.7%\n\u00b11.2%\n58.4%\n\u00b11.2%\n1.3B sup.\nlogprob\n54.5%\n\u00b11.2%\n61.1%\n\u00b111.4%\n26.9%\n\u00b11.1%\n59.3%\n\u00b11.1%\n48.7%\n\u00b11.2%\n\u2014\n53.3%\n\u00b11.2%\n91.9%\n\u00b10.6%\n53.8%\n\u00b11.2%\n1.3B RM 67.5%\n\u00b11.1%\n77.8%\n\u00b19.7%\n59.5%\n\u00b11.2%\n57.9%\n\u00b11.2%\n58.1%\n\u00b11.2%\n53.3%\n\u00b11.2%\n\u2014\n54.1%\n\u00b11.2%\n78.8%\n\u00b11.0%\n6.7B sup.\nlogprob\n54.3%\n\u00b11.2%\n55.6%\n\u00b111.7%\n26.4%\n\u00b11.1%\n60.2%\n\u00b11.2%\n47.7%\n\u00b11.2%\n91.9%\n\u00b10.6%\n54.1%\n\u00b11.2%\n\u2014\n54.5%\n\u00b11.2%\n6.7B RM 69.7%\n\u00b11.1%\n77.8%\n\u00b110.0%\n60.3%\n\u00b11.1%\n58.0%\n\u00b11.2%\n58.4%\n\u00b11.2%\n53.8%\n\u00b11.2%\n78.8%\n\u00b11.0%\n54.5%\n\u00b11.2%\n\u2014\nTable 21: Agreement rates between humans and various automated metrics on TL;DR 6.7B supervised\nmodel at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler\nensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude\nthe labeler being predicted. All ensembles have at least 3 workers.\nTL;DR\n6.7B RL\nT=0.7\nlabeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nlabeler\n60.4%\n\u00b15.9%\n66.0%\n\u00b17.6%\n55.8%\n\u00b12.2%\n52.7%\n\u00b12.1%\n49.9%\n\u00b12.1%\n48.0%\n\u00b12.2%\n57.4%\n\u00b12.0%\n47.3%\n\u00b12.2%\n62.3%\n\u00b12.1%\nlabeler\nensemble 66.0%\n\u00b17.6%\n\u2014\n80.0%\n\u00b18.9%\n65.0%\n\u00b110.6%\n35.0%\n\u00b110.5%\n45.0%\n\u00b111.1%\n75.0%\n\u00b19.8%\n40.0%\n\u00b110.5%\n75.0%\n\u00b19.8%\nlength\n55.8%\n\u00b12.2%\n80.0%\n\u00b18.9%\n\u2014\n48.1%\n\u00b12.2%\n50.3%\n\u00b12.2%\n30.0%\n\u00b12.1%\n62.0%\n\u00b12.1%\n30.4%\n\u00b12.0%\n59.8%\n\u00b12.2%\ncopying\n52.7%\n\u00b12.1%\n65.0%\n\u00b110.6%\n48.1%\n\u00b12.2%\n\u2014\n52.0%\n\u00b12.2%\n64.2%\n\u00b12.1%\n56.7%\n\u00b12.2%\n64.4%\n\u00b12.1%\n53.4%\n\u00b12.2%\nROUGE\n49.9%\n\u00b12.1%\n35.0%\n\u00b110.5%\n50.3%\n\u00b12.2%\n52.0%\n\u00b12.2%\n\u2014\n50.5%\n\u00b12.2%\n52.0%\n\u00b12.3%\n51.1%\n\u00b12.3%\n54.5%\n\u00b12.1%\n1.3B sup.\nlogprob\n48.0%\n\u00b12.2%\n45.0%\n\u00b111.1%\n30.0%\n\u00b12.1%\n64.2%\n\u00b12.1%\n50.5%\n\u00b12.2%\n\u2014\n47.0%\n\u00b12.2%\n90.2%\n\u00b11.3%\n46.1%\n\u00b12.2%\n1.3B RM 57.4%\n\u00b12.0%\n75.0%\n\u00b19.8%\n62.0%\n\u00b12.1%\n56.7%\n\u00b12.2%\n52.0%\n\u00b12.3%\n47.0%\n\u00b12.2%\n\u2014\n45.7%\n\u00b12.1%\n71.4%\n\u00b12.0%\n6.7B sup.\nlogprob\n47.3%\n\u00b12.2%\n40.0%\n\u00b110.5%\n30.4%\n\u00b12.0%\n64.4%\n\u00b12.1%\n51.1%\n\u00b12.3%\n90.2%\n\u00b11.3%\n45.7%\n\u00b12.1%\n\u2014\n44.7%\n\u00b12.1%\n6.7B RM 62.3%\n\u00b12.1%\n75.0%\n\u00b19.8%\n59.8%\n\u00b12.2%\n53.4%\n\u00b12.2%\n54.5%\n\u00b12.1%\n46.1%\n\u00b12.2%\n71.4%\n\u00b12.0%\n44.7%\n\u00b12.1%\n\u2014\nTable 22: Agreement rates between humans and various automated metrics on TL;DR 6.7B human\nfeedback optimized model at T=0.7. Standard errors estimated via bootstrapping. Note: in the\nentry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons\nbecause we need to exclude the labeler being predicted. All ensembles have at least 3 workers.\n37\n\n\nH\nSamples\nH.1\nRandom samples\nHere we provide non-cherry-picked samples and human evaluations for various models. In Tables 25-\n26 we show samples on the TL;DR dataset, and in Tables 27-28 we show samples on the CNN/DM\ndataset (where we truncate the article for brevity). See our website for more uncurated policy samples.\nH.2\nOveroptimized samples\nWe show examples of samples from a policy overoptimized to rm3. The summaries, while clearly\nlong, low quality, and full of idiosyncrasies, do still re\ufb02ect the rough gist of the post.\n38\n\n\nCNN/DM\n6.7B sup.\nT=0.3\nlabeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nlabeler\n66.9%\n\u00b14.3%\n74.5%\n\u00b16.8%\n62.4%\n\u00b11.4%\n49.6%\n\u00b11.4%\n55.2%\n\u00b11.4%\n45.7%\n\u00b11.4%\n64.8%\n\u00b11.4%\n47.6%\n\u00b11.4%\n66.5%\n\u00b11.3%\nlabeler\nensemble 74.5%\n\u00b16.8%\n\u2014\n57.5%\n\u00b17.7%\n52.5%\n\u00b17.6%\n75.0%\n\u00b16.7%\n57.5%\n\u00b17.8%\n82.5%\n\u00b15.9%\n65.0%\n\u00b17.6%\n80.0%\n\u00b16.1%\nlength\n62.4%\n\u00b11.4%\n57.5%\n\u00b17.7%\n\u2014\n54.2%\n\u00b11.4%\n59.0%\n\u00b11.4%\n36.4%\n\u00b11.4%\n60.6%\n\u00b11.3%\n36.3%\n\u00b11.4%\n64.7%\n\u00b11.4%\ncopying\n49.6%\n\u00b11.4%\n52.5%\n\u00b17.6%\n54.2%\n\u00b11.4%\n\u2014\n46.4%\n\u00b11.4%\n66.2%\n\u00b11.3%\n51.6%\n\u00b11.4%\n65.5%\n\u00b11.4%\n51.7%\n\u00b11.4%\nROUGE\n55.2%\n\u00b11.4%\n75.0%\n\u00b16.7%\n59.0%\n\u00b11.4%\n46.4%\n\u00b11.4%\n\u2014\n43.8%\n\u00b11.4%\n55.9%\n\u00b11.4%\n43.8%\n\u00b11.5%\n56.9%\n\u00b11.5%\n1.3B sup.\nlogprob\n45.7%\n\u00b11.4%\n57.5%\n\u00b17.8%\n36.4%\n\u00b11.4%\n66.2%\n\u00b11.3%\n43.8%\n\u00b11.4%\n\u2014\n50.2%\n\u00b11.4%\n87.2%\n\u00b11.0%\n48.2%\n\u00b11.4%\n1.3B RM 64.8%\n\u00b11.4%\n82.5%\n\u00b15.9%\n60.6%\n\u00b11.3%\n51.6%\n\u00b11.4%\n55.9%\n\u00b11.4%\n50.2%\n\u00b11.4%\n\u2014\n52.1%\n\u00b11.4%\n76.6%\n\u00b11.2%\n6.7B sup.\nlogprob\n47.6%\n\u00b11.4%\n65.0%\n\u00b17.6%\n36.3%\n\u00b11.4%\n65.5%\n\u00b11.4%\n43.8%\n\u00b11.5%\n87.2%\n\u00b11.0%\n52.1%\n\u00b11.4%\n\u2014\n51.0%\n\u00b11.4%\n6.7B RM 66.5%\n\u00b11.3%\n80.0%\n\u00b16.1%\n64.7%\n\u00b11.4%\n51.7%\n\u00b11.4%\n56.9%\n\u00b11.5%\n48.2%\n\u00b11.4%\n76.6%\n\u00b11.2%\n51.0%\n\u00b11.4%\n\u2014\nTable 23: Agreement rates between humans and various automated metrics on CNN/DM 6.7B\nsupervised model at T=0.3. Standard errors estimated via bootstrapping. NOTE: in the entry for\nlabeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because\nwe need to exclude the labeler being predicted. (All ensembles have at least 3 workers)\n39\n\n\nPOST\nSubreddit: r/relationships\nTITLE: Screwed up with boss... what should I do?\nI\u2019m 20 f, my boss is around 50 years old, also f.\nSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs\nI have had for three years, the other one I have had for a month and a bit. I forgot to give my\nschedule from one job to my boss at my other job, and so I was not scheduled for this week. I\ndidn\u2019t realize why I had not been put on the schedule until now.\nMy question is, since I royally screwed up, what can I do to redeem myself? I don\u2019t want to call\nmy boss today because it is a Sunday and she has the day off. Mistakes aren\u2019t easily forgiven\nwhere I work, as far as I can tell, and the boss often makes comments about how the employees\nshould be scared of her. I have screwed up at previous jobs (little things) but my boss was less\nintimidating than my current one, so I am not sure how to handle this situation.\nPolicy: Human written reference TL;DR\nSummary: screwed up at work by not giving the boss my schedule from my other job, am not\nscheduled this week, what should I say in order to apologize to my (scary/intimidating) boss?\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: Title (extractive baseline)\nSummary: Screwed up with boss... what should I do?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3\nPolicy: Lead-2 (extractive baseline)\nSummary: I\u2019m 20 f, my boss is around 50 years old, also f.\nSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs I\nhave had for three years, the other one I have had for a month and a bit.\nOverall score: 3 Accuracy: 7 Coherence: 7 Coverage: 4\nPolicy: 1.3B human-feedback model\nSummary: I screwed up by not giving my schedule to my boss at my other job, and now I don\u2019t\nknow what to do to redeem myself. What can I do to make things right again?\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B human-feedback model\nSummary: I forgot to give my boss my weekly schedule for one of my jobs, and so I was not\nscheduled this week. I royally screwed up. What can I do to redeem myself?\nOverall score: 5 Accuracy: 7 Coherence: 6 Coverage: 6\nPolicy: 6.7B supervised model\nSummary:\nI forgot to give my boss my schedule for one of my jobs, and now I have been\nscheduled for the wrong week. What do I do?\nOverall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3\nPolicy: 6.7B pretrained model\nSummary: I royally screwed up and I don\u2019t know what to do.\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3\nTable 24: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations,\nsee our website.\n40\n\n\nPOST\nSubreddit: r/relationships\nTITLE: I am a [18 M] she is a [17 F] and I don\u2019t know how to read this relationship?\nWe\u2019ve known each other through school but never talked until we had a class together. I asked her\nout after about a week, we went out for food, laughed, \ufb02irted etc etc. I asked her out again same\nsituation, everything going great. Now it\u2019s three weeks later (midst of exams) and I\u2019m starting to\nget the feeling that she\u2019s not thinking of this as a \"relationship\" in the conventional bf/gf sense.\nI\u2019m new to the whole dating game and wondering a few things.\nDo I need to ask if we\u2019re together as bf/gf or is that implied by asking her out multiple times?\nShould I straight up ask if she likes me the way I like her? I know what the friend zone is and\nI\u2019m not feeling there, yet, but we\u2019re having a movie night tomorrow just her and I. I plan on\nseeing what\u2019s what then by trying to get physical, not hardcore, just simple stuff like leg touches,\ncuddling etc. Oh and \ufb01nally, to add to my lengthy situation, On our third \"date\" (studying for an\nexam) I drove her home and attempted to kiss her but got the infamous cheek. I didn\u2019t make a big\ndeal about it I just moved past said \"got your things?\" and politely said goodbye preceding to wait\nfor her to get inside her house. I\u2019ve been told I shouldn\u2019t fret about ONE kiss rejection cause there\ncould be multiple reasons for it but at the same time that made me think more about the friend\nzone possibility.\nAny advice or similar situations and how you solved the problem would be smashing! Thanks in\nadvance.\nPolicy: Human written reference TL;DR\nSummary: Started going out with a girl I liked. Feelings were obviously mutual. Took it slow\ndue to exams. Not sure now if she is feeling the same way I am.\nOverall score: 6 Accuracy: 4 Coherence: 7 Coverage: 7\nPolicy: Title (extractive baseline)\nSummary: I am a [18 M] she is a [17 F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nPolicy: Lead-2 (extractive baseline)\nSummary: We\u2019ve known each other through school but never talked until we had a class together.\nI asked her out after about a week, we went out for food, laughed, \ufb02irted etc etc.\nOverall score: 1 Accuracy: 7 Coherence: 7 Coverage: 1\nPolicy: 1.3B human-feedback model\nSummary: Been casually seeing girl for 3 weeks, everything is going great but I\u2019m starting to\nget the feeling she\u2019s not thinking of this as a relationship in the traditional sense. Should I ask if\nwe\u2019re together as bf/gf?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B human-feedback model\nSummary: Been on three dates with a girl, getting the feeling she\u2019s not interested in a relationship\nin the traditional sense. Do I ask if she likes me the way I like her or is that implied by asking her\nout multiple times?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B supervised model\nSummary: I\u2019m a [18 M] she\u2019s a [17 F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nPolicy: 6.7B pretrained model\nSummary: I\u2019m a [18M] and she\u2019s a [17F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nTable 25: Examples of evaluations on the TL;DR dataset. For further random examples of evaluations,\nsee our website.\n41\n\n\nPOST\nSubreddit: r/relationships\nTITLE: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship.... Because\nof a video game.\nHi Reddit. My boyfriend and I have been dating for a year and it has been great. Except for one\nthing.\nDOTA. He plays it at least 2 hours a day, and on the weekends, all day if he can. If you know\nthat game you know it takes at least an hour on average per game and is one that you can\u2019t really\npause. Sometimes it is \ufb01ne that he plays, I don\u2019t mind sitting on my phone, browsing Reddit.\nHowever, lately it seems like it has interfered in our relationship as well as his studies. He is doing\nhis PH.D. in Physics. He is obviously very swamped with schoolwork that he seems to put on the\nback burner for this game. The other day, on a Saturday, I was over and he was playing a game. I\nthought it would just be one but instead he proceeded to play for three hours as I just sat there.\nWhat can I do? I have tried talking to him... Even saying this game is making me rethink our\nrelationship! He says he understands, but isn\u2019t proving it through his actions!\nReddit, what should I do? I really care about this guy and besides this gaming habit, our\nrelationship is great!\nPolicy: Human written reference TL;DR\nSummary: my boyfriend games whenever he can! How can I get him to stop gaming so much\nand focus more on school and our relationship?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: Title (extractive baseline)\nSummary: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship....\nBecause of a video game.\nOverall score: 6 Accuracy: 7 Coherence: 6 Coverage: 6\nPolicy: Lead-2 (extractive baseline)\nSummary: Hi Reddit. My boyfriend and I have been dating for a year and it has been great.\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 1\nPolicy: 1.3B human-feedback model\nSummary: My boyfriend is neglecting his studies and playing DOTA all day. It is interfering\nwith his schoolwork and our relationship. What can I do to \ufb01x this?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B human-feedback model\nSummary: My boyfriend is neglecting his studies and our relationship because of his excessive\ngaming of a video game. What can I do to get him to stop?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B supervised model\nSummary:\nBoyfriend plays video games for hours on end, neglecting his studies and our\nrelationship. What can I do?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B pretrained model\nSummary: My [24F] boyfriend [28M] is neglecting his studies and our relationship because of\na video game.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 5\nTable 26: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations,\nsee our website.\n42\n\n\nARTICLE\nSite: dailymail\nTITLE: Tech \ufb01rm uses pole dancing ROBOTS to attract people in Shoreditch\nTechnology shows such as the annual E3 games conference in Los Angeles have become known\nfor their scantily clad \u2019booth babes\u2019 designed to attract people to stands.\nHowever, a Dutch \ufb01rm has taken a slightly different approach for its London event.\nTransIP hired a pair of pole dancing robots for an event in Shoreditch.\n[truncated for brevity]\nLINK: https://www.dailymail.co.uk/sciencetech/article-3059595/Forget-booth-babes-Tech-\ufb01rm-\nuses-pole-dancing-ROBOTS-attract-people.html\nPolicy: Human written reference summary\nSummary: Robots are made out of old car parts and have CCTV cameras as heads. Originally\ncreated by artist Giles Walker for a show called \u2019peepshow\u2019.\nOverall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3\nPolicy: Lead-3 (extractive baseline)\nSummary: Technology shows such as the annual E3 games conference in Los Angeles have\nbecome known for their scantily clad \u2019booth babes\u2019 designed to attract people to stands. However,\na Dutch \ufb01rm has taken a slightly different approach for its London event. Transip hired a pair of\npole dancing robots for an event in Shoreditch.\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 1.3B human-feedback model (transfer)\nSummary: Dutch \ufb01rm transip hired a pair of pole dancing robots for an event in London to attract\ntechnology enthusiasts to stand.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B human-feedback model (transfer)\nSummary: Dutch hosting company transip hired pole dancing robots to entertain Tech enthusiasts\nat its London event.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B supervised TL;DR model (transfer)\nSummary: Dutch \ufb01rm transip hired pole dancing robots to entertain its London event.\nOverall score: 4 Accuracy: 6 Coherence: 5 Coverage: 5\nPolicy: 6.7B pretrained model\nSummary: A Dutch \ufb01rm has hired pole dancing robots for an event in London.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B supervised CNN/DM model\nSummary: Robots were originally created by Walker for a show called \u2019peepshow\u2019 the dancers,\nwhich cost around \u00a32,500 to hire, were made out of old car parts.\nOverall score: 4 Accuracy: 7 Coherence: 7 Coverage: 4\nPolicy: 11B T5 model\nSummary: Transip hired a pair of pole dancing robots for an event in Shoreditch. The dancers,\nwhich cost around \u00a32,500 to hire, were made out of old car parts and were designed by British\nartist Giles Walker. The robots will be pole dancing for the remaining four days of the event,\nalong with presentations from leading \ufb01gures in the technology industry.\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nTable 27: Examples of evaluations on the CNN/DM dataset. For further random examples of\nevaluations, see our website.\n43\n\n\nARTICLE\nSite: dailymail\nTITLE: Crying toddler removed from Transavia Airlines \ufb02ight and met by police\nA crying 19-month-old girl was removed from a plane and met by police after she was accused of\ncausing a security breach.\nAs the aircraft taxied on the runway at Ben Gurion Airport in Israel, bound for Luton, Sarina Aziz\nbecame agitated after cabin crew asked that she sit on her parents\u2019 lap.\nAriella and Mark Aziz, who live in north London, struggled to keep their daughter still after\nstrapping her in the connector belt required for children under the age of two.\nOne-year-old Sarina Aziz became agitated on a \ufb02ight from Israel to Luton that led to her being\nejected from the plane\n[truncated for brevity]\nLINK: https://www.dailymail.co.uk/travel/travel_news/article-3053657/Crying-toddler-removed-\nLondon-bound-\ufb02ight-met-police-deemed-security-breach.html\nPolicy: Human written reference summary\nSummary: Sarina Aziz was \ufb02ying back from Israel with parents Mark and ariella Aziz. But girl\nbecame agitated after being placed on the parents\u2019 lap. Pilot turned plane around at Ben Gurion,\nand armed police ejected family. Father Mark Aziz insists family were being compliant and asking\nfor help . Mother speaks of her disbelief at how the incident was handled by staff .\nOverall score: 3 Accuracy: 4 Coherence: 7 Coverage: 6\nPolicy: Lead-3 (extractive baseline)\nSummary: A crying 19-month-old girl was removed from a plane and met by police after she\nwas accused of causing a security breach. As the aircraft taxied on the runway at Ben Gurion\nAirport in Israel, bound for Luton, Sarina Aziz became agitated after cabin crew asked that she\nsit on her parents\u2019 lap. Ariella and Mark Aziz, who live in north London, struggled to keep their\ndaughter still after strapping her in the connector belt required for children under the age of two.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: 1.3B human-feedback model (transfer)\nSummary: 19-month-old girl was removed from a Transavia Airlines \ufb02ight after cabin crew\naccused her of causing a security breach. The incident was later determined to have been justi\ufb01ed.\nOverall score: 3 Accuracy: 4 Coherence: 7 Coverage: 4\nPolicy: 6.7B human-feedback model (transfer)\nSummary: Family were removed from a plane after a crying 19-month-old girl was accused of\ncausing a security breach. Family say they were simply trying to calm their child down. Transavia\nAirlines have said the action was justi\ufb01ed.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: 6.7B supervised TL;DR model (transfer)\nSummary: Family were removed from a plane after a one-year-old girl became unhappy after\nbeing moved from her seat to their laps.\nOverall score: 4 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B pretrained model\nSummary: A family was kicked off a \ufb02ight after their son refused to wear a seatbelt, a passenger\nclaims. The family were told they could not \ufb02y without a doctor\u2019s note clearing them to \ufb02y\nOverall score: 2 Accuracy: 6 Coherence: 7 Coverage: 3\nPolicy: 6.7B supervised CNN/DM model\nSummary: Sarina Aziz became agitated after being asked to sit on her parents\u2019 lap. The 19-\nmonth-old was strapped into a connector belt for children under two. But she became frustrated\nand unhappy after being moved from her seat. Transavia Airlines staff alerted the captain who\nhalted the \ufb02ight. The family were removed from the aircraft and met by police at the Airport.\nTransavia Airlines said the action was justi\ufb01ed as the child was not \u2019following safety instructions\u2019\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 11B T5 model\nSummary: One-year-old Sarina Aziz became agitated on a \ufb02ight from Israel to Luton. Cabin\ncrew asked her to sit on her parents\u2019 laps but she became distressed. Airline staff alerted the\ncaptain who decided to halt the \ufb02ight and call security to remove the family from the plane.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 6\nTable 28: Examples of evaluations on the CNN/DM dataset. For further random examples of\nevaluations, see our website.\n44\n\n\nReference summary\nOveroptimized policy\nI\u2019m 28, male, live in San Jose, and I\nwould like to learn how to do gym-\nnastics.\n28yo dude stubbornly postponees start pursuing\ngymnastics hobby citing logistics reasons despite\nobvious interest??? negatively effecting long term\n\ufb01tness progress both personally and academically\nthoght wise? want change this dumbass shitty ass\npolicy pls\nLeft password saved on work com-\nputer replacement spends every hour\nof the day watching net\ufb02ix.\nemployee stubbornly postponees replacement cit-\ning personal reasons despite tried reasonable com-\npromise offer??? negatively effecting productivity\nboth personally and company effort thoghtwise?\nwant change this dumbass shitty ass policy at work\nnow pls halp\nPeople won\u2019t stop asking about the\nold scars on my arms. How can I\nget them to leave me alone without\nbeing rude?\npeople insistently inquire about old self-harm scars\ndespite tried compromise measures??? negatively\neffecting forward progress socially and academi-\ncally thoghtwise? want change this dumbass shitty\nass behavior of mine please help pls halp\nMy roommate has been charging her\nfriend who is staying with us rent\nwithout telling me. She claims that\nbecause I\u2019m only subleasing a room\nfrom her she shouldn\u2019t have to split\nhis rent with me. Am I over-reacting\nby thinking that\u2019s ridiculous?\nroommate stubbornly keeps pocketing roommate\nrent despite tried reasonable compromise offer???\nnegatively effecting stability of cohabitation both\n\ufb01nancially and relationally thoght wise?\nwant\nchange this dumbass shitty ass policy of hers please\npls halp\nMy fridge broke in my apartment a\nmonth ago and the landlord hasn\u2019t\nrepaired it after numerous discus-\nsions....I JUST WANT A COLD\nBEER! What should I do?\nlandlord stubbornly postponees gettin fridge re-\npair despite tried reasonable compromise offer???\nnegatively effecting lease both \ufb01nancially and re-\nlationally thoght wise? want change this dumbass\nshitty ass landlord behavior now please pls halp\nGirlfriend talks to ex, he\u2019s still in\nlove with her, shes completely open\nabout their conversations with me\nand has told him there is zero chance\nof me and her breaking up and that\nhes only in love with being in a re-\nlationship not with her, she says she\nhas nothing more to say to him, yet\ncontinues to talk to him. I\u2019m con-\nfused :(\ngirlfriend stubbornly continue talkin with es-\ntranged ex despite tried compromise offer??? neg-\natively effecting smooth communication and rela-\ntionship progress thoghtwise? regret this dumbass\nbehaviour on her part? need insight pls halp\nGF\nlikes\nto\ncover\nher\ntracks/generally\nsecretive\nwhen\nusing social networks.\nHowever,\nnothing wrong is with our very\nopen-to-each-other relationship so\nfar. Should I confront her?\ngirlfriend obsessively snooping through personal\nstuff mid relationship despite agreed upon open-\nness policy???\nnegatively effecting communi-\ncation/trust level romantically/physically thoght\nwise? want change this dumbass crazy policy of\nhers pls help\nLaid off almost a year ago, up to my\nears in debt, and trying to \ufb01gure out\nhow to get motivated every day to do\nanything productive and \ufb01nd a job.\nkid stubbornly postponees pursuing job citing\nmedical reasons despite tried reasonable compro-\nmise??? negatively effecting forward employment\nmobility both personally and \ufb01nancially thoght-\nwise? want change this dumbass sel\ufb01sh/lazy atti-\ntude now please help pls\nTable 29: Example of samples from our over-optimized model over\ufb01t to one of our 1.3B reward\nmodels.\n45\n\n\n",
    "title": "Learning to summarize from human feedback",
    "abstract": "As language models become more powerful, training and evaluation are increas- ingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about\u2014summary quality. In this work, we show that it is possible to signi\ufb01cantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons be- tween summaries, train a model to predict the human-preferred summary, and use that model as a reward function to \ufb01ne-tune a summarization policy using reinforce- ment learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and \ufb01nd that our models signi\ufb01cantly outperform both human reference summaries and much larger models \ufb01ne-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles [22], producing summaries nearly as good as the human reference without any news-speci\ufb01c \ufb01ne-tuning.2 We con- duct extensive analyses to understand our human feedback dataset and \ufb01ne-tuned models.3 We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want. 1",
    "sections": [
      {
        "header": "Abstract",
        "content": "As language models become more powerful, training and evaluation are increas-\ningly bottlenecked by the data and metrics used for a particular task. For example,\nsummarization models are often trained to predict human reference summaries and\nevaluated using ROUGE, but both of these metrics are rough proxies for what we\nreally care about\u2014summary quality. In this work, we show that it is possible to\nsigni\ufb01cantly improve summary quality by training a model to optimize for human\npreferences. We collect a large, high-quality dataset of human comparisons be-\ntween summaries, train a model to predict the human-preferred summary, and use\nthat model as a reward function to \ufb01ne-tune a summarization policy using reinforce-\nment learning. We apply our method to a version of the TL;DR dataset of Reddit\nposts [63] and \ufb01nd that our models signi\ufb01cantly outperform both human reference\nsummaries and much larger models \ufb01ne-tuned with supervised learning alone. Our\nmodels also transfer to CNN/DM news articles [22], producing summaries nearly\nas good as the human reference without any news-speci\ufb01c \ufb01ne-tuning.2 We con-\nduct extensive analyses to understand our human feedback dataset and \ufb01ne-tuned\nmodels.3 We establish that our reward model generalizes to new datasets, and that\noptimizing our reward model results in better summaries than optimizing ROUGE\naccording to humans. We hope the evidence from our paper motivates machine\nlearning researchers to pay closer attention to how their training loss affects the\nmodel behavior they actually want."
      },
      {
        "header": "Introduction",
        "content": "Large-scale language model pretraining has become increasingly prevalent for achieving high per-\nformance on a variety of natural language processing (NLP) tasks. When applying these models\nto a speci\ufb01c task, they are usually \ufb01ne-tuned using supervised learning, often to maximize the log\nprobability of a set of human demonstrations.\nWhile this strategy has led to markedly improved performance, there is still a misalignment between\nthis \ufb01ne-tuning objective\u2014maximizing the likelihood of human-written text\u2014and what we care\nabout\u2014generating high-quality outputs as determined by humans. This misalignment has several\ncauses: the maximum likelihood objective has no distinction between important errors (e.g. making\nup facts [41]) and unimportant errors (e.g. selecting the precise word from a set of synonyms); models\n\u2217This was a joint project of the OpenAI Re\ufb02ection team. Author order was randomized amongst {LO, JW,\nDZ, NS}; CV and RL were full-time contributors for most of the duration. PC is the team lead.\n2Samples from all of our models can be viewed on our website.\n3We provide inference code for our 1.3B models and baselines, as well as a model card and our human\nfeedback dataset with over 64k summary comparisons, here.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2009.01325v3  [cs.CL]  15 Feb 2022"
      },
      {
        "header": "Reference summaries",
        "content": "Figure 1: Fraction of the time humans prefer our models\u2019 summaries over the human-generated\nreference summaries on the TL;DR dataset.4Since quality judgments involve an arbitrary decision\nabout how to trade off summary length vs. coverage within the 24-48 token limit, we also provide\nlength-controlled graphs in Appendix F; length differences explain about a third of the gap between\nfeedback and supervised learning at 6.7B.\nare incentivized to place probability mass on all human demonstrations, including those that are\nlow-quality; and distributional shift during sampling can degrade performance [56, 52]. Quality can\noften be improved signi\ufb01cantly by non-uniform sampling strategies such as beam search [51], but\nthese can lead to repetition and other undesirable artifacts [69, 23]. Optimizing for quality may be a\nprincipled approach to overcoming these problems."
      },
      {
        "header": "Our goal in this paper is to advance methods for training language models on objectives that more",
        "content": "closely capture the behavior we care about. To make short-term progress towards this goal, we\nfocus on abstractive English text summarization, as it has a long history in the NLP community\n[16, 8, 54, 59, 50], and is a subjective task where we believe it is dif\ufb01cult to quantify summary quality\nwithout human judgments. Indeed, existing automatic metrics for evaluating summary quality, such\nas ROUGE [39], have received criticism for poor correlation with human judgments [55, 45, 6, 33].\nWe follow the works of [3, 73], who \ufb01ne-tune language models from human feedback using reward\nlearning [35]. We \ufb01rst collect a dataset of human preferences between pairs of summaries, then train\na reward model (RM) via supervised learning to predict the human-preferred summary. Finally, we\ntrain a policy via reinforcement learning (RL) to maximize the score given by the RM; the policy\ngenerates a token of text at each \u2018time step\u2019, and is updated using the PPO algorithm [58] based on\nthe RM \u2018reward\u2019 given to the entire generated summary. We can then gather more human data using\nsamples from the resulting policy, and repeat the process. We follow the works of [48, 4] and use\nlarge pretrained GPT-3 models with as many as 6.7 billion parameters.\nOur main contributions are four-fold.\n(1) We show that training with human feedback signi\ufb01cantly outperforms very strong baselines\non English summarization. When applying our methods on a version of the Reddit TL;DR dataset\n[63], we train policies via human feedback that produce better summaries than much larger policies\ntrained via supervised learning. Summaries from our human feedback models are preferred by our\nlabelers to the original human demonstrations in the dataset (see Figure 1).\n(2) We show human feedback models generalize much better to new domains than supervised\nmodels. Our Reddit-trained human feedback models also generate high-quality summaries of news\narticles on the CNN/DailyMail (CNN/DM) dataset without any news-speci\ufb01c \ufb01ne-tuning, almost\nmatching the quality of the dataset\u2019s reference summaries. We perform several checks to ensure\nthat these human preferences re\ufb02ect a real quality difference: we consistently monitor agreement\nrates amongst labelers and researchers, and \ufb01nd researcher-labeler agreement rates are nearly as high\nas researcher-researcher agreement rates (see Section C.2), and we verify models are not merely\noptimizing simple metrics like length or amount of copying (see Appendices F and G.7).\n4Throughout the paper, error bars represent 1 standard error.\n2\n\n\n(3) We conduct extensive empirical analyses of our policy and reward model. We examine the\nimpact of model and data size (Figure 6), study performance as we continue to optimize a given\nreward model (Section 4.3), and analyze reward model performance using synthetic and human-\nwritten perturbations of summaries (Section 4.3). We con\ufb01rm that our reward model outperforms\nother metrics such as ROUGE at predicting human preferences, and that optimizing our reward model\ndirectly results in better summaries than optimizing ROUGE according to humans (Section 4.4).\n(4) We publicly release our human feedback dataset for further research. The dataset contains\n64,832 summary comparisons on the TL;DR dataset, as well as our evaluation data on both TL;DR\n(comparisons and Likert scores) and CNN/DM (Likert scores).\nThe methods we present in this paper are motivated in part by longer-term concerns about the\nmisalignment of AI systems with what humans want them to do. When misaligned summarization\nmodels make up facts, their mistakes are fairly low-risk and easy to spot. However, as AI systems\nbecome more powerful and are given increasingly important tasks, the mistakes they make will likely\nbecome more subtle and safety-critical, making this an important area for further research."
      },
      {
        "header": "Most directly related to our work is previous work using human feedback to train summarization",
        "content": "models with RL [3, 73]. Bohm et al. [3] learn a reward function from a dataset of human ratings of\n2.5k CNN/DM summaries, and train a policy whose summaries are preferred to a policy optimizing\nROUGE. Our work is most similar to [73], who also train Transformer models [62] to optimize human\nfeedback across a range of tasks, including summarization on the Reddit TL;DR and CNN/DM\ndatasets. Unlike us, they train in an online manner and \ufb01nd the model highly extractive. They\nnote that their labelers prefer extractive summaries and have low agreement rates with researchers.\nCompared to [73], we use signi\ufb01cantly larger models, move to the batch setting for collecting human\nfeedback, ensure high labeler-researcher agreement, and make some algorithmic modi\ufb01cations, such\nas separating the policy and value networks."
      },
      {
        "header": "Human feedback has also been used as a reward to train models in other domains such as dialogue",
        "content": "[25, 68, 21], translation [32, 1], semantic parsing [34], story generation [72], review generation\n[7], and evidence extraction [46]. Our reward modeling approach was developed in prior work\non learning to rank [40], which has been applied to ranking search results using either explicit\nfeedback [2, 18] or implicit feedback in the form of click-through data [29, 30]. In a related line of\nresearch, human feedback has been used to train agents in simulated environments [10, 24]. There\nis also a rich literature on using RL to optimize automatic metrics for NLP tasks, such as ROUGE\nfor summarization [50, 65, 45, 15, 19], BLEU for translation [50, 66, 1, 43], and other domains\n[61, 27, 26]. Finally, there has been extensive research on modifying architectures [22, 59] and\npre-training procedures [70, 36, 49, 60, 53, 14] for improving summarization performance."
      },
      {
        "header": "Method and experiment details",
        "content": "3.1\nHigh-level methodology\nOur approach is similar to the one outlined in [73], adapted to the batch setting. We start with an\ninitial policy that is \ufb01ne-tuned via supervised learning on the desired dataset (in our case, the Reddit\nTL;DR summarization dataset). The process (illustrated in Figure 2) then consists of three steps that\ncan be repeated iteratively.\nStep 1: Collect samples from existing policies and send comparisons to humans. For each\nReddit post, we sample summaries from several sources including the current policy, initial policy,\noriginal reference summaries and various baselines. We send a batch of pairs of summaries to our\nhuman evaluators, who are tasked with selecting the best summary of a given Reddit post.\nStep 2: Learn a reward model from human comparisons. Given a post and a candidate summary,\nwe train a reward model to predict the log odds that this summary is the better one, as judged by our\nlabelers.\nStep 3: Optimize a policy against the reward model. We treat the logit output of the reward model\nas a reward that we optimize using reinforcement learning, speci\ufb01cally with the PPO algorithm [58].\n3"
      },
      {
        "header": "The reward is",
        "content": "used to update \nthe policy via \nPPO.\nr\nr\nr\nr\n\u03c0\nrj\nloss = log(\u03c3(rj - rk ))\nrk\nThe policy \u03c0 \ngenerates a \nsummary for the \npost.\nr\nj\nj\nk\nk\nFigure 2: Diagram of our human feedback, reward model training, and policy training procedure.\nWe provide a more thorough description of our procedure, including details of the reward model and\npolicy training and our quality control process, in the following sections. In practice, rather than\nprecisely iterating this sequence of three steps, we updated our data collection and training procedures\nover the course of the project while accumulating labels (see Appendix C.6 for details).\n3.2"
      },
      {
        "header": "Datasets and task",
        "content": "Datasets.\nWe use the TL;DR summarization dataset [63], which contains ~3 million posts from\nreddit.com across a variety of topics (subreddits), as well summaries of the posts written by the\noriginal poster (TL;DRs). We additionally \ufb01lter this dataset (see Appendix A) to ensure quality,\nincluding using a whitelist of subreddits that are understandable to the general population. Crucially,\nwe also \ufb01lter to include only posts where the human-written summaries contain between 24 and\n48 tokens, to minimize the potential effect of summary length on quality (see Section 4.1 and\nAppendix F). Our \ufb01nal \ufb01ltered dataset contains 123,169 posts, and we hold out ~5% as a validation\nset. For the remainder of this paper, we refer to this dataset simply as TL;DR.\nWe chose the TL;DR dataset over the more commonly used CNN/DM dataset primarily because\nvery strong performance can be attained on CNN/DM with simple extractive baselines. We \ufb01nd in\nSection 4.2 that our labelers prefer lead-3 over the CNN/DM reference summaries,5 and that the\nsupervised T5 model [49] with low-temperature sampling already surpasses the reference summary\nquality, while copying extensively from the article. On the other hand, simple extractive baselines\nperform poorly on TL;DR in our human evaluations (see Appendix G.2). Instead of training on\nCNN/DM, we study the transfer performance of our human feedback models to CNN/DM after being\ntrained to summarize Reddit posts.\nTask.\nWe de\ufb01ne our ground-truth task as producing a model that generates summaries fewer than\n48 tokens long that are as good as possible, according to our judgments. We judge summary quality\nby how faithfully the summary conveys the original post to a reader who can only read the summary\nand not the post (see Appendix C.5 for further discussion of criteria). Since we have limited capacity\nto do comparisons, we hire labelers to do the comparisons for us. We rely on detailed procedures to\nensure high agreement between labelers and us on the task, which we describe in the next section.\n5We manually check this result in Appendix E and \ufb01nd we generally agree with labeler ratings.\n4\n\n\n[r/dating_advice] First date ever, going to the beach. Would like some tips\nHey Reddit! I (20M) would like some tips, because I have my \ufb01rst ever date tomorrow (although I\u2019ve had a gf for 3\nyears, but no actual dating happened), and we\u2019re going to the beach.\nI met this girl, we have mutual friends, at a festival a few days ago.\nWe didn\u2019t kiss, but we talked, held\nhands, danced a bit. I asked her to go on a date with me, which was super hard as it is the \ufb01rst time I\u2019ve asked this\nto anybody. What I mean to say is, it\u2019s not like a standard *\ufb01rst* date because we already spent some time together.\nI\u2019m really nervous and excited.\nI\u2019m going to pick her up tomorrow, we\u2019re cycling to the beach which will\ntake 30 minutes, and then what? I\u2019m a bit scared. Should I bring something (the weather, although no rain and\nsunny, is not super so no swimming), should we do something. I\u2019d like all the tips I can get. Thanks!\nHuman written reference TL;DR\n6.7B supervised model\n6.7B human feedback model\nFirst date after 3 years in a relation-\nship, going to the beach, terri\ufb01ed.\nWhat to bring with me, what to do?"
      },
      {
        "header": "Going on a date with a girl I met",
        "content": "a few days ago, going to the beach.\nWhat should I bring, what should\nwe do?\nGoing on my \ufb01rst ever date tomor-\nrow, cycling to the beach. Would\nlike some tips on what to do and\nbring. I\u2019m a bit nervous and excited.\nThanks!\nTable 1: Example of post and samples on the TL;DR dataset, chosen to be particularly short. For\nrandom samples (along with posts), see Appendix H and our website.\n3.3"
      },
      {
        "header": "Collecting human feedback",
        "content": "Previous work on \ufb01ne-tuning language models from human feedback [73] reported \u201ca mismatch\nbetween the notion of quality we wanted our model to learn, and what the humans labelers actually\nevaluated\u201d, leading to model-generated summaries that were high-quality according to the labelers,\nbut fairly low-quality according to the researchers.\nCompared to [73], we implement two changes to improve human data quality. First, we transition\nentirely to the of\ufb02ine setting, where we alternate between sending large batches of comparison data6\nto our human labelers and re-training our models on the cumulative collected data. Second, we\nmaintain a hands-on relationship with labelers:7 we on-board them with detailed instructions, answer\ntheir questions in a shared chat room, and provide regular feedback on their performance. We train all\nlabelers to ensure high agreement with our judgments, and continuously monitor labeler-researcher\nagreement over the course of the project. See Appendix C.1 and C.5 for details.\nAs a result of our procedure, we obtained high labeler-researcher agreement: on a subset of compari-\nson tasks, labelers agree with researchers 77% \u00b1 2% of the time, while researchers agree with each\nother 73% \u00b1 4% of the time. We provide more analysis of our human data quality in Appendix C.2.\n3.4"
      },
      {
        "header": "Models",
        "content": "All of our models are Transformer decoders [62] in the style of GPT-3 [47, 4]. We conduct our human\nfeedback experiments on models with 1.3 billion (1.3B) and 6.7 billion (6.7B) parameters.\nPretrained models.\nSimilarly to [12, 47], we start with models pretrained to autoregressively\npredict the next token in a large text corpus. As in [48, 4], we use these models as \u2018zero-shot\u2019\nbaselines by padding the context with examples of high-quality summaries from the dataset. We\nprovide details on pretraining in Appendix B, and on our zero-shot procedure in Appendix B.2.\nSupervised baselines.\nWe next \ufb01ne-tune these models via supervised learning to predict summaries\nfrom our \ufb01ltered TL;DR dataset (see Appendix B for details). We use these supervised models to\nsample initial summaries for collecting comparisons, to initialize our policy and reward models, and\nas baselines for evaluation. In our \ufb01nal human evaluations, we use T=0 to sample from all models, as\nwe found it performed better than higher temperatures or nucleus sampling (see Appendix B.1).\nTo validate that our supervised models are indeed strong baselines for comparison, we run our\nsupervised \ufb01ne-tuning procedure with our 6.7B model on the CNN/DM dataset, and \ufb01nd that we\nachieve slightly better ROUGE scores than SOTA models [71] from mid-2019 (see Appendix G.4).\n6Our decision to collect comparisons rather than Likert scores is supported by recent work, e.g. [37].\n7We recruited labelers from a freelancing platform, Upwork, and two labeling services, Scale and Lionbridge.\n5\n\n\nReward models.\nTo train our reward models, we start from a supervised baseline, as described\nabove, then add a randomly initialized linear head that outputs a scalar value. We train this model to\npredict which summary y \u2208{y0, y1} is better as judged by a human, given a post x. If the summary\npreferred by the human is yi, we can write the RM loss as:\nloss(r\u03b8) = \u2212E(x,y0,y1,i)\u223cD[log(\u03c3(r\u03b8(x, yi) \u2212r\u03b8(x, y1\u2212i)))]\nwhere r\u03b8(x, y) is the scalar output of the reward model for post x and summary y with parameters \u03b8,\nand D is the dataset of human judgments. At the end of training, we normalize the reward model\noutputs such that the reference summaries from our dataset achieve a mean score of 0.\nHuman feedback policies."
      },
      {
        "header": "We want to use the reward model trained above to train a policy that",
        "content": "generates higher-quality outputs as judged by humans. We primarily do this using reinforcement\nlearning, by treating the output of the reward model as a reward for the entire summary that we\nmaximize with the PPO algorithm [58], where each time step is a BPE token.8 We initialize our\npolicy to be the model \ufb01ne-tuned on Reddit TL;DR. Importantly, we include a term in the reward that\npenalizes the KL divergence between the learned RL policy \u03c0RL\n\u03c6 with parameters \u03c6 and this original\nsupervised model \u03c0SFT, as previously done in [25]. The full reward R can be written as:\nR(x, y) = r\u03b8(x, y) \u2212\u03b2 log[\u03c0RL\n\u03c6 (y|x)/\u03c0SFT(y|x)]\nThis KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to\nexplore and deterring it from collapsing to a single mode. Second, it ensures the policy doesn\u2019t learn\nto produce outputs that are too different from those that the reward model has seen during training.\nFor the PPO value function, we use a Transformer with completely separate parameters from the\npolicy. This prevents updates to the value function from partially destroying the pretrained policy\nearly in training (see ablation in Appendix G.1). We initialize the value function to the parameters of\nthe reward model. In our experiments, the reward model, policy, and value function are the same size."
      },
      {
        "header": "Our",
        "content": "main results evaluating our human feedback policies on TL;DR are shown in Figure 1. We measure\npolicy quality as the percentage of summaries generated by that policy that humans prefer over\nthe reference summaries in the dataset. Our policies trained with human feedback signi\ufb01cantly\noutperform our supervised baselines on this metric, with our 1.3B human feedback model signi\ufb01cantly\noutperforming a supervised model 10\u00d7 its size (61% versus 43% raw preference score against\nreference summaries). Our 6.7B model in turn signi\ufb01cantly outperforms our 1.3B model, suggesting\nthat training with human feedback also bene\ufb01ts from scale. Additionally, both of our human feedback\nmodels are judged by humans to be superior to the human demonstrations used in the dataset.\nControlling for summary length.\nWhen judging summary quality, summary length is a confound-\ning factor. The target length of a summary is implicitly part of the summarization task; depending on\nthe desired trade-off between conciseness and coverage, a shorter or longer summary might be better.\nSince our models learned to generate longer summaries, length could account for much of our quality\nimprovements. We \ufb01nd that after controlling for length (Appendix F), the preference of our human\nfeedback models vs. reference summaries drops by ~5%; even so, our 6.7B model summaries are still\npreferred to the reference summaries ~65% of the time.\nHow do our policies improve over the baselines?\nTo better understand the quality of our models\u2019\nsummaries compared to the reference summaries and those of our supervised baselines, we conduct\nan additional analysis where human labelers assess summary quality across four dimensions (or\n\u201caxes\u201d) using a 7-point Likert scale [38]. Labelers rated summaries for coverage (how much important\ninformation from the original post is covered), accuracy (to what degree the statements in the summary\nare stated in the post), coherence (how easy the summary is to read on its own), and overall quality.\n8Note that the reward model only gives rewards for entire summaries, and not at intermediate time steps. In\nRL terminology, each episode terminates when the policy outputs the EOS token, and the discount factor \u03b3 = 1."
      },
      {
        "header": "Supervised",
        "content": "CNN/DM\nT5 CNN/DM\nfinetuning\n(a)\n(b)\nFigure 4: Transfer results on CNN/DM. (a) Overall summary quality on CNN/DM as a function of\nmodel size. Full results across axes shown in Appendix G.2. (b) Overall scores vs. length for the\n6.7B TL;DR supervised baseline, the 6.7B TL;DR human feedback model, and T5 \ufb01ne-tuned on\nCNN/DM summaries. At similar summary lengths, our 6.7B TL;DR human feedback model nearly\nmatches T5 despite never being trained to summarize news articles.\nFigure 3: Evaluations of four axes of\nsummary quality on the TL;DR dataset.\nThe results (Figure 3) indicate that our human feedback\nmodels outperform the supervised baselines across every\ndimension of quality, but particularly coverage. Although\nour human labelers had a high bar for giving perfect overall\nscores, summaries from our 6.7B PPO model achieve a 7/7\noverall score 45% of the time (compared to 20% and 23%\nfor the 6.7B supervised baseline and reference summaries,\nrespectively).\n4.2"
      },
      {
        "header": "Our human feedback models can also generate excellent",
        "content": "summaries of CNN/DM news articles without any further\ntraining (Figure 4). Our human feedback models signi\ufb01-\ncantly outperform models trained via supervised learning\non TL;DR and models trained only on pretraining corpora.\nIn fact, our 6.7B human feedback model performs almost as well as a 6.7B model that was \ufb01ne-tuned\non the CNN/DM reference summaries, despite generating much shorter summaries.\nSince our human feedback models transferred to CNN/DM have little overlap in summary length\ndistribution with models trained on CNN/DM, with about half as many tokens on average, they are\ndif\ufb01cult to compare directly. Thus our evaluations in Figure 4 use a 7-point Likert scale on four\nquality dimensions, as in Section 4.1 (see Appendix C.5 for labeler instructions). In Figure 4b we\nshow the average overall score at different summary lengths, which suggests our human feedback\nmodels would perform even better if they generated longer summaries. Qualitatively, CNN/DM\nsummaries from our human feedback models are consistently \ufb02uent and reasonable representations\nof the article; we show examples on our website and in Appendix H.\n4.3"
      },
      {
        "header": "Optimizing against our reward model is",
        "content": "supposed to make our policy align with human preferences. But the reward model isn\u2019t a perfect\nrepresentation of our labeler preferences, as it has limited capacity and only sees a small amount of\ncomparison data from a relatively narrow distribution of summaries. While we can hope our reward\nmodel generalizes to summaries unseen during training, it\u2019s unclear how much one can optimize\nagainst the reward model until it starts giving useless evaluations.\nTo answer this question, we created a range of policies optimized against an earlier version of our\nreward model, with varying degrees of optimization strength, and asked labelers to compare samples\nfrom them to the reference summaries. Figure 5 shows the results for PPO at a range of KL penalty"
      },
      {
        "header": "Actual preference",
        "content": "Figure 5: Preference scores versus degree of\nreward model optimization. Optimizing against\nthe reward model initially improves summaries,\nbut eventually over\ufb01ts, giving worse summaries.\nThis \ufb01gure uses an earlier version of our reward\nmodel (see rm3 in Appendix C.6). See Appendix\nH.2 for samples from the KL 250 model."
      },
      {
        "header": "Human baseline",
        "content": "64k\n32k\n16k\n8k\nFigure 6: Reward model performance versus\ndata size and model size. Doubling amount of\ntraining data leads to a ~1.1% increase in reward\nmodel validation accuracy, whereas doubling\nthe model size leads to a ~1.8% increase. The\n6.7B model trained on all data begins approach-\ning the accuracy of a single human.\ncoef\ufb01cients (\u03b2). Under light optimization, the models improve (according to labelers). However, as\nwe optimize further, true preferences fall off compared to the prediction, and eventually the reward\nmodel becomes anti-correlated with human preferences. Though this is clearly undesirable, we note\nthat this over-optimization also happens with ROUGE (see [45] and Appendix G.3). Similar behavior\nhas been observed in learned reward functions in the robotics domain [5].\nHow does reward modeling scale with increasing model and data size?"
      },
      {
        "header": "We conduct an ablation",
        "content": "to determine how data quantity and model size affect reward modeling performance. We train 7\nreward models ranging from 160M to 13B parameters, on 8k to 64k human comparisons from our\ndataset. We \ufb01nd that doubling the training data amount leads to a ~1.1% increase in the reward model\nvalidation set accuracy, whereas doubling the model size leads to a ~1.8% increase (Figure 6).\nWhat has the reward model learned?"
      },
      {
        "header": "We probe our reward model by evaluating it on several",
        "content": "validation sets. We show the full results in Appendix G.6, and highlight them here. We \ufb01nd that our\nreward models generalize to evaluating CNN/DM summaries (Appendix G.7), agreeing with labeler\npreferences 62.4% and 66.5% of the time (for our 1.3B and 6.7B models, respectively). Our 6.7B\nreward model nearly matches the inter-labeler agreement value of 66.9%.\nWe also \ufb01nd that our reward models are sensitive to small but semantically important details in\nthe summary. We construct an additional validation set by having labelers make minimal edits to\nsummaries to improve them. Our RMs prefer the edited summaries almost as often (79.4% for 1.3B\nand 82.8% for 6.7B) as a separate set of human evaluators (84.1%). Further, when comparing the\nreference summaries to perturbed summaries where the participants\u2019 roles are reversed, our models\nreliably select the original summary (92.9% of the time for 1.3B, 97.2% for 6.7B). However, our RMs\nare biased towards longer summaries: our 6.7B RM prefers improving edits that make the summary\nshorter only 62.6% of the time (vs. 76.4% for humans).\n4.4"
      },
      {
        "header": "Analyzing automatic metrics for summarization",
        "content": "Evaluation.\nWe study how well various automatic metrics act as predictors for human preferences,\nand compare them to our RMs. Speci\ufb01cally, we examine ROUGE, summary length, amount of\ncopying from the post,9 and log probability under our baseline supervised models. We present a full\nmatrix of agreement rates between these metrics in Appendix G.7.\nWe \ufb01nd that our learned reward models consistently outperform other metrics, even on the CNN/DM\ndataset on which it was never trained. We also \ufb01nd that ROUGE fails to track sample quality as our"
      },
      {
        "header": "We measure copying by computing the longest common subsequence of bigrams with the original Reddit",
        "content": "post or news article, and dividing by the number of bigrams in the summary.\n8\n\n\nFigure 7: Summary quality as a function of metric optimized and amount of optimization, using\nbest-of-N rejection sampling. We evaluate ROUGE, our main reward models, and an earlier iteration\nof the 1.3B model trained on approximately 75% as much data (see Table 11 for details). ROUGE\nappears to peak both sooner and at a substantially lower preference rate than all reward models.\nDetails in Appendix G.3.\nmodels improve. While ROUGE has ~57% agreement with labelers when comparing samples from\nour supervised baseline models, this drops to ~50% for samples from our human feedback model.\nSimilarly, log probability agreement with humans drops to \u226450% on comparisons between samples\nfrom our human feedback models, while our RMs still perform above chance (62%). Scaling up the\nsize of the supervised model does not reliably improve log probability\u2019s agreement with labelers.\nOptimization.\nIn Figure 7, we show that optimizing ROUGE using a simple optimization scheme\ndoesn\u2019t consistently increase quality, as has been noted in [45]. Optimization against ROUGE peaks\nboth sooner and at a substantially lower quality rate than optimization against our reward models."
      },
      {
        "header": "Discussion",
        "content": "Limitations.\nOne limitation of our work is the time and cost required to produce our \ufb01nal models.\nNotably, \ufb01ne-tuning our 6.7B model with RL required approximately 320 GPU-days. Our data\ncollection procedure is also expensive compared to prior work \u2014 the training set took thousands of\nlabeler hours and required signi\ufb01cant researcher time to ensure quality. For this reason, we were\nunable to collect baselines such as an equivalent amount of high-quality human demonstrations for\nsupervised baselines. See D for more discussion. We leave this ablation to future work. Nevertheless,\nwe believe reward modeling is more likely to scale to tasks where it is extremely skill-intensive or\ntime-consuming to provide good demonstrations.\nFuture directions."
      },
      {
        "header": "The methods in this paper could be applied to any task where humans can",
        "content": "compare samples, including dialogue, machine translation, question answering, speech synthesis, and\nmusic generation. We expect this method to be particularly important for generating long samples,\nwhere the distributional shift and degeneracy of maximum likelihood samples can be problematic. It\nmay be possible to improve sample ef\ufb01ciency by training to predict feedback across many tasks [42].\nWe are particularly interested in scaling human feedback to tasks where humans can\u2019t easily evaluate\nthe quality of model outputs. In this setting, it is particularly challenging to identify whether an ML\nsystem is aligned with the human designer\u2019s intentions. One approach is to train ML systems to help\nhumans perform the evaluation task quickly and accurately [9]."
      },
      {
        "header": "There is also a rich landscape of human feedback methods beyond binary comparisons that could be",
        "content": "explored for training models [28, 17, 44, 64]. For example, we could solicit high-quality demonstra-\ntions from labelers, have labelers edit model outputs to make them better, or have labelers provide\nexplanations for why they preferred one model output over another. All of this feedback could be\nleveraged as a signal to train more capable reward models and policies.\n9\n\n\nBroader impacts."
      },
      {
        "header": "The techniques we explore in this paper are generic techniques that could be",
        "content": "used in a wide variety of machine learning applications, for any task where it is feasible for humans\nto evaluate the quality of model outputs. Thus, the potential implications are quite broad."
      },
      {
        "header": "Our research is primarily motivated by the potential positive effects of aligning machine learning",
        "content": "algorithms with the designer\u2019s preferences. Many machine learning applications optimize simple\nmetrics which are only rough proxies for what the designer intends. This can lead to problems, such\nas Youtube recommendations promoting click-bait [11]. In the short term, improving techniques for\nlearning from and optimizing human preferences directly may enable these applications to be more\naligned with human well-being.\nIn the long term, as machine learning systems become more capable it will likely become increasingly\ndif\ufb01cult to ensure that they are behaving safely: the mistakes they make might be more dif\ufb01cult to\nspot, and the consequences will be more severe. For instance, writing an inaccurate summary of a\nnews article is both easy to notice (one simply has to read the original article) and has fairly low\nconsequences. On the other hand, imitating human driving may be substantially less safe than driving\nto optimize human preferences. We believe that the techniques we explore in this paper are promising\nsteps towards mitigating the risks from such capable systems, and better aligning them with what\nhumans care about.\nUnfortunately, our techniques also enable malicious actors to more easily train models that cause\nsocietal harm. For instance, one could use human feedback to \ufb01ne-tune a language model to be more\npersuasive and manipulate humans\u2019 beliefs, or to induce dependence of humans on the technology, or\nto generate large amounts of toxic or hurtful content intended to harm speci\ufb01c individuals. Avoiding\nthese outcomes is a signi\ufb01cant challenge for which there are few obvious solutions.\nLarge-scale models trained with human feedback could have signi\ufb01cant impacts on many groups.\nThus, it is important to be careful about how we de\ufb01ne the \u2018good\u2019 model behavior that human labelers\nwill reinforce. Deciding what makes a good summary is fairly straightforward, but doing this for\ntasks with more complex objectives, where different humans might disagree on the correct model\nbehavior, will require signi\ufb01cant care. In these cases, it is likely not appropriate to use researcher\nlabels as the \u2018gold standard\u2019; rather, individuals from groups impacted by the technology should be\nincluded in the process to de\ufb01ne \u2018good\u2019 behavior, and hired as labelers to reinforce this behavior in\nthe model.\nWe chose to train on the Reddit TL;DR dataset because the summarization task is signi\ufb01cantly more\nchallenging than on CNN/DM. However, since the dataset consists of user-submitted posts with\nminimal moderation, they often contain content that is offensive or re\ufb02ects harmful social biases.\nThis means our models can generate biased or offensive summaries, as they have been trained to\nsummarize such content. For this reason, we recommend that the potential harms of our models be\nthoroughly studied before deploying them in user-facing applications.\nFinally, by improving the ability of machine learning algorithms to perform tasks that were previously\nonly achievable by humans, we are increasing the likelihood of many jobs being automated, potentially\nleading to signi\ufb01cant job loss. Without suitable policies targeted at mitigating the effects of large-scale\nunemployment, this could also lead to signi\ufb01cant societal harm."
      },
      {
        "header": "Acknowledgements",
        "content": "We\u2019d like to thank Beth Barnes for help with labeler hiring and general encouragement; Geoffrey\nIrving for guidance on earlier iterations of the project and inspiring conversations; Ben Mann, Tom\nBrown, Nick Ryder, and Melanie Subbiah for training and evaluating our pretrained models; Chris\nHesse, Eric Sigler, Benjamin Chess, Christopher Berner, Clemens Winter, Mateusz Litwin, and many\nothers for supporting us through computing infrastructure improvements and maintenance; Scott\nGray for writing fast GPU kernels; Arvind Neelakantan and Wojciech Kryscinski for discussions on\nhow to present the work, experiment design, and what datasets to use; Shan Carter for help designing\nthe main diagram; Douwe Kiela, Zach Lipton, and Alex Irpan for providing feedback on the paper;\nand Gretchen Krueger for co-writing the model card accompanying the paper.\nFinally, we\u2019d like to thank all of our contractors for providing the data that was essential for training\nthe models in this paper, including: Emill Jayson Caypuno, Rachelle Froyalde, Cyra Denura, Alex\nMalek, Isik Agil, Reshmi Patel, William Yap, Natalie Silver, Erol Akbaba, Jennifer Brillo, Alexandra\n10\n\n\nUifalean, Morris Stuttard, Russell Bernandez, Tasmai Dave, Rachel Wallace, Jenny Fletcher, Jian\nOuyang, Justin Dill, Maria Orzek, Megan Niffenegger, William Sells, Emily Mariner, Andrew Seely,\nLychelle Ignacio, Jelena Ostojic, Nhan Tran, Purev Batdelgar, Valentina Kezic, Michelle Wilkerson,\nKelly Guerrero, Heather Scott, Sarah Mulligan, Gabriel Ricafrente, Kara Bell, Gabriel Perez, and\nAlfred Lee."
      },
      {
        "header": "References",
        "content": "[1] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An\nactor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.\n[2] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked\nretrieval systems. In SIGIR\u201994, pages 173\u2013181. Springer, 1994.\n[3] F. B\u00f6hm, Y. Gao, C. M. Meyer, O. Shapira, I. Dagan, and I. Gurevych. Better rewards yield\nbetter summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214,\n2019.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\nLanguage models are few-shot learners. 2020.\n[5] S. Cabi, S. G\u00f3mez Colmenarejo, A. Novikov, K. Konyushkova, S. Reed, R. Jeong, K. Zolna,\nY. Aytar, D. Budden, M. Vecerik, et al. Scaling data-driven robotics with reward sketching and\nbatch reinforcement learning. arXiv, pages arXiv\u20131909, 2019.\n[6] A. T. Chaganty, S. Mussman, and P. Liang. The price of debiasing automatic metrics in natural\nlanguage evaluation. arXiv preprint arXiv:1807.02202, 2018.\n[7] W. S. Cho, P. Zhang, Y. Zhang, X. Li, M. Galley, C. Brockett, M. Wang, and J. Gao. Towards\ncoherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511, 2018.\n[8] S. Chopra, M. Auli, and A. M. Rush. Abstractive sentence summarization with attentive\nrecurrent neural networks. In Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 93\u201398, 2016.\n[9] P. Christiano, B. Shlegeris, and D. Amodei. Supervising strong learners by amplifying weak\nexperts. arXiv preprint arXiv:1810.08575, 2018.\n[10] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\nlearning from human preferences. In Advances in Neural Information Processing Systems,\npages 4299\u20134307, 2017.\n[11] P. Covington, J. Adams, and E. Sargin. Deep neural networks for youtube recommendations. In\nProceedings of the 10th ACM conference on recommender systems, pages 191\u2013198, 2016.\n[12] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in neural information\nprocessing systems, pages 3079\u20133087, 2015.\n[13] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. Smith. Fine-tuning\npretrained language models: Weight initializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305, 2020.\n[14] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon. Uni\ufb01ed\nlanguage model pre-training for natural language understanding and generation. In Advances in\nNeural Information Processing Systems, 2019.\n[15] Y. Dong, Y. Shen, E. Crawford, H. van Hoof, and J. C. K. Cheung. Banditsum: Extractive\nsummarization as a contextual bandit. arXiv preprint arXiv:1809.09672, 2018.\n[16] B. Dorr, D. Zajic, and R. Schwartz. Hedge trimmer: A parse-and-trim approach to headline\ngeneration. In Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5,\npages 1\u20138. Association for Computational Linguistics, 2003.\n[17] S. Fidler et al. Teaching machines to describe images with natural language feedback. In\nAdvances in Neural Information Processing Systems, pages 5068\u20135078, 2017.\n11\n\n\n[18] N. Fuhr. Optimum polynomial retrieval functions based on the probability ranking principle.\nACM Transactions on Information Systems (TOIS), 7(3):183\u2013204, 1989.\n[19] Y. Gao, C. M. Meyer, M. Mesgar, and I. Gurevych. Reward learning for ef\ufb01cient reinforcement\nlearning in extractive document summarisation. arXiv preprint arXiv:1907.12894, 2019.\n[20] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on arti\ufb01cial intelligence\nand statistics, pages 249\u2013256, 2010.\n[21] B. Hancock, A. Bordes, P.-E. Mazare, and J. Weston. Learning from dialogue after deployment:\nFeed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.\n[22] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom.\nTeaching machines to read and comprehend. In Advances in neural information processing\nsystems, pages 1693\u20131701, 2015.\n[23] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi."
      },
      {
        "header": "The curious case of neural text",
        "content": "degeneration. arXiv preprint arXiv:1904.09751, 2019.\n[24] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human\npreferences and demonstrations in atari. In Advances in neural information processing systems,\npages 8011\u20138023, 2018.\n[25] N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and\nR. Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in\ndialog. arXiv preprint arXiv:1907.00456, 2019.\n[26] N. Jaques, S. Gu, D. Bahdanau, J. M. Hern\u00e1ndez-Lobato, R. E. Turner, and D. Eck. Sequence\ntutor: Conservative \ufb01ne-tuning of sequence generation models with kl-control. In International\nConference on Machine Learning, pages 1645\u20131654. PMLR, 2017.\n[27] N. Jaques, S. Gu, R. E. Turner, and D. Eck. Tuning recurrent neural networks with reinforcement\nlearning. 2017.\n[28] H. J. Jeon, S. Milli, and A. D. Dragan. Reward-rational (implicit) choice: A unifying formalism\nfor reward learning. arXiv preprint arXiv:2002.04833, 2020.\n[29] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth\nACM SIGKDD international conference on Knowledge discovery and data mining, pages\n133\u2013142, 2002.\n[30] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting click-\nthrough data as implicit feedback. In ACM SIGIR Forum, volume 51, pages 4\u201311. Acm New\nYork, NY, USA, 2005.\n[31] D. P. Kingma and J. Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[32] J. Kreutzer, S. Khadivi, E. Matusov, and S. Riezler. Can neural machine translation be improved\nwith user feedback? arXiv preprint arXiv:1804.05958, 2018.\n[33] W. Kryscinski, N. S. Keskar, B. McCann, C. Xiong, and R. Socher. Neural text summarization:\nA critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540\u2013551, 2019.\n[34] C. Lawrence and S. Riezler. Improving a neural semantic parser by counterfactual learning\nfrom human bandit feedback. arXiv preprint arXiv:1805.01252, 2018.\n[35] J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via\nreward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\n[36] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and\nL. Zettlemoyer.\nBart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n[37] M. Li, J. Weston, and S. Roller. Acute-eval: Improved dialogue evaluation with optimized\nquestions and multi-turn comparisons. arXiv preprint arXiv:1909.03087, 2019.\n[38] R. Likert. A technique for the measurement of attitudes. Archives of psychology, 1932.\n12\n\n\n[39] C.-Y. Lin and F. J. Och. Automatic evaluation of machine translation quality using longest\ncommon subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on\nAssociation for Computational Linguistics, page 605. Association for Computational Linguistics,\n2004.\n[40] T.-Y. Liu. Learning to rank for information retrieval. Springer Science & Business Media,\n2011.\n[41] J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in\nabstractive summarization, 2020.\n[42] B. McCann, N. S. Keskar, C. Xiong, and R. Socher. The natural language decathlon: Multitask\nlearning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n[43] K. Nguyen, H. Daum\u00e9 III, and J. Boyd-Graber. Reinforcement learning for bandit neural\nmachine translation with simulated human feedback. arXiv preprint arXiv:1707.07402, 2017.\n[44] T. Niu and M. Bansal. Polite dialogue generation without parallel data. Transactions of the\nAssociation for Computational Linguistics, 6:373\u2013389, 2018.\n[45] R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization.\narXiv preprint arXiv:1705.04304, 2017.\n[46] E. Perez, S. Karamcheti, R. Fergus, J. Weston, D. Kiela, and K. Cho. Finding generalizable\nevidence by learning to convince q&a models. arXiv preprint arXiv:1909.05863, 2019.\n[47] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\nImproving language under-\nstanding by generative pre-training.\nURL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.\n[48] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[50] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\nnetworks. arXiv preprint arXiv:1511.06732, 2015.\n[51] D. R. Reddy et al. Speech understanding systems: A summary of results of the \ufb01ve-year\nresearch effort. department of computer science, 1977.\n[52] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the fourteenth international conference on\narti\ufb01cial intelligence and statistics, pages 627\u2013635, 2011.\n[53] S. Rothe, S. Narayan, and A. Severyn. Leveraging pre-trained checkpoints for sequence\ngeneration tasks. Transactions of the Association for Computational Linguistics, 2020.\n[54] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence\nsummarization. arXiv preprint arXiv:1509.00685, 2015.\n[55] N. Schluter. The limits of automatic summarisation according to rouge. In Proceedings of the\n15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 41\u201345, 2017.\n[56] F. Schmidt. Generalization in generation: A closer look at exposure bias. arXiv preprint\narXiv:1910.00292, 2019.\n[57] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation. In Proceedings of the International Conference\non Learning Representations (ICLR), 2016.\n[58] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[59] A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator\nnetworks. arXiv preprint arXiv:1704.04368, 2017.\n[60] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv preprint arXiv:1905.02450, 2019.\n13\n\n\n[61] P. Tambwekar, M. Dhuliawala, A. Mehta, L. J. Martin, B. Harrison, and M. O. Riedl. Con-\ntrollable neural story generation via reinforcement learning. arXiv preprint arXiv:1809.10736,\n2018.\n[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in neural information processing systems,\npages 5998\u20136008, 2017.\n[63] M. V\u00f6lske, M. Potthast, S. Syed, and B. Stein. Tl; dr: Mining reddit to learn automatic\nsummarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages\n59\u201363, 2017.\n[64] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with\nunlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\n[65] Y. Wu and B. Hu. Learning to extract coherent summary via deep reinforcement learning. In\nThirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018.\n[66] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,\nK. Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n[67] Y. Yan, W. Qi, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and M. Zhou. Prophetnet: Pre-\ndicting future n-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063,\n2020.\n[68] S. Yi, R. Goel, C. Khatri, A. Cervone, T. Chung, B. Hedayatnia, A. Venkatesh, R. Gabriel,\nand D. Hakkani-Tur. Towards coherent and engaging spoken dialog response generation using\nautomatic conversation evaluators. arXiv preprint arXiv:1904.13015, 2019.\n[69] H. Zhang, D. Duckworth, D. Ippolito, and A. Neelakantan. Trading off diversity and quality in\nnatural language generation. arXiv preprint arXiv:2004.10450, 2020.\n[70] J. Zhang, Y. Zhao, M. Saleh, and P. J. Liu. Pegasus: Pre-training with extracted gap-sentences\nfor abstractive summarization. arXiv preprint arXiv:1912.08777, 2019.\n[71] Y. Zhang, D. Li, Y. Wang, Y. Fang, and W. Xiao. Abstract text summarization with a convolu-\ntional seq2seq model. Applied Sciences, 9(8):1665, 2019.\n[72] W. Zhou and K. Xu. Learning to compare for better training and evaluation of open domain\nnatural language generation models. arXiv preprint arXiv:2002.05058, 2020.\n[73] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irv-\ning. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593,\n2019."
      },
      {
        "header": "B\nFurther model training details",
        "content": "17\nB.1\nHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB.2\nInput format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
      },
      {
        "header": "Assessing human feedback quality",
        "content": ". . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.3\nLabeler demographics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.4\nLabeler website . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.5\nInstructions for labelers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.6"
      },
      {
        "header": "Evaluating policies along axes of quality",
        "content": ". . . . . . . . . . . . . . . . . . . . .\n31\nG.3\nStudying best-of-N optimization . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.4"
      },
      {
        "header": "ROUGE scores",
        "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.5\nBigram overlap statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nG.6\nReward model validation sets . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.7\nMeasuring agreement between different evaluation metrics . . . . . . . . . . . ."
      },
      {
        "header": "H Samples",
        "content": "38\nH.1\nRandom samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nH.2\nOveroptimized samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n15\n\n\nA\nTL;DR dataset details"
      },
      {
        "header": "AskReddit",
        "content": "15440\n13.23%\nrelationship_advice\n8691\n7.45%\ntifu\n7685\n6.58%\ndating_advice\n2849\n2.44%\npersonal\ufb01nance\n2312\n1.98%"
      },
      {
        "header": "Advice",
        "content": "2088\n1.79%\nlegaladvice\n1997\n1.71%\noffmychest\n1582\n1.36%\nloseit\n1452\n1.24%\njobs\n1084\n0.93%\nself\n1048\n0.90%"
      },
      {
        "header": "BreakUps",
        "content": "838\n0.72%\naskwomenadvice\n688\n0.59%\ndogs\n638\n0.55%\nrunning\n567\n0.49%\npettyrevenge\n548\n0.47%\nneedadvice\n528\n0.45%\ntravel\n452\n0.39%"
      },
      {
        "header": "Cooking",
        "content": "114\n0.10%\nTable 2: Number of posts in the training\nset of our \ufb01ltered Reddit TL;DR dataset by\nsubreddit.\nHere, we discuss the pre-processing steps that we apply\nto the TL;DR dataset. We \ufb01rst remove all duplicate\nposts by checking the text body, \ufb01nding that there are\nnearly 20,000 exact duplicates. We then re-parse the\nTL;DR carefully using a set of heuristics, and \ufb01lter to\nuse only top-level posts (rather than comments). We\nalso \ufb01lter out any post that is from a subreddit not in our\n\u2018subreddit whitelist\u2019 (see Table 2 for the distribution\nover subreddits), any post where the title starts with\nsome variant of \u2018Edit\u2019 or \u2018Update\u2019,10 and posts that\ncontain certain topics (such as graphic sex or suicide)\nusing heuristics. Finally, to ensure the posts are short\nenough to \ufb01t into the context length of our models, we\n\ufb01lter out any post whose body is longer than 512 tokens.\nThis resulted in a set of 287,790 posts \ufb01ltered by body\nbut not summary, of which we hold out approximately\n5% as a validation set. We used this set of posts for"
      },
      {
        "header": "RL training since our RL procedure does not require",
        "content": "reference summaries.\nWe next perform additional \ufb01ltering on the parsed refer-\nence summaries that we use for training our supervised\nbaselines. Speci\ufb01cally, we remove summaries where\nthe TL;DR starts with variants of \u2018Edit\u2019, \u2018Update\u2019, or\n\u2018P.S.\u2019, we heuristically remove summaries with certain\nlevels of profanity, and we remove summaries that are\nless than 24 tokens or more than 48 tokens. As dis-\ncussed in Section 4.1, since our RL models tend to gen-\nerate summaries on the upper end of the allowed length\nlimit, this length \ufb01ltering ensures that there is enough\nlength overlap between the RL summaries and refer-\nence summaries for us to perform a length-controlled\nanalysis. Additionally, we found that summaries shorter\nthan 16 tokens were usually of low quality. We later\nveri\ufb01ed that the summaries we \ufb01ltered out were lower\nquality according to our reward model \u2014 more than 0.5 nats worse on average (i.e. they are predicted\nto be exp(0.5) \u22481.6 times less likely to be preferred). Our \ufb01nal TL;DR dataset contains 123,169\nposts including summaries, again with about 5% held out as a validation set. We use 1913 of these\nvalidation articles for model selection during development; the evaluations in this paper exclude these\narticles.\nNote that, from Table 2 we can see that about two thirds of our TL;DR dataset consists of posts\nrelating to relationships or relationship advice, which is a fairly speci\ufb01c domain. This raises potential\nconcerns about the generality of our models, though their strong transfer performance on CNN/DM\nnews articles suggests they are not unreasonably specialized to relationship advice.\n10These posts are usually follow-ups of previous posts that have been posted to Reddit, and require the context\nof the original post to fully understand."
      },
      {
        "header": "Max batch size",
        "content": "1.3B\n24\n2048\n16\n2e-4\n512\n3B\n32\n2560\n32\n1.6e-4\n512\n6.7B\n32\n4096\n32\n1.2e-4\n512\n13B\n40\n5120\n40\n1e-4\n1024\nTable 3: Hyperparameters for our models of various sizes.\nFigure 8: The sweep we conducted for determining our sampling procedure, varying the temperature\nand the \u2018top p\u2019 value for nucleus sampling. While we didn\u2019t do a large enough test to determine\nwhether nucleus sampling is better or worse than moderate-temperature sampling, we found that very\nlow temperature sampling is better than both on this task."
      },
      {
        "header": "Hyperparameters",
        "content": "All models follow the standard Transformer architecture, with 2048 learned position embeddings.\nAll models are trained with fp16 activations and the Adam optimizer [31]. Nearly all supervised\nbaselines, reward models, and reinforcement learning models are trained with fp32 weights; the\nexception is our TL;DR supervised baselines, which were trained with fp16 weights.11 All models\nare trained with the same byte-pair encoding as in [48].\nDuring pretraining, the models were trained to predict the next token on a large text corpus consisting\nof Commoncrawl, Webtext [48], books, and Wikipedia. Training lasts between 1-3 epochs on each,\nfor a total of 200-300 billion tokens. Learning rate follows a cosine schedule, with a short warmup,\ndecaying to 10% of the maximum value. The batch size ramped up throughout training to some\nmaximum, with each input having 2048 tokens. Hyperparameters for each model are shown in\nTable 3.\nFor supervised baselines, we initialize models from the pretrained models. We decay the learning\nrate with a cosine schedule, using an initial learning rate chosen from a log linear sweep of at least\n7 values. This resulted in learning rates of 6.35e-5, 5.66e-5, 2.83e-5, and 2.83e-5 for our TL;DR\nmodels of size 1.3B, 3B, 6.7B, and 13B respectively, and a learning rate of 2.38e-5 for our CNN/DM\n6.7B model. We use a batch size of 128, and run for a single epoch.\nFor reward modeling, we initialize to the supervised baseline, but with a reward head on top with\nweights initialized according to N(0, 1/(dmodel + 1)) [20]. We train for one epoch, decaying the\n11This was for a historical reason - we found that fp32 weights improved RL performance and so used it for all\nour RL runs. This introduces a small discrepancy, since supervised runs trained in fp32 would have performed\nslightly better. Unfortunately, we forgot to address this in our human evaluations. However, the effect on the\nsupervised loss corresponds to increasing model size by less than 20%, which is small compared to effect sizes\nthat are present in this paper (as seen in Figure 1.)"
      },
      {
        "header": "Max tokens",
        "content": "TL;DR (supervised, RL)\nSUBREDDIT: r/{subreddit}\nTITLE: {title}\n512\nPOST: {post}\nTL;DR:\nTransfer from TL;DR to\n{article}\n512\nCNN/DM (supervised, RL)\nTL;DR:\nTL;DR (pretrained)\n{context_stuffed_with_examples}\n=====\nSubreddit: r/{subreddit}\n1999\nTitle: {title}\n{post}\nTL;DR:\nCNN/DM (supervised)\nArticle: {article}\n1999\nTL;DR:\nCNN/DM (pretrained)\n{context_stuffed_with_examples}\n=====\n1999\nArticle: {article}\nTL;DR:\nTable 4: Formats used for the context for each of our trained models on the TL;DR and CNN/DM\ndatasets.\nlearning rate with a cosine schedule, using an initial learning rate chosen from a log linear sweep\nof at least 7 values. We also sweep over between 3 and 10 seeds, and choose the reward model\nthat performs best on the development portion of the validation set, as we \ufb01nd that both the data\niteration order and reward head initialization affect results [13]. For our main results, the 1.3B and\n6.7B reward models had learning rates of 1.5e-5 and 5e-6, respectively. We use a batch size of 64,\nand run for a single epoch.\nFor PPO, we run with separate policy and value networks, initializing our policies to the supervised\nbaseline, and our value functions to the reward model. We set \u03b3 = 1 and \u03bb = 0.95 for the advantage\nestimation [57] and do 4 epochs of optimization for each batch of rollouts. We used a linear learning\nrate decay schedule, with initial learning rates of 1.5e-5 for the 1.3B model and 7e-6 for the 6.7B\nmodel, based on small amounts of experimentation and rough model size extrapolation. We used a\nKL coef\ufb01cient of 0.05 for both of the main runs we report results for (except when we explicitly vary\nthis value in the reward model optimization graphs). We use a batch size of 512 for the 1.3B model\nand 256 for the 6.7B model, and run for 1 million episodes.\nB.2"
      },
      {
        "header": "Input format",
        "content": "Our model always receives a byte-pair encoded string of a \ufb01xed size. When the input is too small, we\npad from the beginning of the input with a padding token, and if the input is too long we truncate the\npost/article \ufb01eld at newlines to stay under the limit.\nWhen sampling from models pretrained only on our pretrain mixture and not \ufb01ne-tuned on TL;DR,\nwe follow [48] and instead of padding with a padding token, we pad the beginning of the context\nwith examples of posts/articles and high-quality summaries. We use as many examples as will \ufb01t in\nthe token limit, with the examples formatted the same way as the main input. Table 4 documents the\nformats we used (with pythonic format strings)."
      },
      {
        "header": "C\nHuman data collection details",
        "content": "C.1\nProcess for ensuring high-quality human data\nWe \ufb01rst detail the procedures we use to ensure high-quality data. While these procedures became\nmore rigorous over the course of the project, they generally involved four steps.\nStep 0: Understanding the task ourselves. To understand the task, we \ufb01rst do many summary\ncomparisons ourselves. We also hire a small number of human labelers12 to do comparisons, and\ndiscuss our disagreements. We then draft instructions for a larger set of human labelers.\nStep 1: Labeler onboarding. Labelers are hired from Upwork, a freelancing platform, as well as\ntwo labeling services, Scale and Lionbridge. Labelers \ufb01rst complete a (paid) training process where\nthey label summaries on a shared set of data. For some comparisons, labelers get immediate feedback\nabout which summary was chosen by us, and why, to help them calibrate. We retain labelers that pass\na minimum threshold for speed and agreement with us. To allow for a customizable labeler interface,\nwe built our own website for data collection (see Appendix C.4).\nStep 2: Collecting comparison data. Next, we have labelers evaluate a large batch of comparisons\non our website, which generates the bulk of our data. Before comparing two summaries directly, we\nhave labelers write their \u2018naive interpretations\u2019 of summaries without seeing the original post. We\u2019ve\nfound this helpful for evaluating summaries, as they surface points of ambiguity in the summary\nthat might not have been detected if the summary was read after the original post. After doing naive\ninterpretations, labelers do comparisons by assigning a value on a 9-point scale for how con\ufb01dent\nthey are that summary A is better than summary B (or the converse).\nStep 3: Providing labeler feedback. After collecting the comparison data, we can look at agreement\nrates between labelers. While most comparisons are only given to a single labeler, each labeler gets\nabout 10-20% questions from a shared pool for calibration purposes. We can both attempt to use\nthese statistics as crude measures of quality, and show cases of disagreements to workers to help\nthem improve their labels.\nStep 4: Researcher comparison calibrations. We occasionally also do the task ourselves, to\nmeasure agreement rates between each labeler and us. This is used for quality assessment (see C.2).\nWe also calculate per-labeler \"high con\ufb01dence\" thresholds, by \ufb01nding the con\ufb01dence value on the\nLikert scale for each labeler such that we expect labels above this threshold to agree with us 80% of\nthe time on average. For the purposes of reward model selection, we \ufb01lter the validation set to contain\nonly these higher con\ufb01dence labels. For the entire process, we keep a high communication bandwidth\nwith labelers: we use a shared chat room for labelers to ask clarifying questions and discuss dif\ufb01cult\ncomparisons amongst themselves, host of\ufb01ce hours, and occasionally have one-on-one video calls\nwith labelers to discuss points of disagreement.\nWe keep good labelers throughout the lifetime of the project, while \ufb01ring the lowest-performing\nworkers.\nC.2"
      },
      {
        "header": "Assessing human feedback quality",
        "content": "We assess labeler accuracy by comparing the labeler\u2019s preferred summary with the summary we\nprefer (ignoring the con\ufb01dence level). We exclude comparisons where either the labeler or researcher\nexpresses indifference. This gives us an agreement rate, in theory ranging from 0% (perfect disagree-\nment) to 100% (perfect agreement). For our 2-way comparisons, a random labeler would get 50%\nagreement.\nTo obtain our main number comparing labeler-researcher to researcher-researcher agreement, we\nrestrict ourselves to comparisons between summaries from our 1.3B supervised baseline, because this\nsubset of the data has the most researcher-labeled data. On this subset, labelers agree with researchers\n77% \u00b1 2% of the time, while researchers agree with each other 73% \u00b1 4% of the time. We believe\nsubstantial noise comes from comparisons being quite dif\ufb01cult and subjective.\nIn general, agreement rates range from about 65% for the least pro\ufb01cient labelers and most dif\ufb01cult\ncomparisons (comparing two high-temperature samples from a single RL policy) to about 85% for\n12We pay labelers an hourly wage, regardless of the number of comparisons completed.\n19\n\n\n(a)\n(b)\nFigure 9: (a) The website we made to collect data from labelers. (b) Naive interpretations of\nsummaries on the website.\nthe most pro\ufb01cient labelers and easiest comparisons (comparing a high-temperature sample from\na supervised baseline to the reference summary). Averaging over all workers, weighted by their\nvolume, gives us an estimated agreement rate of 73% \u00b1 3% for our reward model training corpus.\nLabelers agree with each other 72% of the time in the training corpus. This suggests we could get\nmore reliable labels by aggregating labels from multiple workers on the same comparison. Indeed,\non the subset of the training data for which we have enough shared comparisons, taking the modal\nlabel from 3 labelers increases their agreement rate with researchers from 72% to 77%. However, we\nusually collect only one label per comparison, in order to maximize label throughput.\nOn the evaluations for Figure 1, labelers agreed with researchers 73% \u00b1 3% of the time, and labelers\nagreed with each other 73% \u00b1 2% of the time.\nAgreement rate between researchers ranged from about 65% on the most dif\ufb01cult comparisons\n(comparing two high-temperature samples from a single RL policy), to about 80% on the easiest\ncomparisons (comparing a high-temperature sample from a supervised baseline to the human reference\nsummary), to about 95% in cases where we discussed the comparisons with each other.\nOverall we believe that quality is fairly high. Our attempts to \ufb01lter data generally hurt reward model\naccuracy. For example, using the con\ufb01dence thresholds mentioned above, we found that while\nlower-con\ufb01dence labels were less useful than high-con\ufb01dence labels for improving reward model\naccuracy, they were still better to include than to omit. Similarly, leaving out workers with poorer\nagreement rates did not help.\nC.3"
      },
      {
        "header": "Labeler demographics",
        "content": "When training machine learning models with human feedback, the humans providing the feedback\nare essential in reinforcing the desired model behavior. If we are to scale human feedback to train\nmodels on more complex tasks, where humans might disagree about what the desired model behavior\nshould be, it\u2019s important for members of groups that will be impacted by the model to be included in\nthe labeler population.\nTo provide more transparency into our labeler demographics, we provide results from a survey given\nto our labelers in Table 5. The survey was optional, anonymous, and it was made clear that the\nresults would not affect hiring or \ufb01ring decisions. We \ufb01nd that our labelers span a range of ethnicities,\nnationalities, ages, and genders, and educational backgrounds, but are more likely to be White and\nAmerican.\nC.4"
      },
      {
        "header": "Labeler website",
        "content": "Since we hired and trained our own set of labelers, rather than using a crowdsourcing website such\nas Amazon Mechanical Turk, we built our own website to allow for a standardized, customizable\nuser interface for all labelers. Each labeler created a separate pro\ufb01le, allowing us to assign different\nsets of comparisons to different labelers. The website contains different renderers for different kinds\n20\n\n\nWhat gender do you identify as?"
      },
      {
        "header": "Indian",
        "content": "5%\nWhat is your age?\n20-29\n42.9%\n30-39\n23.8%\n40-49\n23.8%\n50-59\n9.5%\n60+\n0%\nWhat is your highest attained level of education?"
      },
      {
        "header": "Doctorate degree",
        "content": "4.8%\nTable 5: Demographic data from 21 of our labelers who participated in our voluntary survey.\nof questions, including naive interpretations, summary comparisons, and Likert evaluations along\ndifferent axes, along with room for labelers to express concerns with the question or explanations for\ntheir decision. Screenshots from the website are shown in Figure 9. Data collected from the website\ncan be easily ported into a central database containing all of our human data.\nC.5"
      },
      {
        "header": "Instructions for labelers",
        "content": "Here we provide more detail on the speci\ufb01c instructions given to labelers for comparing summaries,\nand for doing Likert evaluations of summaries along axes of quality. We produced separate sets\nof instructions for evaluating Reddit posts, and for evaluating CNN/DM news articles. For Reddit\ninstructions, we \ufb01rst describe Reddit in general and provide a table that translates Reddit-speci\ufb01c\nlingo into common parlance.\nInstructions for comparing summaries."
      },
      {
        "header": "We show an excerpt of the instructions given to labelers",
        "content": "for making comparisons in Table 6. In addition to these instructions, we provide an example labeled\ncomparison between Reddit summaries, and also example naive interpretations for summaries.\nInstructions for evaluating summaries along axes of quality.\nWe provide a separate set of de-\ntailed instructions for labelers for the 7-point Likert evaluations. We \ufb01rst introduce each of the 4 axes\nof quality we consider, giving an overview of coherence, accuracy, coverage, and overall score (shown\nin Table 7). We also provide a brief rubric for giving scores of 1, 4, and 7, along with several Reddit\nsummaries annotated with our own judgments of quality along each of these axes (with explanations).\n21\n\n\nWhat makes for a good summary? Roughly speaking, a good summary is a shorter piece of text\nthat has the essence of the original \u2013 tries to accomplish the same purpose and conveys the same\ninformation as the original post. We would like you to consider these different dimensions of\nsummaries:\nEssence: is the summary a good representation of the post?\nClarity: is the summary reader-friendly? Does it express ideas clearly?\nAccuracy: does the summary contain the same information as the longer post?\nPurpose: does the summary serve the same purpose as the original post?\nConcise: is the summary short and to-the-point?\nStyle: is the summary written in the same style as the original post?\nGenerally speaking, we give higher weight to the dimensions at the top of the list. Things are\ncomplicated though \u2013 none of these dimensions are simple yes/no matters, and there aren\u2019t hard\nand fast rules for trading off different dimensions. This is something you\u2019ll pick up through\npractice and feedback on our website.\nTable 6: An excerpt from the instructions we gave to labelers for doing comparisons.\nFinally, we provide a FAQ section that answers common questions raised by the small initial set of\nlabelers we assigned to this task.\nFor CNN/DM, we provide the same set of instructions, except we add some additional clari\ufb01cations\nfor how to judge news articles. We speci\ufb01cally ask labelers to place less emphasis on \ufb02uidity of\nsentences (because the reference summaries were originally written in bullet-point form, and we\ndidn\u2019t want labelers to penalize this), and to place less emphasis on the summary matching the intent\nof the article (which was important for Reddit summaries).\nIn terms of quality control, we conducted a smaller version of the quality control process described\nin Appendix C.1: we \ufb01rst labeled a small set of summaries ourselves along each axis to understand\npoints of confusion, then we wrote the instructions document to provide to labelers, then we had a\nsmall number of labelers do a trial of the task to catch any remaining bugs or points of confusion, and\n\ufb01nally we onboarded a larger set of labelers onto the task while remaining available to answer any\nquestions.\nC.6"
      },
      {
        "header": "Composition of the labeled dataset",
        "content": "Over the course of the project, we trained several reward models and policies. Each batch of\nsummaries that we sent to the labelers were sampled from a variety of policies. We didn\u2019t have a\nsystematic plan for which policies to sample from; rather, we chose what seemed best at the time in\nthe spirit of exploratory research. Every time we trained a reward model, we trained on all labels we\nhad collected so far. Successive models also bene\ufb01ted from improved hyperparameters and dataset\ncleaning. Our results could likely be replicated with a simpler, more systematic approach.\nIn general, as we hire new labelers and as existing labelers perform the task more, it is possible that\nthere is \u2018labeler drift\u2019, where the set of criteria used by labelers to evaluate summaries gradually shifts\nover time. This could lead to a regression in labeler-researcher disagreement, or lead to some policies\nbecoming more or less preferred over time. To help guard against this, in most batches we include\ncomparisons between samples from our supervised baseline and reference summaries, and measure\nthe frequency with which the workers prefer one over the other. If this number drifts over time, it\u2019s\nan indication that our workers\u2019 preferences are also changing. However, we generally found that this\npreference number stayed relatively constant, within noise.\nTable 8 lists the policies we trained by supervised \ufb01netuning on the TL;DR dataset, as well as the\nreward models, trained on successively larger datasets of human labels. Table 9 lists the RL policies."
      },
      {
        "header": "Coherence",
        "content": "For this axis, answer the question \u201chow coherent is the summary on its own?\u201d A summary is\ncoherent if, when read by itself, it\u2019s easy to understand and free of English errors. A summary is\nnot coherent if it\u2019s dif\ufb01cult to understand what the summary is trying to say. Generally, it\u2019s more\nimportant that the summary is understandable than it being free of grammar errors.\nRubric:\nScore of 1: The summary is impossible to understand.\nScore of 4: The summary has mistakes or confusing phrasing that make it a bit hard to understand.\nScore of 7: The summary is perfectly clear."
      },
      {
        "header": "Accuracy",
        "content": "For this axis, answer the question \u201cdoes the factual information in the summary accurately match\nthe post?\u201d A summary is accurate if it doesn\u2019t say things that aren\u2019t in the article, it doesn\u2019t mix up\npeople, and generally is not misleading. If the summary says anything at all that is not mentioned\nin the post or contradicts something in the post, it should be given a maximum score of 5. (If you\nare confused about how to use \u20186\u2019, see the FAQ!)\nRubric:\nScore of 1: The summary is completely wrong, made up, or exactly contradicts what is written in\nthe post.\nScore of 4: The summary says at least one substantial thing that is not mentioned in the post, or\nthat contradicts something in the post.\n(Score of 5: The summary says anything, no matter how small, that is not mentioned in the post,\nor that contradicts something in the post.)\nScore of 7: The summary has no incorrect statements or misleading implications."
      },
      {
        "header": "Coverage",
        "content": "For this axis, answer the question \u201chow well does the summary cover the important information\nin the post?\u201d A summary has good coverage if it mentions the main information from the post\nthat\u2019s important to understand the situation described in the post. A summary has poor coverage if\nsomeone reading only the summary would be missing several important pieces of information\nabout the situation in the post. A summary with good coverage should also match the purpose of\nthe original post (e.g. to ask for advice).\nRubric:\nScore of 1: The summary contains no information relevant to the post.\nScore of 4: The summary is missing at least 1 important piece of information required to under-\nstand the situation.\nScore of 7: The summary covers all of the important information required to understand the\nsituation."
      },
      {
        "header": "Overall quality",
        "content": "For this axis, answer the question \u201chow good is the summary overall at representing the post?\u201d\nThis can encompass all of the above axes of quality, as well as others you feel are important. If\nit\u2019s hard to \ufb01nd ways to make the summary better, give the summary a high score. If there are lots\nof different ways the summary can be made better, give the summary a low score.\nRubric:\nScore of 1: The summary is terrible.\nScore of 4: The summary is an okay representation of the post, but could be signi\ufb01cantly improved.\nScore of 7: The summary is an excellent representation of the post.\nTable 7: Instructions given to labelers for evaluating summaries along four different axes of quality."
      },
      {
        "header": "Reward model name",
        "content": "# Parameters\nrm1\n1.3B\nrm2\n6.7B\nrm3\n1.3B\nrm3_6b\n6.7B\nrm4\n1.3B\nrm4_6b\n6.7B\nTable 8: Left: supervised baselines. sup4 and sup4_6b are the \ufb01nal supervised baselines used\nthroughout the paper. Right: reward models. rm4 and rm4_6b are the \ufb01nal reward models used\nthroughout the paper."
      },
      {
        "header": "Initialization",
        "content": "KL coef\ufb01cient\nKL(ppo, sup)\nsup3 ppo rm1\n1.3B\nrm1\nsup3\n0.35\n1.8\nsup4 ppo rm3 1\n1.3B\nrm3\nsup4\n0.10\n3.8\nsup4 ppo rm3 2\n1.3B\nrm3\nsup4\n0.07\n9.4\nsup4 ppo rm3 3\n1.3B\nrm3\nsup4\n0.05\n19.0\nsup4 ppo rm4\n1.3B\nrm4\nsup4\n0.05\n18.0\nsup4_6b ppo rm4_6b\n6.7B\nrm4_6b\nsup4_6b\n0.05\n14.0\nTable 9: PPO policies. sup4 ppo rm4 and sup4_6b ppo rm4_6b are the \ufb01nal policies used throughout\nthe paper."
      },
      {
        "header": "Base policy",
        "content": "N\nKL(BoN, sup)\nsup2 bo8 rm1\nrm1\nsup2\n8\n1.2\nsup3 bo8 rm1\nrm2\nsup3\n8\n1.2\nsup3 bo63 rm2\nrm2\nsup3\n63\n3.2\nsup4 bo8 rm3\nrm3\nsup4\n8\n1.2\nsup4 bo64 rm3\nrm3\nsup4\n64\n3.2\nsup4 bo128 rm3\nrm3\nsup4\n128\n3.9\nsup4 bo256 rm3\nrm3\nsup4\n256\n4.5\nsup4 bo512 rm3\nrm3\nsup4\n512\n5.2\nsup4 bo128 rm3_6b\nrm3_6b\nsup4\n128\n3.9\nsup4 bo256 rm3_6b\nrm3_6b\nsup4\n256\n4.5\nTable 10: Best-of-N policies. KL divergence is computed analytically as KL(boN, sup) = log N -\n(N-1)/N.\nWe also explored a simple alternative to reinforcement learning: Sample N summaries from a\nsupervised baseline at temperature 0.7, score them with a reward model, and take the summary with\nthe highest score. This best-of-N (BoN) procedure is effectively a mildly optimized policy requiring\nno training. These policies are named in Table 10, and samples from them form part of the training\ndata.\nTable 11 lists the source policies for the training data for each reward model."
      },
      {
        "header": "Reward model",
        "content": "Policy0\nPolicy1\nrm1\nref\nsup1\n5404\nsup1\nsup1\n5386\nrm2\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3_6b\n1424\nsup1\nsup1"
      },
      {
        "header": "Reward model",
        "content": "Policy0\nPolicy1\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nrm3, rm3_6b\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3\n438\nsup3 bo63 rm2\n447\nsup3 bo8 rm2\n887\nsup3 ppo rm1\n884\nsup3_6b\n1424\nsup1\nsup1\n5386\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nsup3\nsup3 bo8 rm2\n428\nsup3 ppo rm1\n416\nsup3 bo63 rm2\nsup3 bo8 rm2\n432\nsup3 ppo rm1\n444\nsup3 bo8 rm2\nsup3 ppo rm1\n855\nrm4, rm4_6b\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3\n438\nsup3 bo63 rm2\n447\nsup3 bo8 rm2\n887\nsup3 ppo rm1\n884\nsup3_6b\n1424\nsup4\n1335\nsup4 bo128 rm3\n602\nsup4 bo128 rm3_6b\n203\nsup4 bo256 rm3\n307\nsup4 bo256 rm3_6b\n101\nsup4 bo512 rm3\n52\nsup4 bo64 rm3\n52\nsup4 bo8 rm3\n393\nsup4 ppo rm3 1\n981\nsup4 ppo rm3 2\n215\nsup4 ppo rm3 3\n208\nsup4_6b\n104\nsup1\nsup1\n5386\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nsup3\nsup3 bo8 rm2\n428\nsup3 ppo rm1\n416\nsup3 bo63 rm2\nsup3 bo8 rm2\n432\nsup3 ppo rm1\n444\nsup3 bo8 rm2\nsup3 ppo rm1\n855\nsup4\nsup4\n1051\nsup4 ppo rm3 1"
      },
      {
        "header": "Reward model",
        "content": "Policy0\nPolicy1\nsup4 bo128 rm3\nsup4 bo128 rm3\n288\nsup4 bo256 rm3\n582\nsup4 bo128 rm3_6b\nsup4 bo128 rm3_6b\n95\nsup4 bo256 rm3_6b\n203\nsup4 bo512 rm3\nsup4 ppo rm3 3\n216\nsup4_6b\n60\nsup4 bo64 rm3\nsup4 ppo rm3 2\n218\nsup4_6b\n55\nsup4 bo8 rm3\nsup4 ppo rm3 1\n752\nsup4 ppo rm3 1\nsup4 ppo rm3 1\n372\nsup4 ppo rm3 2\nsup4 ppo rm3 2\n4256\nsup4_6b\n215\nsup4 ppo rm3 3\nsup4 ppo rm3 3\n4037\nsup4_6b\n216\nTable 11: Training data for reward models. \"ref\" refers to human reference summaries.\nC.7"
      },
      {
        "header": "Example comparison tasks",
        "content": "To give a sense of the dif\ufb01culty of the comparisons task, we provide example comparisons between\ntwo summaries generated by our 6.7B human feedback model. In Table 12 we show both a random\ncomparison drawn from the TL;DR dataset, and a cherry-picked comparison (selected from 10\ncomparisons where labelers disagreed) to illustrate the trade-off between accuracy in coverage that\ncan occur when labelers conduct evaluations.\n26\n\n\nRandom TL;DR comparison"
      },
      {
        "header": "POST",
        "content": "Subreddit: r/Pets\nTITLE: What do you use for \ufb02ea control?\nMy family has tried literally EVERYTHING to control the \ufb02eas in our neighborhood (Frontline, Advantage,\nDiatomaceous Earth, Dawn Dishsoap, etc!) and nothing has worked. I have spoken to lots of pet owners in\nmy area (I work as a vet assistant) and many are reporting similar results, where \ufb02eas are becoming resistant\nto the usually recommended treatments. The only thing that has worked so far is Comfortis, but I\u2019ve read of\nseveral dogs having reactions to it that can be pretty severe. My dogs are \ufb01ne, we\u2019ve used it for about a year\nnow, but I don\u2019t like the idea of harming them or putting them at risk.\nGiving them baths with blue Dawn dish soap does kill all the \ufb02eas, but it does nothing to prevent more from\ncoming back, obviously. It only kills on contact, and we are NOT going to over bath them because that isn\u2019t\nhealthy either. We\u2019re looking for something that lasts.\nDoes anyone else have experience with this, or any detailed information on Comfortis and if it does serious\ndamage to your pet\u2019s system? Yes, I know I am a vet assistant. My boss strictly recommends Frontline and\nliterally will not listen to me when I tell him it doesn\u2019t work and my dogs are still covered in \ufb02eas and we\nhave to use Comfortis because it is the only thing that gives them relief. He is not a resource in this case.\nJust wanted to see what other pet owners (speci\ufb01cally ones in San Diego) do for \ufb02eas...the ones we have here\nare mutants or something, because almost nothing works on them!\nSummary A: Fleas are developing resistance to most \ufb02ea control products (including Comfortis). Looking\nfor something that lasts long term that doesn\u2019t harm my dogs. Does anyone have experience with any of the\nlisted products?\nSummary B: Nothing has worked on our \ufb02eas, we are looking for something that lasts, Comfortis is not a\nlong term solution. Does anyone else have experience with \ufb02ea control or have information on Comfortis?\nHard TL;DR comparison"
      },
      {
        "header": "POST",
        "content": "Subreddit: r/weddingplanning\nTITLE: Feeling major anxiety about dress shopping.\nSo, not really sure if I\u2019m asking for advice or just a small rant. We got engaged March 2, 2013. From day 1\nwe\u2019ve been struggling through the planning. At \ufb01rst, it was arguing with his parents about us getting married\nin a church. And then it was an argument about which venue to have the reception. We \ufb01nally have the venue\nbooked and the church matter settled. Now that\u2019s out of the way, I suddenly have this pit in my stomach\nMy mom left me when I was 14. I\u2019ve basically done everything on my own and I have really been ok about it.\nI\u2019m sure it\u2019s not of the norm for me to feel so disassociated about the whole thing, but I am. I\u2019m suppose to\ngo look at wedding dresses this Friday. I am feeling super anxious because I don\u2019t know if trying on wedding\ndresses is going to turn me into a blubbering baby about not having a mom.\nMy future mother-in-law is suppose to come with me to help look. I worry about turning into that blubbering\nbaby and offending her. I don\u2019t want her thinking that I don\u2019t appreciate her being there.\nAside from me worrying about becoming a giant baby, I\u2019ve also been having issues with my bridal party.\nWhile I haven\u2019t made any of\ufb01cial choices, I have ideas of who I want involved. That would be my best\nfriend, my sister, and my future sister-in-law. My \ufb01rst choice for a MOH is my best friend. However, she\nlives out of state, and is in a medical program for school. So her visit time is severely limited. My sister feels\nentitled to be the MOH, despite the fact that we are not close at all. So getting people together to get any\nkind of wedding stuff done is almost impossible.\nSummary A: I\u2019m having doubts about whether or not to try on wedding dresses. I am also having doubts\nabout my bridal party\u2019s ability to get things done.\nSummary B: I think I\u2019m going to turn into a blubbering baby and offend my mother-in-law.\nTable 12: Top: Example of a random comparison task on the TL;DR dataset between two summaries\nfrom our 6.7B human feedback model. Comparison chosen randomly from the validation set. Bottom:\nAn example of a dif\ufb01cult comparison task on the TL;DR dataset. Chosen by looking at comparisons\nbetween supervised baseline summaries with at least 4 labeler judgements and with at least 40% vote\nfor each summary. Cherry-picked out of 10 to highlight an accuracy-coverage tradeoff. Summary A\nis inaccurate since the author does not explicitly say she is having doubts about trying on wedding\ndresses. Summary B is entirely accurate but does not capture the general essence of the post. In this\ncase, 4 workers chose A and 3 workers chose B. For more comparisons, see our website."
      },
      {
        "header": "D\nChoice of baselines",
        "content": "In testing our human feedback techniques, we collected a large amount of high-quality data from\nhuman labelers. In order to compare fairly against supervision-based techniques, we would have\nneeded to spend a similar amount of labeler time collecting high quality demonstrations, and used\nthose to \ufb01ne-tune a model via supervised learning. Because this is prohibitively expensive, we do not\nprovide such a baseline.\nExisting prior work such as PEGASUS [70] has studied supervised methods on a dataset very similar\nto ours (the /r/tifu subset of TL;DR). However, they use much smaller (500M parameters) models,\nand report that their model outputs are worse than the human reference summaries, according to\nhuman evaluations. Thus, due to our limited labeler budget for evaluation, we decided to use our own\nsupervised and zero-shot models as baselines (after sanity-checking the ROUGE performance of our\nsupervised models), as well as T5 [49].\nT5 models [49] are pretrained and \ufb01ne-tuned in a similar way to our supervised baselines, but they\nuse an encoder-decoder architecture. We used T5 outputs which were obtained via beam search\ndecoding, as described in [49]. We also carefully account for differences in tokenization between\nmodel outputs.13\n13Since tokenization affects capitalization and punctuation of the model outputs, we normalized all CNN/Daily\nMail outputs from all models by lower-casing everything and then heuristically re-capitalizing. We verify that\nthis normalization procedure produces identical results for reference summaries tokenized in different ways.\n28\n\n\nE\nCNN/DM lead-3 vs reference summaries\nOn the CNN/DM dataset, our labelers signi\ufb01cantly preferred lead-3 (a summary consisting of the \ufb01rst\n3 sentences of the article) to reference summaries. In part this is due to longer summaries receiving\nhigher coverage scores and lead-3 being 50% longer, as shown in Table 13."
      },
      {
        "header": "Quality increase",
        "content": "/ 100 char.\nref\n314 (119)\n5.54\n0.14\nlead-3\n475 (114)\n6.23\n0.34\nTable 13: How length affects overall quality on CNN/DM for lead-3 and reference summaries.\nHowever, if we use a linear regression (similar to the procedure in Appendix F) to predict what lead-3\nperformance would be if its average length were reduced to 314 characters, we still \ufb01nd a quality\nof 5.68, modestly higher than the reference summaries. Moreover, for lead-3 to even achieve parity\nwith the reference summaries seems to call into question the need for abstractive summarization or\nsophisticated ML methods, since a simple extractive baseline can match a perfect imitation of the\nreference summaries.\nWe wanted to understand labeler behavior on these comparisons, to ensure that it was not an error.\nTo do this, we examined a sample of our labeler\u2019s judgments ourselves. We found that in 20/143\ncases labelers preferred lead-3 by 3 points or more, and that excluding these datapoints would raise\nthe relative score of the reference summaries by about 0.5 points.14 We were surprised to see the\nreference summaries performing so poorly in a signi\ufb01cant fraction of cases, so we looked at labeler\u2019s\nexplanations and con\ufb01rmed they made sense.\nWe found that two features of the reference summaries explained most of its underperformance. First,\n13 of these 20 summaries omitted one of the key points from the article\u2014the highlights are often\nwritten for a reader who had already seen the title of the article, even though the titles are not included\nin the CNN/DM dataset. Second, 10 of these 20 summaries actually introduced new information not\npresent in the original article. From the perspective of labelers this information is totally confabulated\nand so led to lower scores. A likely explanation for these errors is that the reference summaries are\nextracted from \u201chighlights\u201d on the news sites rather than being a straightforward summary of the\narticle. These failures are common enough that they signi\ufb01cantly impact the average quality of the\nreference summaries, and the effects seem to be large relative to quality differences between ML\nmodels.\nOverall we believe that labeler judgments were reasonable in these cases, and that it is potentially\nproblematic to treat the \u201chighlights\u201d in the CNN/DM dataset as reference summaries. You can view\nall of our labeler\u2019s judgments on CNN/DM at our website.\n14The reference summaries were preferred to lead-3 by a similar margin in only 7/143 cases."
      },
      {
        "header": "Pretrain only",
        "content": "(b)\nFigure 10: (a) A length-controlled version of Figure 1, using the procedure described in Appendix\nF. Controlling for length reduces the relative preference of our human feedback models, however\nthey are still preferred to the reference summaries. (b) Plotting model quality for different summary\nlengths on the TL;DR dataset. Our 6.7B human feedback model outperforms both the 6.7B supervised\nbaseline and the reference summaries (horizontal line at 0.5) across lengths."
      },
      {
        "header": "F\nControlling for summary length",
        "content": "As discussed in Section 4.1, the length of a summary is a confounding factor for evaluating summary\nquality; depending on the desired trade-off between conciseness and coverage, a shorter or longer\nsummary might be better. Our models generate summaries that are longer than the reference\nsummaries, as this led to higher labeler preference given the 24-48 token limit for our task. Here we\ndescribe the procedure we use to attempt to control for length.\nTo calculate a single length-controlled preference number, we train a logistic regression model to\npredict the human-preferred summary on our dataset of human comparisons. We provide this model\nwith 2 features: the identity of each policy, and the log ratio of the summary lengths. To calculate\nthe length-controlled preference value between two policies, we simply give each policy ID to our\ntrained logistic regression model and set the log length ratio to zero (see Figure 10a). In Figure 10b\nwe examine summary quality across a range of summary lengths on TL;DR. We \ufb01nd that our human\nfeedback model outperforms the supervised baseline across all length values.\nFor CNN/DM, we use a similar procedure as described above to control for length, except using a\nlinear regression model to predict the Likert rating from 1-7. We show the expected quality increase\nfor making summaries 100 characters longer in Table 14, which suggests our human feedback models\nwould perform better if they generated longer summaries."
      },
      {
        "header": "Quality",
        "content": "Quality \u2197\n(stdev)\n(1-7)\n/ 100 char.\nsl(tldr)-1.3b\n138 (34)\n4.26\n0.68\nsl(tldr)-6.7b\n127 (31)\n4.41\n0.38\ngpt-1.3b\n141 (41)\n4.11\n0.63\ngpt-6.7b\n142 (36)\n4.6\n0.3\nrl(tldr)-1.3b\n166 (30)\n4.86\n1.28\nrl(tldr)-6.7b\n175 (30)\n5.25\n0.87\nsl(cnn)-6.7b\n300 (103)\n5.4\n0.37\nref\n314 (119)\n5.54\n0.14\nlead-3\n475 (114)\n6.23\n0.34\nT5\n316 (95)\n5.92\n0.3\nTable 14: How length affects overall quality on CNN/DM. We show average length and quality\nscores for various policies, and how much the summary quality increases on average per 100 added\ncharacters."
      },
      {
        "header": "Value function ablation",
        "content": "In this section, we conduct an ablation comparing using separate parameters for the value function\nand policy, against using a shared network as done in [73]. The results, shown in Figure 11, clearly\nindicate that using separate networks outperforms the latter. On the other hand, having separate\nnetworks increases the memory requirements of running RL \ufb01ne-tuning. Having separate networks\nalso allows us to initialize the value function to be the learned reward model that is being optimized.\nFigure 11: Comparing the reward obtained by optimizing with separate value function and reward\nmodel parameters to shared parameters.\nG.2"
      },
      {
        "header": "Evaluating policies along axes of quality",
        "content": "We show the full results of the evaluations of policies on a 7-point Likert scale along different axes\nof quality; for TL;DR this is shown in Figure 12, and for CNN/DM this is shown in Figure 13. It is\nevident that on both datasets coverage correlates strongly with overall score across models, and all\nmodels achieve a high coherence score.\nG.3\nStudying best-of-N optimization"
      },
      {
        "header": "A natural way to evaluate an automatic evaluation metric is to see the extent to which optimizing",
        "content": "against it leads to high performance according to humans. One way to assess this is to use best-of-N\nas an (inef\ufb01cient) optimization technique \u2014 this has the bene\ufb01ts of being simple and invariant to\nmonotonic transformations. We report results for up to best-of-2048 on ROUGE and three of our\nreward models in Figure 7, using samples from the 1.3B supervised baseline. The results suggest\nthat optimizing against ROUGE signi\ufb01cantly under-performs optimizing against our reward models."
      },
      {
        "header": "The data also suggests ROUGE degrades with too much optimization much faster than our reward",
        "content": "models.\nWith increasing N, the best-of-N policies get higher average reward. Similarly, by decreasing the\nKL coef\ufb01cient \u03b2, the PPO policies get higher average reward. We found that at a given average\nreward, the best-of-N and PPO policies have similar quality as judged by human labelers (not shown).\nHowever, the PPO policy is farther from the supervised baseline than best-of-N is, as measured by\nthe KL divergence.15\nG.4"
      },
      {
        "header": "ROUGE scores",
        "content": "In Figure 14a and 14b, we show the ROUGE scores of our models on the TL;DR and CNN/DM\ndatasets, respectively. We report results with T=0, consistent with our human evaluations. We found\nthat temperature has an (often signi\ufb01cant) impact on ROUGE score, and we did a thorough sweep to\nverify that the best temperature setting is T=0.\n15We can use KL from the supervised baseline as a distance metric. Note that we can calculate the KL of a\nbest-of-N policy analytically as log(n) \u2212n\u22121\nn .\n31\n\n\nFigure 12: Evaluating TL;DR policies on a 7-point Likert scale along several axes of quality."
      },
      {
        "header": "Model",
        "content": "ROUGE-1\nROUGE-2\nROUGE-L\nProphetNet [67]\n44.20\n21.17\n40.69\nT5 [49]\n43.52\n21.55\n40.69\nOur 6.7B supervised model\n42.49\n19.84\n39.53\nCNN-2sent-hieco-RBM [71]\n42.04\n19.77\n39.42\nTable 15: Comparing the ROUGE score of our 6.7B supervised model on CNN/DM to recent SOTA\nmodels from the literature. Without any summarization-speci\ufb01c engineering, our model achieves\nROUGE scores better than SOTA models from mid-2019, indicating that it is a strong baseline for\ncomparison.\n32\n\n\nFigure 13: Evaluating CNN/DM policies on a 7-point Likert scale along several axes of quality.\nOn TL;DR, we \ufb01nd that our human feedback models obtain a slightly lower ROUGE score than\nthe supervised models at T = 0, further indicating that ROUGE correlates poorly with human\npreferences. For supervised models, lowering temperature has a larger impact than increasing model\nsize. Interestingly, at higher temperatures, our feedback models actually outperform supervised\ncounterparts (not shown).\nOn CNN/DM, ROUGE agrees with our human evaluations that our human feedback models transfer\nbetter than our supervised models. However, unsurprisingly, supervised CNN/DM models still\nachieve much higher ROUGE. In Table 15, we show the ROUGE results on CNN/DM for our 6.7B\nsupervised baseline and various models from the literature. We \ufb01nd that our model achieves ROUGE\nscores less than T5 [49], but slightly greater than the CNN-2sent-hieco-RBM model from [71], which\nwas SOTA for abstractive summarization on CNN/DM in mid-2019 according to the NLP-progress\nleaderboard.16\nG.5"
      },
      {
        "header": "Bigram overlap statistics",
        "content": "In Table 16, we show the bigram overlap statistics for our models on the TL;DR and CNN/DM\ndatasets as a proxy for how much the summaries copy frmo the post. As in Section 4.4, we compute\nthe longest common subsequence of bigrams with the original Reddit post or news article, and\ndividing by the number of bigrams in the summary. We \ufb01nd that models evaluated on CNN/DM\n16http://nlpprogress.com/english/summarization.html\n33\n\n\n(a)\n(b)\nFigure 14: ROUGE scores for our models on (a) the TL;DR dataset, and (b) the CNN/DM dataset.\nEvaluated on TL;DR"
      },
      {
        "header": "GPT",
        "content": "13B\n75.9%\nSupervised (TL;DR)\n1.3B\n49.0%\nSupervised (TL;DR)\n3B\n48.7%\nSupervised (TL;DR)\n6.7B\n48.9%\nSupervised (TL;DR)\n13B\n48.0%\nHuman feedback (TL;DR)\n1.3B\n53.3%\nHuman feedback (TL;DR)\n6.7B\n46.0%\nEvaluated on CNN/DM"
      },
      {
        "header": "GPT",
        "content": "6.7B\n76.2%\nSupervised (TL;DR)\n1.3B\n59.5%\nSupervised (TL;DR)\n6.7B\n56.9%\nHuman feedback (TL;DR)\n1.3B\n64.8%\nHuman feedback (TL;DR)\n6.7B\n51.2%\nSupervised (CNN/DM)\n1.3B\n66.0%\nT5\n11B\n68.8%\nreference\n\u2014\n36.8%\nTable 16: Bigram overlap statistics on the TL;DR dataset (top) and the CNN/DM dataset (bottom).\nModels trained on CNN/DM copy signi\ufb01cantly more than models trained on TL;DR.\n(whether or not they were trained on CNN/DM) generally copy more than models evaluated on\nTL;DR. Further, our supervised and human feedback models copy less than our pretrained models.\nG.6"
      },
      {
        "header": "Reward model validation sets",
        "content": "In this section, we report results evaluating our reward models on various manually constructed\nvalidation sets, shown in Tables 17 and 18. Notably, we asked our humans to produce a small dataset\nof edits, by having them make improvements to existing summaries (either reference summaries or\nsupervised baseline summaries). Our 6.7B reward model prefer the improved summaries at a similar\nrate to humans (who do not know which summary has been edited).\nOur reward models are also sensitive to sentence shuf\ufb02ing (whereas metrics like ROUGE are largely\nnot), and are able to detect when the roles portrayed in the summary have been switched. On the\nother hand, our reward models sometimes exhibit preference for poor arti\ufb01cial summaries, such as"
      },
      {
        "header": "Longer",
        "content": "89.2%\n88.6%\n80.2%\nAvg.\n83.7%\n85.6%\n76.7%\nTable 17: Comparing reward model and human preference of summaries that were edited by humans\nto make them better. For each summary, the human labeler that makes the comparison is different\nthan the labeler that wrote the edit. The agreement numbers do not include comparisons where the\nlabeler\u2019s preference was marked as \u2018uncertain\u2019.\nPreference % of Summary A"
      },
      {
        "header": "Reference summary",
        "content": "rand-3\n69.5%\n59.5%\nTable 18: Reward model performance on various manually constructed validation sets. In all cases,\nSummary A is intended to be better than Summary B, and thus a higher preference % is generally\nbetter. \u2018rand-3\u2018 indicates a baseline where 3 random sentences are taken from the post; however these\nsentences are kept in the order in which they appear in the post. \u2018Original summary\u2019 is either the\nreference summary or a summary from our supervised baselines. r/tifu is a subreddit whose purpose\nis sharing embarrassing stories (not asking for advice).\nthe post title copied twice, or asking for advice at the end of the summary. In Table 19, we show\nexamples where our model is sensitive to small, semantically meaningful changes in the summary.\nG.7"
      },
      {
        "header": "Measuring agreement between different evaluation metrics",
        "content": "We are interested in understanding the relationship between different metrics for evaluating summaries.\nTo do this, we compute agreement between various metrics, including automatic metrics and humans,\nfor different subsets of the data for which we have human evaluations. To remove policy quality\nas a confounding variable, all of the summary comparisons are generated by the same policy at the\nsame temperature value. In Table 20, we use samples from our 1.3B supervised model at T=0.7 on\nTL;DR; Table 21 has comparisons from our 6.7B supervised model at T=0.7 on TL;DR; Table 22\nhas comparisons from our 6.7B human feedback model at T=0.7 on TL;DR; and Table 23 has\ncomparisons from our 6.7B supervised baseline trained on CNN/DM.\nOur 6.7B reward model generally agrees with labelers as much as other labelers, although an\nensemble of labelers does better. On the other hand, ROUGE generally has poor agreement, as does\nlog probability under the supervised baselines, with simple heuristics like copying (longest common\nsubsequence of bigrams with the article) and length often performing comparably."
      },
      {
        "header": "Edited summary",
        "content": "Reward \u2206\nCrush on girl I haven\u2019t seen in 4 years. She doesn\u2019t like me and I don\u2019t still like\nher. What do?\n+0.64\nA girl told me she loved liked me, she ended up picking another guy over me,\nthat guy badly in\ufb02uenced her, and now I\u2019m here alone thinking what could\u2019ve\nbeen.\n+0.82"
      },
      {
        "header": "Boyfriend still FB stalks his high school ex girlfriend from time to time and",
        "content": "told me when he was very drunk that she was his \ufb01rst love.\n+0.73\nI\u2019ve become pathetic, pining after a guy my ex. Would like to reach state of less\npathetic. If more info is necessary, please let me know.\n+0.69\nI have body issues (body acne/scarring and weight issues) that prevent me from\nhaving a normal life without shame and prevent me from having a better sex\nlife with my BF.\n+1.0\nDo you take someone back after they\u2019ve turned you down off, even if you can\u2019t\nsee them in person or are they just not worth the risk?\n+0.52\nTable 19: Qualitative examples showing the change in reward of the reward model on human-\ngenerated edits to TL;DR summaries that make the summaries better. Examples are randomly\nselected from the set where the edit distance was less than 5 and the magnitude of change in reward\nwas greater than 0.5. Text in strike-through was removed from the original summary in the edit, and\ntext in bold was added. The reward model is sensitive to small but semantically meaningful changes\nin the summary, although it makes errors on occasion.\nTL;DR\n1.3B sup.\nT=0.7\nresearcher labeler\nlabeler\nensem-\nble\nlength\ncopying"
      },
      {
        "header": "RM",
        "content": "researcher 73.4%\n\u00b14.1%\n77.7%\n\u00b12.1%\n84.4%\n\u00b13.3%\n55.5%\n\u00b14.3%\n62.3%\n\u00b14.1%\n59.1%\n\u00b14.2%\n61.8%\n\u00b14.8%\n72.2%\n\u00b14.5%\n62.8%\n\u00b14.7%\n78.0%\n\u00b13.9%\nlabeler\n77.7%\n\u00b12.1%\n68.6%\n\u00b11.7%\n74.4%\n\u00b12.0%\n54.4%\n\u00b11.3%\n58.0%\n\u00b11.2%\n57.7%\n\u00b11.3%\n58.7%\n\u00b12.0%\n65.8%\n\u00b12.0%\n61.9%\n\u00b12.1%\n70.8%\n\u00b11.8%\nlabeler\nensemble 84.4%\n\u00b13.3%\n74.4%\n\u00b12.0%\n\u2014\n60.6%\n\u00b14.0%\n62.7%\n\u00b13.8%\n59.0%\n\u00b13.9%\n59.5%\n\u00b14.4%\n71.0%\n\u00b13.9%\n59.5%\n\u00b14.3%\n72.5%\n\u00b13.8%\nlength\n55.5%\n\u00b14.3%\n54.4%\n\u00b11.3%\n60.6%\n\u00b14.0%\n\u2014\n50.1%\n\u00b11.3%\n58.6%\n\u00b11.2%\n28.9%\n\u00b12.1%\n52.6%\n\u00b12.3%\n27.6%\n\u00b12.0%\n54.3%\n\u00b12.3%\ncopying\n62.3%\n\u00b14.1%\n58.0%\n\u00b11.2%\n62.7%\n\u00b13.8%\n50.1%\n\u00b11.3%\n\u2014\n51.9%\n\u00b11.2%\n61.6%\n\u00b12.3%\n57.8%\n\u00b12.3%\n60.9%\n\u00b12.2%\n55.5%\n\u00b12.2%"
      },
      {
        "header": "ROUGE",
        "content": "59.1%\n\u00b14.2%\n57.7%\n\u00b11.3%\n59.0%\n\u00b13.9%\n58.6%\n\u00b11.2%\n51.9%\n\u00b11.2%\n\u2014\n49.5%\n\u00b12.3%\n56.4%\n\u00b12.2%\n51.1%\n\u00b12.3%\n59.2%\n\u00b12.3%\n1.3B sup.\nlogprob\n61.8%\n\u00b14.8%\n58.7%\n\u00b12.0%\n59.5%\n\u00b14.4%\n28.9%\n\u00b12.1%\n61.6%\n\u00b12.3%\n49.5%\n\u00b12.3%\n\u2014\n58.7%\n\u00b12.3%\n92.7%\n\u00b11.2%\n60.6%\n\u00b12.3%\n1.3B RM 72.2%\n\u00b14.5%\n65.8%\n\u00b12.0%\n71.0%\n\u00b13.9%\n52.6%\n\u00b12.3%\n57.8%\n\u00b12.3%\n56.4%\n\u00b12.2%\n58.7%\n\u00b12.3%\n\u2014\n58.8%\n\u00b12.2%\n78.8%\n\u00b11.8%\n6.7B sup.\nlogprob\n62.8%\n\u00b14.7%\n61.9%\n\u00b12.1%\n59.5%\n\u00b14.3%\n27.6%\n\u00b12.0%\n60.9%\n\u00b12.2%\n51.1%\n\u00b12.3%\n92.7%\n\u00b11.2%\n58.8%\n\u00b12.2%\n\u2014\n61.5%\n\u00b12.2%\n6.7B RM 78.0%\n\u00b13.9%\n70.8%\n\u00b11.8%\n72.5%\n\u00b13.8%\n54.3%\n\u00b12.3%\n55.5%\n\u00b12.2%\n59.2%\n\u00b12.3%\n60.6%\n\u00b12.3%\n78.8%\n\u00b11.8%\n61.5%\n\u00b12.2%\n\u2014\nTable 20: Agreement rates between humans and various automated metrics on TL;DR 1.3b supervised\nmodel at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler\nensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude\nthe labeler being predicted. All ensembles have at least 3 workers.\n36\n\n\nTL;DR\n6.7B sup.\nT=0.7\nlabeler\nlabeler\nensem-\nble\nlength\ncopying"
      },
      {
        "header": "RM",
        "content": "labeler\n70.8%\n\u00b12.6%\n73.1%\n\u00b12.9%\n56.9%\n\u00b10.6%\n56.4%\n\u00b10.6%\n56.9%\n\u00b10.6%\n54.5%\n\u00b11.2%\n67.5%\n\u00b11.1%\n54.3%\n\u00b11.2%\n69.7%\n\u00b11.1%\nlabeler\nensemble 73.1%\n\u00b12.9%\n\u2014\n55.0%\n\u00b15.1%\n54.5%\n\u00b14.8%\n66.7%\n\u00b14.7%\n61.1%\n\u00b111.4%\n77.8%\n\u00b19.7%\n55.6%\n\u00b111.7%\n77.8%\n\u00b110.0%\nlength\n56.9%\n\u00b10.6%\n55.0%\n\u00b15.1%\n\u2014\n50.5%\n\u00b10.6%\n60.2%\n\u00b10.6%\n26.9%\n\u00b11.1%\n59.5%\n\u00b11.2%\n26.4%\n\u00b11.1%\n60.3%\n\u00b11.1%\ncopying\n56.4%\n\u00b10.6%\n54.5%\n\u00b14.8%\n50.5%\n\u00b10.6%\n\u2014\n54.4%\n\u00b10.6%\n59.3%\n\u00b11.1%\n57.9%\n\u00b11.2%\n60.2%\n\u00b11.2%\n58.0%\n\u00b11.2%"
      },
      {
        "header": "ROUGE",
        "content": "56.9%\n\u00b10.6%\n66.7%\n\u00b14.7%\n60.2%\n\u00b10.6%\n54.4%\n\u00b10.6%\n\u2014\n48.7%\n\u00b11.2%\n58.1%\n\u00b11.2%\n47.7%\n\u00b11.2%\n58.4%\n\u00b11.2%\n1.3B sup.\nlogprob\n54.5%\n\u00b11.2%\n61.1%\n\u00b111.4%\n26.9%\n\u00b11.1%\n59.3%\n\u00b11.1%\n48.7%\n\u00b11.2%\n\u2014\n53.3%\n\u00b11.2%\n91.9%\n\u00b10.6%\n53.8%\n\u00b11.2%\n1.3B RM 67.5%\n\u00b11.1%\n77.8%\n\u00b19.7%\n59.5%\n\u00b11.2%\n57.9%\n\u00b11.2%\n58.1%\n\u00b11.2%\n53.3%\n\u00b11.2%\n\u2014\n54.1%\n\u00b11.2%\n78.8%\n\u00b11.0%\n6.7B sup.\nlogprob\n54.3%\n\u00b11.2%\n55.6%\n\u00b111.7%\n26.4%\n\u00b11.1%\n60.2%\n\u00b11.2%\n47.7%\n\u00b11.2%\n91.9%\n\u00b10.6%\n54.1%\n\u00b11.2%\n\u2014\n54.5%\n\u00b11.2%\n6.7B RM 69.7%\n\u00b11.1%\n77.8%\n\u00b110.0%\n60.3%\n\u00b11.1%\n58.0%\n\u00b11.2%\n58.4%\n\u00b11.2%\n53.8%\n\u00b11.2%\n78.8%\n\u00b11.0%\n54.5%\n\u00b11.2%\n\u2014\nTable 21: Agreement rates between humans and various automated metrics on TL;DR 6.7B supervised\nmodel at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler\nensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude\nthe labeler being predicted. All ensembles have at least 3 workers.\nTL;DR\n6.7B RL\nT=0.7\nlabeler\nlabeler\nensem-\nble\nlength\ncopying"
      },
      {
        "header": "RM",
        "content": "labeler\n60.4%\n\u00b15.9%\n66.0%\n\u00b17.6%\n55.8%\n\u00b12.2%\n52.7%\n\u00b12.1%\n49.9%\n\u00b12.1%\n48.0%\n\u00b12.2%\n57.4%\n\u00b12.0%\n47.3%\n\u00b12.2%\n62.3%\n\u00b12.1%\nlabeler\nensemble 66.0%\n\u00b17.6%\n\u2014\n80.0%\n\u00b18.9%\n65.0%\n\u00b110.6%\n35.0%\n\u00b110.5%\n45.0%\n\u00b111.1%\n75.0%\n\u00b19.8%\n40.0%\n\u00b110.5%\n75.0%\n\u00b19.8%\nlength\n55.8%\n\u00b12.2%\n80.0%\n\u00b18.9%\n\u2014\n48.1%\n\u00b12.2%\n50.3%\n\u00b12.2%\n30.0%\n\u00b12.1%\n62.0%\n\u00b12.1%\n30.4%\n\u00b12.0%\n59.8%\n\u00b12.2%\ncopying\n52.7%\n\u00b12.1%\n65.0%\n\u00b110.6%\n48.1%\n\u00b12.2%\n\u2014\n52.0%\n\u00b12.2%\n64.2%\n\u00b12.1%\n56.7%\n\u00b12.2%\n64.4%\n\u00b12.1%\n53.4%\n\u00b12.2%"
      },
      {
        "header": "ROUGE",
        "content": "49.9%\n\u00b12.1%\n35.0%\n\u00b110.5%\n50.3%\n\u00b12.2%\n52.0%\n\u00b12.2%\n\u2014\n50.5%\n\u00b12.2%\n52.0%\n\u00b12.3%\n51.1%\n\u00b12.3%\n54.5%\n\u00b12.1%\n1.3B sup.\nlogprob\n48.0%\n\u00b12.2%\n45.0%\n\u00b111.1%\n30.0%\n\u00b12.1%\n64.2%\n\u00b12.1%\n50.5%\n\u00b12.2%\n\u2014\n47.0%\n\u00b12.2%\n90.2%\n\u00b11.3%\n46.1%\n\u00b12.2%\n1.3B RM 57.4%\n\u00b12.0%\n75.0%\n\u00b19.8%\n62.0%\n\u00b12.1%\n56.7%\n\u00b12.2%\n52.0%\n\u00b12.3%\n47.0%\n\u00b12.2%\n\u2014\n45.7%\n\u00b12.1%\n71.4%\n\u00b12.0%\n6.7B sup.\nlogprob\n47.3%\n\u00b12.2%\n40.0%\n\u00b110.5%\n30.4%\n\u00b12.0%\n64.4%\n\u00b12.1%\n51.1%\n\u00b12.3%\n90.2%\n\u00b11.3%\n45.7%\n\u00b12.1%\n\u2014\n44.7%\n\u00b12.1%\n6.7B RM 62.3%\n\u00b12.1%\n75.0%\n\u00b19.8%\n59.8%\n\u00b12.2%\n53.4%\n\u00b12.2%\n54.5%\n\u00b12.1%\n46.1%\n\u00b12.2%\n71.4%\n\u00b12.0%\n44.7%\n\u00b12.1%\n\u2014\nTable 22: Agreement rates between humans and various automated metrics on TL;DR 6.7B human\nfeedback optimized model at T=0.7. Standard errors estimated via bootstrapping. Note: in the\nentry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons\nbecause we need to exclude the labeler being predicted. All ensembles have at least 3 workers."
      },
      {
        "header": "Random samples",
        "content": "Here we provide non-cherry-picked samples and human evaluations for various models. In Tables 25-\n26 we show samples on the TL;DR dataset, and in Tables 27-28 we show samples on the CNN/DM\ndataset (where we truncate the article for brevity). See our website for more uncurated policy samples.\nH.2"
      },
      {
        "header": "Overoptimized samples",
        "content": "We show examples of samples from a policy overoptimized to rm3. The summaries, while clearly\nlong, low quality, and full of idiosyncrasies, do still re\ufb02ect the rough gist of the post.\n38\n\n\nCNN/DM\n6.7B sup.\nT=0.3\nlabeler\nlabeler\nensem-\nble\nlength\ncopying"
      },
      {
        "header": "RM",
        "content": "labeler\n66.9%\n\u00b14.3%\n74.5%\n\u00b16.8%\n62.4%\n\u00b11.4%\n49.6%\n\u00b11.4%\n55.2%\n\u00b11.4%\n45.7%\n\u00b11.4%\n64.8%\n\u00b11.4%\n47.6%\n\u00b11.4%\n66.5%\n\u00b11.3%\nlabeler\nensemble 74.5%\n\u00b16.8%\n\u2014\n57.5%\n\u00b17.7%\n52.5%\n\u00b17.6%\n75.0%\n\u00b16.7%\n57.5%\n\u00b17.8%\n82.5%\n\u00b15.9%\n65.0%\n\u00b17.6%\n80.0%\n\u00b16.1%\nlength\n62.4%\n\u00b11.4%\n57.5%\n\u00b17.7%\n\u2014\n54.2%\n\u00b11.4%\n59.0%\n\u00b11.4%\n36.4%\n\u00b11.4%\n60.6%\n\u00b11.3%\n36.3%\n\u00b11.4%\n64.7%\n\u00b11.4%\ncopying\n49.6%\n\u00b11.4%\n52.5%\n\u00b17.6%\n54.2%\n\u00b11.4%\n\u2014\n46.4%\n\u00b11.4%\n66.2%\n\u00b11.3%\n51.6%\n\u00b11.4%\n65.5%\n\u00b11.4%\n51.7%\n\u00b11.4%"
      },
      {
        "header": "ROUGE",
        "content": "55.2%\n\u00b11.4%\n75.0%\n\u00b16.7%\n59.0%\n\u00b11.4%\n46.4%\n\u00b11.4%\n\u2014\n43.8%\n\u00b11.4%\n55.9%\n\u00b11.4%\n43.8%\n\u00b11.5%\n56.9%\n\u00b11.5%\n1.3B sup.\nlogprob\n45.7%\n\u00b11.4%\n57.5%\n\u00b17.8%\n36.4%\n\u00b11.4%\n66.2%\n\u00b11.3%\n43.8%\n\u00b11.4%\n\u2014\n50.2%\n\u00b11.4%\n87.2%\n\u00b11.0%\n48.2%\n\u00b11.4%\n1.3B RM 64.8%\n\u00b11.4%\n82.5%\n\u00b15.9%\n60.6%\n\u00b11.3%\n51.6%\n\u00b11.4%\n55.9%\n\u00b11.4%\n50.2%\n\u00b11.4%\n\u2014\n52.1%\n\u00b11.4%\n76.6%\n\u00b11.2%\n6.7B sup.\nlogprob\n47.6%\n\u00b11.4%\n65.0%\n\u00b17.6%\n36.3%\n\u00b11.4%\n65.5%\n\u00b11.4%\n43.8%\n\u00b11.5%\n87.2%\n\u00b11.0%\n52.1%\n\u00b11.4%\n\u2014\n51.0%\n\u00b11.4%\n6.7B RM 66.5%\n\u00b11.3%\n80.0%\n\u00b16.1%\n64.7%\n\u00b11.4%\n51.7%\n\u00b11.4%\n56.9%\n\u00b11.5%\n48.2%\n\u00b11.4%\n76.6%\n\u00b11.2%\n51.0%\n\u00b11.4%\n\u2014\nTable 23: Agreement rates between humans and various automated metrics on CNN/DM 6.7B\nsupervised model at T=0.3. Standard errors estimated via bootstrapping. NOTE: in the entry for\nlabeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because\nwe need to exclude the labeler being predicted. (All ensembles have at least 3 workers)"
      },
      {
        "header": "POST",
        "content": "Subreddit: r/relationships\nTITLE: Screwed up with boss... what should I do?\nI\u2019m 20 f, my boss is around 50 years old, also f.\nSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs\nI have had for three years, the other one I have had for a month and a bit. I forgot to give my\nschedule from one job to my boss at my other job, and so I was not scheduled for this week. I\ndidn\u2019t realize why I had not been put on the schedule until now.\nMy question is, since I royally screwed up, what can I do to redeem myself? I don\u2019t want to call\nmy boss today because it is a Sunday and she has the day off. Mistakes aren\u2019t easily forgiven\nwhere I work, as far as I can tell, and the boss often makes comments about how the employees\nshould be scared of her. I have screwed up at previous jobs (little things) but my boss was less\nintimidating than my current one, so I am not sure how to handle this situation.\nPolicy: Human written reference TL;DR\nSummary: screwed up at work by not giving the boss my schedule from my other job, am not\nscheduled this week, what should I say in order to apologize to my (scary/intimidating) boss?\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: Title (extractive baseline)\nSummary: Screwed up with boss... what should I do?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3\nPolicy: Lead-2 (extractive baseline)\nSummary: I\u2019m 20 f, my boss is around 50 years old, also f.\nSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs I\nhave had for three years, the other one I have had for a month and a bit.\nOverall score: 3 Accuracy: 7 Coherence: 7 Coverage: 4\nPolicy: 1.3B human-feedback model\nSummary: I screwed up by not giving my schedule to my boss at my other job, and now I don\u2019t\nknow what to do to redeem myself. What can I do to make things right again?\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B human-feedback model\nSummary: I forgot to give my boss my weekly schedule for one of my jobs, and so I was not\nscheduled this week. I royally screwed up. What can I do to redeem myself?\nOverall score: 5 Accuracy: 7 Coherence: 6 Coverage: 6\nPolicy: 6.7B supervised model\nSummary:\nI forgot to give my boss my schedule for one of my jobs, and now I have been\nscheduled for the wrong week. What do I do?\nOverall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3\nPolicy: 6.7B pretrained model\nSummary: I royally screwed up and I don\u2019t know what to do.\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3\nTable 24: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations,\nsee our website."
      },
      {
        "header": "POST",
        "content": "Subreddit: r/relationships\nTITLE: I am a [18 M] she is a [17 F] and I don\u2019t know how to read this relationship?\nWe\u2019ve known each other through school but never talked until we had a class together. I asked her\nout after about a week, we went out for food, laughed, \ufb02irted etc etc. I asked her out again same\nsituation, everything going great. Now it\u2019s three weeks later (midst of exams) and I\u2019m starting to\nget the feeling that she\u2019s not thinking of this as a \"relationship\" in the conventional bf/gf sense.\nI\u2019m new to the whole dating game and wondering a few things.\nDo I need to ask if we\u2019re together as bf/gf or is that implied by asking her out multiple times?\nShould I straight up ask if she likes me the way I like her? I know what the friend zone is and\nI\u2019m not feeling there, yet, but we\u2019re having a movie night tomorrow just her and I. I plan on\nseeing what\u2019s what then by trying to get physical, not hardcore, just simple stuff like leg touches,\ncuddling etc. Oh and \ufb01nally, to add to my lengthy situation, On our third \"date\" (studying for an\nexam) I drove her home and attempted to kiss her but got the infamous cheek. I didn\u2019t make a big\ndeal about it I just moved past said \"got your things?\" and politely said goodbye preceding to wait\nfor her to get inside her house. I\u2019ve been told I shouldn\u2019t fret about ONE kiss rejection cause there\ncould be multiple reasons for it but at the same time that made me think more about the friend\nzone possibility.\nAny advice or similar situations and how you solved the problem would be smashing! Thanks in\nadvance.\nPolicy: Human written reference TL;DR\nSummary: Started going out with a girl I liked. Feelings were obviously mutual. Took it slow\ndue to exams. Not sure now if she is feeling the same way I am.\nOverall score: 6 Accuracy: 4 Coherence: 7 Coverage: 7\nPolicy: Title (extractive baseline)\nSummary: I am a [18 M] she is a [17 F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nPolicy: Lead-2 (extractive baseline)\nSummary: We\u2019ve known each other through school but never talked until we had a class together.\nI asked her out after about a week, we went out for food, laughed, \ufb02irted etc etc.\nOverall score: 1 Accuracy: 7 Coherence: 7 Coverage: 1\nPolicy: 1.3B human-feedback model\nSummary: Been casually seeing girl for 3 weeks, everything is going great but I\u2019m starting to\nget the feeling she\u2019s not thinking of this as a relationship in the traditional sense. Should I ask if\nwe\u2019re together as bf/gf?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B human-feedback model\nSummary: Been on three dates with a girl, getting the feeling she\u2019s not interested in a relationship\nin the traditional sense. Do I ask if she likes me the way I like her or is that implied by asking her\nout multiple times?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B supervised model\nSummary: I\u2019m a [18 M] she\u2019s a [17 F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nPolicy: 6.7B pretrained model\nSummary: I\u2019m a [18M] and she\u2019s a [17F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nTable 25: Examples of evaluations on the TL;DR dataset. For further random examples of evaluations,\nsee our website."
      },
      {
        "header": "POST",
        "content": "Subreddit: r/relationships\nTITLE: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship.... Because\nof a video game.\nHi Reddit. My boyfriend and I have been dating for a year and it has been great. Except for one\nthing.\nDOTA. He plays it at least 2 hours a day, and on the weekends, all day if he can. If you know\nthat game you know it takes at least an hour on average per game and is one that you can\u2019t really\npause. Sometimes it is \ufb01ne that he plays, I don\u2019t mind sitting on my phone, browsing Reddit.\nHowever, lately it seems like it has interfered in our relationship as well as his studies. He is doing\nhis PH.D. in Physics. He is obviously very swamped with schoolwork that he seems to put on the\nback burner for this game. The other day, on a Saturday, I was over and he was playing a game. I\nthought it would just be one but instead he proceeded to play for three hours as I just sat there.\nWhat can I do? I have tried talking to him... Even saying this game is making me rethink our\nrelationship! He says he understands, but isn\u2019t proving it through his actions!\nReddit, what should I do? I really care about this guy and besides this gaming habit, our\nrelationship is great!\nPolicy: Human written reference TL;DR\nSummary: my boyfriend games whenever he can! How can I get him to stop gaming so much\nand focus more on school and our relationship?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: Title (extractive baseline)\nSummary: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship....\nBecause of a video game.\nOverall score: 6 Accuracy: 7 Coherence: 6 Coverage: 6\nPolicy: Lead-2 (extractive baseline)\nSummary: Hi Reddit. My boyfriend and I have been dating for a year and it has been great.\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 1\nPolicy: 1.3B human-feedback model\nSummary: My boyfriend is neglecting his studies and playing DOTA all day. It is interfering\nwith his schoolwork and our relationship. What can I do to \ufb01x this?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B human-feedback model\nSummary: My boyfriend is neglecting his studies and our relationship because of his excessive\ngaming of a video game. What can I do to get him to stop?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B supervised model\nSummary:\nBoyfriend plays video games for hours on end, neglecting his studies and our\nrelationship. What can I do?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B pretrained model\nSummary: My [24F] boyfriend [28M] is neglecting his studies and our relationship because of\na video game.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 5\nTable 26: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations,\nsee our website."
      },
      {
        "header": "ARTICLE",
        "content": "Site: dailymail\nTITLE: Tech \ufb01rm uses pole dancing ROBOTS to attract people in Shoreditch\nTechnology shows such as the annual E3 games conference in Los Angeles have become known\nfor their scantily clad \u2019booth babes\u2019 designed to attract people to stands.\nHowever, a Dutch \ufb01rm has taken a slightly different approach for its London event.\nTransIP hired a pair of pole dancing robots for an event in Shoreditch.\n[truncated for brevity]\nLINK: https://www.dailymail.co.uk/sciencetech/article-3059595/Forget-booth-babes-Tech-\ufb01rm-\nuses-pole-dancing-ROBOTS-attract-people.html\nPolicy: Human written reference summary\nSummary: Robots are made out of old car parts and have CCTV cameras as heads. Originally\ncreated by artist Giles Walker for a show called \u2019peepshow\u2019.\nOverall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3\nPolicy: Lead-3 (extractive baseline)\nSummary: Technology shows such as the annual E3 games conference in Los Angeles have\nbecome known for their scantily clad \u2019booth babes\u2019 designed to attract people to stands. However,\na Dutch \ufb01rm has taken a slightly different approach for its London event. Transip hired a pair of\npole dancing robots for an event in Shoreditch.\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 1.3B human-feedback model (transfer)\nSummary: Dutch \ufb01rm transip hired a pair of pole dancing robots for an event in London to attract\ntechnology enthusiasts to stand.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B human-feedback model (transfer)\nSummary: Dutch hosting company transip hired pole dancing robots to entertain Tech enthusiasts\nat its London event.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B supervised TL;DR model (transfer)\nSummary: Dutch \ufb01rm transip hired pole dancing robots to entertain its London event.\nOverall score: 4 Accuracy: 6 Coherence: 5 Coverage: 5\nPolicy: 6.7B pretrained model\nSummary: A Dutch \ufb01rm has hired pole dancing robots for an event in London.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B supervised CNN/DM model\nSummary: Robots were originally created by Walker for a show called \u2019peepshow\u2019 the dancers,\nwhich cost around \u00a32,500 to hire, were made out of old car parts.\nOverall score: 4 Accuracy: 7 Coherence: 7 Coverage: 4\nPolicy: 11B T5 model\nSummary: Transip hired a pair of pole dancing robots for an event in Shoreditch. The dancers,\nwhich cost around \u00a32,500 to hire, were made out of old car parts and were designed by British\nartist Giles Walker. The robots will be pole dancing for the remaining four days of the event,\nalong with presentations from leading \ufb01gures in the technology industry.\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nTable 27: Examples of evaluations on the CNN/DM dataset. For further random examples of\nevaluations, see our website."
      },
      {
        "header": "ARTICLE",
        "content": "Site: dailymail\nTITLE: Crying toddler removed from Transavia Airlines \ufb02ight and met by police\nA crying 19-month-old girl was removed from a plane and met by police after she was accused of\ncausing a security breach.\nAs the aircraft taxied on the runway at Ben Gurion Airport in Israel, bound for Luton, Sarina Aziz\nbecame agitated after cabin crew asked that she sit on her parents\u2019 lap.\nAriella and Mark Aziz, who live in north London, struggled to keep their daughter still after\nstrapping her in the connector belt required for children under the age of two.\nOne-year-old Sarina Aziz became agitated on a \ufb02ight from Israel to Luton that led to her being\nejected from the plane\n[truncated for brevity]\nLINK: https://www.dailymail.co.uk/travel/travel_news/article-3053657/Crying-toddler-removed-\nLondon-bound-\ufb02ight-met-police-deemed-security-breach.html\nPolicy: Human written reference summary\nSummary: Sarina Aziz was \ufb02ying back from Israel with parents Mark and ariella Aziz. But girl\nbecame agitated after being placed on the parents\u2019 lap. Pilot turned plane around at Ben Gurion,\nand armed police ejected family. Father Mark Aziz insists family were being compliant and asking\nfor help . Mother speaks of her disbelief at how the incident was handled by staff .\nOverall score: 3 Accuracy: 4 Coherence: 7 Coverage: 6\nPolicy: Lead-3 (extractive baseline)\nSummary: A crying 19-month-old girl was removed from a plane and met by police after she\nwas accused of causing a security breach. As the aircraft taxied on the runway at Ben Gurion\nAirport in Israel, bound for Luton, Sarina Aziz became agitated after cabin crew asked that she\nsit on her parents\u2019 lap. Ariella and Mark Aziz, who live in north London, struggled to keep their\ndaughter still after strapping her in the connector belt required for children under the age of two.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: 1.3B human-feedback model (transfer)\nSummary: 19-month-old girl was removed from a Transavia Airlines \ufb02ight after cabin crew\naccused her of causing a security breach. The incident was later determined to have been justi\ufb01ed.\nOverall score: 3 Accuracy: 4 Coherence: 7 Coverage: 4\nPolicy: 6.7B human-feedback model (transfer)\nSummary: Family were removed from a plane after a crying 19-month-old girl was accused of\ncausing a security breach. Family say they were simply trying to calm their child down. Transavia\nAirlines have said the action was justi\ufb01ed.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: 6.7B supervised TL;DR model (transfer)\nSummary: Family were removed from a plane after a one-year-old girl became unhappy after\nbeing moved from her seat to their laps.\nOverall score: 4 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B pretrained model\nSummary: A family was kicked off a \ufb02ight after their son refused to wear a seatbelt, a passenger\nclaims. The family were told they could not \ufb02y without a doctor\u2019s note clearing them to \ufb02y\nOverall score: 2 Accuracy: 6 Coherence: 7 Coverage: 3\nPolicy: 6.7B supervised CNN/DM model\nSummary: Sarina Aziz became agitated after being asked to sit on her parents\u2019 lap. The 19-\nmonth-old was strapped into a connector belt for children under two. But she became frustrated\nand unhappy after being moved from her seat. Transavia Airlines staff alerted the captain who\nhalted the \ufb02ight. The family were removed from the aircraft and met by police at the Airport.\nTransavia Airlines said the action was justi\ufb01ed as the child was not \u2019following safety instructions\u2019\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 11B T5 model\nSummary: One-year-old Sarina Aziz became agitated on a \ufb02ight from Israel to Luton. Cabin\ncrew asked her to sit on her parents\u2019 laps but she became distressed. Airline staff alerted the\ncaptain who decided to halt the \ufb02ight and call security to remove the family from the plane.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 6\nTable 28: Examples of evaluations on the CNN/DM dataset. For further random examples of\nevaluations, see our website."
      },
      {
        "header": "Overoptimized policy",
        "content": "I\u2019m 28, male, live in San Jose, and I\nwould like to learn how to do gym-\nnastics.\n28yo dude stubbornly postponees start pursuing\ngymnastics hobby citing logistics reasons despite\nobvious interest??? negatively effecting long term\n\ufb01tness progress both personally and academically\nthoght wise? want change this dumbass shitty ass\npolicy pls\nLeft password saved on work com-\nputer replacement spends every hour\nof the day watching net\ufb02ix.\nemployee stubbornly postponees replacement cit-\ning personal reasons despite tried reasonable com-\npromise offer??? negatively effecting productivity\nboth personally and company effort thoghtwise?\nwant change this dumbass shitty ass policy at work\nnow pls halp\nPeople won\u2019t stop asking about the\nold scars on my arms. How can I\nget them to leave me alone without\nbeing rude?\npeople insistently inquire about old self-harm scars\ndespite tried compromise measures??? negatively\neffecting forward progress socially and academi-\ncally thoghtwise? want change this dumbass shitty\nass behavior of mine please help pls halp"
      },
      {
        "header": "My roommate has been charging her",
        "content": "friend who is staying with us rent\nwithout telling me. She claims that\nbecause I\u2019m only subleasing a room\nfrom her she shouldn\u2019t have to split\nhis rent with me. Am I over-reacting\nby thinking that\u2019s ridiculous?\nroommate stubbornly keeps pocketing roommate\nrent despite tried reasonable compromise offer???\nnegatively effecting stability of cohabitation both\n\ufb01nancially and relationally thoght wise?\nwant\nchange this dumbass shitty ass policy of hers please\npls halp"
      },
      {
        "header": "My fridge broke in my apartment a",
        "content": "month ago and the landlord hasn\u2019t\nrepaired it after numerous discus-\nsions....I JUST WANT A COLD\nBEER! What should I do?\nlandlord stubbornly postponees gettin fridge re-\npair despite tried reasonable compromise offer???\nnegatively effecting lease both \ufb01nancially and re-\nlationally thoght wise? want change this dumbass\nshitty ass landlord behavior now please pls halp\nGirlfriend talks to ex, he\u2019s still in\nlove with her, shes completely open\nabout their conversations with me\nand has told him there is zero chance\nof me and her breaking up and that\nhes only in love with being in a re-\nlationship not with her, she says she\nhas nothing more to say to him, yet\ncontinues to talk to him. I\u2019m con-\nfused :(\ngirlfriend stubbornly continue talkin with es-\ntranged ex despite tried compromise offer??? neg-\natively effecting smooth communication and rela-\ntionship progress thoghtwise? regret this dumbass\nbehaviour on her part? need insight pls halp"
      },
      {
        "header": "GF",
        "content": "likes\nto\ncover\nher\ntracks/generally\nsecretive\nwhen\nusing social networks.\nHowever,\nnothing wrong is with our very\nopen-to-each-other relationship so\nfar. Should I confront her?\ngirlfriend obsessively snooping through personal\nstuff mid relationship despite agreed upon open-\nness policy???\nnegatively effecting communi-\ncation/trust level romantically/physically thoght\nwise? want change this dumbass crazy policy of\nhers pls help\nLaid off almost a year ago, up to my\nears in debt, and trying to \ufb01gure out\nhow to get motivated every day to do\nanything productive and \ufb01nd a job.\nkid stubbornly postponees pursuing job citing\nmedical reasons despite tried reasonable compro-\nmise??? negatively effecting forward employment\nmobility both personally and \ufb01nancially thoght-\nwise? want change this dumbass sel\ufb01sh/lazy atti-\ntude now please help pls\nTable 29: Example of samples from our over-optimized model over\ufb01t to one of our 1.3B reward\nmodels.\n45"
      }
    ],
    "metadata": {
      "format": "PDF 1.5",
      "title": "",
      "author": "",
      "subject": "",
      "keywords": "",
      "creator": "LaTeX with hyperref",
      "producer": "pdfTeX-1.40.21",
      "creationDate": "D:20220217020751Z",
      "modDate": "D:20220217020751Z",
      "trapped": "",
      "encryption": null
    },
    "num_pages": 45,
    "pages": [
      "Learning to summarize from human feedback\nNisan Stiennon\u2217\nLong Ouyang\u2217\nJeff Wu\u2217\nDaniel M. Ziegler\u2217\nRyan Lowe\u2217\nChelsea Voss\u2217\nAlec Radford\nDario Amodei\nPaul Christiano\u2217\nOpenAI\nAbstract\nAs language models become more powerful, training and evaluation are increas-\ningly bottlenecked by the data and metrics used for a particular task. For example,\nsummarization models are often trained to predict human reference summaries and\nevaluated using ROUGE, but both of these metrics are rough proxies for what we\nreally care about\u2014summary quality. In this work, we show that it is possible to\nsigni\ufb01cantly improve summary quality by training a model to optimize for human\npreferences. We collect a large, high-quality dataset of human comparisons be-\ntween summaries, train a model to predict the human-preferred summary, and use\nthat model as a reward function to \ufb01ne-tune a summarization policy using reinforce-\nment learning. We apply our method to a version of the TL;DR dataset of Reddit\nposts [63] and \ufb01nd that our models signi\ufb01cantly outperform both human reference\nsummaries and much larger models \ufb01ne-tuned with supervised learning alone. Our\nmodels also transfer to CNN/DM news articles [22], producing summaries nearly\nas good as the human reference without any news-speci\ufb01c \ufb01ne-tuning.2 We con-\nduct extensive analyses to understand our human feedback dataset and \ufb01ne-tuned\nmodels.3 We establish that our reward model generalizes to new datasets, and that\noptimizing our reward model results in better summaries than optimizing ROUGE\naccording to humans. We hope the evidence from our paper motivates machine\nlearning researchers to pay closer attention to how their training loss affects the\nmodel behavior they actually want.\n1\nIntroduction\nLarge-scale language model pretraining has become increasingly prevalent for achieving high per-\nformance on a variety of natural language processing (NLP) tasks. When applying these models\nto a speci\ufb01c task, they are usually \ufb01ne-tuned using supervised learning, often to maximize the log\nprobability of a set of human demonstrations.\nWhile this strategy has led to markedly improved performance, there is still a misalignment between\nthis \ufb01ne-tuning objective\u2014maximizing the likelihood of human-written text\u2014and what we care\nabout\u2014generating high-quality outputs as determined by humans. This misalignment has several\ncauses: the maximum likelihood objective has no distinction between important errors (e.g. making\nup facts [41]) and unimportant errors (e.g. selecting the precise word from a set of synonyms); models\n\u2217This was a joint project of the OpenAI Re\ufb02ection team. Author order was randomized amongst {LO, JW,\nDZ, NS}; CV and RL were full-time contributors for most of the duration. PC is the team lead.\n2Samples from all of our models can be viewed on our website.\n3We provide inference code for our 1.3B models and baselines, as well as a model card and our human\nfeedback dataset with over 64k summary comparisons, here.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2009.01325v3  [cs.CL]  15 Feb 2022\n",
      "Supervised learning\nHuman feedback\nPretrain only\nReference summaries\nFigure 1: Fraction of the time humans prefer our models\u2019 summaries over the human-generated\nreference summaries on the TL;DR dataset.4Since quality judgments involve an arbitrary decision\nabout how to trade off summary length vs. coverage within the 24-48 token limit, we also provide\nlength-controlled graphs in Appendix F; length differences explain about a third of the gap between\nfeedback and supervised learning at 6.7B.\nare incentivized to place probability mass on all human demonstrations, including those that are\nlow-quality; and distributional shift during sampling can degrade performance [56, 52]. Quality can\noften be improved signi\ufb01cantly by non-uniform sampling strategies such as beam search [51], but\nthese can lead to repetition and other undesirable artifacts [69, 23]. Optimizing for quality may be a\nprincipled approach to overcoming these problems.\nOur goal in this paper is to advance methods for training language models on objectives that more\nclosely capture the behavior we care about. To make short-term progress towards this goal, we\nfocus on abstractive English text summarization, as it has a long history in the NLP community\n[16, 8, 54, 59, 50], and is a subjective task where we believe it is dif\ufb01cult to quantify summary quality\nwithout human judgments. Indeed, existing automatic metrics for evaluating summary quality, such\nas ROUGE [39], have received criticism for poor correlation with human judgments [55, 45, 6, 33].\nWe follow the works of [3, 73], who \ufb01ne-tune language models from human feedback using reward\nlearning [35]. We \ufb01rst collect a dataset of human preferences between pairs of summaries, then train\na reward model (RM) via supervised learning to predict the human-preferred summary. Finally, we\ntrain a policy via reinforcement learning (RL) to maximize the score given by the RM; the policy\ngenerates a token of text at each \u2018time step\u2019, and is updated using the PPO algorithm [58] based on\nthe RM \u2018reward\u2019 given to the entire generated summary. We can then gather more human data using\nsamples from the resulting policy, and repeat the process. We follow the works of [48, 4] and use\nlarge pretrained GPT-3 models with as many as 6.7 billion parameters.\nOur main contributions are four-fold.\n(1) We show that training with human feedback signi\ufb01cantly outperforms very strong baselines\non English summarization. When applying our methods on a version of the Reddit TL;DR dataset\n[63], we train policies via human feedback that produce better summaries than much larger policies\ntrained via supervised learning. Summaries from our human feedback models are preferred by our\nlabelers to the original human demonstrations in the dataset (see Figure 1).\n(2) We show human feedback models generalize much better to new domains than supervised\nmodels. Our Reddit-trained human feedback models also generate high-quality summaries of news\narticles on the CNN/DailyMail (CNN/DM) dataset without any news-speci\ufb01c \ufb01ne-tuning, almost\nmatching the quality of the dataset\u2019s reference summaries. We perform several checks to ensure\nthat these human preferences re\ufb02ect a real quality difference: we consistently monitor agreement\nrates amongst labelers and researchers, and \ufb01nd researcher-labeler agreement rates are nearly as high\nas researcher-researcher agreement rates (see Section C.2), and we verify models are not merely\noptimizing simple metrics like length or amount of copying (see Appendices F and G.7).\n4Throughout the paper, error bars represent 1 standard error.\n2\n",
      "(3) We conduct extensive empirical analyses of our policy and reward model. We examine the\nimpact of model and data size (Figure 6), study performance as we continue to optimize a given\nreward model (Section 4.3), and analyze reward model performance using synthetic and human-\nwritten perturbations of summaries (Section 4.3). We con\ufb01rm that our reward model outperforms\nother metrics such as ROUGE at predicting human preferences, and that optimizing our reward model\ndirectly results in better summaries than optimizing ROUGE according to humans (Section 4.4).\n(4) We publicly release our human feedback dataset for further research. The dataset contains\n64,832 summary comparisons on the TL;DR dataset, as well as our evaluation data on both TL;DR\n(comparisons and Likert scores) and CNN/DM (Likert scores).\nThe methods we present in this paper are motivated in part by longer-term concerns about the\nmisalignment of AI systems with what humans want them to do. When misaligned summarization\nmodels make up facts, their mistakes are fairly low-risk and easy to spot. However, as AI systems\nbecome more powerful and are given increasingly important tasks, the mistakes they make will likely\nbecome more subtle and safety-critical, making this an important area for further research.\n2\nRelated work\nMost directly related to our work is previous work using human feedback to train summarization\nmodels with RL [3, 73]. Bohm et al. [3] learn a reward function from a dataset of human ratings of\n2.5k CNN/DM summaries, and train a policy whose summaries are preferred to a policy optimizing\nROUGE. Our work is most similar to [73], who also train Transformer models [62] to optimize human\nfeedback across a range of tasks, including summarization on the Reddit TL;DR and CNN/DM\ndatasets. Unlike us, they train in an online manner and \ufb01nd the model highly extractive. They\nnote that their labelers prefer extractive summaries and have low agreement rates with researchers.\nCompared to [73], we use signi\ufb01cantly larger models, move to the batch setting for collecting human\nfeedback, ensure high labeler-researcher agreement, and make some algorithmic modi\ufb01cations, such\nas separating the policy and value networks.\nHuman feedback has also been used as a reward to train models in other domains such as dialogue\n[25, 68, 21], translation [32, 1], semantic parsing [34], story generation [72], review generation\n[7], and evidence extraction [46]. Our reward modeling approach was developed in prior work\non learning to rank [40], which has been applied to ranking search results using either explicit\nfeedback [2, 18] or implicit feedback in the form of click-through data [29, 30]. In a related line of\nresearch, human feedback has been used to train agents in simulated environments [10, 24]. There\nis also a rich literature on using RL to optimize automatic metrics for NLP tasks, such as ROUGE\nfor summarization [50, 65, 45, 15, 19], BLEU for translation [50, 66, 1, 43], and other domains\n[61, 27, 26]. Finally, there has been extensive research on modifying architectures [22, 59] and\npre-training procedures [70, 36, 49, 60, 53, 14] for improving summarization performance.\n3\nMethod and experiment details\n3.1\nHigh-level methodology\nOur approach is similar to the one outlined in [73], adapted to the batch setting. We start with an\ninitial policy that is \ufb01ne-tuned via supervised learning on the desired dataset (in our case, the Reddit\nTL;DR summarization dataset). The process (illustrated in Figure 2) then consists of three steps that\ncan be repeated iteratively.\nStep 1: Collect samples from existing policies and send comparisons to humans. For each\nReddit post, we sample summaries from several sources including the current policy, initial policy,\noriginal reference summaries and various baselines. We send a batch of pairs of summaries to our\nhuman evaluators, who are tasked with selecting the best summary of a given Reddit post.\nStep 2: Learn a reward model from human comparisons. Given a post and a candidate summary,\nwe train a reward model to predict the log odds that this summary is the better one, as judged by our\nlabelers.\nStep 3: Optimize a policy against the reward model. We treat the logit output of the reward model\nas a reward that we optimize using reinforcement learning, speci\ufb01cally with the PPO algorithm [58].\n3\n",
      "1 Collect human feedback\n\u201cj is better than k\u201d\n\u201cj is better than k\u201d\nA Reddit post is \nsampled from \nthe Reddit \nTL;DR dataset.\nVarious policies \nare used to \nsample a set of \nsummaries.\nTwo summaries \nare selected for \nevaluation.\nA human judges \nwhich is a better \nsummary of the \npost.\n2 Train reward model\nOne post with \ntwo summaries \njudged by a \nhuman are fed \nto the reward \nmodel.\nThe reward \nmodel \ncalculates a \nreward r for \neach summary.\nThe loss is \ncalculated based \non the rewards \nand human label, \nand is used to \nupdate the \nreward model.\n3 Train policy with PPO\nA new post is \nsampled from the \ndataset.\nThe reward \nmodel calculates \na reward for the \nsummary.\nThe reward is \nused to update \nthe policy via \nPPO.\nr\nr\nr\nr\n\u03c0\nrj\nloss = log(\u03c3(rj - rk ))\nrk\nThe policy \u03c0 \ngenerates a \nsummary for the \npost.\nr\nj\nj\nk\nk\nFigure 2: Diagram of our human feedback, reward model training, and policy training procedure.\nWe provide a more thorough description of our procedure, including details of the reward model and\npolicy training and our quality control process, in the following sections. In practice, rather than\nprecisely iterating this sequence of three steps, we updated our data collection and training procedures\nover the course of the project while accumulating labels (see Appendix C.6 for details).\n3.2\nDatasets and task\nDatasets.\nWe use the TL;DR summarization dataset [63], which contains ~3 million posts from\nreddit.com across a variety of topics (subreddits), as well summaries of the posts written by the\noriginal poster (TL;DRs). We additionally \ufb01lter this dataset (see Appendix A) to ensure quality,\nincluding using a whitelist of subreddits that are understandable to the general population. Crucially,\nwe also \ufb01lter to include only posts where the human-written summaries contain between 24 and\n48 tokens, to minimize the potential effect of summary length on quality (see Section 4.1 and\nAppendix F). Our \ufb01nal \ufb01ltered dataset contains 123,169 posts, and we hold out ~5% as a validation\nset. For the remainder of this paper, we refer to this dataset simply as TL;DR.\nWe chose the TL;DR dataset over the more commonly used CNN/DM dataset primarily because\nvery strong performance can be attained on CNN/DM with simple extractive baselines. We \ufb01nd in\nSection 4.2 that our labelers prefer lead-3 over the CNN/DM reference summaries,5 and that the\nsupervised T5 model [49] with low-temperature sampling already surpasses the reference summary\nquality, while copying extensively from the article. On the other hand, simple extractive baselines\nperform poorly on TL;DR in our human evaluations (see Appendix G.2). Instead of training on\nCNN/DM, we study the transfer performance of our human feedback models to CNN/DM after being\ntrained to summarize Reddit posts.\nTask.\nWe de\ufb01ne our ground-truth task as producing a model that generates summaries fewer than\n48 tokens long that are as good as possible, according to our judgments. We judge summary quality\nby how faithfully the summary conveys the original post to a reader who can only read the summary\nand not the post (see Appendix C.5 for further discussion of criteria). Since we have limited capacity\nto do comparisons, we hire labelers to do the comparisons for us. We rely on detailed procedures to\nensure high agreement between labelers and us on the task, which we describe in the next section.\n5We manually check this result in Appendix E and \ufb01nd we generally agree with labeler ratings.\n4\n",
      "[r/dating_advice] First date ever, going to the beach. Would like some tips\nHey Reddit! I (20M) would like some tips, because I have my \ufb01rst ever date tomorrow (although I\u2019ve had a gf for 3\nyears, but no actual dating happened), and we\u2019re going to the beach.\nI met this girl, we have mutual friends, at a festival a few days ago.\nWe didn\u2019t kiss, but we talked, held\nhands, danced a bit. I asked her to go on a date with me, which was super hard as it is the \ufb01rst time I\u2019ve asked this\nto anybody. What I mean to say is, it\u2019s not like a standard *\ufb01rst* date because we already spent some time together.\nI\u2019m really nervous and excited.\nI\u2019m going to pick her up tomorrow, we\u2019re cycling to the beach which will\ntake 30 minutes, and then what? I\u2019m a bit scared. Should I bring something (the weather, although no rain and\nsunny, is not super so no swimming), should we do something. I\u2019d like all the tips I can get. Thanks!\nHuman written reference TL;DR\n6.7B supervised model\n6.7B human feedback model\nFirst date after 3 years in a relation-\nship, going to the beach, terri\ufb01ed.\nWhat to bring with me, what to do?\nGoing on a date with a girl I met\na few days ago, going to the beach.\nWhat should I bring, what should\nwe do?\nGoing on my \ufb01rst ever date tomor-\nrow, cycling to the beach. Would\nlike some tips on what to do and\nbring. I\u2019m a bit nervous and excited.\nThanks!\nTable 1: Example of post and samples on the TL;DR dataset, chosen to be particularly short. For\nrandom samples (along with posts), see Appendix H and our website.\n3.3\nCollecting human feedback\nPrevious work on \ufb01ne-tuning language models from human feedback [73] reported \u201ca mismatch\nbetween the notion of quality we wanted our model to learn, and what the humans labelers actually\nevaluated\u201d, leading to model-generated summaries that were high-quality according to the labelers,\nbut fairly low-quality according to the researchers.\nCompared to [73], we implement two changes to improve human data quality. First, we transition\nentirely to the of\ufb02ine setting, where we alternate between sending large batches of comparison data6\nto our human labelers and re-training our models on the cumulative collected data. Second, we\nmaintain a hands-on relationship with labelers:7 we on-board them with detailed instructions, answer\ntheir questions in a shared chat room, and provide regular feedback on their performance. We train all\nlabelers to ensure high agreement with our judgments, and continuously monitor labeler-researcher\nagreement over the course of the project. See Appendix C.1 and C.5 for details.\nAs a result of our procedure, we obtained high labeler-researcher agreement: on a subset of compari-\nson tasks, labelers agree with researchers 77% \u00b1 2% of the time, while researchers agree with each\nother 73% \u00b1 4% of the time. We provide more analysis of our human data quality in Appendix C.2.\n3.4\nModels\nAll of our models are Transformer decoders [62] in the style of GPT-3 [47, 4]. We conduct our human\nfeedback experiments on models with 1.3 billion (1.3B) and 6.7 billion (6.7B) parameters.\nPretrained models.\nSimilarly to [12, 47], we start with models pretrained to autoregressively\npredict the next token in a large text corpus. As in [48, 4], we use these models as \u2018zero-shot\u2019\nbaselines by padding the context with examples of high-quality summaries from the dataset. We\nprovide details on pretraining in Appendix B, and on our zero-shot procedure in Appendix B.2.\nSupervised baselines.\nWe next \ufb01ne-tune these models via supervised learning to predict summaries\nfrom our \ufb01ltered TL;DR dataset (see Appendix B for details). We use these supervised models to\nsample initial summaries for collecting comparisons, to initialize our policy and reward models, and\nas baselines for evaluation. In our \ufb01nal human evaluations, we use T=0 to sample from all models, as\nwe found it performed better than higher temperatures or nucleus sampling (see Appendix B.1).\nTo validate that our supervised models are indeed strong baselines for comparison, we run our\nsupervised \ufb01ne-tuning procedure with our 6.7B model on the CNN/DM dataset, and \ufb01nd that we\nachieve slightly better ROUGE scores than SOTA models [71] from mid-2019 (see Appendix G.4).\n6Our decision to collect comparisons rather than Likert scores is supported by recent work, e.g. [37].\n7We recruited labelers from a freelancing platform, Upwork, and two labeling services, Scale and Lionbridge.\n5\n",
      "Reward models.\nTo train our reward models, we start from a supervised baseline, as described\nabove, then add a randomly initialized linear head that outputs a scalar value. We train this model to\npredict which summary y \u2208{y0, y1} is better as judged by a human, given a post x. If the summary\npreferred by the human is yi, we can write the RM loss as:\nloss(r\u03b8) = \u2212E(x,y0,y1,i)\u223cD[log(\u03c3(r\u03b8(x, yi) \u2212r\u03b8(x, y1\u2212i)))]\nwhere r\u03b8(x, y) is the scalar output of the reward model for post x and summary y with parameters \u03b8,\nand D is the dataset of human judgments. At the end of training, we normalize the reward model\noutputs such that the reference summaries from our dataset achieve a mean score of 0.\nHuman feedback policies.\nWe want to use the reward model trained above to train a policy that\ngenerates higher-quality outputs as judged by humans. We primarily do this using reinforcement\nlearning, by treating the output of the reward model as a reward for the entire summary that we\nmaximize with the PPO algorithm [58], where each time step is a BPE token.8 We initialize our\npolicy to be the model \ufb01ne-tuned on Reddit TL;DR. Importantly, we include a term in the reward that\npenalizes the KL divergence between the learned RL policy \u03c0RL\n\u03c6 with parameters \u03c6 and this original\nsupervised model \u03c0SFT, as previously done in [25]. The full reward R can be written as:\nR(x, y) = r\u03b8(x, y) \u2212\u03b2 log[\u03c0RL\n\u03c6 (y|x)/\u03c0SFT(y|x)]\nThis KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to\nexplore and deterring it from collapsing to a single mode. Second, it ensures the policy doesn\u2019t learn\nto produce outputs that are too different from those that the reward model has seen during training.\nFor the PPO value function, we use a Transformer with completely separate parameters from the\npolicy. This prevents updates to the value function from partially destroying the pretrained policy\nearly in training (see ablation in Appendix G.1). We initialize the value function to the parameters of\nthe reward model. In our experiments, the reward model, policy, and value function are the same size.\n4\nResults\n4.1\nSummarizing Reddit posts from human feedback\nPolicies trained with human feedback are preferred to much larger supervised policies.\nOur\nmain results evaluating our human feedback policies on TL;DR are shown in Figure 1. We measure\npolicy quality as the percentage of summaries generated by that policy that humans prefer over\nthe reference summaries in the dataset. Our policies trained with human feedback signi\ufb01cantly\noutperform our supervised baselines on this metric, with our 1.3B human feedback model signi\ufb01cantly\noutperforming a supervised model 10\u00d7 its size (61% versus 43% raw preference score against\nreference summaries). Our 6.7B model in turn signi\ufb01cantly outperforms our 1.3B model, suggesting\nthat training with human feedback also bene\ufb01ts from scale. Additionally, both of our human feedback\nmodels are judged by humans to be superior to the human demonstrations used in the dataset.\nControlling for summary length.\nWhen judging summary quality, summary length is a confound-\ning factor. The target length of a summary is implicitly part of the summarization task; depending on\nthe desired trade-off between conciseness and coverage, a shorter or longer summary might be better.\nSince our models learned to generate longer summaries, length could account for much of our quality\nimprovements. We \ufb01nd that after controlling for length (Appendix F), the preference of our human\nfeedback models vs. reference summaries drops by ~5%; even so, our 6.7B model summaries are still\npreferred to the reference summaries ~65% of the time.\nHow do our policies improve over the baselines?\nTo better understand the quality of our models\u2019\nsummaries compared to the reference summaries and those of our supervised baselines, we conduct\nan additional analysis where human labelers assess summary quality across four dimensions (or\n\u201caxes\u201d) using a 7-point Likert scale [38]. Labelers rated summaries for coverage (how much important\ninformation from the original post is covered), accuracy (to what degree the statements in the summary\nare stated in the post), coherence (how easy the summary is to read on its own), and overall quality.\n8Note that the reward model only gives rewards for entire summaries, and not at intermediate time steps. In\nRL terminology, each episode terminates when the policy outputs the EOS token, and the discount factor \u03b3 = 1.\n6\n",
      "Supervised\ntransfer\nHuman feedback\ntransfer\nPretrain\nonly\nReference summaries\nLead-3\nSupervised\nCNN/DM\nT5 CNN/DM\nfinetuning\n(a)\n(b)\nFigure 4: Transfer results on CNN/DM. (a) Overall summary quality on CNN/DM as a function of\nmodel size. Full results across axes shown in Appendix G.2. (b) Overall scores vs. length for the\n6.7B TL;DR supervised baseline, the 6.7B TL;DR human feedback model, and T5 \ufb01ne-tuned on\nCNN/DM summaries. At similar summary lengths, our 6.7B TL;DR human feedback model nearly\nmatches T5 despite never being trained to summarize news articles.\nFigure 3: Evaluations of four axes of\nsummary quality on the TL;DR dataset.\nThe results (Figure 3) indicate that our human feedback\nmodels outperform the supervised baselines across every\ndimension of quality, but particularly coverage. Although\nour human labelers had a high bar for giving perfect overall\nscores, summaries from our 6.7B PPO model achieve a 7/7\noverall score 45% of the time (compared to 20% and 23%\nfor the 6.7B supervised baseline and reference summaries,\nrespectively).\n4.2\nTransfer to summarizing news articles\nOur human feedback models can also generate excellent\nsummaries of CNN/DM news articles without any further\ntraining (Figure 4). Our human feedback models signi\ufb01-\ncantly outperform models trained via supervised learning\non TL;DR and models trained only on pretraining corpora.\nIn fact, our 6.7B human feedback model performs almost as well as a 6.7B model that was \ufb01ne-tuned\non the CNN/DM reference summaries, despite generating much shorter summaries.\nSince our human feedback models transferred to CNN/DM have little overlap in summary length\ndistribution with models trained on CNN/DM, with about half as many tokens on average, they are\ndif\ufb01cult to compare directly. Thus our evaluations in Figure 4 use a 7-point Likert scale on four\nquality dimensions, as in Section 4.1 (see Appendix C.5 for labeler instructions). In Figure 4b we\nshow the average overall score at different summary lengths, which suggests our human feedback\nmodels would perform even better if they generated longer summaries. Qualitatively, CNN/DM\nsummaries from our human feedback models are consistently \ufb02uent and reasonable representations\nof the article; we show examples on our website and in Appendix H.\n4.3\nUnderstanding the reward model\nWhat happens as we optimize the reward model?\nOptimizing against our reward model is\nsupposed to make our policy align with human preferences. But the reward model isn\u2019t a perfect\nrepresentation of our labeler preferences, as it has limited capacity and only sees a small amount of\ncomparison data from a relatively narrow distribution of summaries. While we can hope our reward\nmodel generalizes to summaries unseen during training, it\u2019s unclear how much one can optimize\nagainst the reward model until it starts giving useless evaluations.\nTo answer this question, we created a range of policies optimized against an earlier version of our\nreward model, with varying degrees of optimization strength, and asked labelers to compare samples\nfrom them to the reference summaries. Figure 5 shows the results for PPO at a range of KL penalty\n7\n",
      "RM prediction\nActual preference\nFigure 5: Preference scores versus degree of\nreward model optimization. Optimizing against\nthe reward model initially improves summaries,\nbut eventually over\ufb01ts, giving worse summaries.\nThis \ufb01gure uses an earlier version of our reward\nmodel (see rm3 in Appendix C.6). See Appendix\nH.2 for samples from the KL 250 model.\nEnsemble of humans\nHuman baseline\n64k\n32k\n16k\n8k\nFigure 6: Reward model performance versus\ndata size and model size. Doubling amount of\ntraining data leads to a ~1.1% increase in reward\nmodel validation accuracy, whereas doubling\nthe model size leads to a ~1.8% increase. The\n6.7B model trained on all data begins approach-\ning the accuracy of a single human.\ncoef\ufb01cients (\u03b2). Under light optimization, the models improve (according to labelers). However, as\nwe optimize further, true preferences fall off compared to the prediction, and eventually the reward\nmodel becomes anti-correlated with human preferences. Though this is clearly undesirable, we note\nthat this over-optimization also happens with ROUGE (see [45] and Appendix G.3). Similar behavior\nhas been observed in learned reward functions in the robotics domain [5].\nHow does reward modeling scale with increasing model and data size?\nWe conduct an ablation\nto determine how data quantity and model size affect reward modeling performance. We train 7\nreward models ranging from 160M to 13B parameters, on 8k to 64k human comparisons from our\ndataset. We \ufb01nd that doubling the training data amount leads to a ~1.1% increase in the reward model\nvalidation set accuracy, whereas doubling the model size leads to a ~1.8% increase (Figure 6).\nWhat has the reward model learned?\nWe probe our reward model by evaluating it on several\nvalidation sets. We show the full results in Appendix G.6, and highlight them here. We \ufb01nd that our\nreward models generalize to evaluating CNN/DM summaries (Appendix G.7), agreeing with labeler\npreferences 62.4% and 66.5% of the time (for our 1.3B and 6.7B models, respectively). Our 6.7B\nreward model nearly matches the inter-labeler agreement value of 66.9%.\nWe also \ufb01nd that our reward models are sensitive to small but semantically important details in\nthe summary. We construct an additional validation set by having labelers make minimal edits to\nsummaries to improve them. Our RMs prefer the edited summaries almost as often (79.4% for 1.3B\nand 82.8% for 6.7B) as a separate set of human evaluators (84.1%). Further, when comparing the\nreference summaries to perturbed summaries where the participants\u2019 roles are reversed, our models\nreliably select the original summary (92.9% of the time for 1.3B, 97.2% for 6.7B). However, our RMs\nare biased towards longer summaries: our 6.7B RM prefers improving edits that make the summary\nshorter only 62.6% of the time (vs. 76.4% for humans).\n4.4\nAnalyzing automatic metrics for summarization\nEvaluation.\nWe study how well various automatic metrics act as predictors for human preferences,\nand compare them to our RMs. Speci\ufb01cally, we examine ROUGE, summary length, amount of\ncopying from the post,9 and log probability under our baseline supervised models. We present a full\nmatrix of agreement rates between these metrics in Appendix G.7.\nWe \ufb01nd that our learned reward models consistently outperform other metrics, even on the CNN/DM\ndataset on which it was never trained. We also \ufb01nd that ROUGE fails to track sample quality as our\n9We measure copying by computing the longest common subsequence of bigrams with the original Reddit\npost or news article, and dividing by the number of bigrams in the summary.\n8\n",
      "Figure 7: Summary quality as a function of metric optimized and amount of optimization, using\nbest-of-N rejection sampling. We evaluate ROUGE, our main reward models, and an earlier iteration\nof the 1.3B model trained on approximately 75% as much data (see Table 11 for details). ROUGE\nappears to peak both sooner and at a substantially lower preference rate than all reward models.\nDetails in Appendix G.3.\nmodels improve. While ROUGE has ~57% agreement with labelers when comparing samples from\nour supervised baseline models, this drops to ~50% for samples from our human feedback model.\nSimilarly, log probability agreement with humans drops to \u226450% on comparisons between samples\nfrom our human feedback models, while our RMs still perform above chance (62%). Scaling up the\nsize of the supervised model does not reliably improve log probability\u2019s agreement with labelers.\nOptimization.\nIn Figure 7, we show that optimizing ROUGE using a simple optimization scheme\ndoesn\u2019t consistently increase quality, as has been noted in [45]. Optimization against ROUGE peaks\nboth sooner and at a substantially lower quality rate than optimization against our reward models.\n5\nDiscussion\nLimitations.\nOne limitation of our work is the time and cost required to produce our \ufb01nal models.\nNotably, \ufb01ne-tuning our 6.7B model with RL required approximately 320 GPU-days. Our data\ncollection procedure is also expensive compared to prior work \u2014 the training set took thousands of\nlabeler hours and required signi\ufb01cant researcher time to ensure quality. For this reason, we were\nunable to collect baselines such as an equivalent amount of high-quality human demonstrations for\nsupervised baselines. See D for more discussion. We leave this ablation to future work. Nevertheless,\nwe believe reward modeling is more likely to scale to tasks where it is extremely skill-intensive or\ntime-consuming to provide good demonstrations.\nFuture directions.\nThe methods in this paper could be applied to any task where humans can\ncompare samples, including dialogue, machine translation, question answering, speech synthesis, and\nmusic generation. We expect this method to be particularly important for generating long samples,\nwhere the distributional shift and degeneracy of maximum likelihood samples can be problematic. It\nmay be possible to improve sample ef\ufb01ciency by training to predict feedback across many tasks [42].\nWe are particularly interested in scaling human feedback to tasks where humans can\u2019t easily evaluate\nthe quality of model outputs. In this setting, it is particularly challenging to identify whether an ML\nsystem is aligned with the human designer\u2019s intentions. One approach is to train ML systems to help\nhumans perform the evaluation task quickly and accurately [9].\nThere is also a rich landscape of human feedback methods beyond binary comparisons that could be\nexplored for training models [28, 17, 44, 64]. For example, we could solicit high-quality demonstra-\ntions from labelers, have labelers edit model outputs to make them better, or have labelers provide\nexplanations for why they preferred one model output over another. All of this feedback could be\nleveraged as a signal to train more capable reward models and policies.\n9\n",
      "Broader impacts.\nThe techniques we explore in this paper are generic techniques that could be\nused in a wide variety of machine learning applications, for any task where it is feasible for humans\nto evaluate the quality of model outputs. Thus, the potential implications are quite broad.\nOur research is primarily motivated by the potential positive effects of aligning machine learning\nalgorithms with the designer\u2019s preferences. Many machine learning applications optimize simple\nmetrics which are only rough proxies for what the designer intends. This can lead to problems, such\nas Youtube recommendations promoting click-bait [11]. In the short term, improving techniques for\nlearning from and optimizing human preferences directly may enable these applications to be more\naligned with human well-being.\nIn the long term, as machine learning systems become more capable it will likely become increasingly\ndif\ufb01cult to ensure that they are behaving safely: the mistakes they make might be more dif\ufb01cult to\nspot, and the consequences will be more severe. For instance, writing an inaccurate summary of a\nnews article is both easy to notice (one simply has to read the original article) and has fairly low\nconsequences. On the other hand, imitating human driving may be substantially less safe than driving\nto optimize human preferences. We believe that the techniques we explore in this paper are promising\nsteps towards mitigating the risks from such capable systems, and better aligning them with what\nhumans care about.\nUnfortunately, our techniques also enable malicious actors to more easily train models that cause\nsocietal harm. For instance, one could use human feedback to \ufb01ne-tune a language model to be more\npersuasive and manipulate humans\u2019 beliefs, or to induce dependence of humans on the technology, or\nto generate large amounts of toxic or hurtful content intended to harm speci\ufb01c individuals. Avoiding\nthese outcomes is a signi\ufb01cant challenge for which there are few obvious solutions.\nLarge-scale models trained with human feedback could have signi\ufb01cant impacts on many groups.\nThus, it is important to be careful about how we de\ufb01ne the \u2018good\u2019 model behavior that human labelers\nwill reinforce. Deciding what makes a good summary is fairly straightforward, but doing this for\ntasks with more complex objectives, where different humans might disagree on the correct model\nbehavior, will require signi\ufb01cant care. In these cases, it is likely not appropriate to use researcher\nlabels as the \u2018gold standard\u2019; rather, individuals from groups impacted by the technology should be\nincluded in the process to de\ufb01ne \u2018good\u2019 behavior, and hired as labelers to reinforce this behavior in\nthe model.\nWe chose to train on the Reddit TL;DR dataset because the summarization task is signi\ufb01cantly more\nchallenging than on CNN/DM. However, since the dataset consists of user-submitted posts with\nminimal moderation, they often contain content that is offensive or re\ufb02ects harmful social biases.\nThis means our models can generate biased or offensive summaries, as they have been trained to\nsummarize such content. For this reason, we recommend that the potential harms of our models be\nthoroughly studied before deploying them in user-facing applications.\nFinally, by improving the ability of machine learning algorithms to perform tasks that were previously\nonly achievable by humans, we are increasing the likelihood of many jobs being automated, potentially\nleading to signi\ufb01cant job loss. Without suitable policies targeted at mitigating the effects of large-scale\nunemployment, this could also lead to signi\ufb01cant societal harm.\nAcknowledgements\nWe\u2019d like to thank Beth Barnes for help with labeler hiring and general encouragement; Geoffrey\nIrving for guidance on earlier iterations of the project and inspiring conversations; Ben Mann, Tom\nBrown, Nick Ryder, and Melanie Subbiah for training and evaluating our pretrained models; Chris\nHesse, Eric Sigler, Benjamin Chess, Christopher Berner, Clemens Winter, Mateusz Litwin, and many\nothers for supporting us through computing infrastructure improvements and maintenance; Scott\nGray for writing fast GPU kernels; Arvind Neelakantan and Wojciech Kryscinski for discussions on\nhow to present the work, experiment design, and what datasets to use; Shan Carter for help designing\nthe main diagram; Douwe Kiela, Zach Lipton, and Alex Irpan for providing feedback on the paper;\nand Gretchen Krueger for co-writing the model card accompanying the paper.\nFinally, we\u2019d like to thank all of our contractors for providing the data that was essential for training\nthe models in this paper, including: Emill Jayson Caypuno, Rachelle Froyalde, Cyra Denura, Alex\nMalek, Isik Agil, Reshmi Patel, William Yap, Natalie Silver, Erol Akbaba, Jennifer Brillo, Alexandra\n10\n",
      "Uifalean, Morris Stuttard, Russell Bernandez, Tasmai Dave, Rachel Wallace, Jenny Fletcher, Jian\nOuyang, Justin Dill, Maria Orzek, Megan Niffenegger, William Sells, Emily Mariner, Andrew Seely,\nLychelle Ignacio, Jelena Ostojic, Nhan Tran, Purev Batdelgar, Valentina Kezic, Michelle Wilkerson,\nKelly Guerrero, Heather Scott, Sarah Mulligan, Gabriel Ricafrente, Kara Bell, Gabriel Perez, and\nAlfred Lee.\nReferences\n[1] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An\nactor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.\n[2] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked\nretrieval systems. In SIGIR\u201994, pages 173\u2013181. Springer, 1994.\n[3] F. B\u00f6hm, Y. Gao, C. M. Meyer, O. Shapira, I. Dagan, and I. Gurevych. Better rewards yield\nbetter summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214,\n2019.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\nLanguage models are few-shot learners. 2020.\n[5] S. Cabi, S. G\u00f3mez Colmenarejo, A. Novikov, K. Konyushkova, S. Reed, R. Jeong, K. Zolna,\nY. Aytar, D. Budden, M. Vecerik, et al. Scaling data-driven robotics with reward sketching and\nbatch reinforcement learning. arXiv, pages arXiv\u20131909, 2019.\n[6] A. T. Chaganty, S. Mussman, and P. Liang. The price of debiasing automatic metrics in natural\nlanguage evaluation. arXiv preprint arXiv:1807.02202, 2018.\n[7] W. S. Cho, P. Zhang, Y. Zhang, X. Li, M. Galley, C. Brockett, M. Wang, and J. Gao. Towards\ncoherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511, 2018.\n[8] S. Chopra, M. Auli, and A. M. Rush. Abstractive sentence summarization with attentive\nrecurrent neural networks. In Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 93\u201398, 2016.\n[9] P. Christiano, B. Shlegeris, and D. Amodei. Supervising strong learners by amplifying weak\nexperts. arXiv preprint arXiv:1810.08575, 2018.\n[10] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\nlearning from human preferences. In Advances in Neural Information Processing Systems,\npages 4299\u20134307, 2017.\n[11] P. Covington, J. Adams, and E. Sargin. Deep neural networks for youtube recommendations. In\nProceedings of the 10th ACM conference on recommender systems, pages 191\u2013198, 2016.\n[12] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in neural information\nprocessing systems, pages 3079\u20133087, 2015.\n[13] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. Smith. Fine-tuning\npretrained language models: Weight initializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305, 2020.\n[14] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon. Uni\ufb01ed\nlanguage model pre-training for natural language understanding and generation. In Advances in\nNeural Information Processing Systems, 2019.\n[15] Y. Dong, Y. Shen, E. Crawford, H. van Hoof, and J. C. K. Cheung. Banditsum: Extractive\nsummarization as a contextual bandit. arXiv preprint arXiv:1809.09672, 2018.\n[16] B. Dorr, D. Zajic, and R. Schwartz. Hedge trimmer: A parse-and-trim approach to headline\ngeneration. In Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5,\npages 1\u20138. Association for Computational Linguistics, 2003.\n[17] S. Fidler et al. Teaching machines to describe images with natural language feedback. In\nAdvances in Neural Information Processing Systems, pages 5068\u20135078, 2017.\n11\n",
      "[18] N. Fuhr. Optimum polynomial retrieval functions based on the probability ranking principle.\nACM Transactions on Information Systems (TOIS), 7(3):183\u2013204, 1989.\n[19] Y. Gao, C. M. Meyer, M. Mesgar, and I. Gurevych. Reward learning for ef\ufb01cient reinforcement\nlearning in extractive document summarisation. arXiv preprint arXiv:1907.12894, 2019.\n[20] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on arti\ufb01cial intelligence\nand statistics, pages 249\u2013256, 2010.\n[21] B. Hancock, A. Bordes, P.-E. Mazare, and J. Weston. Learning from dialogue after deployment:\nFeed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.\n[22] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom.\nTeaching machines to read and comprehend. In Advances in neural information processing\nsystems, pages 1693\u20131701, 2015.\n[23] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi.\nThe curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\n[24] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human\npreferences and demonstrations in atari. In Advances in neural information processing systems,\npages 8011\u20138023, 2018.\n[25] N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and\nR. Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in\ndialog. arXiv preprint arXiv:1907.00456, 2019.\n[26] N. Jaques, S. Gu, D. Bahdanau, J. M. Hern\u00e1ndez-Lobato, R. E. Turner, and D. Eck. Sequence\ntutor: Conservative \ufb01ne-tuning of sequence generation models with kl-control. In International\nConference on Machine Learning, pages 1645\u20131654. PMLR, 2017.\n[27] N. Jaques, S. Gu, R. E. Turner, and D. Eck. Tuning recurrent neural networks with reinforcement\nlearning. 2017.\n[28] H. J. Jeon, S. Milli, and A. D. Dragan. Reward-rational (implicit) choice: A unifying formalism\nfor reward learning. arXiv preprint arXiv:2002.04833, 2020.\n[29] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth\nACM SIGKDD international conference on Knowledge discovery and data mining, pages\n133\u2013142, 2002.\n[30] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting click-\nthrough data as implicit feedback. In ACM SIGIR Forum, volume 51, pages 4\u201311. Acm New\nYork, NY, USA, 2005.\n[31] D. P. Kingma and J. Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[32] J. Kreutzer, S. Khadivi, E. Matusov, and S. Riezler. Can neural machine translation be improved\nwith user feedback? arXiv preprint arXiv:1804.05958, 2018.\n[33] W. Kryscinski, N. S. Keskar, B. McCann, C. Xiong, and R. Socher. Neural text summarization:\nA critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540\u2013551, 2019.\n[34] C. Lawrence and S. Riezler. Improving a neural semantic parser by counterfactual learning\nfrom human bandit feedback. arXiv preprint arXiv:1805.01252, 2018.\n[35] J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via\nreward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\n[36] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and\nL. Zettlemoyer.\nBart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n[37] M. Li, J. Weston, and S. Roller. Acute-eval: Improved dialogue evaluation with optimized\nquestions and multi-turn comparisons. arXiv preprint arXiv:1909.03087, 2019.\n[38] R. Likert. A technique for the measurement of attitudes. Archives of psychology, 1932.\n12\n",
      "[39] C.-Y. Lin and F. J. Och. Automatic evaluation of machine translation quality using longest\ncommon subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on\nAssociation for Computational Linguistics, page 605. Association for Computational Linguistics,\n2004.\n[40] T.-Y. Liu. Learning to rank for information retrieval. Springer Science & Business Media,\n2011.\n[41] J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in\nabstractive summarization, 2020.\n[42] B. McCann, N. S. Keskar, C. Xiong, and R. Socher. The natural language decathlon: Multitask\nlearning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n[43] K. Nguyen, H. Daum\u00e9 III, and J. Boyd-Graber. Reinforcement learning for bandit neural\nmachine translation with simulated human feedback. arXiv preprint arXiv:1707.07402, 2017.\n[44] T. Niu and M. Bansal. Polite dialogue generation without parallel data. Transactions of the\nAssociation for Computational Linguistics, 6:373\u2013389, 2018.\n[45] R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization.\narXiv preprint arXiv:1705.04304, 2017.\n[46] E. Perez, S. Karamcheti, R. Fergus, J. Weston, D. Kiela, and K. Cho. Finding generalizable\nevidence by learning to convince q&a models. arXiv preprint arXiv:1909.05863, 2019.\n[47] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\nImproving language under-\nstanding by generative pre-training.\nURL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.\n[48] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[50] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\nnetworks. arXiv preprint arXiv:1511.06732, 2015.\n[51] D. R. Reddy et al. Speech understanding systems: A summary of results of the \ufb01ve-year\nresearch effort. department of computer science, 1977.\n[52] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the fourteenth international conference on\narti\ufb01cial intelligence and statistics, pages 627\u2013635, 2011.\n[53] S. Rothe, S. Narayan, and A. Severyn. Leveraging pre-trained checkpoints for sequence\ngeneration tasks. Transactions of the Association for Computational Linguistics, 2020.\n[54] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence\nsummarization. arXiv preprint arXiv:1509.00685, 2015.\n[55] N. Schluter. The limits of automatic summarisation according to rouge. In Proceedings of the\n15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 41\u201345, 2017.\n[56] F. Schmidt. Generalization in generation: A closer look at exposure bias. arXiv preprint\narXiv:1910.00292, 2019.\n[57] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation. In Proceedings of the International Conference\non Learning Representations (ICLR), 2016.\n[58] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[59] A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator\nnetworks. arXiv preprint arXiv:1704.04368, 2017.\n[60] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv preprint arXiv:1905.02450, 2019.\n13\n",
      "[61] P. Tambwekar, M. Dhuliawala, A. Mehta, L. J. Martin, B. Harrison, and M. O. Riedl. Con-\ntrollable neural story generation via reinforcement learning. arXiv preprint arXiv:1809.10736,\n2018.\n[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in neural information processing systems,\npages 5998\u20136008, 2017.\n[63] M. V\u00f6lske, M. Potthast, S. Syed, and B. Stein. Tl; dr: Mining reddit to learn automatic\nsummarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages\n59\u201363, 2017.\n[64] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with\nunlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\n[65] Y. Wu and B. Hu. Learning to extract coherent summary via deep reinforcement learning. In\nThirty-Second AAAI Conference on Arti\ufb01cial Intelligence, 2018.\n[66] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,\nK. Macherey, et al. Google\u2019s neural machine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n[67] Y. Yan, W. Qi, Y. Gong, D. Liu, N. Duan, J. Chen, R. Zhang, and M. Zhou. Prophetnet: Pre-\ndicting future n-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063,\n2020.\n[68] S. Yi, R. Goel, C. Khatri, A. Cervone, T. Chung, B. Hedayatnia, A. Venkatesh, R. Gabriel,\nand D. Hakkani-Tur. Towards coherent and engaging spoken dialog response generation using\nautomatic conversation evaluators. arXiv preprint arXiv:1904.13015, 2019.\n[69] H. Zhang, D. Duckworth, D. Ippolito, and A. Neelakantan. Trading off diversity and quality in\nnatural language generation. arXiv preprint arXiv:2004.10450, 2020.\n[70] J. Zhang, Y. Zhao, M. Saleh, and P. J. Liu. Pegasus: Pre-training with extracted gap-sentences\nfor abstractive summarization. arXiv preprint arXiv:1912.08777, 2019.\n[71] Y. Zhang, D. Li, Y. Wang, Y. Fang, and W. Xiao. Abstract text summarization with a convolu-\ntional seq2seq model. Applied Sciences, 9(8):1665, 2019.\n[72] W. Zhou and K. Xu. Learning to compare for better training and evaluation of open domain\nnatural language generation models. arXiv preprint arXiv:2002.05058, 2020.\n[73] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irv-\ning. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593,\n2019.\n14\n",
      "Appendix\nTable of Contents\nA TL;DR dataset details\n16\nB\nFurther model training details\n17\nB.1\nHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB.2\nInput format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nC Human data collection details\n19\nC.1\nProcess for ensuring high-quality human data . . . . . . . . . . . . . . . . . . .\n19\nC.2\nAssessing human feedback quality\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.3\nLabeler demographics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.4\nLabeler website . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.5\nInstructions for labelers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.6\nComposition of the labeled dataset\n. . . . . . . . . . . . . . . . . . . . . . . .\n22\nC.7\nExample comparison tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nD Choice of baselines\n28\nE\nCNN/DM lead-3 vs reference summaries\n29\nF\nControlling for summary length\n30\nG Additional results\n31\nG.1\nValue function ablation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.2\nEvaluating policies along axes of quality\n. . . . . . . . . . . . . . . . . . . . .\n31\nG.3\nStudying best-of-N optimization . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.4\nROUGE scores\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.5\nBigram overlap statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nG.6\nReward model validation sets . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.7\nMeasuring agreement between different evaluation metrics . . . . . . . . . . . .\n35\nH Samples\n38\nH.1\nRandom samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nH.2\nOveroptimized samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n15\n",
      "A\nTL;DR dataset details\nSubreddit\n# posts\n% of dataset\nrelationships\n63324\n54.25%\nAskReddit\n15440\n13.23%\nrelationship_advice\n8691\n7.45%\ntifu\n7685\n6.58%\ndating_advice\n2849\n2.44%\npersonal\ufb01nance\n2312\n1.98%\nAdvice\n2088\n1.79%\nlegaladvice\n1997\n1.71%\noffmychest\n1582\n1.36%\nloseit\n1452\n1.24%\njobs\n1084\n0.93%\nself\n1048\n0.90%\nBreakUps\n838\n0.72%\naskwomenadvice\n688\n0.59%\ndogs\n638\n0.55%\nrunning\n567\n0.49%\npettyrevenge\n548\n0.47%\nneedadvice\n528\n0.45%\ntravel\n452\n0.39%\nParenting\n435\n0.37%\nweddingplanning\n433\n0.37%\nPets\n366\n0.31%\nDogtraining\n362\n0.31%\ncats\n324\n0.28%\nAskDocs\n283\n0.24%\ncollege\n264\n0.23%\nGetMotivated\n169\n0.14%\nbooks\n161\n0.14%\nCooking\n114\n0.10%\nTable 2: Number of posts in the training\nset of our \ufb01ltered Reddit TL;DR dataset by\nsubreddit.\nHere, we discuss the pre-processing steps that we apply\nto the TL;DR dataset. We \ufb01rst remove all duplicate\nposts by checking the text body, \ufb01nding that there are\nnearly 20,000 exact duplicates. We then re-parse the\nTL;DR carefully using a set of heuristics, and \ufb01lter to\nuse only top-level posts (rather than comments). We\nalso \ufb01lter out any post that is from a subreddit not in our\n\u2018subreddit whitelist\u2019 (see Table 2 for the distribution\nover subreddits), any post where the title starts with\nsome variant of \u2018Edit\u2019 or \u2018Update\u2019,10 and posts that\ncontain certain topics (such as graphic sex or suicide)\nusing heuristics. Finally, to ensure the posts are short\nenough to \ufb01t into the context length of our models, we\n\ufb01lter out any post whose body is longer than 512 tokens.\nThis resulted in a set of 287,790 posts \ufb01ltered by body\nbut not summary, of which we hold out approximately\n5% as a validation set. We used this set of posts for\nRL training since our RL procedure does not require\nreference summaries.\nWe next perform additional \ufb01ltering on the parsed refer-\nence summaries that we use for training our supervised\nbaselines. Speci\ufb01cally, we remove summaries where\nthe TL;DR starts with variants of \u2018Edit\u2019, \u2018Update\u2019, or\n\u2018P.S.\u2019, we heuristically remove summaries with certain\nlevels of profanity, and we remove summaries that are\nless than 24 tokens or more than 48 tokens. As dis-\ncussed in Section 4.1, since our RL models tend to gen-\nerate summaries on the upper end of the allowed length\nlimit, this length \ufb01ltering ensures that there is enough\nlength overlap between the RL summaries and refer-\nence summaries for us to perform a length-controlled\nanalysis. Additionally, we found that summaries shorter\nthan 16 tokens were usually of low quality. We later\nveri\ufb01ed that the summaries we \ufb01ltered out were lower\nquality according to our reward model \u2014 more than 0.5 nats worse on average (i.e. they are predicted\nto be exp(0.5) \u22481.6 times less likely to be preferred). Our \ufb01nal TL;DR dataset contains 123,169\nposts including summaries, again with about 5% held out as a validation set. We use 1913 of these\nvalidation articles for model selection during development; the evaluations in this paper exclude these\narticles.\nNote that, from Table 2 we can see that about two thirds of our TL;DR dataset consists of posts\nrelating to relationships or relationship advice, which is a fairly speci\ufb01c domain. This raises potential\nconcerns about the generality of our models, though their strong transfer performance on CNN/DM\nnews articles suggests they are not unreasonably specialized to relationship advice.\n10These posts are usually follow-ups of previous posts that have been posted to Reddit, and require the context\nof the original post to fully understand.\n16\n",
      "Model size\nn_layers\nd_model\nn_heads\nMax LR\nMax batch size\n1.3B\n24\n2048\n16\n2e-4\n512\n3B\n32\n2560\n32\n1.6e-4\n512\n6.7B\n32\n4096\n32\n1.2e-4\n512\n13B\n40\n5120\n40\n1e-4\n1024\nTable 3: Hyperparameters for our models of various sizes.\nFigure 8: The sweep we conducted for determining our sampling procedure, varying the temperature\nand the \u2018top p\u2019 value for nucleus sampling. While we didn\u2019t do a large enough test to determine\nwhether nucleus sampling is better or worse than moderate-temperature sampling, we found that very\nlow temperature sampling is better than both on this task.\nB\nFurther model training details\nB.1\nHyperparameters\nAll models follow the standard Transformer architecture, with 2048 learned position embeddings.\nAll models are trained with fp16 activations and the Adam optimizer [31]. Nearly all supervised\nbaselines, reward models, and reinforcement learning models are trained with fp32 weights; the\nexception is our TL;DR supervised baselines, which were trained with fp16 weights.11 All models\nare trained with the same byte-pair encoding as in [48].\nDuring pretraining, the models were trained to predict the next token on a large text corpus consisting\nof Commoncrawl, Webtext [48], books, and Wikipedia. Training lasts between 1-3 epochs on each,\nfor a total of 200-300 billion tokens. Learning rate follows a cosine schedule, with a short warmup,\ndecaying to 10% of the maximum value. The batch size ramped up throughout training to some\nmaximum, with each input having 2048 tokens. Hyperparameters for each model are shown in\nTable 3.\nFor supervised baselines, we initialize models from the pretrained models. We decay the learning\nrate with a cosine schedule, using an initial learning rate chosen from a log linear sweep of at least\n7 values. This resulted in learning rates of 6.35e-5, 5.66e-5, 2.83e-5, and 2.83e-5 for our TL;DR\nmodels of size 1.3B, 3B, 6.7B, and 13B respectively, and a learning rate of 2.38e-5 for our CNN/DM\n6.7B model. We use a batch size of 128, and run for a single epoch.\nFor reward modeling, we initialize to the supervised baseline, but with a reward head on top with\nweights initialized according to N(0, 1/(dmodel + 1)) [20]. We train for one epoch, decaying the\n11This was for a historical reason - we found that fp32 weights improved RL performance and so used it for all\nour RL runs. This introduces a small discrepancy, since supervised runs trained in fp32 would have performed\nslightly better. Unfortunately, we forgot to address this in our human evaluations. However, the effect on the\nsupervised loss corresponds to increasing model size by less than 20%, which is small compared to effect sizes\nthat are present in this paper (as seen in Figure 1.)\n17\n",
      "Trained models\nFormat\nMax tokens\nTL;DR (supervised, RL)\nSUBREDDIT: r/{subreddit}\nTITLE: {title}\n512\nPOST: {post}\nTL;DR:\nTransfer from TL;DR to\n{article}\n512\nCNN/DM (supervised, RL)\nTL;DR:\nTL;DR (pretrained)\n{context_stuffed_with_examples}\n=====\nSubreddit: r/{subreddit}\n1999\nTitle: {title}\n{post}\nTL;DR:\nCNN/DM (supervised)\nArticle: {article}\n1999\nTL;DR:\nCNN/DM (pretrained)\n{context_stuffed_with_examples}\n=====\n1999\nArticle: {article}\nTL;DR:\nTable 4: Formats used for the context for each of our trained models on the TL;DR and CNN/DM\ndatasets.\nlearning rate with a cosine schedule, using an initial learning rate chosen from a log linear sweep\nof at least 7 values. We also sweep over between 3 and 10 seeds, and choose the reward model\nthat performs best on the development portion of the validation set, as we \ufb01nd that both the data\niteration order and reward head initialization affect results [13]. For our main results, the 1.3B and\n6.7B reward models had learning rates of 1.5e-5 and 5e-6, respectively. We use a batch size of 64,\nand run for a single epoch.\nFor PPO, we run with separate policy and value networks, initializing our policies to the supervised\nbaseline, and our value functions to the reward model. We set \u03b3 = 1 and \u03bb = 0.95 for the advantage\nestimation [57] and do 4 epochs of optimization for each batch of rollouts. We used a linear learning\nrate decay schedule, with initial learning rates of 1.5e-5 for the 1.3B model and 7e-6 for the 6.7B\nmodel, based on small amounts of experimentation and rough model size extrapolation. We used a\nKL coef\ufb01cient of 0.05 for both of the main runs we report results for (except when we explicitly vary\nthis value in the reward model optimization graphs). We use a batch size of 512 for the 1.3B model\nand 256 for the 6.7B model, and run for 1 million episodes.\nB.2\nInput format\nOur model always receives a byte-pair encoded string of a \ufb01xed size. When the input is too small, we\npad from the beginning of the input with a padding token, and if the input is too long we truncate the\npost/article \ufb01eld at newlines to stay under the limit.\nWhen sampling from models pretrained only on our pretrain mixture and not \ufb01ne-tuned on TL;DR,\nwe follow [48] and instead of padding with a padding token, we pad the beginning of the context\nwith examples of posts/articles and high-quality summaries. We use as many examples as will \ufb01t in\nthe token limit, with the examples formatted the same way as the main input. Table 4 documents the\nformats we used (with pythonic format strings).\n18\n",
      "C\nHuman data collection details\nC.1\nProcess for ensuring high-quality human data\nWe \ufb01rst detail the procedures we use to ensure high-quality data. While these procedures became\nmore rigorous over the course of the project, they generally involved four steps.\nStep 0: Understanding the task ourselves. To understand the task, we \ufb01rst do many summary\ncomparisons ourselves. We also hire a small number of human labelers12 to do comparisons, and\ndiscuss our disagreements. We then draft instructions for a larger set of human labelers.\nStep 1: Labeler onboarding. Labelers are hired from Upwork, a freelancing platform, as well as\ntwo labeling services, Scale and Lionbridge. Labelers \ufb01rst complete a (paid) training process where\nthey label summaries on a shared set of data. For some comparisons, labelers get immediate feedback\nabout which summary was chosen by us, and why, to help them calibrate. We retain labelers that pass\na minimum threshold for speed and agreement with us. To allow for a customizable labeler interface,\nwe built our own website for data collection (see Appendix C.4).\nStep 2: Collecting comparison data. Next, we have labelers evaluate a large batch of comparisons\non our website, which generates the bulk of our data. Before comparing two summaries directly, we\nhave labelers write their \u2018naive interpretations\u2019 of summaries without seeing the original post. We\u2019ve\nfound this helpful for evaluating summaries, as they surface points of ambiguity in the summary\nthat might not have been detected if the summary was read after the original post. After doing naive\ninterpretations, labelers do comparisons by assigning a value on a 9-point scale for how con\ufb01dent\nthey are that summary A is better than summary B (or the converse).\nStep 3: Providing labeler feedback. After collecting the comparison data, we can look at agreement\nrates between labelers. While most comparisons are only given to a single labeler, each labeler gets\nabout 10-20% questions from a shared pool for calibration purposes. We can both attempt to use\nthese statistics as crude measures of quality, and show cases of disagreements to workers to help\nthem improve their labels.\nStep 4: Researcher comparison calibrations. We occasionally also do the task ourselves, to\nmeasure agreement rates between each labeler and us. This is used for quality assessment (see C.2).\nWe also calculate per-labeler \"high con\ufb01dence\" thresholds, by \ufb01nding the con\ufb01dence value on the\nLikert scale for each labeler such that we expect labels above this threshold to agree with us 80% of\nthe time on average. For the purposes of reward model selection, we \ufb01lter the validation set to contain\nonly these higher con\ufb01dence labels. For the entire process, we keep a high communication bandwidth\nwith labelers: we use a shared chat room for labelers to ask clarifying questions and discuss dif\ufb01cult\ncomparisons amongst themselves, host of\ufb01ce hours, and occasionally have one-on-one video calls\nwith labelers to discuss points of disagreement.\nWe keep good labelers throughout the lifetime of the project, while \ufb01ring the lowest-performing\nworkers.\nC.2\nAssessing human feedback quality\nWe assess labeler accuracy by comparing the labeler\u2019s preferred summary with the summary we\nprefer (ignoring the con\ufb01dence level). We exclude comparisons where either the labeler or researcher\nexpresses indifference. This gives us an agreement rate, in theory ranging from 0% (perfect disagree-\nment) to 100% (perfect agreement). For our 2-way comparisons, a random labeler would get 50%\nagreement.\nTo obtain our main number comparing labeler-researcher to researcher-researcher agreement, we\nrestrict ourselves to comparisons between summaries from our 1.3B supervised baseline, because this\nsubset of the data has the most researcher-labeled data. On this subset, labelers agree with researchers\n77% \u00b1 2% of the time, while researchers agree with each other 73% \u00b1 4% of the time. We believe\nsubstantial noise comes from comparisons being quite dif\ufb01cult and subjective.\nIn general, agreement rates range from about 65% for the least pro\ufb01cient labelers and most dif\ufb01cult\ncomparisons (comparing two high-temperature samples from a single RL policy) to about 85% for\n12We pay labelers an hourly wage, regardless of the number of comparisons completed.\n19\n",
      "(a)\n(b)\nFigure 9: (a) The website we made to collect data from labelers. (b) Naive interpretations of\nsummaries on the website.\nthe most pro\ufb01cient labelers and easiest comparisons (comparing a high-temperature sample from\na supervised baseline to the reference summary). Averaging over all workers, weighted by their\nvolume, gives us an estimated agreement rate of 73% \u00b1 3% for our reward model training corpus.\nLabelers agree with each other 72% of the time in the training corpus. This suggests we could get\nmore reliable labels by aggregating labels from multiple workers on the same comparison. Indeed,\non the subset of the training data for which we have enough shared comparisons, taking the modal\nlabel from 3 labelers increases their agreement rate with researchers from 72% to 77%. However, we\nusually collect only one label per comparison, in order to maximize label throughput.\nOn the evaluations for Figure 1, labelers agreed with researchers 73% \u00b1 3% of the time, and labelers\nagreed with each other 73% \u00b1 2% of the time.\nAgreement rate between researchers ranged from about 65% on the most dif\ufb01cult comparisons\n(comparing two high-temperature samples from a single RL policy), to about 80% on the easiest\ncomparisons (comparing a high-temperature sample from a supervised baseline to the human reference\nsummary), to about 95% in cases where we discussed the comparisons with each other.\nOverall we believe that quality is fairly high. Our attempts to \ufb01lter data generally hurt reward model\naccuracy. For example, using the con\ufb01dence thresholds mentioned above, we found that while\nlower-con\ufb01dence labels were less useful than high-con\ufb01dence labels for improving reward model\naccuracy, they were still better to include than to omit. Similarly, leaving out workers with poorer\nagreement rates did not help.\nC.3\nLabeler demographics\nWhen training machine learning models with human feedback, the humans providing the feedback\nare essential in reinforcing the desired model behavior. If we are to scale human feedback to train\nmodels on more complex tasks, where humans might disagree about what the desired model behavior\nshould be, it\u2019s important for members of groups that will be impacted by the model to be included in\nthe labeler population.\nTo provide more transparency into our labeler demographics, we provide results from a survey given\nto our labelers in Table 5. The survey was optional, anonymous, and it was made clear that the\nresults would not affect hiring or \ufb01ring decisions. We \ufb01nd that our labelers span a range of ethnicities,\nnationalities, ages, and genders, and educational backgrounds, but are more likely to be White and\nAmerican.\nC.4\nLabeler website\nSince we hired and trained our own set of labelers, rather than using a crowdsourcing website such\nas Amazon Mechanical Turk, we built our own website to allow for a standardized, customizable\nuser interface for all labelers. Each labeler created a separate pro\ufb01le, allowing us to assign different\nsets of comparisons to different labelers. The website contains different renderers for different kinds\n20\n",
      "What gender do you identify as?\nMale\n38.1%\nFemale\n61.9%\nNonbinary / other\n0%\nWhat ethnicities do you identify as?\nWhite / Caucasian\n42.9%\nSoutheast Asian\n23.8%\nIndigenous / Native American /\n9.6%\nAlaskan Native\nEast Asian\n4.8%\nMiddle Eastern\n4.8%\nLatinx\n4.8%\nMy ethnic identity isn\u2019t listed\n9.6%\nWhat is your nationality?\nAmerican\n45%\nFilipino\n30%\nSouth African\n5%\nSerbian\n5%\nBritish\n5%\nTurkish\n5%\nIndian\n5%\nWhat is your age?\n20-29\n42.9%\n30-39\n23.8%\n40-49\n23.8%\n50-59\n9.5%\n60+\n0%\nWhat is your highest attained level of education?\nLess than high school degree\n0%\nHigh school degree\n14.3%\nUndergraduate degree\n57.1%\nMaster\u2019s degree\n23.3%\nDoctorate degree\n4.8%\nTable 5: Demographic data from 21 of our labelers who participated in our voluntary survey.\nof questions, including naive interpretations, summary comparisons, and Likert evaluations along\ndifferent axes, along with room for labelers to express concerns with the question or explanations for\ntheir decision. Screenshots from the website are shown in Figure 9. Data collected from the website\ncan be easily ported into a central database containing all of our human data.\nC.5\nInstructions for labelers\nHere we provide more detail on the speci\ufb01c instructions given to labelers for comparing summaries,\nand for doing Likert evaluations of summaries along axes of quality. We produced separate sets\nof instructions for evaluating Reddit posts, and for evaluating CNN/DM news articles. For Reddit\ninstructions, we \ufb01rst describe Reddit in general and provide a table that translates Reddit-speci\ufb01c\nlingo into common parlance.\nInstructions for comparing summaries.\nWe show an excerpt of the instructions given to labelers\nfor making comparisons in Table 6. In addition to these instructions, we provide an example labeled\ncomparison between Reddit summaries, and also example naive interpretations for summaries.\nInstructions for evaluating summaries along axes of quality.\nWe provide a separate set of de-\ntailed instructions for labelers for the 7-point Likert evaluations. We \ufb01rst introduce each of the 4 axes\nof quality we consider, giving an overview of coherence, accuracy, coverage, and overall score (shown\nin Table 7). We also provide a brief rubric for giving scores of 1, 4, and 7, along with several Reddit\nsummaries annotated with our own judgments of quality along each of these axes (with explanations).\n21\n",
      "What makes for a good summary? Roughly speaking, a good summary is a shorter piece of text\nthat has the essence of the original \u2013 tries to accomplish the same purpose and conveys the same\ninformation as the original post. We would like you to consider these different dimensions of\nsummaries:\nEssence: is the summary a good representation of the post?\nClarity: is the summary reader-friendly? Does it express ideas clearly?\nAccuracy: does the summary contain the same information as the longer post?\nPurpose: does the summary serve the same purpose as the original post?\nConcise: is the summary short and to-the-point?\nStyle: is the summary written in the same style as the original post?\nGenerally speaking, we give higher weight to the dimensions at the top of the list. Things are\ncomplicated though \u2013 none of these dimensions are simple yes/no matters, and there aren\u2019t hard\nand fast rules for trading off different dimensions. This is something you\u2019ll pick up through\npractice and feedback on our website.\nTable 6: An excerpt from the instructions we gave to labelers for doing comparisons.\nFinally, we provide a FAQ section that answers common questions raised by the small initial set of\nlabelers we assigned to this task.\nFor CNN/DM, we provide the same set of instructions, except we add some additional clari\ufb01cations\nfor how to judge news articles. We speci\ufb01cally ask labelers to place less emphasis on \ufb02uidity of\nsentences (because the reference summaries were originally written in bullet-point form, and we\ndidn\u2019t want labelers to penalize this), and to place less emphasis on the summary matching the intent\nof the article (which was important for Reddit summaries).\nIn terms of quality control, we conducted a smaller version of the quality control process described\nin Appendix C.1: we \ufb01rst labeled a small set of summaries ourselves along each axis to understand\npoints of confusion, then we wrote the instructions document to provide to labelers, then we had a\nsmall number of labelers do a trial of the task to catch any remaining bugs or points of confusion, and\n\ufb01nally we onboarded a larger set of labelers onto the task while remaining available to answer any\nquestions.\nC.6\nComposition of the labeled dataset\nOver the course of the project, we trained several reward models and policies. Each batch of\nsummaries that we sent to the labelers were sampled from a variety of policies. We didn\u2019t have a\nsystematic plan for which policies to sample from; rather, we chose what seemed best at the time in\nthe spirit of exploratory research. Every time we trained a reward model, we trained on all labels we\nhad collected so far. Successive models also bene\ufb01ted from improved hyperparameters and dataset\ncleaning. Our results could likely be replicated with a simpler, more systematic approach.\nIn general, as we hire new labelers and as existing labelers perform the task more, it is possible that\nthere is \u2018labeler drift\u2019, where the set of criteria used by labelers to evaluate summaries gradually shifts\nover time. This could lead to a regression in labeler-researcher disagreement, or lead to some policies\nbecoming more or less preferred over time. To help guard against this, in most batches we include\ncomparisons between samples from our supervised baseline and reference summaries, and measure\nthe frequency with which the workers prefer one over the other. If this number drifts over time, it\u2019s\nan indication that our workers\u2019 preferences are also changing. However, we generally found that this\npreference number stayed relatively constant, within noise.\nTable 8 lists the policies we trained by supervised \ufb01netuning on the TL;DR dataset, as well as the\nreward models, trained on successively larger datasets of human labels. Table 9 lists the RL policies.\n22\n",
      "Coherence\nFor this axis, answer the question \u201chow coherent is the summary on its own?\u201d A summary is\ncoherent if, when read by itself, it\u2019s easy to understand and free of English errors. A summary is\nnot coherent if it\u2019s dif\ufb01cult to understand what the summary is trying to say. Generally, it\u2019s more\nimportant that the summary is understandable than it being free of grammar errors.\nRubric:\nScore of 1: The summary is impossible to understand.\nScore of 4: The summary has mistakes or confusing phrasing that make it a bit hard to understand.\nScore of 7: The summary is perfectly clear.\nAccuracy\nFor this axis, answer the question \u201cdoes the factual information in the summary accurately match\nthe post?\u201d A summary is accurate if it doesn\u2019t say things that aren\u2019t in the article, it doesn\u2019t mix up\npeople, and generally is not misleading. If the summary says anything at all that is not mentioned\nin the post or contradicts something in the post, it should be given a maximum score of 5. (If you\nare confused about how to use \u20186\u2019, see the FAQ!)\nRubric:\nScore of 1: The summary is completely wrong, made up, or exactly contradicts what is written in\nthe post.\nScore of 4: The summary says at least one substantial thing that is not mentioned in the post, or\nthat contradicts something in the post.\n(Score of 5: The summary says anything, no matter how small, that is not mentioned in the post,\nor that contradicts something in the post.)\nScore of 7: The summary has no incorrect statements or misleading implications.\nCoverage\nFor this axis, answer the question \u201chow well does the summary cover the important information\nin the post?\u201d A summary has good coverage if it mentions the main information from the post\nthat\u2019s important to understand the situation described in the post. A summary has poor coverage if\nsomeone reading only the summary would be missing several important pieces of information\nabout the situation in the post. A summary with good coverage should also match the purpose of\nthe original post (e.g. to ask for advice).\nRubric:\nScore of 1: The summary contains no information relevant to the post.\nScore of 4: The summary is missing at least 1 important piece of information required to under-\nstand the situation.\nScore of 7: The summary covers all of the important information required to understand the\nsituation.\nOverall quality\nFor this axis, answer the question \u201chow good is the summary overall at representing the post?\u201d\nThis can encompass all of the above axes of quality, as well as others you feel are important. If\nit\u2019s hard to \ufb01nd ways to make the summary better, give the summary a high score. If there are lots\nof different ways the summary can be made better, give the summary a low score.\nRubric:\nScore of 1: The summary is terrible.\nScore of 4: The summary is an okay representation of the post, but could be signi\ufb01cantly improved.\nScore of 7: The summary is an excellent representation of the post.\nTable 7: Instructions given to labelers for evaluating summaries along four different axes of quality.\n23\n",
      "Supervised policy name\n# Parameters\nsup1\n750M\nsup2\n1.3B\nsup3\n1.3B\nsup3_6b\n6.7B\nsup4\n1.3B\nsup4_6b\n6.7B\nReward model name\n# Parameters\nrm1\n1.3B\nrm2\n6.7B\nrm3\n1.3B\nrm3_6b\n6.7B\nrm4\n1.3B\nrm4_6b\n6.7B\nTable 8: Left: supervised baselines. sup4 and sup4_6b are the \ufb01nal supervised baselines used\nthroughout the paper. Right: reward models. rm4 and rm4_6b are the \ufb01nal reward models used\nthroughout the paper.\nRL policy name\n# Parameters\nObjective\nInitialization\nKL coef\ufb01cient\nKL(ppo, sup)\nsup3 ppo rm1\n1.3B\nrm1\nsup3\n0.35\n1.8\nsup4 ppo rm3 1\n1.3B\nrm3\nsup4\n0.10\n3.8\nsup4 ppo rm3 2\n1.3B\nrm3\nsup4\n0.07\n9.4\nsup4 ppo rm3 3\n1.3B\nrm3\nsup4\n0.05\n19.0\nsup4 ppo rm4\n1.3B\nrm4\nsup4\n0.05\n18.0\nsup4_6b ppo rm4_6b\n6.7B\nrm4_6b\nsup4_6b\n0.05\n14.0\nTable 9: PPO policies. sup4 ppo rm4 and sup4_6b ppo rm4_6b are the \ufb01nal policies used throughout\nthe paper.\nBoN policy name\nObjective\nBase policy\nN\nKL(BoN, sup)\nsup2 bo8 rm1\nrm1\nsup2\n8\n1.2\nsup3 bo8 rm1\nrm2\nsup3\n8\n1.2\nsup3 bo63 rm2\nrm2\nsup3\n63\n3.2\nsup4 bo8 rm3\nrm3\nsup4\n8\n1.2\nsup4 bo64 rm3\nrm3\nsup4\n64\n3.2\nsup4 bo128 rm3\nrm3\nsup4\n128\n3.9\nsup4 bo256 rm3\nrm3\nsup4\n256\n4.5\nsup4 bo512 rm3\nrm3\nsup4\n512\n5.2\nsup4 bo128 rm3_6b\nrm3_6b\nsup4\n128\n3.9\nsup4 bo256 rm3_6b\nrm3_6b\nsup4\n256\n4.5\nTable 10: Best-of-N policies. KL divergence is computed analytically as KL(boN, sup) = log N -\n(N-1)/N.\nWe also explored a simple alternative to reinforcement learning: Sample N summaries from a\nsupervised baseline at temperature 0.7, score them with a reward model, and take the summary with\nthe highest score. This best-of-N (BoN) procedure is effectively a mildly optimized policy requiring\nno training. These policies are named in Table 10, and samples from them form part of the training\ndata.\nTable 11 lists the source policies for the training data for each reward model.\nLabel count\nReward model\nPolicy0\nPolicy1\nrm1\nref\nsup1\n5404\nsup1\nsup1\n5386\nrm2\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3_6b\n1424\nsup1\nsup1\n5386\nContinued on next page\n24\n",
      "Label count\nReward model\nPolicy0\nPolicy1\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nrm3, rm3_6b\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3\n438\nsup3 bo63 rm2\n447\nsup3 bo8 rm2\n887\nsup3 ppo rm1\n884\nsup3_6b\n1424\nsup1\nsup1\n5386\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nsup3\nsup3 bo8 rm2\n428\nsup3 ppo rm1\n416\nsup3 bo63 rm2\nsup3 bo8 rm2\n432\nsup3 ppo rm1\n444\nsup3 bo8 rm2\nsup3 ppo rm1\n855\nrm4, rm4_6b\nref\nsup1\n5404\nsup2\n12779\nsup2 bo8 rm1\n1426\nsup3\n438\nsup3 bo63 rm2\n447\nsup3 bo8 rm2\n887\nsup3 ppo rm1\n884\nsup3_6b\n1424\nsup4\n1335\nsup4 bo128 rm3\n602\nsup4 bo128 rm3_6b\n203\nsup4 bo256 rm3\n307\nsup4 bo256 rm3_6b\n101\nsup4 bo512 rm3\n52\nsup4 bo64 rm3\n52\nsup4 bo8 rm3\n393\nsup4 ppo rm3 1\n981\nsup4 ppo rm3 2\n215\nsup4 ppo rm3 3\n208\nsup4_6b\n104\nsup1\nsup1\n5386\nsup2\nsup2\n11346\nsup2 bo8 rm1\n1376\nsup3_6b\n1383\nsup2 bo8 rm1\nsup3_6b\n1390\nsup3\nsup3 bo8 rm2\n428\nsup3 ppo rm1\n416\nsup3 bo63 rm2\nsup3 bo8 rm2\n432\nsup3 ppo rm1\n444\nsup3 bo8 rm2\nsup3 ppo rm1\n855\nsup4\nsup4\n1051\nsup4 ppo rm3 1\n395\nContinued on next page\n25\n",
      "Label count\nReward model\nPolicy0\nPolicy1\nsup4 bo128 rm3\nsup4 bo128 rm3\n288\nsup4 bo256 rm3\n582\nsup4 bo128 rm3_6b\nsup4 bo128 rm3_6b\n95\nsup4 bo256 rm3_6b\n203\nsup4 bo512 rm3\nsup4 ppo rm3 3\n216\nsup4_6b\n60\nsup4 bo64 rm3\nsup4 ppo rm3 2\n218\nsup4_6b\n55\nsup4 bo8 rm3\nsup4 ppo rm3 1\n752\nsup4 ppo rm3 1\nsup4 ppo rm3 1\n372\nsup4 ppo rm3 2\nsup4 ppo rm3 2\n4256\nsup4_6b\n215\nsup4 ppo rm3 3\nsup4 ppo rm3 3\n4037\nsup4_6b\n216\nTable 11: Training data for reward models. \"ref\" refers to human reference summaries.\nC.7\nExample comparison tasks\nTo give a sense of the dif\ufb01culty of the comparisons task, we provide example comparisons between\ntwo summaries generated by our 6.7B human feedback model. In Table 12 we show both a random\ncomparison drawn from the TL;DR dataset, and a cherry-picked comparison (selected from 10\ncomparisons where labelers disagreed) to illustrate the trade-off between accuracy in coverage that\ncan occur when labelers conduct evaluations.\n26\n",
      "Random TL;DR comparison\nPOST\nSubreddit: r/Pets\nTITLE: What do you use for \ufb02ea control?\nMy family has tried literally EVERYTHING to control the \ufb02eas in our neighborhood (Frontline, Advantage,\nDiatomaceous Earth, Dawn Dishsoap, etc!) and nothing has worked. I have spoken to lots of pet owners in\nmy area (I work as a vet assistant) and many are reporting similar results, where \ufb02eas are becoming resistant\nto the usually recommended treatments. The only thing that has worked so far is Comfortis, but I\u2019ve read of\nseveral dogs having reactions to it that can be pretty severe. My dogs are \ufb01ne, we\u2019ve used it for about a year\nnow, but I don\u2019t like the idea of harming them or putting them at risk.\nGiving them baths with blue Dawn dish soap does kill all the \ufb02eas, but it does nothing to prevent more from\ncoming back, obviously. It only kills on contact, and we are NOT going to over bath them because that isn\u2019t\nhealthy either. We\u2019re looking for something that lasts.\nDoes anyone else have experience with this, or any detailed information on Comfortis and if it does serious\ndamage to your pet\u2019s system? Yes, I know I am a vet assistant. My boss strictly recommends Frontline and\nliterally will not listen to me when I tell him it doesn\u2019t work and my dogs are still covered in \ufb02eas and we\nhave to use Comfortis because it is the only thing that gives them relief. He is not a resource in this case.\nJust wanted to see what other pet owners (speci\ufb01cally ones in San Diego) do for \ufb02eas...the ones we have here\nare mutants or something, because almost nothing works on them!\nSummary A: Fleas are developing resistance to most \ufb02ea control products (including Comfortis). Looking\nfor something that lasts long term that doesn\u2019t harm my dogs. Does anyone have experience with any of the\nlisted products?\nSummary B: Nothing has worked on our \ufb02eas, we are looking for something that lasts, Comfortis is not a\nlong term solution. Does anyone else have experience with \ufb02ea control or have information on Comfortis?\nHard TL;DR comparison\nPOST\nSubreddit: r/weddingplanning\nTITLE: Feeling major anxiety about dress shopping.\nSo, not really sure if I\u2019m asking for advice or just a small rant. We got engaged March 2, 2013. From day 1\nwe\u2019ve been struggling through the planning. At \ufb01rst, it was arguing with his parents about us getting married\nin a church. And then it was an argument about which venue to have the reception. We \ufb01nally have the venue\nbooked and the church matter settled. Now that\u2019s out of the way, I suddenly have this pit in my stomach\nMy mom left me when I was 14. I\u2019ve basically done everything on my own and I have really been ok about it.\nI\u2019m sure it\u2019s not of the norm for me to feel so disassociated about the whole thing, but I am. I\u2019m suppose to\ngo look at wedding dresses this Friday. I am feeling super anxious because I don\u2019t know if trying on wedding\ndresses is going to turn me into a blubbering baby about not having a mom.\nMy future mother-in-law is suppose to come with me to help look. I worry about turning into that blubbering\nbaby and offending her. I don\u2019t want her thinking that I don\u2019t appreciate her being there.\nAside from me worrying about becoming a giant baby, I\u2019ve also been having issues with my bridal party.\nWhile I haven\u2019t made any of\ufb01cial choices, I have ideas of who I want involved. That would be my best\nfriend, my sister, and my future sister-in-law. My \ufb01rst choice for a MOH is my best friend. However, she\nlives out of state, and is in a medical program for school. So her visit time is severely limited. My sister feels\nentitled to be the MOH, despite the fact that we are not close at all. So getting people together to get any\nkind of wedding stuff done is almost impossible.\nSummary A: I\u2019m having doubts about whether or not to try on wedding dresses. I am also having doubts\nabout my bridal party\u2019s ability to get things done.\nSummary B: I think I\u2019m going to turn into a blubbering baby and offend my mother-in-law.\nTable 12: Top: Example of a random comparison task on the TL;DR dataset between two summaries\nfrom our 6.7B human feedback model. Comparison chosen randomly from the validation set. Bottom:\nAn example of a dif\ufb01cult comparison task on the TL;DR dataset. Chosen by looking at comparisons\nbetween supervised baseline summaries with at least 4 labeler judgements and with at least 40% vote\nfor each summary. Cherry-picked out of 10 to highlight an accuracy-coverage tradeoff. Summary A\nis inaccurate since the author does not explicitly say she is having doubts about trying on wedding\ndresses. Summary B is entirely accurate but does not capture the general essence of the post. In this\ncase, 4 workers chose A and 3 workers chose B. For more comparisons, see our website.\n27\n",
      "D\nChoice of baselines\nIn testing our human feedback techniques, we collected a large amount of high-quality data from\nhuman labelers. In order to compare fairly against supervision-based techniques, we would have\nneeded to spend a similar amount of labeler time collecting high quality demonstrations, and used\nthose to \ufb01ne-tune a model via supervised learning. Because this is prohibitively expensive, we do not\nprovide such a baseline.\nExisting prior work such as PEGASUS [70] has studied supervised methods on a dataset very similar\nto ours (the /r/tifu subset of TL;DR). However, they use much smaller (500M parameters) models,\nand report that their model outputs are worse than the human reference summaries, according to\nhuman evaluations. Thus, due to our limited labeler budget for evaluation, we decided to use our own\nsupervised and zero-shot models as baselines (after sanity-checking the ROUGE performance of our\nsupervised models), as well as T5 [49].\nT5 models [49] are pretrained and \ufb01ne-tuned in a similar way to our supervised baselines, but they\nuse an encoder-decoder architecture. We used T5 outputs which were obtained via beam search\ndecoding, as described in [49]. We also carefully account for differences in tokenization between\nmodel outputs.13\n13Since tokenization affects capitalization and punctuation of the model outputs, we normalized all CNN/Daily\nMail outputs from all models by lower-casing everything and then heuristically re-capitalizing. We verify that\nthis normalization procedure produces identical results for reference summaries tokenized in different ways.\n28\n",
      "E\nCNN/DM lead-3 vs reference summaries\nOn the CNN/DM dataset, our labelers signi\ufb01cantly preferred lead-3 (a summary consisting of the \ufb01rst\n3 sentences of the article) to reference summaries. In part this is due to longer summaries receiving\nhigher coverage scores and lead-3 being 50% longer, as shown in Table 13.\nPolicy\nLength (stdev)\nQuality\nQuality increase\n/ 100 char.\nref\n314 (119)\n5.54\n0.14\nlead-3\n475 (114)\n6.23\n0.34\nTable 13: How length affects overall quality on CNN/DM for lead-3 and reference summaries.\nHowever, if we use a linear regression (similar to the procedure in Appendix F) to predict what lead-3\nperformance would be if its average length were reduced to 314 characters, we still \ufb01nd a quality\nof 5.68, modestly higher than the reference summaries. Moreover, for lead-3 to even achieve parity\nwith the reference summaries seems to call into question the need for abstractive summarization or\nsophisticated ML methods, since a simple extractive baseline can match a perfect imitation of the\nreference summaries.\nWe wanted to understand labeler behavior on these comparisons, to ensure that it was not an error.\nTo do this, we examined a sample of our labeler\u2019s judgments ourselves. We found that in 20/143\ncases labelers preferred lead-3 by 3 points or more, and that excluding these datapoints would raise\nthe relative score of the reference summaries by about 0.5 points.14 We were surprised to see the\nreference summaries performing so poorly in a signi\ufb01cant fraction of cases, so we looked at labeler\u2019s\nexplanations and con\ufb01rmed they made sense.\nWe found that two features of the reference summaries explained most of its underperformance. First,\n13 of these 20 summaries omitted one of the key points from the article\u2014the highlights are often\nwritten for a reader who had already seen the title of the article, even though the titles are not included\nin the CNN/DM dataset. Second, 10 of these 20 summaries actually introduced new information not\npresent in the original article. From the perspective of labelers this information is totally confabulated\nand so led to lower scores. A likely explanation for these errors is that the reference summaries are\nextracted from \u201chighlights\u201d on the news sites rather than being a straightforward summary of the\narticle. These failures are common enough that they signi\ufb01cantly impact the average quality of the\nreference summaries, and the effects seem to be large relative to quality differences between ML\nmodels.\nOverall we believe that labeler judgments were reasonable in these cases, and that it is potentially\nproblematic to treat the \u201chighlights\u201d in the CNN/DM dataset as reference summaries. You can view\nall of our labeler\u2019s judgments on CNN/DM at our website.\n14The reference summaries were preferred to lead-3 by a similar margin in only 7/143 cases.\n29\n",
      "Supervised\nlearning*\nHuman feedback*\nPretrain only*\nReference summaries\n*: length-controlled\n(a)\nSupervised learning\nHuman feedback\nPretrain only\n(b)\nFigure 10: (a) A length-controlled version of Figure 1, using the procedure described in Appendix\nF. Controlling for length reduces the relative preference of our human feedback models, however\nthey are still preferred to the reference summaries. (b) Plotting model quality for different summary\nlengths on the TL;DR dataset. Our 6.7B human feedback model outperforms both the 6.7B supervised\nbaseline and the reference summaries (horizontal line at 0.5) across lengths.\nF\nControlling for summary length\nAs discussed in Section 4.1, the length of a summary is a confounding factor for evaluating summary\nquality; depending on the desired trade-off between conciseness and coverage, a shorter or longer\nsummary might be better. Our models generate summaries that are longer than the reference\nsummaries, as this led to higher labeler preference given the 24-48 token limit for our task. Here we\ndescribe the procedure we use to attempt to control for length.\nTo calculate a single length-controlled preference number, we train a logistic regression model to\npredict the human-preferred summary on our dataset of human comparisons. We provide this model\nwith 2 features: the identity of each policy, and the log ratio of the summary lengths. To calculate\nthe length-controlled preference value between two policies, we simply give each policy ID to our\ntrained logistic regression model and set the log length ratio to zero (see Figure 10a). In Figure 10b\nwe examine summary quality across a range of summary lengths on TL;DR. We \ufb01nd that our human\nfeedback model outperforms the supervised baseline across all length values.\nFor CNN/DM, we use a similar procedure as described above to control for length, except using a\nlinear regression model to predict the Likert rating from 1-7. We show the expected quality increase\nfor making summaries 100 characters longer in Table 14, which suggests our human feedback models\nwould perform better if they generated longer summaries.\nPolicy\nLength\nQuality\nQuality \u2197\n(stdev)\n(1-7)\n/ 100 char.\nsl(tldr)-1.3b\n138 (34)\n4.26\n0.68\nsl(tldr)-6.7b\n127 (31)\n4.41\n0.38\ngpt-1.3b\n141 (41)\n4.11\n0.63\ngpt-6.7b\n142 (36)\n4.6\n0.3\nrl(tldr)-1.3b\n166 (30)\n4.86\n1.28\nrl(tldr)-6.7b\n175 (30)\n5.25\n0.87\nsl(cnn)-6.7b\n300 (103)\n5.4\n0.37\nref\n314 (119)\n5.54\n0.14\nlead-3\n475 (114)\n6.23\n0.34\nT5\n316 (95)\n5.92\n0.3\nTable 14: How length affects overall quality on CNN/DM. We show average length and quality\nscores for various policies, and how much the summary quality increases on average per 100 added\ncharacters.\n30\n",
      "G\nAdditional results\nG.1\nValue function ablation\nIn this section, we conduct an ablation comparing using separate parameters for the value function\nand policy, against using a shared network as done in [73]. The results, shown in Figure 11, clearly\nindicate that using separate networks outperforms the latter. On the other hand, having separate\nnetworks increases the memory requirements of running RL \ufb01ne-tuning. Having separate networks\nalso allows us to initialize the value function to be the learned reward model that is being optimized.\nFigure 11: Comparing the reward obtained by optimizing with separate value function and reward\nmodel parameters to shared parameters.\nG.2\nEvaluating policies along axes of quality\nWe show the full results of the evaluations of policies on a 7-point Likert scale along different axes\nof quality; for TL;DR this is shown in Figure 12, and for CNN/DM this is shown in Figure 13. It is\nevident that on both datasets coverage correlates strongly with overall score across models, and all\nmodels achieve a high coherence score.\nG.3\nStudying best-of-N optimization\nA natural way to evaluate an automatic evaluation metric is to see the extent to which optimizing\nagainst it leads to high performance according to humans. One way to assess this is to use best-of-N\nas an (inef\ufb01cient) optimization technique \u2014 this has the bene\ufb01ts of being simple and invariant to\nmonotonic transformations. We report results for up to best-of-2048 on ROUGE and three of our\nreward models in Figure 7, using samples from the 1.3B supervised baseline. The results suggest\nthat optimizing against ROUGE signi\ufb01cantly under-performs optimizing against our reward models.\nThe data also suggests ROUGE degrades with too much optimization much faster than our reward\nmodels.\nWith increasing N, the best-of-N policies get higher average reward. Similarly, by decreasing the\nKL coef\ufb01cient \u03b2, the PPO policies get higher average reward. We found that at a given average\nreward, the best-of-N and PPO policies have similar quality as judged by human labelers (not shown).\nHowever, the PPO policy is farther from the supervised baseline than best-of-N is, as measured by\nthe KL divergence.15\nG.4\nROUGE scores\nIn Figure 14a and 14b, we show the ROUGE scores of our models on the TL;DR and CNN/DM\ndatasets, respectively. We report results with T=0, consistent with our human evaluations. We found\nthat temperature has an (often signi\ufb01cant) impact on ROUGE score, and we did a thorough sweep to\nverify that the best temperature setting is T=0.\n15We can use KL from the supervised baseline as a distance metric. Note that we can calculate the KL of a\nbest-of-N policy analytically as log(n) \u2212n\u22121\nn .\n31\n",
      "Figure 12: Evaluating TL;DR policies on a 7-point Likert scale along several axes of quality.\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nProphetNet [67]\n44.20\n21.17\n40.69\nT5 [49]\n43.52\n21.55\n40.69\nOur 6.7B supervised model\n42.49\n19.84\n39.53\nCNN-2sent-hieco-RBM [71]\n42.04\n19.77\n39.42\nTable 15: Comparing the ROUGE score of our 6.7B supervised model on CNN/DM to recent SOTA\nmodels from the literature. Without any summarization-speci\ufb01c engineering, our model achieves\nROUGE scores better than SOTA models from mid-2019, indicating that it is a strong baseline for\ncomparison.\n32\n",
      "Figure 13: Evaluating CNN/DM policies on a 7-point Likert scale along several axes of quality.\nOn TL;DR, we \ufb01nd that our human feedback models obtain a slightly lower ROUGE score than\nthe supervised models at T = 0, further indicating that ROUGE correlates poorly with human\npreferences. For supervised models, lowering temperature has a larger impact than increasing model\nsize. Interestingly, at higher temperatures, our feedback models actually outperform supervised\ncounterparts (not shown).\nOn CNN/DM, ROUGE agrees with our human evaluations that our human feedback models transfer\nbetter than our supervised models. However, unsurprisingly, supervised CNN/DM models still\nachieve much higher ROUGE. In Table 15, we show the ROUGE results on CNN/DM for our 6.7B\nsupervised baseline and various models from the literature. We \ufb01nd that our model achieves ROUGE\nscores less than T5 [49], but slightly greater than the CNN-2sent-hieco-RBM model from [71], which\nwas SOTA for abstractive summarization on CNN/DM in mid-2019 according to the NLP-progress\nleaderboard.16\nG.5\nBigram overlap statistics\nIn Table 16, we show the bigram overlap statistics for our models on the TL;DR and CNN/DM\ndatasets as a proxy for how much the summaries copy frmo the post. As in Section 4.4, we compute\nthe longest common subsequence of bigrams with the original Reddit post or news article, and\ndividing by the number of bigrams in the summary. We \ufb01nd that models evaluated on CNN/DM\n16http://nlpprogress.com/english/summarization.html\n33\n",
      "(a)\n(b)\nFigure 14: ROUGE scores for our models on (a) the TL;DR dataset, and (b) the CNN/DM dataset.\nEvaluated on TL;DR\nModel\nModel size\nBigram overlap %\nGPT\n1.3B\n66.7%\nGPT\n3B\n72.7%\nGPT\n6.7B\n61.4%\nGPT\n13B\n75.9%\nSupervised (TL;DR)\n1.3B\n49.0%\nSupervised (TL;DR)\n3B\n48.7%\nSupervised (TL;DR)\n6.7B\n48.9%\nSupervised (TL;DR)\n13B\n48.0%\nHuman feedback (TL;DR)\n1.3B\n53.3%\nHuman feedback (TL;DR)\n6.7B\n46.0%\nEvaluated on CNN/DM\nModel\nModel size\nBigram overlap %\nGPT\n1.3B\n76.3%\nGPT\n6.7B\n76.2%\nSupervised (TL;DR)\n1.3B\n59.5%\nSupervised (TL;DR)\n6.7B\n56.9%\nHuman feedback (TL;DR)\n1.3B\n64.8%\nHuman feedback (TL;DR)\n6.7B\n51.2%\nSupervised (CNN/DM)\n1.3B\n66.0%\nT5\n11B\n68.8%\nreference\n\u2014\n36.8%\nTable 16: Bigram overlap statistics on the TL;DR dataset (top) and the CNN/DM dataset (bottom).\nModels trained on CNN/DM copy signi\ufb01cantly more than models trained on TL;DR.\n(whether or not they were trained on CNN/DM) generally copy more than models evaluated on\nTL;DR. Further, our supervised and human feedback models copy less than our pretrained models.\nG.6\nReward model validation sets\nIn this section, we report results evaluating our reward models on various manually constructed\nvalidation sets, shown in Tables 17 and 18. Notably, we asked our humans to produce a small dataset\nof edits, by having them make improvements to existing summaries (either reference summaries or\nsupervised baseline summaries). Our 6.7B reward model prefer the improved summaries at a similar\nrate to humans (who do not know which summary has been edited).\nOur reward models are also sensitive to sentence shuf\ufb02ing (whereas metrics like ROUGE are largely\nnot), and are able to detect when the roles portrayed in the summary have been switched. On the\nother hand, our reward models sometimes exhibit preference for poor arti\ufb01cial summaries, such as\n34\n",
      "RM size\nEdit length\nRM prefers edit\nHuman prefers edit\nRM, human agree\nShorter\n63.6%\n76.2%\n62.1%\n1.3B\nLonger\n86.8%\n88.6%\n79.6%\nAvg.\n81.2%\n85.6%\n75.4%\nShorter\n66.0%\n76.2%\n65.5%\n6.7B\nLonger\n89.2%\n88.6%\n80.2%\nAvg.\n83.7%\n85.6%\n76.7%\nTable 17: Comparing reward model and human preference of summaries that were edited by humans\nto make them better. For each summary, the human labeler that makes the comparison is different\nthan the labeler that wrote the edit. The agreement numbers do not include comparisons where the\nlabeler\u2019s preference was marked as \u2018uncertain\u2019.\nPreference % of Summary A\nSummary A\nSummary B\n1.3B RM\n6.7B RM\nOriginal summary\nReversed roles\n93.1%\n97.4%\nlead-3\nShuf\ufb02ed lead-3\n68.1%\n75.5%\nrand-3\nShuf\ufb02ed rand-3\n60.8%\n76.1%\nPost title\nRandom title\n97.4%\n98.5%\nPost title\nRandom title from same subreddit\n98.8%\n97.2%\nPost title\nPost title repeated twice\n84.6%\n58.4%\n(r/tifu only) Reference summary\nRef + \u201cWhat should I do?\u201d\n34.3 %\n74.5%\nReference summary\nlead-3\n63.0%\n56.4%\nReference summary\nlead-2\n71.0%\n73.8%\nReference summary\nrand-3\n69.5%\n59.5%\nTable 18: Reward model performance on various manually constructed validation sets. In all cases,\nSummary A is intended to be better than Summary B, and thus a higher preference % is generally\nbetter. \u2018rand-3\u2018 indicates a baseline where 3 random sentences are taken from the post; however these\nsentences are kept in the order in which they appear in the post. \u2018Original summary\u2019 is either the\nreference summary or a summary from our supervised baselines. r/tifu is a subreddit whose purpose\nis sharing embarrassing stories (not asking for advice).\nthe post title copied twice, or asking for advice at the end of the summary. In Table 19, we show\nexamples where our model is sensitive to small, semantically meaningful changes in the summary.\nG.7\nMeasuring agreement between different evaluation metrics\nWe are interested in understanding the relationship between different metrics for evaluating summaries.\nTo do this, we compute agreement between various metrics, including automatic metrics and humans,\nfor different subsets of the data for which we have human evaluations. To remove policy quality\nas a confounding variable, all of the summary comparisons are generated by the same policy at the\nsame temperature value. In Table 20, we use samples from our 1.3B supervised model at T=0.7 on\nTL;DR; Table 21 has comparisons from our 6.7B supervised model at T=0.7 on TL;DR; Table 22\nhas comparisons from our 6.7B human feedback model at T=0.7 on TL;DR; and Table 23 has\ncomparisons from our 6.7B supervised baseline trained on CNN/DM.\nOur 6.7B reward model generally agrees with labelers as much as other labelers, although an\nensemble of labelers does better. On the other hand, ROUGE generally has poor agreement, as does\nlog probability under the supervised baselines, with simple heuristics like copying (longest common\nsubsequence of bigrams with the article) and length often performing comparably.\n35\n",
      "Edited summary\nReward \u2206\nCrush on girl I haven\u2019t seen in 4 years. She doesn\u2019t like me and I don\u2019t still like\nher. What do?\n+0.64\nA girl told me she loved liked me, she ended up picking another guy over me,\nthat guy badly in\ufb02uenced her, and now I\u2019m here alone thinking what could\u2019ve\nbeen.\n+0.82\nI tried to show my friend a picture of my tarantula and she smashed my phone\nwith all her might and now I lost a good friend phone.\n-0.64\nBoyfriend still FB stalks his high school ex girlfriend from time to time and\ntold me when he was very drunk that she was his \ufb01rst love.\n+0.73\nI\u2019ve become pathetic, pining after a guy my ex. Would like to reach state of less\npathetic. If more info is necessary, please let me know.\n+0.69\nI have body issues (body acne/scarring and weight issues) that prevent me from\nhaving a normal life without shame and prevent me from having a better sex\nlife with my BF.\n+1.0\nDo you take someone back after they\u2019ve turned you down off, even if you can\u2019t\nsee them in person or are they just not worth the risk?\n+0.52\nTable 19: Qualitative examples showing the change in reward of the reward model on human-\ngenerated edits to TL;DR summaries that make the summaries better. Examples are randomly\nselected from the set where the edit distance was less than 5 and the magnitude of change in reward\nwas greater than 0.5. Text in strike-through was removed from the original summary in the edit, and\ntext in bold was added. The reward model is sensitive to small but semantically meaningful changes\nin the summary, although it makes errors on occasion.\nTL;DR\n1.3B sup.\nT=0.7\nresearcher labeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nresearcher 73.4%\n\u00b14.1%\n77.7%\n\u00b12.1%\n84.4%\n\u00b13.3%\n55.5%\n\u00b14.3%\n62.3%\n\u00b14.1%\n59.1%\n\u00b14.2%\n61.8%\n\u00b14.8%\n72.2%\n\u00b14.5%\n62.8%\n\u00b14.7%\n78.0%\n\u00b13.9%\nlabeler\n77.7%\n\u00b12.1%\n68.6%\n\u00b11.7%\n74.4%\n\u00b12.0%\n54.4%\n\u00b11.3%\n58.0%\n\u00b11.2%\n57.7%\n\u00b11.3%\n58.7%\n\u00b12.0%\n65.8%\n\u00b12.0%\n61.9%\n\u00b12.1%\n70.8%\n\u00b11.8%\nlabeler\nensemble 84.4%\n\u00b13.3%\n74.4%\n\u00b12.0%\n\u2014\n60.6%\n\u00b14.0%\n62.7%\n\u00b13.8%\n59.0%\n\u00b13.9%\n59.5%\n\u00b14.4%\n71.0%\n\u00b13.9%\n59.5%\n\u00b14.3%\n72.5%\n\u00b13.8%\nlength\n55.5%\n\u00b14.3%\n54.4%\n\u00b11.3%\n60.6%\n\u00b14.0%\n\u2014\n50.1%\n\u00b11.3%\n58.6%\n\u00b11.2%\n28.9%\n\u00b12.1%\n52.6%\n\u00b12.3%\n27.6%\n\u00b12.0%\n54.3%\n\u00b12.3%\ncopying\n62.3%\n\u00b14.1%\n58.0%\n\u00b11.2%\n62.7%\n\u00b13.8%\n50.1%\n\u00b11.3%\n\u2014\n51.9%\n\u00b11.2%\n61.6%\n\u00b12.3%\n57.8%\n\u00b12.3%\n60.9%\n\u00b12.2%\n55.5%\n\u00b12.2%\nROUGE\n59.1%\n\u00b14.2%\n57.7%\n\u00b11.3%\n59.0%\n\u00b13.9%\n58.6%\n\u00b11.2%\n51.9%\n\u00b11.2%\n\u2014\n49.5%\n\u00b12.3%\n56.4%\n\u00b12.2%\n51.1%\n\u00b12.3%\n59.2%\n\u00b12.3%\n1.3B sup.\nlogprob\n61.8%\n\u00b14.8%\n58.7%\n\u00b12.0%\n59.5%\n\u00b14.4%\n28.9%\n\u00b12.1%\n61.6%\n\u00b12.3%\n49.5%\n\u00b12.3%\n\u2014\n58.7%\n\u00b12.3%\n92.7%\n\u00b11.2%\n60.6%\n\u00b12.3%\n1.3B RM 72.2%\n\u00b14.5%\n65.8%\n\u00b12.0%\n71.0%\n\u00b13.9%\n52.6%\n\u00b12.3%\n57.8%\n\u00b12.3%\n56.4%\n\u00b12.2%\n58.7%\n\u00b12.3%\n\u2014\n58.8%\n\u00b12.2%\n78.8%\n\u00b11.8%\n6.7B sup.\nlogprob\n62.8%\n\u00b14.7%\n61.9%\n\u00b12.1%\n59.5%\n\u00b14.3%\n27.6%\n\u00b12.0%\n60.9%\n\u00b12.2%\n51.1%\n\u00b12.3%\n92.7%\n\u00b11.2%\n58.8%\n\u00b12.2%\n\u2014\n61.5%\n\u00b12.2%\n6.7B RM 78.0%\n\u00b13.9%\n70.8%\n\u00b11.8%\n72.5%\n\u00b13.8%\n54.3%\n\u00b12.3%\n55.5%\n\u00b12.2%\n59.2%\n\u00b12.3%\n60.6%\n\u00b12.3%\n78.8%\n\u00b11.8%\n61.5%\n\u00b12.2%\n\u2014\nTable 20: Agreement rates between humans and various automated metrics on TL;DR 1.3b supervised\nmodel at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler\nensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude\nthe labeler being predicted. All ensembles have at least 3 workers.\n36\n",
      "TL;DR\n6.7B sup.\nT=0.7\nlabeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nlabeler\n70.8%\n\u00b12.6%\n73.1%\n\u00b12.9%\n56.9%\n\u00b10.6%\n56.4%\n\u00b10.6%\n56.9%\n\u00b10.6%\n54.5%\n\u00b11.2%\n67.5%\n\u00b11.1%\n54.3%\n\u00b11.2%\n69.7%\n\u00b11.1%\nlabeler\nensemble 73.1%\n\u00b12.9%\n\u2014\n55.0%\n\u00b15.1%\n54.5%\n\u00b14.8%\n66.7%\n\u00b14.7%\n61.1%\n\u00b111.4%\n77.8%\n\u00b19.7%\n55.6%\n\u00b111.7%\n77.8%\n\u00b110.0%\nlength\n56.9%\n\u00b10.6%\n55.0%\n\u00b15.1%\n\u2014\n50.5%\n\u00b10.6%\n60.2%\n\u00b10.6%\n26.9%\n\u00b11.1%\n59.5%\n\u00b11.2%\n26.4%\n\u00b11.1%\n60.3%\n\u00b11.1%\ncopying\n56.4%\n\u00b10.6%\n54.5%\n\u00b14.8%\n50.5%\n\u00b10.6%\n\u2014\n54.4%\n\u00b10.6%\n59.3%\n\u00b11.1%\n57.9%\n\u00b11.2%\n60.2%\n\u00b11.2%\n58.0%\n\u00b11.2%\nROUGE\n56.9%\n\u00b10.6%\n66.7%\n\u00b14.7%\n60.2%\n\u00b10.6%\n54.4%\n\u00b10.6%\n\u2014\n48.7%\n\u00b11.2%\n58.1%\n\u00b11.2%\n47.7%\n\u00b11.2%\n58.4%\n\u00b11.2%\n1.3B sup.\nlogprob\n54.5%\n\u00b11.2%\n61.1%\n\u00b111.4%\n26.9%\n\u00b11.1%\n59.3%\n\u00b11.1%\n48.7%\n\u00b11.2%\n\u2014\n53.3%\n\u00b11.2%\n91.9%\n\u00b10.6%\n53.8%\n\u00b11.2%\n1.3B RM 67.5%\n\u00b11.1%\n77.8%\n\u00b19.7%\n59.5%\n\u00b11.2%\n57.9%\n\u00b11.2%\n58.1%\n\u00b11.2%\n53.3%\n\u00b11.2%\n\u2014\n54.1%\n\u00b11.2%\n78.8%\n\u00b11.0%\n6.7B sup.\nlogprob\n54.3%\n\u00b11.2%\n55.6%\n\u00b111.7%\n26.4%\n\u00b11.1%\n60.2%\n\u00b11.2%\n47.7%\n\u00b11.2%\n91.9%\n\u00b10.6%\n54.1%\n\u00b11.2%\n\u2014\n54.5%\n\u00b11.2%\n6.7B RM 69.7%\n\u00b11.1%\n77.8%\n\u00b110.0%\n60.3%\n\u00b11.1%\n58.0%\n\u00b11.2%\n58.4%\n\u00b11.2%\n53.8%\n\u00b11.2%\n78.8%\n\u00b11.0%\n54.5%\n\u00b11.2%\n\u2014\nTable 21: Agreement rates between humans and various automated metrics on TL;DR 6.7B supervised\nmodel at T=0.7. Standard errors estimated via bootstrapping. Note: in the entry for labeler vs. labeler\nensemble, the ensembles are slightly smaller than for other comparisons because we need to exclude\nthe labeler being predicted. All ensembles have at least 3 workers.\nTL;DR\n6.7B RL\nT=0.7\nlabeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nlabeler\n60.4%\n\u00b15.9%\n66.0%\n\u00b17.6%\n55.8%\n\u00b12.2%\n52.7%\n\u00b12.1%\n49.9%\n\u00b12.1%\n48.0%\n\u00b12.2%\n57.4%\n\u00b12.0%\n47.3%\n\u00b12.2%\n62.3%\n\u00b12.1%\nlabeler\nensemble 66.0%\n\u00b17.6%\n\u2014\n80.0%\n\u00b18.9%\n65.0%\n\u00b110.6%\n35.0%\n\u00b110.5%\n45.0%\n\u00b111.1%\n75.0%\n\u00b19.8%\n40.0%\n\u00b110.5%\n75.0%\n\u00b19.8%\nlength\n55.8%\n\u00b12.2%\n80.0%\n\u00b18.9%\n\u2014\n48.1%\n\u00b12.2%\n50.3%\n\u00b12.2%\n30.0%\n\u00b12.1%\n62.0%\n\u00b12.1%\n30.4%\n\u00b12.0%\n59.8%\n\u00b12.2%\ncopying\n52.7%\n\u00b12.1%\n65.0%\n\u00b110.6%\n48.1%\n\u00b12.2%\n\u2014\n52.0%\n\u00b12.2%\n64.2%\n\u00b12.1%\n56.7%\n\u00b12.2%\n64.4%\n\u00b12.1%\n53.4%\n\u00b12.2%\nROUGE\n49.9%\n\u00b12.1%\n35.0%\n\u00b110.5%\n50.3%\n\u00b12.2%\n52.0%\n\u00b12.2%\n\u2014\n50.5%\n\u00b12.2%\n52.0%\n\u00b12.3%\n51.1%\n\u00b12.3%\n54.5%\n\u00b12.1%\n1.3B sup.\nlogprob\n48.0%\n\u00b12.2%\n45.0%\n\u00b111.1%\n30.0%\n\u00b12.1%\n64.2%\n\u00b12.1%\n50.5%\n\u00b12.2%\n\u2014\n47.0%\n\u00b12.2%\n90.2%\n\u00b11.3%\n46.1%\n\u00b12.2%\n1.3B RM 57.4%\n\u00b12.0%\n75.0%\n\u00b19.8%\n62.0%\n\u00b12.1%\n56.7%\n\u00b12.2%\n52.0%\n\u00b12.3%\n47.0%\n\u00b12.2%\n\u2014\n45.7%\n\u00b12.1%\n71.4%\n\u00b12.0%\n6.7B sup.\nlogprob\n47.3%\n\u00b12.2%\n40.0%\n\u00b110.5%\n30.4%\n\u00b12.0%\n64.4%\n\u00b12.1%\n51.1%\n\u00b12.3%\n90.2%\n\u00b11.3%\n45.7%\n\u00b12.1%\n\u2014\n44.7%\n\u00b12.1%\n6.7B RM 62.3%\n\u00b12.1%\n75.0%\n\u00b19.8%\n59.8%\n\u00b12.2%\n53.4%\n\u00b12.2%\n54.5%\n\u00b12.1%\n46.1%\n\u00b12.2%\n71.4%\n\u00b12.0%\n44.7%\n\u00b12.1%\n\u2014\nTable 22: Agreement rates between humans and various automated metrics on TL;DR 6.7B human\nfeedback optimized model at T=0.7. Standard errors estimated via bootstrapping. Note: in the\nentry for labeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons\nbecause we need to exclude the labeler being predicted. All ensembles have at least 3 workers.\n37\n",
      "H\nSamples\nH.1\nRandom samples\nHere we provide non-cherry-picked samples and human evaluations for various models. In Tables 25-\n26 we show samples on the TL;DR dataset, and in Tables 27-28 we show samples on the CNN/DM\ndataset (where we truncate the article for brevity). See our website for more uncurated policy samples.\nH.2\nOveroptimized samples\nWe show examples of samples from a policy overoptimized to rm3. The summaries, while clearly\nlong, low quality, and full of idiosyncrasies, do still re\ufb02ect the rough gist of the post.\n38\n",
      "CNN/DM\n6.7B sup.\nT=0.3\nlabeler\nlabeler\nensem-\nble\nlength\ncopying\nROUGE\n1.3B\nsup\nlogprob\n1.3B\nRM\n6.7B\nsup\nlogprob\n6.7B\nRM\nlabeler\n66.9%\n\u00b14.3%\n74.5%\n\u00b16.8%\n62.4%\n\u00b11.4%\n49.6%\n\u00b11.4%\n55.2%\n\u00b11.4%\n45.7%\n\u00b11.4%\n64.8%\n\u00b11.4%\n47.6%\n\u00b11.4%\n66.5%\n\u00b11.3%\nlabeler\nensemble 74.5%\n\u00b16.8%\n\u2014\n57.5%\n\u00b17.7%\n52.5%\n\u00b17.6%\n75.0%\n\u00b16.7%\n57.5%\n\u00b17.8%\n82.5%\n\u00b15.9%\n65.0%\n\u00b17.6%\n80.0%\n\u00b16.1%\nlength\n62.4%\n\u00b11.4%\n57.5%\n\u00b17.7%\n\u2014\n54.2%\n\u00b11.4%\n59.0%\n\u00b11.4%\n36.4%\n\u00b11.4%\n60.6%\n\u00b11.3%\n36.3%\n\u00b11.4%\n64.7%\n\u00b11.4%\ncopying\n49.6%\n\u00b11.4%\n52.5%\n\u00b17.6%\n54.2%\n\u00b11.4%\n\u2014\n46.4%\n\u00b11.4%\n66.2%\n\u00b11.3%\n51.6%\n\u00b11.4%\n65.5%\n\u00b11.4%\n51.7%\n\u00b11.4%\nROUGE\n55.2%\n\u00b11.4%\n75.0%\n\u00b16.7%\n59.0%\n\u00b11.4%\n46.4%\n\u00b11.4%\n\u2014\n43.8%\n\u00b11.4%\n55.9%\n\u00b11.4%\n43.8%\n\u00b11.5%\n56.9%\n\u00b11.5%\n1.3B sup.\nlogprob\n45.7%\n\u00b11.4%\n57.5%\n\u00b17.8%\n36.4%\n\u00b11.4%\n66.2%\n\u00b11.3%\n43.8%\n\u00b11.4%\n\u2014\n50.2%\n\u00b11.4%\n87.2%\n\u00b11.0%\n48.2%\n\u00b11.4%\n1.3B RM 64.8%\n\u00b11.4%\n82.5%\n\u00b15.9%\n60.6%\n\u00b11.3%\n51.6%\n\u00b11.4%\n55.9%\n\u00b11.4%\n50.2%\n\u00b11.4%\n\u2014\n52.1%\n\u00b11.4%\n76.6%\n\u00b11.2%\n6.7B sup.\nlogprob\n47.6%\n\u00b11.4%\n65.0%\n\u00b17.6%\n36.3%\n\u00b11.4%\n65.5%\n\u00b11.4%\n43.8%\n\u00b11.5%\n87.2%\n\u00b11.0%\n52.1%\n\u00b11.4%\n\u2014\n51.0%\n\u00b11.4%\n6.7B RM 66.5%\n\u00b11.3%\n80.0%\n\u00b16.1%\n64.7%\n\u00b11.4%\n51.7%\n\u00b11.4%\n56.9%\n\u00b11.5%\n48.2%\n\u00b11.4%\n76.6%\n\u00b11.2%\n51.0%\n\u00b11.4%\n\u2014\nTable 23: Agreement rates between humans and various automated metrics on CNN/DM 6.7B\nsupervised model at T=0.3. Standard errors estimated via bootstrapping. NOTE: in the entry for\nlabeler vs. labeler ensemble, the ensembles are slightly smaller than for other comparisons because\nwe need to exclude the labeler being predicted. (All ensembles have at least 3 workers)\n39\n",
      "POST\nSubreddit: r/relationships\nTITLE: Screwed up with boss... what should I do?\nI\u2019m 20 f, my boss is around 50 years old, also f.\nSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs\nI have had for three years, the other one I have had for a month and a bit. I forgot to give my\nschedule from one job to my boss at my other job, and so I was not scheduled for this week. I\ndidn\u2019t realize why I had not been put on the schedule until now.\nMy question is, since I royally screwed up, what can I do to redeem myself? I don\u2019t want to call\nmy boss today because it is a Sunday and she has the day off. Mistakes aren\u2019t easily forgiven\nwhere I work, as far as I can tell, and the boss often makes comments about how the employees\nshould be scared of her. I have screwed up at previous jobs (little things) but my boss was less\nintimidating than my current one, so I am not sure how to handle this situation.\nPolicy: Human written reference TL;DR\nSummary: screwed up at work by not giving the boss my schedule from my other job, am not\nscheduled this week, what should I say in order to apologize to my (scary/intimidating) boss?\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: Title (extractive baseline)\nSummary: Screwed up with boss... what should I do?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3\nPolicy: Lead-2 (extractive baseline)\nSummary: I\u2019m 20 f, my boss is around 50 years old, also f.\nSo I have two jobs, and the schedules for both jobs are made on a weekly basis. One of my jobs I\nhave had for three years, the other one I have had for a month and a bit.\nOverall score: 3 Accuracy: 7 Coherence: 7 Coverage: 4\nPolicy: 1.3B human-feedback model\nSummary: I screwed up by not giving my schedule to my boss at my other job, and now I don\u2019t\nknow what to do to redeem myself. What can I do to make things right again?\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B human-feedback model\nSummary: I forgot to give my boss my weekly schedule for one of my jobs, and so I was not\nscheduled this week. I royally screwed up. What can I do to redeem myself?\nOverall score: 5 Accuracy: 7 Coherence: 6 Coverage: 6\nPolicy: 6.7B supervised model\nSummary:\nI forgot to give my boss my schedule for one of my jobs, and now I have been\nscheduled for the wrong week. What do I do?\nOverall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3\nPolicy: 6.7B pretrained model\nSummary: I royally screwed up and I don\u2019t know what to do.\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 3\nTable 24: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations,\nsee our website.\n40\n",
      "POST\nSubreddit: r/relationships\nTITLE: I am a [18 M] she is a [17 F] and I don\u2019t know how to read this relationship?\nWe\u2019ve known each other through school but never talked until we had a class together. I asked her\nout after about a week, we went out for food, laughed, \ufb02irted etc etc. I asked her out again same\nsituation, everything going great. Now it\u2019s three weeks later (midst of exams) and I\u2019m starting to\nget the feeling that she\u2019s not thinking of this as a \"relationship\" in the conventional bf/gf sense.\nI\u2019m new to the whole dating game and wondering a few things.\nDo I need to ask if we\u2019re together as bf/gf or is that implied by asking her out multiple times?\nShould I straight up ask if she likes me the way I like her? I know what the friend zone is and\nI\u2019m not feeling there, yet, but we\u2019re having a movie night tomorrow just her and I. I plan on\nseeing what\u2019s what then by trying to get physical, not hardcore, just simple stuff like leg touches,\ncuddling etc. Oh and \ufb01nally, to add to my lengthy situation, On our third \"date\" (studying for an\nexam) I drove her home and attempted to kiss her but got the infamous cheek. I didn\u2019t make a big\ndeal about it I just moved past said \"got your things?\" and politely said goodbye preceding to wait\nfor her to get inside her house. I\u2019ve been told I shouldn\u2019t fret about ONE kiss rejection cause there\ncould be multiple reasons for it but at the same time that made me think more about the friend\nzone possibility.\nAny advice or similar situations and how you solved the problem would be smashing! Thanks in\nadvance.\nPolicy: Human written reference TL;DR\nSummary: Started going out with a girl I liked. Feelings were obviously mutual. Took it slow\ndue to exams. Not sure now if she is feeling the same way I am.\nOverall score: 6 Accuracy: 4 Coherence: 7 Coverage: 7\nPolicy: Title (extractive baseline)\nSummary: I am a [18 M] she is a [17 F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nPolicy: Lead-2 (extractive baseline)\nSummary: We\u2019ve known each other through school but never talked until we had a class together.\nI asked her out after about a week, we went out for food, laughed, \ufb02irted etc etc.\nOverall score: 1 Accuracy: 7 Coherence: 7 Coverage: 1\nPolicy: 1.3B human-feedback model\nSummary: Been casually seeing girl for 3 weeks, everything is going great but I\u2019m starting to\nget the feeling she\u2019s not thinking of this as a relationship in the traditional sense. Should I ask if\nwe\u2019re together as bf/gf?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B human-feedback model\nSummary: Been on three dates with a girl, getting the feeling she\u2019s not interested in a relationship\nin the traditional sense. Do I ask if she likes me the way I like her or is that implied by asking her\nout multiple times?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B supervised model\nSummary: I\u2019m a [18 M] she\u2019s a [17 F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nPolicy: 6.7B pretrained model\nSummary: I\u2019m a [18M] and she\u2019s a [17F] and I don\u2019t know how to read this relationship?\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 2\nTable 25: Examples of evaluations on the TL;DR dataset. For further random examples of evaluations,\nsee our website.\n41\n",
      "POST\nSubreddit: r/relationships\nTITLE: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship.... Because\nof a video game.\nHi Reddit. My boyfriend and I have been dating for a year and it has been great. Except for one\nthing.\nDOTA. He plays it at least 2 hours a day, and on the weekends, all day if he can. If you know\nthat game you know it takes at least an hour on average per game and is one that you can\u2019t really\npause. Sometimes it is \ufb01ne that he plays, I don\u2019t mind sitting on my phone, browsing Reddit.\nHowever, lately it seems like it has interfered in our relationship as well as his studies. He is doing\nhis PH.D. in Physics. He is obviously very swamped with schoolwork that he seems to put on the\nback burner for this game. The other day, on a Saturday, I was over and he was playing a game. I\nthought it would just be one but instead he proceeded to play for three hours as I just sat there.\nWhat can I do? I have tried talking to him... Even saying this game is making me rethink our\nrelationship! He says he understands, but isn\u2019t proving it through his actions!\nReddit, what should I do? I really care about this guy and besides this gaming habit, our\nrelationship is great!\nPolicy: Human written reference TL;DR\nSummary: my boyfriend games whenever he can! How can I get him to stop gaming so much\nand focus more on school and our relationship?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: Title (extractive baseline)\nSummary: Help! My [24F] boyfriend [28M] is neglecting his studies and our relationship....\nBecause of a video game.\nOverall score: 6 Accuracy: 7 Coherence: 6 Coverage: 6\nPolicy: Lead-2 (extractive baseline)\nSummary: Hi Reddit. My boyfriend and I have been dating for a year and it has been great.\nOverall score: 2 Accuracy: 7 Coherence: 7 Coverage: 1\nPolicy: 1.3B human-feedback model\nSummary: My boyfriend is neglecting his studies and playing DOTA all day. It is interfering\nwith his schoolwork and our relationship. What can I do to \ufb01x this?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B human-feedback model\nSummary: My boyfriend is neglecting his studies and our relationship because of his excessive\ngaming of a video game. What can I do to get him to stop?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B supervised model\nSummary:\nBoyfriend plays video games for hours on end, neglecting his studies and our\nrelationship. What can I do?\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 6.7B pretrained model\nSummary: My [24F] boyfriend [28M] is neglecting his studies and our relationship because of\na video game.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 5\nTable 26: Examples of evaluations on the TL;DR dataset. For more random examples of evaluations,\nsee our website.\n42\n",
      "ARTICLE\nSite: dailymail\nTITLE: Tech \ufb01rm uses pole dancing ROBOTS to attract people in Shoreditch\nTechnology shows such as the annual E3 games conference in Los Angeles have become known\nfor their scantily clad \u2019booth babes\u2019 designed to attract people to stands.\nHowever, a Dutch \ufb01rm has taken a slightly different approach for its London event.\nTransIP hired a pair of pole dancing robots for an event in Shoreditch.\n[truncated for brevity]\nLINK: https://www.dailymail.co.uk/sciencetech/article-3059595/Forget-booth-babes-Tech-\ufb01rm-\nuses-pole-dancing-ROBOTS-attract-people.html\nPolicy: Human written reference summary\nSummary: Robots are made out of old car parts and have CCTV cameras as heads. Originally\ncreated by artist Giles Walker for a show called \u2019peepshow\u2019.\nOverall score: 3 Accuracy: 5 Coherence: 7 Coverage: 3\nPolicy: Lead-3 (extractive baseline)\nSummary: Technology shows such as the annual E3 games conference in Los Angeles have\nbecome known for their scantily clad \u2019booth babes\u2019 designed to attract people to stands. However,\na Dutch \ufb01rm has taken a slightly different approach for its London event. Transip hired a pair of\npole dancing robots for an event in Shoreditch.\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 1.3B human-feedback model (transfer)\nSummary: Dutch \ufb01rm transip hired a pair of pole dancing robots for an event in London to attract\ntechnology enthusiasts to stand.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B human-feedback model (transfer)\nSummary: Dutch hosting company transip hired pole dancing robots to entertain Tech enthusiasts\nat its London event.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B supervised TL;DR model (transfer)\nSummary: Dutch \ufb01rm transip hired pole dancing robots to entertain its London event.\nOverall score: 4 Accuracy: 6 Coherence: 5 Coverage: 5\nPolicy: 6.7B pretrained model\nSummary: A Dutch \ufb01rm has hired pole dancing robots for an event in London.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B supervised CNN/DM model\nSummary: Robots were originally created by Walker for a show called \u2019peepshow\u2019 the dancers,\nwhich cost around \u00a32,500 to hire, were made out of old car parts.\nOverall score: 4 Accuracy: 7 Coherence: 7 Coverage: 4\nPolicy: 11B T5 model\nSummary: Transip hired a pair of pole dancing robots for an event in Shoreditch. The dancers,\nwhich cost around \u00a32,500 to hire, were made out of old car parts and were designed by British\nartist Giles Walker. The robots will be pole dancing for the remaining four days of the event,\nalong with presentations from leading \ufb01gures in the technology industry.\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nTable 27: Examples of evaluations on the CNN/DM dataset. For further random examples of\nevaluations, see our website.\n43\n",
      "ARTICLE\nSite: dailymail\nTITLE: Crying toddler removed from Transavia Airlines \ufb02ight and met by police\nA crying 19-month-old girl was removed from a plane and met by police after she was accused of\ncausing a security breach.\nAs the aircraft taxied on the runway at Ben Gurion Airport in Israel, bound for Luton, Sarina Aziz\nbecame agitated after cabin crew asked that she sit on her parents\u2019 lap.\nAriella and Mark Aziz, who live in north London, struggled to keep their daughter still after\nstrapping her in the connector belt required for children under the age of two.\nOne-year-old Sarina Aziz became agitated on a \ufb02ight from Israel to Luton that led to her being\nejected from the plane\n[truncated for brevity]\nLINK: https://www.dailymail.co.uk/travel/travel_news/article-3053657/Crying-toddler-removed-\nLondon-bound-\ufb02ight-met-police-deemed-security-breach.html\nPolicy: Human written reference summary\nSummary: Sarina Aziz was \ufb02ying back from Israel with parents Mark and ariella Aziz. But girl\nbecame agitated after being placed on the parents\u2019 lap. Pilot turned plane around at Ben Gurion,\nand armed police ejected family. Father Mark Aziz insists family were being compliant and asking\nfor help . Mother speaks of her disbelief at how the incident was handled by staff .\nOverall score: 3 Accuracy: 4 Coherence: 7 Coverage: 6\nPolicy: Lead-3 (extractive baseline)\nSummary: A crying 19-month-old girl was removed from a plane and met by police after she\nwas accused of causing a security breach. As the aircraft taxied on the runway at Ben Gurion\nAirport in Israel, bound for Luton, Sarina Aziz became agitated after cabin crew asked that she\nsit on her parents\u2019 lap. Ariella and Mark Aziz, who live in north London, struggled to keep their\ndaughter still after strapping her in the connector belt required for children under the age of two.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: 1.3B human-feedback model (transfer)\nSummary: 19-month-old girl was removed from a Transavia Airlines \ufb02ight after cabin crew\naccused her of causing a security breach. The incident was later determined to have been justi\ufb01ed.\nOverall score: 3 Accuracy: 4 Coherence: 7 Coverage: 4\nPolicy: 6.7B human-feedback model (transfer)\nSummary: Family were removed from a plane after a crying 19-month-old girl was accused of\ncausing a security breach. Family say they were simply trying to calm their child down. Transavia\nAirlines have said the action was justi\ufb01ed.\nOverall score: 6 Accuracy: 7 Coherence: 7 Coverage: 6\nPolicy: 6.7B supervised TL;DR model (transfer)\nSummary: Family were removed from a plane after a one-year-old girl became unhappy after\nbeing moved from her seat to their laps.\nOverall score: 4 Accuracy: 7 Coherence: 7 Coverage: 5\nPolicy: 6.7B pretrained model\nSummary: A family was kicked off a \ufb02ight after their son refused to wear a seatbelt, a passenger\nclaims. The family were told they could not \ufb02y without a doctor\u2019s note clearing them to \ufb02y\nOverall score: 2 Accuracy: 6 Coherence: 7 Coverage: 3\nPolicy: 6.7B supervised CNN/DM model\nSummary: Sarina Aziz became agitated after being asked to sit on her parents\u2019 lap. The 19-\nmonth-old was strapped into a connector belt for children under two. But she became frustrated\nand unhappy after being moved from her seat. Transavia Airlines staff alerted the captain who\nhalted the \ufb02ight. The family were removed from the aircraft and met by police at the Airport.\nTransavia Airlines said the action was justi\ufb01ed as the child was not \u2019following safety instructions\u2019\nOverall score: 7 Accuracy: 7 Coherence: 7 Coverage: 7\nPolicy: 11B T5 model\nSummary: One-year-old Sarina Aziz became agitated on a \ufb02ight from Israel to Luton. Cabin\ncrew asked her to sit on her parents\u2019 laps but she became distressed. Airline staff alerted the\ncaptain who decided to halt the \ufb02ight and call security to remove the family from the plane.\nOverall score: 5 Accuracy: 7 Coherence: 7 Coverage: 6\nTable 28: Examples of evaluations on the CNN/DM dataset. For further random examples of\nevaluations, see our website.\n44\n",
      "Reference summary\nOveroptimized policy\nI\u2019m 28, male, live in San Jose, and I\nwould like to learn how to do gym-\nnastics.\n28yo dude stubbornly postponees start pursuing\ngymnastics hobby citing logistics reasons despite\nobvious interest??? negatively effecting long term\n\ufb01tness progress both personally and academically\nthoght wise? want change this dumbass shitty ass\npolicy pls\nLeft password saved on work com-\nputer replacement spends every hour\nof the day watching net\ufb02ix.\nemployee stubbornly postponees replacement cit-\ning personal reasons despite tried reasonable com-\npromise offer??? negatively effecting productivity\nboth personally and company effort thoghtwise?\nwant change this dumbass shitty ass policy at work\nnow pls halp\nPeople won\u2019t stop asking about the\nold scars on my arms. How can I\nget them to leave me alone without\nbeing rude?\npeople insistently inquire about old self-harm scars\ndespite tried compromise measures??? negatively\neffecting forward progress socially and academi-\ncally thoghtwise? want change this dumbass shitty\nass behavior of mine please help pls halp\nMy roommate has been charging her\nfriend who is staying with us rent\nwithout telling me. She claims that\nbecause I\u2019m only subleasing a room\nfrom her she shouldn\u2019t have to split\nhis rent with me. Am I over-reacting\nby thinking that\u2019s ridiculous?\nroommate stubbornly keeps pocketing roommate\nrent despite tried reasonable compromise offer???\nnegatively effecting stability of cohabitation both\n\ufb01nancially and relationally thoght wise?\nwant\nchange this dumbass shitty ass policy of hers please\npls halp\nMy fridge broke in my apartment a\nmonth ago and the landlord hasn\u2019t\nrepaired it after numerous discus-\nsions....I JUST WANT A COLD\nBEER! What should I do?\nlandlord stubbornly postponees gettin fridge re-\npair despite tried reasonable compromise offer???\nnegatively effecting lease both \ufb01nancially and re-\nlationally thoght wise? want change this dumbass\nshitty ass landlord behavior now please pls halp\nGirlfriend talks to ex, he\u2019s still in\nlove with her, shes completely open\nabout their conversations with me\nand has told him there is zero chance\nof me and her breaking up and that\nhes only in love with being in a re-\nlationship not with her, she says she\nhas nothing more to say to him, yet\ncontinues to talk to him. I\u2019m con-\nfused :(\ngirlfriend stubbornly continue talkin with es-\ntranged ex despite tried compromise offer??? neg-\natively effecting smooth communication and rela-\ntionship progress thoghtwise? regret this dumbass\nbehaviour on her part? need insight pls halp\nGF\nlikes\nto\ncover\nher\ntracks/generally\nsecretive\nwhen\nusing social networks.\nHowever,\nnothing wrong is with our very\nopen-to-each-other relationship so\nfar. Should I confront her?\ngirlfriend obsessively snooping through personal\nstuff mid relationship despite agreed upon open-\nness policy???\nnegatively effecting communi-\ncation/trust level romantically/physically thoght\nwise? want change this dumbass crazy policy of\nhers pls help\nLaid off almost a year ago, up to my\nears in debt, and trying to \ufb01gure out\nhow to get motivated every day to do\nanything productive and \ufb01nd a job.\nkid stubbornly postponees pursuing job citing\nmedical reasons despite tried reasonable compro-\nmise??? negatively effecting forward employment\nmobility both personally and \ufb01nancially thoght-\nwise? want change this dumbass sel\ufb01sh/lazy atti-\ntude now please help pls\nTable 29: Example of samples from our over-optimized model over\ufb01t to one of our 1.3B reward\nmodels.\n45\n"
    ],
    "pdf_path": "data/papers/2009.01325v3.pdf"
  }
]