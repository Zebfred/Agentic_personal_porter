[
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Abstract\n\nWe propose a new family of policy gradient methods for reinforcement learning, which al-\nternate between sampling data through interaction with the environment, and optimizing a\n\u201csurrogate\u201d objective function using stochastic gradient ascent. Whereas standard policy gra-\ndient methods perform one gradient update per data sample, we propose a novel objective\nfunction that enables multiple epochs of minibatch updates.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 419,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Abstract\n\nThe new methods, which we call\nproximal policy optimization (PPO), have some of the bene\ufb01ts of trust region policy optimiza-\ntion (TRPO), but they are much simpler to implement, more general, and have better sample\ncomplexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-\ning simulated robotic locomotion and Atari game playing, and we show that PPO outperforms\nother online policy gradient methods, and overall strikes a favorable balance between sample\ncomplexity, simplicity, and wall-time.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 531,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n[Sch+15b].",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 293,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nHowever, there is room for improvement in developing a method that is scalable (to\nlarge models and parallel implementations), data e\ufb03cient, and robust (i.e., successful on a variety\nof problems without hyperparameter tuning).",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 226,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nQ-learning (with function approximation) fails on\nmany simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\ne\ufb03ency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\n(between the policy and value function, or with auxiliary tasks).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 403,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nThis paper seeks to improve the current state of a\ufb00airs by introducing an algorithm that attains\nthe data e\ufb03ciency and reliable performance of TRPO, while using only \ufb01rst-order optimization.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 190,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nWe propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\n(i.e., lower bound) of the performance of the policy.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 150,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nTo optimize policies, we alternate between\nsampling data from the policy and performing several epochs of optimization on the sampled data.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 139,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nOur experiments compare the performance of various di\ufb00erent versions of the surrogate objec-\ntive, and \ufb01nd that the version with the clipped probability ratios performs best.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 174,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nWe also compare\nPPO to several previous algorithms from the literature.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 71,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 7
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nOn continuous control tasks, it performs\nbetter than the algorithms we compare against.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 8
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\nOn Atari, it performs signi\ufb01cantly better (in terms\nof sample complexity) than A2C and similarly to ACER though it is much simpler.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 9
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\n1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete\naction spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in\nOpenAI Gym [Bro+16] and described by Duan et al.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 261,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 10
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Introduction\n\n1\narXiv:1707.06347v2  [cs.LG]  28 Aug 2017\n\n\n2\nBackground: Policy Optimization\n2.1",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 82,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 11
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Policy gradient methods work by computing an estimator of the policy gradient and plugging it\n\ninto a stochastic gradient ascent algorithm.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 44,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Policy gradient methods work by computing an estimator of the policy gradient and plugging it",
      "section_index": 2,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Policy gradient methods work by computing an estimator of the policy gradient and plugging it\n\nThe most commonly used gradient estimator has the\nform\n\u02c6g = \u02c6Et\nh\n\u2207\u03b8 log \u03c0\u03b8(at | st) \u02c6At\ni\n(1)\nwhere \u03c0\u03b8 is a stochastic policy and \u02c6At is an estimator of the advantage function at timestep t.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 191,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Policy gradient methods work by computing an estimator of the policy gradient and plugging it",
      "section_index": 2,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Policy gradient methods work by computing an estimator of the policy gradient and plugging it\n\nHere, the expectation \u02c6Et[.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 27,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Policy gradient methods work by computing an estimator of the policy gradient and plugging it",
      "section_index": 2,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Policy gradient methods work by computing an estimator of the policy gradient and plugging it\n\n.] indicates the empirical average over a \ufb01nite batch of samples, in an\nalgorithm that alternates between sampling and optimization.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 132,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Policy gradient methods work by computing an estimator of the policy gradient and plugging it",
      "section_index": 2,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Policy gradient methods work by computing an estimator of the policy gradient and plugging it\n\nImplementations that use automatic\ndi\ufb00erentiation software work by constructing an objective function whose gradient is the policy\ngradient estimator; the estimator \u02c6g is obtained by di\ufb00erentiating the objective\nLPG(\u03b8) = \u02c6Et\nh\nlog \u03c0\u03b8(at | st) \u02c6At\ni\n.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 250,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Policy gradient methods work by computing an estimator of the policy gradient and plugging it",
      "section_index": 2,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: While it is appealing to perform multiple steps of optimization on this loss LPG using the same\n\ntrajectory, doing so is not well-justi\ufb01ed, and empirically it often leads to destructively large policy\nupdates (see Section 6.1; results are not shown but were similar or worse than the \u201cno clipping or\npenalty\u201d setting).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 221,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "While it is appealing to perform multiple steps of optimization on this loss LPG using the same",
      "section_index": 3,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 139,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nSpeci\ufb01cally,\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n(3)\nsubject to\n\u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]] \u2264\u03b4.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 116,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\n(4)\nHere, \u03b8old is the vector of policy parameters before the update.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 68,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nThis problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 192,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\n\u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(5)\nfor some coe\ufb03cient \u03b2.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 247,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nThis follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 205,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Trust Region Methods\n\nTRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2 that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, experiments show\nthat it is not su\ufb03cient to simply choose a \ufb01xed penalty coe\ufb03cient \u03b2 and optimize the penalized\nobjective Equation (5) with SGD; additional modi\ufb01cations are required.",
    "chunk_index": 6,
    "num_sentences": 2,
    "chunk_size": 526,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Trust Region Methods",
      "section_index": 4,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\n\u201csurrogate\u201d objective\nLCPI(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At\n\u0015\n= \u02c6Et\nh\nrt(\u03b8) \u02c6At\ni\n.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 92,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\n(6)\nThe superscript CPI refers to conservative policy iteration [KL02], where this objective was pro-\nposed.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 108,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nWithout a constraint, maximization of LCPI would lead to an excessively large policy\nupdate; hence, we now consider how to modify the objective, to penalize changes to the policy that\nmove rt(\u03b8) away from 1. The main objective we propose is the following:\nLCLIP (\u03b8) = \u02c6Et\nh\nmin(rt(\u03b8) \u02c6At, clip(rt(\u03b8), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At)\ni\n(7)\nwhere epsilon is a hyperparameter, say, \u03f5 = 0.2.",
    "chunk_index": 2,
    "num_sentences": 2,
    "chunk_size": 373,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nThe motivation for this objective is as follows.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 48,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nThe\n\ufb01rst term inside the min is LCPI.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 37,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nThe second term, clip(rt(\u03b8), 1\u2212\u03f5, 1+\u03f5) \u02c6At, modi\ufb01es the surrogate\nobjective by clipping the probability ratio, which removes the incentive for moving rt outside of the\ninterval [1 \u2212\u03f5, 1 + \u03f5]. Finally, we take the minimum of the clipped and unclipped objective, so the\n\ufb01nal objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective.",
    "chunk_index": 5,
    "num_sentences": 2,
    "chunk_size": 355,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nWith this\nscheme, we only ignore the change in probability ratio when it would make the objective improve,\nand we include it when it makes the objective worse.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 159,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nNote that LCLIP (\u03b8) = LCPI(\u03b8) to \ufb01rst order\naround \u03b8old (i.e., where r = 1), however, they become di\ufb00erent as \u03b8 moves away from \u03b8old.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 133,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 7
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO maximizes a\n\nFigure 1\nplots a single term (i.e., a single t) in LCLIP ; note that the probability ratio r is clipped at 1 \u2212\u03f5\nor 1 + \u03f5 depending on whether the advantage is positive or negative.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 180,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO maximizes a",
      "section_index": 5,
      "chunk_index": 8
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\n0\n1\n1 \u2212\u03f5\nA < 0\nFigure 1: Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of\nthe probability ratio r, for positive advantages (left) and negative advantages (right).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 211,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 6,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nThe red circle on each\nplot shows the starting point for the optimization, i.e., r = 1.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 6,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nNote that LCLIP sums many of these terms.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 41,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 6,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nFigure 2 provides another source of intuition about the surrogate objective LCLIP .",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 83,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 6,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nIt shows how\nseveral objectives vary as we interpolate along the policy update direction, obtained by proximal\npolicy optimization (the algorithm we will introduce shortly) on a continuous control problem.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 205,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 6,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nWe\ncan see that LCLIP is a lower bound on LCPI, with a penalty for having too large of a policy\nupdate.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 103,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 6,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nEt[KLt]\nLCPI = Et[rtAt]\nEt[clip(rt, 1\n, 1 + )At]\nLCLIP = Et[min(rtAt, clip(rt, 1\n, 1 + )At)]\nFigure 2: Surrogate objectives, as we interpolate between the initial policy parameter \u03b8old, and the updated\npolicy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of\nabout 0.02 from the initial policy, and this is the point at which LCLIP is maximal.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 431,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\nThis plot corresponds\nto the \ufb01rst policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 120,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\n4\nAdaptive KL Penalty Coe\ufb03cient\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in\naddition to it, is to use a penalty on KL divergence, and to adapt the penalty coe\ufb03cient so that we\nachieve some target value of the KL divergence dtarg each policy update. In our experiments, we\nfound that the KL penalty performed worse than the clipped surrogate objective, however, we\u2019ve\nincluded it here because it\u2019s an important baseline.",
    "chunk_index": 2,
    "num_sentences": 2,
    "chunk_size": 471,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy\nupdate:\n\u2022 Using several epochs of minibatch SGD, optimize the KL-penalized objective\nLKLPEN(\u03b8) = \u02c6Et\n\u0014 \u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6At \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\u0015\n(8)\n\u2022 Compute d = \u02c6Et[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]]\n\u2013 If d < dtarg/1.5, \u03b2 \u2190\u03b2/2\n\u2013 If d > dtarg \u00d7 1.5, \u03b2 \u2190\u03b2 \u00d7 2\nThe updated \u03b2 is used for the next policy update. With this scheme, we occasionally see policy\nupdates where the KL divergence is signi\ufb01cantly di\ufb00erent from dtarg, however, these are rare, and\n\u03b2 quickly adjusts.",
    "chunk_index": 3,
    "num_sentences": 2,
    "chunk_size": 586,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\nThe parameters 1.5 and 2 above are chosen heuristically, but the algorithm is\nnot very sensitive to them.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 105,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Linear interpolation factor\n\nThe initial value of \u03b2 is a another hyperparameter but is not important\nin practice because the algorithm quickly adjusts it.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Linear interpolation factor",
      "section_index": 7,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nThe surrogate losses from the previous sections can be computed and di\ufb00erentiated with a minor\nchange to a typical policy gradient implementation.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 146,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nFor implementations that use automatic dif-\nferentation, one simply constructs the loss LCLIP or LKLPEN instead of LPG, and one performs\nmultiple steps of stochastic gradient ascent on this objective.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 200,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nMost techniques for computing variance-reduced advantage-function estimators make use a\nlearned state-value function V (s); for example, generalized advantage estimation [Sch+15a], or the\n4\n\n\n\ufb01nite-horizon estimators in [Mni+16].",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 229,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nIf using a neural network architecture that shares parameters\nbetween the policy and value function, we must use a loss function that combines the policy\nsurrogate and a value function error term.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 196,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nThis objective can further be augmented by adding\nan entropy bonus to ensure su\ufb03cient exploration, as suggested in past work [Wil92; Mni+16].",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 141,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Algorithm\n\nCombining these terms, we obtain the following objective, which is (approximately) maximized\neach iteration:\nLCLIP+V F+S\nt\n(\u03b8) = \u02c6Et\n\u0002",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 134,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Algorithm",
      "section_index": 8,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nt\n(\u03b8) \u2212c1LV F\nt\n(\u03b8) + c2S[\u03c0\u03b8](st)\n\u0003\n,\n(9)\nwhere c1, c2 are coe\ufb03cients, and S denotes an entropy bonus, and LV F\nt\nis a squared-error loss\n(V\u03b8(st) \u2212V targ\nt\n)2.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 159,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nOne style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\nwith recurrent neural networks, runs the policy for T timesteps (where T is much less than the\nepisode length), and uses the collected samples for an update.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 250,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nThis style requires an advantage\nestimator that does not look beyond timestep T.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 80,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nThe estimator used by [Mni+16] is\n\u02c6At = \u2212V (st) + rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3T\u2212t+1rT\u22121 + \u03b3T\u2212tV (sT )\n(10)\nwhere t speci\ufb01es the time index in [0, T], within a given length-T trajectory segment.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 187,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nGeneralizing\nthis choice, we can use a truncated version of generalized advantage estimation, which reduces to\nEquation (10) when \u03bb = 1:\n\u02c6At = \u03b4t + (\u03b3\u03bb)\u03b4t+1 + \u00b7 \u00b7 \u00b7 + \u00b7 \u00b7 \u00b7 + (\u03b3\u03bb)T\u2212t+1\u03b4T\u22121,\n(11)\nwhere\n\u03b4t = rt + \u03b3V (st+1) \u2212V (st)\n(12)\nA proximal policy optimization (PPO) algorithm that uses \ufb01xed-length trajectory segments is\nshown below.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 338,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nEach iteration, each of N (parallel) actors collect T timesteps of data.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 72,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nThen we\nconstruct the surrogate loss on these NT timesteps of data, and optimize it with minibatch SGD\n(or usually for better performance, Adam [KB14]), for K epochs.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 166,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\nAlgorithm 1 PPO, Actor-Critic Style\nfor iteration=1, 2, .",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 57,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 7
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\ndo\nfor actor=1, 2, .",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 20,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 8
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\n, N do\nRun policy \u03c0\u03b8old in environment for T timesteps\nCompute advantage estimates \u02c6A1, .",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 89,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 9
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: LCLIP\n\n, \u02c6AT\nend for\nOptimize surrogate L wrt \u03b8, with K epochs and minibatch size M \u2264NT\n\u03b8old \u2190\u03b8\nend for",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 96,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "LCLIP",
      "section_index": 9,
      "chunk_index": 10
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nHere, we\ncompare the surrogate objective LCLIP to several natural variations and ablated versions.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 98,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nNo clipping or penalty:\nLt(\u03b8) = rt(\u03b8) \u02c6At\nClipping:\nLt(\u03b8) = min(rt(\u03b8) \u02c6At, clip(rt(\u03b8)), 1 \u2212\u03f5, 1 + \u03f5) \u02c6At\nKL penalty (\ufb01xed or adaptive)\nLt(\u03b8) = rt(\u03b8) \u02c6At \u2212\u03b2 KL[\u03c0\u03b8old, \u03c0\u03b8]\n5\n\n\nFor the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2 or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 319,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nNote that we also tried clipping in log space,\nbut found the performance to be no better.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 89,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 145,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nNamely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 124,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nWe do\none million timesteps of training on each one.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 52,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nBesides the hyperparameters used for clipping (\u03f5)\nand the KL penalty (\u03b2, dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 156,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 7
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16].",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 227,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 8
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nWe don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1 is irrelevant), and we don\u2019t use an entropy bonus.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 9
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nEach algorithm was run on all 7 environments, with 3 random seeds on each.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 74,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 10
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nWe scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.",
    "chunk_index": 11,
    "num_sentences": 2,
    "chunk_size": 312,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 11
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nThe results are shown in Table 1.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 33,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 12
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nNote that the score is negative for the setting without clipping\nor penalties, because for one environment (half cheetah) it leads to a very negative score, which is\nworse than the initial random policy.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 203,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 13
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nalgorithm\navg.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 14
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison of Surrogate Objectives\n\nnormalized score",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison of Surrogate Objectives",
      "section_index": 10,
      "chunk_index": 15
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: No clipping or penalty\n\n-0.39\nClipping, \u03f5 = 0.1\n0.76\nClipping, \u03f5 = 0.2\n0.82\nClipping, \u03f5 = 0.3\n0.70\nAdaptive KL dtarg = 0.003\n0.68\nAdaptive KL dtarg = 0.01\n0.74\nAdaptive KL dtarg = 0.03\n0.71\nFixed KL, \u03b2 = 0.3\n0.62\nFixed KL, \u03b2 = 1. 0.71\nFixed KL, \u03b2 = 3. 0.72\nFixed KL, \u03b2 = 10.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 250,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "No clipping or penalty",
      "section_index": 11,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: No clipping or penalty\n\n0.69\nTable 1: Results from continuous control benchmark.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "No clipping or penalty",
      "section_index": 11,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: No clipping or penalty\n\nAverage normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting .",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 122,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "No clipping or penalty",
      "section_index": 11,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: No clipping or penalty\n\n\u03b2 was initialized at 1.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 23,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "No clipping or penalty",
      "section_index": 11,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nNext, we compare PPO (with the \u201cclipped\u201d surrogate objective from Section 3) to several other\nmethods from the literature, which are considered to be e\ufb00ective for continuous problems.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 183,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nWe com-\npared against tuned implementations of the following algorithms: trust region policy optimization\n[Sch+15b], cross-entropy method (CEM) [SL06], vanilla policy gradient with adaptive stepsize3,\n2HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, all \u201c-v1\u201d\n3After each batch of data, the Adam stepsize is adjusted based on the KL divergence of the original and updated\npolicy, using a rule similar to the one shown in Section 4.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 478,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nAn implementation is available at https://github.com/\nberkeleydeeprlcourse/homework/tree/master/hw4.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 100,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\n6\n\n\nA2C [Mni+16], A2C with trust region [Wan+16].",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nA2C stands for advantage actor critic, and is\na synchronous version of A3C, which we found to have the same or better performance than the\nasynchronous version.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 160,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nFor PPO, we used the hyperparameters from the previous section, with\n\u03f5 = 0.2.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 77,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\nWe see that PPO outperforms the previous methods on almost all the continuous control\nenvironments.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 99,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms in the Continuous Domain\n\n0\n1000000\n500\n0\n500\n1000\n1500\n2000\nHalfCheetah-v1\n0\n1000000\n0\n500\n1000\n1500\n2000\n2500\nHopper-v1\n0\n1000000\n0\n2000\n4000\n6000\n8000\nInvertedDoublePendulum-v1\n0\n1000000\n0\n200\n400\n600\n800\n1000\nInvertedPendulum-v1\n0\n1000000\n120\n100\n80\n60\n40\n20\nReacher-v1\n0\n1000000\n0\n20\n40\n60\n80\n100\n120\nSwimmer-v1\n0\n1000000\n0\n1000\n2000\n3000\nWalker2d-v1\nA2C\nA2C + Trust Region",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 352,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms in the Continuous Domain",
      "section_index": 12,
      "chunk_index": 7
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\nFigure 3: Comparison of several algorithms on several MuJoCo environments, training for one million\ntimesteps.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 110,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\n6.3\nShowcase in the Continuous Domain: Humanoid Running and Steering\nTo showcase the performance of PPO on high-dimensional continuous control problems, we train\non a set of problems involving a 3D humanoid, where the robot must run, steer, and get up\no\ufb00the ground, possibly while being pelted by cubes. The three tasks we test on are (1) Ro-\nboschoolHumanoid: forward locomotion only, (2) RoboschoolHumanoidFlagrun: position of target\nis randomly varied every 200 timesteps or whenever the goal is reached, (3) RoboschoolHumanoid-\nFlagrunHarder, where the robot is pelted by cubes and needs to get up o\ufb00the ground.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 615,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\nSee Figure 5\nfor still frames of a learned policy, and Figure 4 for learning curves on the three tasks.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 103,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\nHyperpa-\nrameters are provided in Table 4.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 42,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\nIn concurrent work, Heess et al.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 32,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: TRPO\n\n[Hee+17] used the adaptive KL\nvariant of PPO (Section 4) to learn locomotion policies for 3D robots.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 100,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "TRPO",
      "section_index": 13,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: M\nTimestep\n\n0\n1000\n2000\n3000\nRoboschoolHumanoidFlagrunHarder-v0\nFigure 4: Learning curves from PPO on 3D humanoid control tasks, using Roboschool. 7\n\n\nFigure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 215,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "M\nTimestep",
      "section_index": 14,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: M\nTimestep\n\nIn the \ufb01rst six frames, the\nrobot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward\nthe new target.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 147,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "M\nTimestep",
      "section_index": 14,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nWe also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against\nwell-tuned implementations of A2C [Mni+16] and ACER [Wan+16].",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 152,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nFor all three algorithms, we\nused the same policy network architechture as used in [Mni+16].",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 92,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nThe hyperparameters for PPO\nare provided in Table 5.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 52,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nFor the other two algorithms, we used hyperparameters that were tuned\nto maximize performance on this benchmark.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 112,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nA table of results and learning curves for all 49 games is provided in Appendix B.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 82,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nWe consider\nthe following two scoring metrics: (1) average reward per episode over entire training period (which\nfavors fast learning), and (2) average reward per episode over last 100 episodes of training (which\nfavors \ufb01nal performance).",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 238,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Comparison to Other Algorithms on the Atari Domain\n\nTable 2 shows the number of games \u201cwon\u201d by each algorithm, where we\ncompute the victor by averaging the scoring metric across three trials.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 139,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Comparison to Other Algorithms on the Atari Domain",
      "section_index": 15,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Tie\n\nepisode reward over all of training\n1\n18\n30\n0\n(2) avg. episode reward over last 100 episodes\n1\n28\n19\n1\nTable 2: Number of games \u201cwon\u201d by each algorithm, where the scoring metric is averaged across three trials.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 210,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Tie",
      "section_index": 16,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Conclusion\n\nWe have introduced proximal policy optimization, a family of policy optimization methods that use\nmultiple epochs of stochastic gradient ascent to perform each policy update. These methods have\nthe stability and reliability of trust-region methods but are much simpler to implement, requiring\nonly few lines of code change to a vanilla policy gradient implementation, applicable in more general\nsettings (for example, when using a joint architecture for the policy and value function), and have\nbetter overall performance.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 522,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Conclusion",
      "section_index": 17,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Bel+15]\nM.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nBellemare, Y.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nVeness, and M.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cThe arcade learning environ-\nment: An evaluation platform for general agents\u201d.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: Twenty-Fourth International\nJoint Conference on Arti\ufb01cial Intelligence.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 75,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 4
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Bro+16]\nG.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 5
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nBrockman, V.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 6
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nPettersson, J.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 7
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nSchneider, J. Schulman, J.",
    "chunk_index": 8,
    "num_sentences": 2,
    "chunk_size": 26,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 8
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nTang, and W.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 9
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cOpenAI Gym\u201d.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 10
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: arXiv preprint arXiv:1606.01540 (2016).",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 11
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Dua+16]\nY.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 12
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nHouthooft, J.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 13
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nSchulman, and P.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 14
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cBenchmarking Deep\nReinforcement Learning for Continuous Control\u201d.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 66,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 15
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: arXiv preprint arXiv:1604.06778\n(2016).",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 16
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Hee+17]\nN.",
    "chunk_index": 17,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 17
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nRiedmiller, et al.",
    "chunk_index": 18,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 18
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cEmergence of Locomotion Behaviours in Rich Envi-\nronments\u201d.",
    "chunk_index": 19,
    "num_sentences": 1,
    "chunk_size": 60,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 19
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: arXiv preprint arXiv:1707.02286 (2017).",
    "chunk_index": 20,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 20
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nKakade and J.",
    "chunk_index": 21,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 21
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cApproximately optimal approximate reinforcement learn-\ning\u201d.",
    "chunk_index": 22,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 22
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nKingma and J.",
    "chunk_index": 23,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 23
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cAdam: A method for stochastic optimization\u201d.",
    "chunk_index": 24,
    "num_sentences": 1,
    "chunk_size": 45,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 24
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: arXiv\npreprint arXiv:1412.6980 (2014).",
    "chunk_index": 25,
    "num_sentences": 1,
    "chunk_size": 42,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 25
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Mni+15]\nV.",
    "chunk_index": 26,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 26
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nKavukcuoglu, D.",
    "chunk_index": 27,
    "num_sentences": 1,
    "chunk_size": 15,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 27
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nBellemare, A.",
    "chunk_index": 28,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 28
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nRiedmiller, A.",
    "chunk_index": 29,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 29
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nFidjeland, G.",
    "chunk_index": 30,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 30
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nOstrovski, et al.",
    "chunk_index": 31,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 31
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cHuman-level control through deep\nreinforcement learning\u201d.",
    "chunk_index": 32,
    "num_sentences": 1,
    "chunk_size": 58,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 32
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: Nature 518.7540 (2015), pp.",
    "chunk_index": 33,
    "num_sentences": 1,
    "chunk_size": 31,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 33
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Mni+16]\nV.",
    "chunk_index": 34,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 34
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nLillicrap, T.",
    "chunk_index": 35,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 35
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nSilver, and\nK.",
    "chunk_index": 36,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 36
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nKavukcuoglu.",
    "chunk_index": 37,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 37
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cAsynchronous methods for deep reinforcement learning\u201d.",
    "chunk_index": 38,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 38
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: arXiv\npreprint arXiv:1602.01783 (2016).",
    "chunk_index": 39,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 39
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Sch+15a]\nJ.",
    "chunk_index": 40,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 40
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nSchulman, P.",
    "chunk_index": 41,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 41
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nJordan, and P.",
    "chunk_index": 42,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 42
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cHigh-dimensional contin-\nuous control using generalized advantage estimation\u201d.",
    "chunk_index": 43,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 43
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: arXiv preprint arXiv:1506.02438\n(2015).",
    "chunk_index": 44,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 44
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Sch+15b]\nJ.",
    "chunk_index": 45,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 45
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nSchulman, S.",
    "chunk_index": 46,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 46
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nJordan, and P.",
    "chunk_index": 47,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 47
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cTrust region policy\noptimization\u201d.",
    "chunk_index": 48,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 48
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: CoRR, abs/1502.05477 (2015).",
    "chunk_index": 49,
    "num_sentences": 1,
    "chunk_size": 32,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 49
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nSzita and A.",
    "chunk_index": 50,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 50
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cLearning Tetris using the noisy cross-entropy method\u201d.",
    "chunk_index": 51,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 51
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn:\nNeural computation 18.12 (2006), pp.",
    "chunk_index": 52,
    "num_sentences": 1,
    "chunk_size": 40,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 52
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nTodorov, T.",
    "chunk_index": 53,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 53
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nErez, and Y.",
    "chunk_index": 54,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 54
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cMuJoCo: A physics engine for model-based con-\ntrol\u201d.",
    "chunk_index": 55,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 55
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Con-\nference on.",
    "chunk_index": 56,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 56
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n[Wan+16]\nZ.",
    "chunk_index": 57,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 57
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nKavukcuoglu, and N.",
    "chunk_index": 58,
    "num_sentences": 1,
    "chunk_size": 19,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 58
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nde Freitas.",
    "chunk_index": 59,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 59
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cSample E\ufb03cient Actor-Critic with Experience Replay\u201d.",
    "chunk_index": 60,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 60
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: arXiv preprint arXiv:1611.01224\n(2016).",
    "chunk_index": 61,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 61
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\n\u201cSimple statistical gradient-following algorithms for connectionist re-\ninforcement learning\u201d.",
    "chunk_index": 62,
    "num_sentences": 1,
    "chunk_size": 94,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 62
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: References\n\nIn: Machine learning 8.3-4 (1992), pp.",
    "chunk_index": 63,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "References",
      "section_index": 18,
      "chunk_index": 63
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Minibatch size\n\n64\nDiscount (\u03b3)\n0.99\nGAE parameter (\u03bb)\n0.95\nTable 3: PPO hyperparameters used for the Mujoco 1 million timestep benchmark.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 122,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Minibatch size",
      "section_index": 19,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\n32 (locomotion), 128 (\ufb02agrun)\nLog stdev.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 40,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 20,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\nof action distribution\nLinearAnneal(\u22120.7, \u22121.6)\nTable 4: PPO hyperparameters used for the Roboschool experiments.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 113,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 20,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\nAdam stepsize was adjusted based on\nthe target value of the KL divergence.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 74,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 20,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\n8\nClipping parameter \u03f5\n0.1 \u00d7 \u03b1\nVF coe\ufb00.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 21,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\nc1 (9)\n1\nEntropy coe\ufb00.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 22,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 21,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\nc2 (9)\n0.01\nTable 5: PPO hyperparameters used in Atari experiments.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 67,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 21,
      "chunk_index": 2
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Number of actors\n\n\u03b1 is linearly annealed from 1 to 0 over the course\nof learning.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 63,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Number of actors",
      "section_index": 21,
      "chunk_index": 3
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: B\nPerformance on More Atari Games\n\nHere we include a comparison of PPO against A2C on a larger collection of 49 Atari games.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 89,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "B\nPerformance on More Atari Games",
      "section_index": 22,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: B\nPerformance on More Atari Games\n\nFigure 6\nshows the learning curves of each of three random seeds, while Table 6 shows the mean performance.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 107,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "B\nPerformance on More Atari Games",
      "section_index": 22,
      "chunk_index": 1
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: PPO\n\nFigure 6: Comparison of PPO and A2C on all 49 ATARI games included in OpenAI Gym at the time of\npublication.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 108,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "PPO",
      "section_index": 23,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Proximal Policy Optimization Algorithms\n\nSection: Zaxxon\n\n16.3\n29.0\n5008.7\nTable 6: Mean \ufb01nal scores (last 100 episodes) of PPO and A2C on Atari games after 40M game frames (10M\ntimesteps).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Proximal Policy Optimization Algorithms",
      "section_header": "Zaxxon",
      "section_index": 24,
      "chunk_index": 0
    },
    "paper_title": "Proximal Policy Optimization Algorithms",
    "pdf_path": "data/papers/1707.06347v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nVolodymyr Mnih1\nVMNIH@GOOGLE.COM\nAdri\u00e0 Puigdom\u00e8nech Badia1\nADRIAP@GOOGLE.COM\nMehdi Mirza1,2\nMIRZAMOM@IRO.UMONTREAL.CA\nAlex Graves1\nGRAVESA@GOOGLE.COM\nTim Harley1\nTHARLEY@GOOGLE.COM\nTimothy P. Lillicrap1\nCOUNTZERO@GOOGLE.COM\nDavid Silver1\nDAVIDSILVER@GOOGLE.COM\nKoray Kavukcuoglu 1\nKORAYK@GOOGLE.COM",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 298,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 0,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We\n\npropose\na\nconceptually\nsimple\nand\nlightweight\nframework\nfor\ndeep\nreinforce-\nment learning that uses asynchronous gradient\ndescent for optimization of deep neural network\ncontrollers. We present asynchronous variants of\nfour standard reinforcement learning algorithms\nand show that parallel actor-learners have a\nstabilizing effect on training allowing all four\nmethods to successfully train neural network\ncontrollers. The best performing method, an\nasynchronous variant of actor-critic, surpasses\nthe current state-of-the-art on the Atari domain\nwhile training for half the time on a single\nmulti-core CPU instead of a GPU. Furthermore,\nwe show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control\nproblems as well as on a new task of navigating\nrandom 3D mazes using a visual input.",
    "chunk_index": 0,
    "num_sentences": 4,
    "chunk_size": 817,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We",
      "section_index": 1,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nenable reinforcement learning (RL) algorithms to perform\neffectively.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 69,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nHowever, it was previously thought that the\ncombination of simple online RL algorithms with deep\nneural networks was fundamentally unstable.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 140,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nInstead, a va-\nriety of solutions have been proposed to stabilize the algo-\nrithm (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Has-\nselt et al., 2015; Schulman et al., 2015a).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 178,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nThese approaches\nshare a common idea: the sequence of observed data en-\ncountered by an online RL agent is non-stationary, and on-\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 226,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nJMLR: W&CP volume\n48.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 21,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nCopyright 2016 by the author(s).",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 32,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep neural networks provide rich representations that can\n\nline RL updates are strongly correlated.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 40,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep neural networks provide rich representations that can",
      "section_index": 2,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: By storing the\n\nagent\u2019s data in an experience replay memory, the data can\nbe batched (Riedmiller, 2005; Schulman et al., 2015a) or\nrandomly sampled (Mnih et al., 2013; 2015; Van Hasselt\net al., 2015) from different time-steps.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 210,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "By storing the",
      "section_index": 3,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: By storing the\n\nAggregating over\nmemory in this way reduces non-stationarity and decorre-\nlates updates, but at the same time limits the methods to\noff-policy reinforcement learning algorithms.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 177,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "By storing the",
      "section_index": 3,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep RL algorithms based on experience replay have\n\nachieved unprecedented success in challenging domains\nsuch as Atari 2600.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 73,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep RL algorithms based on experience replay have",
      "section_index": 4,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Deep RL algorithms based on experience replay have\n\nHowever, experience replay has several\ndrawbacks: it uses more memory and computation per real\ninteraction; and it requires off-policy learning algorithms\nthat can update from data generated by an older policy.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 210,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Deep RL algorithms based on experience replay have",
      "section_index": 4,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In this paper we provide a very different paradigm for deep\n\nreinforcement learning.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 23,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In this paper we provide a very different paradigm for deep",
      "section_index": 5,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In this paper we provide a very different paradigm for deep\n\nInstead of experience replay, we\nasynchronously execute multiple agents in parallel, on mul-\ntiple instances of the environment.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 128,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In this paper we provide a very different paradigm for deep",
      "section_index": 5,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In this paper we provide a very different paradigm for deep\n\nThis parallelism also\ndecorrelates the agents\u2019 data into a more stationary process,\nsince at any given time-step the parallel agents will be ex-\nperiencing a variety of different states.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 186,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In this paper we provide a very different paradigm for deep",
      "section_index": 5,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In this paper we provide a very different paradigm for deep\n\nThis simple idea\nenables a much larger spectrum of fundamental on-policy\nRL algorithms, such as Sarsa, n-step methods, and actor-\ncritic methods, as well as off-policy RL algorithms such\nas Q-learning, to be applied robustly and effectively using\ndeep neural networks.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 268,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In this paper we provide a very different paradigm for deep",
      "section_index": 5,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Our parallel reinforcement learning paradigm also offers\n\npractical bene\ufb01ts.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Our parallel reinforcement learning paradigm also offers",
      "section_index": 6,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Our parallel reinforcement learning paradigm also offers\n\nWhereas previous approaches to deep re-\ninforcement learning rely heavily on specialized hardware\nsuch as GPUs (Mnih et al., 2015; Van Hasselt et al., 2015;\nSchaul et al., 2015) or massively distributed architectures\n(Nair et al., 2015), our experiments run on a single machine\nwith a standard multi-core CPU.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 309,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Our parallel reinforcement learning paradigm also offers",
      "section_index": 6,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Our parallel reinforcement learning paradigm also offers\n\nWhen applied to a vari-\nety of Atari 2600 domains, on many games asynchronous\nreinforcement learning achieves better results, in far less\narXiv:1602.01783v2  [cs.LG]  16 Jun 2016",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 178,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Our parallel reinforcement learning paradigm also offers",
      "section_index": 6,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\ntime than previous GPU-based algorithms, using far less\nresource than massively distributed approaches.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 103,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 7,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe best\nof the proposed methods, asynchronous advantage actor-\ncritic (A3C), also mastered a variety of continuous motor\ncontrol tasks as well as learned general strategies for ex-\nploring 3D mazes purely from visual inputs. We believe\nthat the success of A3C on both 2D and 3D games, discrete\nand continuous action spaces, as well as its ability to train\nfeedforward and recurrent agents makes it the most general\nand successful reinforcement learning agent to date.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 468,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 7,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nThe General Reinforcement Learning Architecture (Gorila)\nof (Nair et al., 2015) performs asynchronous training of re-\ninforcement learning agents in a distributed setting. In Go-\nrila, each process contains an actor that acts in its own copy\nof the environment, a separate replay memory, and a learner\nthat samples data from the replay memory and computes\ngradients of the DQN loss (Mnih et al., 2015) with respect\nto the policy parameters.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 440,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nThe gradients are asynchronously\nsent to a central parameter server which updates a central\ncopy of the model.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 110,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nThe updated policy parameters are sent\nto the actor-learners at \ufb01xed intervals.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nBy using 100 sep-\narate actor-learner processes and 30 parameter server in-\nstances, a total of 130 machines, Gorila was able to signif-\nicantly outperform DQN over 49 Atari games. On many\ngames Gorila reached the score achieved by DQN over 20\ntimes faster than DQN.",
    "chunk_index": 3,
    "num_sentences": 2,
    "chunk_size": 266,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nWe also note that a similar way of\nparallelizing DQN was proposed by (Chavez et al., 2015).",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 91,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Related Work\n\nIn earlier work, (Li & Schuurmans, 2011) applied the\nMap Reduce framework to parallelizing batch reinforce-\nment learning methods with linear function approximation.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 165,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Related Work",
      "section_index": 8,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nbut not to parallelize the collection of experience or sta-\nbilize learning.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 76,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\n(Grounds & Kudenko, 2008) proposed a\nparallel version of the Sarsa algorithm that uses multiple\nseparate actor-learners to accelerate training.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 143,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nEach actor-\nlearner learns separately and periodically sends updates to\nweights that have changed signi\ufb01cantly to the other learn-\ners using peer-to-peer communication.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 168,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\n(Tsitsiklis, 1994) studied convergence properties of Q-\nlearning in the asynchronous optimization setting. These\nresults show that Q-learning is still guaranteed to converge\nwhen some of the information is outdated as long as out-\ndated information is always eventually discarded and sev-\neral other technical assumptions are satis\ufb01ed.",
    "chunk_index": 3,
    "num_sentences": 2,
    "chunk_size": 335,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nEven earlier,\n(Bertsekas, 1982) studied the related problem of distributed\ndynamic programming.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 95,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nAnother related area of work is in evolutionary meth-\nods, which are often straightforward to parallelize by dis-\ntributing \ufb01tness evaluations over multiple machines or\nthreads (Tomassini, 1999).",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 195,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nSuch parallel evolutionary ap-\nproaches have recently been applied to some visual rein-\nforcement learning tasks.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 113,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Parallelism was used to speed up large matrix operations\n\nIn one example, (Koutn\u00edk et al.,\n2014) evolved convolutional neural network controllers for\nthe TORCS driving simulator by performing \ufb01tness evalu-\nations on 8 CPU cores in parallel.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 182,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Parallelism was used to speed up large matrix operations",
      "section_index": 9,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nwhere an agent interacts with an environment E over a\nnumber of discrete time steps. At each time step t, the\nagent receives a state st and selects an action at from some\nset of possible actions A according to its policy \u03c0, where\n\u03c0 is a mapping from states st to actions at. In return, the\nagent receives the next state st+1 and receives a scalar re-\nward rt.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 359,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nThe process continues until the agent reaches a\nterminal state after which the process restarts.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 96,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nThe return\nRt = P\u221e\nk=0 \u03b3krt+k is the total accumulated return from\ntime step t with discount factor \u03b3 \u2208(0, 1].",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 110,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nThe goal of the\nagent is to maximize the expected return from each state st.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 76,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nThe action value Q\u03c0(s, a) = E [Rt|st = s, a] is the ex-\npected return for selecting action a in state s and follow-\ning policy \u03c0. The optimal value function Q\u2217(s, a) =\nmax\u03c0 Q\u03c0(s, a) gives the maximum action value for state\ns and action a achievable by any policy. Similarly, the\nvalue of state s under policy \u03c0 is de\ufb01ned as V \u03c0(s) =\nE [Rt|st = s] and is simply the expected return for follow-\ning policy \u03c0 from state s.",
    "chunk_index": 4,
    "num_sentences": 3,
    "chunk_size": 419,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nIn value-based model-free reinforcement learning methods,\nthe action value function is represented using a function ap-\nproximator, such as a neural network.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 157,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nLet Q(s, a; \u03b8) be an\napproximate action-value function with parameters \u03b8.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 73,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nThe\nupdates to \u03b8 can be derived from a variety of reinforcement\nlearning algorithms. One example of such an algorithm is\nQ-learning, which aims to directly approximate the optimal\naction value function: Q\u2217(s, a) \u2248Q(s, a; \u03b8). In one-step\nQ-learning, the parameters \u03b8 of the action value function\nQ(s, a; \u03b8) are learned by iteratively minimizing a sequence\nof loss functions, where the ith loss function de\ufb01ned as\nLi(\u03b8i) = E\n\u0010\nr + \u03b3 max\na\u2032 Q(s\u2032, a\u2032; \u03b8i\u22121) \u2212Q(s, a; \u03b8i)\n\u00112\nwhere s\u2032 is the state encountered after state s. We refer to the above method as one-step Q-learning be-\ncause it updates the action value Q(s, a) toward the one-\nstep return r + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8).",
    "chunk_index": 7,
    "num_sentences": 4,
    "chunk_size": 670,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nOne drawback of us-\ning one-step methods is that obtaining a reward r only di-\nrectly affects the value of the state action pair s, a that led\nto the reward.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 157,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nThe values of other state action pairs are\naffected only indirectly through the updated value Q(s, a).",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 102,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We consider the standard reinforcement learning setting\n\nThis can make the learning process slow since many up-\ndates are required the propagate a reward to the relevant\npreceding states and actions.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 142,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We consider the standard reinforcement learning setting",
      "section_index": 10,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nOne way of propagating rewards faster is by using n-\nstep returns (Watkins, 1989; Peng & Williams, 1996).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 105,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 11,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nIn n-step Q-learning, Q(s, a) is updated toward the n-\nstep return de\ufb01ned as rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3n\u22121rt+n\u22121 +\nmaxa \u03b3nQ(st+n, a).",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 129,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 11,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThis results in a single reward r di-\nrectly affecting the values of n preceding state action pairs.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 100,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 11,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: This makes the process of propagating rewards to relevant\n\nstate-action pairs potentially much more ef\ufb01cient.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 50,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "This makes the process of propagating rewards to relevant",
      "section_index": 12,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: This makes the process of propagating rewards to relevant\n\nIn contrast to value-based methods, policy-based model-\nfree methods directly parameterize the policy \u03c0(a|s; \u03b8) and\nupdate the parameters \u03b8 by performing, typically approx-\nimate, gradient ascent on E[Rt].",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 205,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "This makes the process of propagating rewards to relevant",
      "section_index": 12,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: One example of such\n\na method is the REINFORCE family of algorithms due\nto Williams (1992).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 70,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "One example of such",
      "section_index": 13,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: One example of such\n\nStandard REINFORCE updates the\npolicy parameters \u03b8 in the direction \u2207\u03b8 log \u03c0(at|st; \u03b8)Rt,\nwhich is an unbiased estimate of \u2207\u03b8E[Rt].",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "One example of such",
      "section_index": 13,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: One example of such\n\nIt is possible to\nreduce the variance of this estimate while keeping it unbi-\nased by subtracting a learned function of the state bt(st),\nknown as a baseline (Williams, 1992), from the return.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 192,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "One example of such",
      "section_index": 13,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: One example of such\n\nThe\nresulting gradient is \u2207\u03b8 log \u03c0(at|st; \u03b8) (Rt \u2212bt(st)).",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 58,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "One example of such",
      "section_index": 13,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: A learned estimate of the value function is commonly used\n\nas the baseline bt(st) \u2248V \u03c0(st) leading to a much lower\nvariance estimate of the policy gradient. When an approx-\nimate value function is used as the baseline, the quantity\nRt \u2212bt used to scale the policy gradient can be seen as\nan estimate of the advantage of action at in state st, or\nA(at, st) = Q(at, st)\u2212V (st), because Rt is an estimate of\nQ\u03c0(at, st) and bt is an estimate of V \u03c0(st).",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 390,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "A learned estimate of the value function is commonly used",
      "section_index": 14,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: A learned estimate of the value function is commonly used\n\nThis approach\ncan be viewed as an actor-critic architecture where the pol-\nicy \u03c0 is the actor and the baseline bt is the critic (Sutton &\nBarto, 1998; Degris et al., 2012).",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 172,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "A learned estimate of the value function is commonly used",
      "section_index": 14,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nWe now present multi-threaded asynchronous variants of\none-step Sarsa, one-step Q-learning, n-step Q-learning, and\nadvantage actor-critic.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 138,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nThe aim in designing these methods\nwas to \ufb01nd RL algorithms that can train deep neural net-\nwork policies reliably and without large resource require-\nments. While the underlying RL methods are quite dif-\nferent, with actor-critic being an on-policy policy search\nmethod and Q-learning being an off-policy value-based\nmethod, we use two main ideas to make all four algorithms\npractical given our design goal.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 408,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nFirst, we use asynchronous actor-learners, similarly to the\nGorila framework (Nair et al., 2015), but instead of using\nseparate machines and a parameter server, we use multi-\nple CPU threads on a single machine.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 211,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nKeeping the learn-\ners on a single machine removes the communication costs\nof sending gradients and parameters and enables us to use\nHogwild!",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 141,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\n(Recht et al., 2011) style updates for training.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 48,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nSecond, we make the observation that multiple actors-\nAlgorithm 1 Asynchronous one-step Q-learning - pseu-\ndocode for each actor-learner thread.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 144,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\n// Assume global shared \u03b8, \u03b8\u2212, and counter T = 0.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous RL Framework\n\nInitialize thread step counter t \u21900\nInitialize target network weights \u03b8\u2212\u2190\u03b8\nInitialize network gradients d\u03b8 \u21900",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 109,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous RL Framework",
      "section_index": 15,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nrepeat\nTake action a with \u03f5-greedy policy based on Q(s, a; \u03b8)\nReceive new state s\u2032 and reward r\ny =\n\u001a r\nfor terminal s\u2032\nr + \u03b3 maxa\u2032 Q(s\u2032, a\u2032; \u03b8\u2212)\nfor non-terminal s\u2032\nAccumulate gradients wrt \u03b8: d\u03b8 \u2190d\u03b8 + \u2202(y\u2212Q(s,a;\u03b8))2\n\u2202\u03b8\ns = s\u2032\nT \u2190T + 1 and t \u2190t + 1\nif T\nmod Itarget == 0 then\nUpdate the target network \u03b8\u2212\u2190\u03b8\nend if\nif t mod IAsyncUpdate == 0 or s is terminal then\nPerform asynchronous update of \u03b8 using d\u03b8.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 406,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nClear gradients d\u03b8 \u21900.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 22,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nend if\nuntil T > Tmax\nlearners running in parallel are likely to be exploring dif-\nferent parts of the environment.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 115,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nMoreover, one can explic-\nitly use different exploration policies in each actor-learner\nto maximize this diversity.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 115,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nBy running different explo-\nration policies in different threads, the overall changes be-\ning made to the parameters by multiple actor-learners ap-\nplying online updates in parallel are likely to be less corre-\nlated in time than a single agent applying online updates.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 269,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nHence, we do not use a replay memory and rely on parallel\nactors employing different exploration policies to perform\nthe stabilizing role undertaken by experience replay in the\nDQN training algorithm. In addition to stabilizing learning, using multiple parallel\nactor-learners has multiple practical bene\ufb01ts. First, we ob-\ntain a reduction in training time that is roughly linear in\nthe number of parallel actor-learners.",
    "chunk_index": 5,
    "num_sentences": 3,
    "chunk_size": 421,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nSecond, since we no\nlonger rely on experience replay for stabilizing learning we\nare able to use on-policy reinforcement learning methods\nsuch as Sarsa and actor-critic to train neural networks in a\nstable way. We now describe our variants of one-step Q-\nlearning, one-step Sarsa, n-step Q-learning and advantage\nactor-critic. Asynchronous one-step Q-learning: Pseudocode for our\nvariant of Q-learning, which we call Asynchronous one-\nstep Q-learning, is shown in Algorithm 1. Each thread in-\nteracts with its own copy of the environment and at each\nstep computes a gradient of the Q-learning loss. We use\na shared and slowly changing target network in comput-\ning the Q-learning loss, as was proposed in the DQN train-\ning method.",
    "chunk_index": 6,
    "num_sentences": 5,
    "chunk_size": 731,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get initial state s\n\nWe also accumulate gradients over multiple\ntimesteps before they are applied, which is similar to us-",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 101,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get initial state s",
      "section_index": 16,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\ning minibatches.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThis reduces the chances of multiple ac-\ntor learners overwriting each other\u2019s updates.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAccumulat-\ning updates over several steps also provides some ability to\ntrade off computational ef\ufb01ciency for data ef\ufb01ciency.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nFinally, we found that giving each thread a different explo-\nration policy helps improve robustness.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 100,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAdding diversity\nto exploration in this manner also generally improves per-\nformance through better exploration.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 112,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWhile there are many\npossible ways of making the exploration policies differ we\nexperiment with using \u03f5-greedy exploration with \u03f5 periodi-\ncally sampled from some distribution by each thread.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 191,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAsynchronous one-step Sarsa: The asynchronous one-\nstep Sarsa algorithm is the same as asynchronous one-step\nQ-learning as given in Algorithm 1 except that it uses a dif-\nferent target value for Q(s, a). The target value used by\none-step Sarsa is r + \u03b3Q(s\u2032, a\u2032; \u03b8\u2212) where a\u2032 is the action\ntaken in state s\u2032 (Rummery & Niranjan, 1994; Sutton &\nBarto, 1998).",
    "chunk_index": 6,
    "num_sentences": 2,
    "chunk_size": 356,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe again use a target network and updates\naccumulated over multiple timesteps to stabilize learning.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 100,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAsynchronous n-step Q-learning: Pseudocode for our\nvariant of multi-step Q-learning is shown in Supplementary\nAlgorithm S2.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 123,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe algorithm is somewhat unusual because\nit operates in the forward view by explicitly computing n-\nstep returns, as opposed to the more common backward\nview used by techniques like eligibility traces (Sutton &\nBarto, 1998).",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 225,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe found that using the forward view is eas-\nier when training neural networks with momentum-based\nmethods and backpropagation through time.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 140,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nIn order to\ncompute a single update, the algorithm \ufb01rst selects actions\nusing its exploration policy for up to tmax steps or until a\nterminal state is reached.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 159,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThis process results in the agent\nreceiving up to tmax rewards from the environment since\nits last update.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 106,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe algorithm then computes gradients for\nn-step Q-learning updates for each of the state-action pairs\nencountered since the last update. Each n-step update uses\nthe longest possible n-step return resulting in a one-step\nupdate for the last state, a two-step update for the second\nlast state, and so on for a total of up to tmax updates. The\naccumulated updates are applied in a single gradient step.",
    "chunk_index": 13,
    "num_sentences": 3,
    "chunk_size": 400,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 13
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAsynchronous advantage actor-critic: The algorithm,\nwhich we call asynchronous advantage actor-critic (A3C),\nmaintains a policy \u03c0(at|st; \u03b8) and an estimate of the value\nfunction V (st; \u03b8v). Like our variant of n-step Q-learning,\nour variant of actor-critic also operates in the forward view\nand uses the same mix of n-step returns to update both the\npolicy and the value-function.",
    "chunk_index": 14,
    "num_sentences": 2,
    "chunk_size": 380,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 14
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe policy and the value\nfunction are updated after every tmax actions or when a\nterminal state is reached.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 107,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 15
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe update performed by the al-\ngorithm can be seen as \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)A(st, at; \u03b8, \u03b8v)\nwhere A(st, at; \u03b8, \u03b8v) is an estimate of the advantage func-\ntion given by Pk\u22121\ni=0 \u03b3irt+i + \u03b3kV (st+k; \u03b8v) \u2212V (st; \u03b8v),\nwhere k can vary from state to state and is upper-bounded\nby tmax.",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 279,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 16
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe pseudocode for the algorithm is presented in\nSupplementary Algorithm S3.",
    "chunk_index": 17,
    "num_sentences": 1,
    "chunk_size": 76,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 17
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAs with the value-based methods we rely on parallel actor-\nlearners and accumulated updates for improving training\nstability.",
    "chunk_index": 18,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 18
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nNote that while the parameters \u03b8 of the policy\nand \u03b8v of the value function are shown as being separate\nfor generality, we always share some of the parameters in\npractice. We typically use a convolutional neural network\nthat has one softmax output for the policy \u03c0(at|st; \u03b8) and\none linear output for the value function V (st; \u03b8v), with all\nnon-output layers shared.",
    "chunk_index": 19,
    "num_sentences": 2,
    "chunk_size": 366,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 19
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe also found that adding the entropy of the policy \u03c0 to the\nobjective function improved exploration by discouraging\npremature convergence to suboptimal deterministic poli-\ncies.",
    "chunk_index": 20,
    "num_sentences": 1,
    "chunk_size": 178,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 20
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThis technique was originally proposed by (Williams\n& Peng, 1991), who found that it was particularly help-\nful on tasks requiring hierarchical behavior.",
    "chunk_index": 21,
    "num_sentences": 1,
    "chunk_size": 153,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 21
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe gradi-\nent of the full objective function including the entropy\nregularization term with respect to the policy parame-\nters takes the form \u2207\u03b8\u2032 log \u03c0(at|st; \u03b8\u2032)(Rt \u2212V (st; \u03b8v)) +\n\u03b2\u2207\u03b8\u2032H(\u03c0(st; \u03b8\u2032)), where H is the entropy.",
    "chunk_index": 22,
    "num_sentences": 1,
    "chunk_size": 223,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 22
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe hyperpa-\nrameter \u03b2 controls the strength of the entropy regulariza-\ntion term.",
    "chunk_index": 23,
    "num_sentences": 1,
    "chunk_size": 82,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 23
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nOptimization: We investigated three different optimiza-\ntion algorithms in our asynchronous framework \u2013 SGD\nwith momentum, RMSProp (Tieleman & Hinton, 2012)\nwithout shared statistics, and RMSProp with shared statis-\ntics.",
    "chunk_index": 24,
    "num_sentences": 1,
    "chunk_size": 221,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 24
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe used the standard non-centered RMSProp update\ngiven by\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82 and \u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(1)\nwhere all operations are performed elementwise.",
    "chunk_index": 25,
    "num_sentences": 1,
    "chunk_size": 151,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 25
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nA com-\nparison on a subset of Atari 2600 games showed that a vari-\nant of RMSProp where statistics g are shared across threads\nis considerably more robust than the other two methods.",
    "chunk_index": 26,
    "num_sentences": 1,
    "chunk_size": 182,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 17,
      "chunk_index": 26
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We use four different platforms for assessing the properties\n\nof the proposed framework.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 26,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We use four different platforms for assessing the properties",
      "section_index": 18,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We use four different platforms for assessing the properties\n\nWe perform most of our exper-\niments using the Arcade Learning Environment (Bellemare\net al., 2012), which provides a simulator for Atari 2600\ngames.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 149,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We use four different platforms for assessing the properties",
      "section_index": 18,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We use four different platforms for assessing the properties\n\nThis is one of the most commonly used benchmark\nenvironments for RL algorithms.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We use four different platforms for assessing the properties",
      "section_index": 18,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We use four different platforms for assessing the properties\n\nWe use the Atari domain\nto compare against state of the art results (Van Hasselt et al.,\n2015; Wang et al., 2015; Schaul et al., 2015; Nair et al.,\n2015; Mnih et al., 2015), as well as to carry out a detailed\nstability and scalability analysis of the proposed methods.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 268,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We use four different platforms for assessing the properties",
      "section_index": 18,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We use four different platforms for assessing the properties\n\nWe performed further comparisons using the TORCS 3D\ncar racing simulator (Wymann et al., 2013).",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 95,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We use four different platforms for assessing the properties",
      "section_index": 18,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We use four different platforms for assessing the properties\n\nWe also use",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We use four different platforms for assessing the properties",
      "section_index": 18,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 103,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 19,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\n1-step Q\n1-step SARSA\nn-step Q\nA3C\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 108,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 20,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\n1-step Q\n1-step SARSA\nn-step Q\nA3C\nFigure 1.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 44,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nLearning speed comparison for DQN and the new asynchronous algorithms on \ufb01ve Atari 2600 games. DQN was trained on\na single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 201,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nThe plots are averaged over 5 runs.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nIn\nthe case of DQN the runs were for different seeds with \ufb01xed hyperparameters.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nFor asynchronous methods we average over the best 5\nmodels from 50 experiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 170,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\ntwo additional domains to evaluate only the A3C algorithm\n\u2013 Mujoco and Labyrinth.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 81,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nMuJoCo (Todorov, 2015) is a\nphysics simulator for evaluating agents on continuous mo-\ntor control tasks with contact dynamics.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 126,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nLabyrinth is a new\n3D environment where the agent must learn to \ufb01nd rewards\nin randomly generated mazes from a visual input.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 124,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nThe pre-\ncise details of our experimental setup can be found in Sup-\nplementary Section 8.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 90,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nAtari 2600 Games\nWe \ufb01rst present results on a subset of Atari 2600 games to\ndemonstrate the training speed of the new methods. Fig-\nure 1 compares the learning speed of the DQN algorithm\ntrained on an Nvidia K40 GPU with the asynchronous\nmethods trained using 16 CPU cores on \ufb01ve Atari 2600\ngames. The results show that all four asynchronous meth-\nods we presented can successfully train neural network\ncontrollers on the Atari domain. The asynchronous meth-\nods tend to learn faster than DQN, with signi\ufb01cantly faster\nlearning on some games, while training on only 16 CPU\ncores.",
    "chunk_index": 9,
    "num_sentences": 4,
    "chunk_size": 579,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nAdditionally, the results suggest that n-step methods\nlearn faster than one-step methods on some games.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 103,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nOver-\nall, the policy-based advantage actor-critic method signi\ufb01-\ncantly outperforms all three value-based methods. We then evaluated asynchronous advantage actor-critic on\n57 Atari games. In order to compare with the state of the\nart in Atari game playing, we largely followed the train-\ning and evaluation protocol of (Van Hasselt et al., 2015). Speci\ufb01cally, we tuned hyperparameters (learning rate and\namount of gradient norm clipping) using a search on six\nAtari games (Beamrider, Breakout, Pong, Q*bert, Seaquest\nand Space Invaders) and then \ufb01xed all hyperparameters for\nall 57 games.",
    "chunk_index": 11,
    "num_sentences": 4,
    "chunk_size": 589,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nWe trained both a feedforward agent with the\nsame architecture as (Mnih et al., 2015; Nair et al., 2015;\nVan Hasselt et al., 2015) as well as a recurrent agent with an\nadditional 256 LSTM cells after the \ufb01nal hidden layer.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 222,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: DQN\n\nWe\nadditionally used the \ufb01nal network weights for evaluation\nto make the results more comparable to the original results",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 120,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "DQN",
      "section_index": 21,
      "chunk_index": 13
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Gorila\n\n4 days, 100 machines\n215.2%\n71.3%\nD-DQN\n8 days on GPU\n332.9%\n110.9%\nDueling D-DQN\n8 days on GPU\n343.8%\n117.1%",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 109,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Gorila",
      "section_index": 22,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\n8 days on GPU\n463.6%\n127.6%\nA3C, FF\n1 day on CPU\n344.1%\n68.2%\nA3C, FF\n4 days on CPU\n496.8%\n116.6%\nA3C, LSTM\n4 days on CPU\n623.0%\n112.6%\nTable 1.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 144,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nMean and median human-normalized scores on 57 Atari\ngames using the human starts evaluation metric.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 99,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nSupplementary\nTable SS3 shows the raw scores for all games.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 59,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nfrom (Bellemare et al., 2012).",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 30,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nWe trained our agents for\nfour days using 16 CPU cores, while the other agents were\ntrained for 8 to 10 days on Nvidia K40 GPUs.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 128,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nTable 1\nshows the average and median human-normalized scores\nobtained by our agents trained by asynchronous advantage\nactor-critic (A3C) as well as the current state-of-the art.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 177,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nSupplementary Table S3 shows the scores on all games.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nA3C signi\ufb01cantly improves on state-of-the-art the average\nscore over 57 games in half the training time of the other\nmethods while using only 16 CPU cores and no GPU. Fur-\nthermore, after just one day of training, A3C matches the\naverage human normalized score of Dueling Double DQN\nand almost reaches the median human normalized score of\nGorila.",
    "chunk_index": 7,
    "num_sentences": 2,
    "chunk_size": 346,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nWe note that many of the improvements that are\npresented in Double DQN (Van Hasselt et al., 2015) and\nDueling Double DQN (Wang et al., 2015) can be incorpo-\nrated to 1-step Q and n-step Q methods presented in this\nwork with similar potential improvements.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 255,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Prioritized DQN\n\nTORCS Car Racing Simulator",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 26,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Prioritized DQN",
      "section_index": 23,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: TORCS not only has more realistic graphics than Atari\n\n2600 games, but also requires the agent to learn the dy-\nnamics of the car it is controlling.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 93,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "TORCS not only has more realistic graphics than Atari",
      "section_index": 24,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: TORCS not only has more realistic graphics than Atari\n\nAt each step, an agent\nreceived only a visual input in the form of an RGB image",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "TORCS not only has more realistic graphics than Atari",
      "section_index": 24,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nof the current frame as well as a reward proportional to the\nagent\u2019s velocity along the center of the track at the agent\u2019s\ncurrent position.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 140,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe used the same neural network archi-\ntecture as the one used in the Atari experiments speci\ufb01ed in\nSupplementary Section 8.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 124,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nWe performed experiments us-\ning four different settings \u2013 the agent controlling a slow car\nwith and without opponent bots, and the agent controlling a\nfast car with and without opponent bots.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 192,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nFull results can be\nfound in Supplementary Figure S6.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nA3C was the best per-\nforming agent, reaching between roughly 75% and 90% of\nthe score obtained by a human tester on all four game con-\n\ufb01gurations in about 12 hours of training.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 177,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nA video showing\nthe learned driving behavior of the A3C agent can be found\nat https://youtu.be/0xo1Ldx3L5Q.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 107,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nContinuous Action Control Using the MuJoCo",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 42,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 25,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We also examined a set of tasks where the action space\n\nis continuous.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We also examined a set of tasks where the action space",
      "section_index": 26,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We also examined a set of tasks where the action space\n\nIn particular, we looked at a set of rigid\nbody physics domains with contact dynamics where the\ntasks include many examples of manipulation and loco-\nmotion.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 157,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We also examined a set of tasks where the action space",
      "section_index": 26,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nphysics engine.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 15,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nWe evaluated only the asynchronous ad-\nvantage actor-critic algorithm since, unlike the value-based\nmethods, it is easily extended to continuous actions.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 153,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nIn all\nproblems, using either the physical state or pixels as in-\nput, Asynchronous Advantage-Critic found good solutions\nin less than 24 hours of training and typically in under a few\nhours.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 191,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nSome successful policies learned by our agent can\nbe seen in the following video https://youtu.be/\nAjjc08-iPx8.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nFurther details about this experiment can\nbe found in Supplementary Section 9.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 78,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nLabyrinth\nWe performed an additional set of experiments with A3C\non a new 3D environment called Labyrinth.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 106,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nThe speci\ufb01c\ntask we considered involved the agent learning to \ufb01nd re-\nwards in randomly generated mazes. At the beginning of\neach episode the agent was placed in a new randomly gen-\nerated maze consisting of rooms and corridors. Each maze\ncontained two types of objects that the agent was rewarded\nfor \ufb01nding \u2013 apples and portals.",
    "chunk_index": 6,
    "num_sentences": 3,
    "chunk_size": 330,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nPicking up an apple led to\na reward of 1. Entering a portal led to a reward of 10 after\nwhich the agent was respawned in a new random location in\nthe maze and all previously collected apples were regener-\nated.",
    "chunk_index": 7,
    "num_sentences": 2,
    "chunk_size": 210,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nAn episode terminated after 60 seconds after which a\nnew episode would begin.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 77,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nThe aim of the agent is to collect\nas many points as possible in the time limit and the optimal\nstrategy involves \ufb01rst \ufb01nding the portal and then repeatedly\ngoing back to it after each respawn.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 193,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: These tasks were simulated using the Mujoco\n\nThis task is much more\nchallenging than the TORCS driving domain because the\nagent is faced with a new maze in each episode and must\nlearn a general strategy for exploring random mazes.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 185,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "These tasks were simulated using the Mujoco",
      "section_index": 27,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\n1\n2\n4\n8\n16\n1-step Q\n1.0\n3.0\n6.3\n13.3\n24.1\n1-step SARSA\n1.0\n2.8\n5.9\n13.1\n22.1\nn-step Q\n1.0\n2.7\n5.9\n10.7\n17.2\nA3C\n1.0\n2.1\n3.7\n6.9\n12.5\nTable 2.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 141,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\nThe average training speedup for each method and num-\nber of threads averaged over seven Atari games. To compute the\ntraining speed-up on a single game we measured the time to re-\nquired reach a \ufb01xed reference score using each method and num-\nber of threads. The speedup from using n threads on a game was\nde\ufb01ned as the time required to reach a \ufb01xed reference score using\none thread divided the time required to reach the reference score\nusing n threads.",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 454,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\nThe table shows the speedups averaged over\nseven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert,\nSeaquest, and Space Invaders).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 135,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\nWe trained an A3C LSTM agent on this task using only\n84 \u00d7 84 RGB images as input.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 81,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\nThe \ufb01nal average score\nof around 50 indicates that the agent learned a reason-\nable strategy for exploring random 3D maxes using only\na visual input.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 149,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\nA video showing one of the agents ex-\nploring previously unseen mazes is included at https:\n//youtu.be/nMR5mjCFZCw.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 115,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Method\n\nScalability and Data Ef\ufb01ciency",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 30,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Method",
      "section_index": 28,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nby looking at how the training time and data ef\ufb01ciency\nchanges with the number of parallel actor-learners.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 106,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nWhen\nusing multiple workers in parallel and updating a shared\nmodel, one would expect that in an ideal case, for a given\ntask and algorithm, the number of training steps to achieve\na certain score would remain the same with varying num-\nbers of workers.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 253,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nTherefore, the advantage would be solely\ndue to the ability of the system to consume more data in\nthe same amount of wall clock time and possibly improved\nexploration.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 167,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nTable 2 shows the training speed-up achieved\nby using increasing numbers of parallel actor-learners av-\neraged over seven Atari games.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 134,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nThese results show that all\nfour methods achieve substantial speedups from using mul-\ntiple worker threads, with 16 threads leading to at least an\norder of magnitude speedup.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 174,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nThis con\ufb01rms that our pro-\nposed framework scales well with the number of parallel\nworkers, making ef\ufb01cient use of resources.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nSomewhat surprisingly, asynchronous one-step Q-learning\nand Sarsa algorithms exhibit superlinear speedups that\ncannot be explained by purely computational gains. We\nobserve that one-step methods (one-step Q and one-step\nSarsa) often require less data to achieve a particular score\nwhen using more parallel actor-learners.",
    "chunk_index": 6,
    "num_sentences": 2,
    "chunk_size": 321,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nWe believe this\nis due to positive effect of multiple threads to reduce the\nbias in one-step methods.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 101,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We analyzed the effectiveness of our proposed framework\n\nThese effects are shown more\nclearly in Figure 3, which shows plots of the average score\nagainst the total number of training frames for different",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 146,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We analyzed the effectiveness of our proposed framework",
      "section_index": 29,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nA3C, Space Invaders\nFigure 2.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 29,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nScatter plots of scores obtained by asynchronous advantage actor-critic on \ufb01ve games (Beamrider, Breakout, Pong, Q*bert,\nSpace Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for\nwhich all random initializations acheive good scores.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 308,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nThis shows that A3C is quite robust to learning rates and initial random weights.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 81,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nnumbers of actor-learners and training methods on \ufb01ve\nAtari games, and Figure 4, which shows plots of the av-\nerage score against wall-clock time.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 146,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nRobustness and Stability\nFinally, we analyzed the stability and robustness of the\nfour proposed asynchronous algorithms.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 120,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nFor each of the\nfour algorithms we trained models on \ufb01ve games (Break-\nout, Beamrider, Pong, Q*bert, Space Invaders) using 50\ndifferent learning rates and random initializations.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 178,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nFigure 2\nshows scatter plots of the resulting scores for A3C, while\nSupplementary Figure S11 shows plots for the other three\nmethods.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 133,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nThere is usually a range of learning rates for each\nmethod and game combination that leads to good scores,\nindicating that all methods are quite robust to the choice of\nlearning rate and random initialization. The fact that there\nare virtually no points with scores of 0 in regions with good\nlearning rates indicates that the methods are stable and do\nnot collapse or diverge once they are learning.",
    "chunk_index": 7,
    "num_sentences": 2,
    "chunk_size": 399,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 30,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We have presented asynchronous versions of four standard\n\nreinforcement learning algorithms and showed that they\nare able to train neural network controllers on a variety\nof domains in a stable manner. Our results show that in\nour proposed framework stable training of neural networks\nthrough reinforcement learning is possible with both value-\nbased and policy-based methods, off-policy as well as on-\npolicy methods, and in discrete as well as continuous do-\nmains.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 409,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We have presented asynchronous versions of four standard",
      "section_index": 31,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We have presented asynchronous versions of four standard\n\nWhen trained on the Atari domain using 16 CPU\ncores, the proposed asynchronous algorithms train faster\nthan DQN trained on an Nvidia K40 GPU, with A3C sur-\npassing the current state-of-the-art in half the training time.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 219,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We have presented asynchronous versions of four standard",
      "section_index": 31,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We have presented asynchronous versions of four standard\n\nOne of our main \ufb01ndings is that using parallel actor-\nlearners to update a shared model had a stabilizing effect on\nthe learning process of the three value-based methods we\nconsidered.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 184,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We have presented asynchronous versions of four standard",
      "section_index": 31,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: We have presented asynchronous versions of four standard\n\nWhile this shows that stable online Q-learning\nis possible without experience replay, which was used for\nthis purpose in DQN, it does not mean that experience re-\nplay is not useful.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 182,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "We have presented asynchronous versions of four standard",
      "section_index": 31,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nthe asynchronous reinforcement learning framework could\nsubstantially improve the data ef\ufb01ciency of these methods\nby reusing old data.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 134,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nThis could in turn lead to much faster\ntraining times in domains like TORCS where interacting\nwith the environment is more expensive than updating the\nmodel for the architecture we used.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 186,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nCombining other existing reinforcement learning meth-\nods or recent advances in deep reinforcement learning\nwith our asynchronous framework presents many possibil-\nities for immediate improvements to the methods we pre-\nsented.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 227,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nWhile our n-step methods operate in the forward\nview (Sutton & Barto, 1998) by using corrected n-step re-\nturns directly as targets, it has been more common to use\nthe backward view to implicitly combine different returns\nthrough eligibility traces (Watkins, 1989; Sutton & Barto,\n1998; Peng & Williams, 1996).",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 310,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nThe asynchronous ad-\nvantage actor-critic method could be potentially improved\nby using other ways of estimating the advantage function,\nsuch as generalized advantage estimation of (Schulman\net al., 2015b).",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 206,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nAll of the value-based methods we inves-\ntigated could bene\ufb01t from different ways of reducing over-\nestimation bias of Q-values (Van Hasselt et al., 2015; Belle-\nmare et al., 2016).",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 181,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nYet another, more speculative, direction\nis to try and combine the recent work on true online tempo-\nral difference methods (van Seijen et al., 2015) with non-\nlinear function approximation.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 190,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nIn addition to these algorithmic improvements, a number\nof complementary improvements to the neural network ar-\nchitecture are possible.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 136,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nThe dueling architecture of (Wang\net al., 2015) has been shown to produce more accurate es-\ntimates of Q-values by including separate streams for the\nstate value and advantage in the network.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 191,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Incorporating experience replay into\n\nThe spatial soft-\nmax proposed by (Levine et al., 2015) could improve both\nvalue-based and policy-based methods by making it easier\nfor the network to represent feature coordinates.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 181,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Incorporating experience replay into",
      "section_index": 32,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Sasha Vezhnevets and Joseph Modayil for many helpful\n\ndiscussions, suggestions and comments on the paper.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Sasha Vezhnevets and Joseph Modayil for many helpful",
      "section_index": 33,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Sasha Vezhnevets and Joseph Modayil for many helpful\n\nWe\nalso thank the DeepMind evaluation team for setting up the\nenvironments used to evaluate the agents in the paper.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 116,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Sasha Vezhnevets and Joseph Modayil for many helpful",
      "section_index": 33,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 34,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 35,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 36,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 118,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 37,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 38,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 39,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 40,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 41,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 118,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 42,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 43,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 3.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 85,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nData ef\ufb01ciency comparison of different numbers of actor-learners for three asynchronous methods on \ufb01ve Atari games.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 115,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nThe\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 124,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nThe y-axis\nshows the average score. Each curve shows the average over the three best learning rates.",
    "chunk_index": 3,
    "num_sentences": 2,
    "chunk_size": 100,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nSingle step methods show increased data\nef\ufb01ciency from more parallel workers.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 77,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nResults for Sarsa are shown in Supplementary Figure S9.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 82,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 44,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 162,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 45,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 165,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 46,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 177,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 47,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 176,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 48,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step Q, 1 threads\n1-step Q, 2 threads\n1-step Q, 4 threads\n1-step Q, 8 threads\n1-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 169,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 49,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 166,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 50,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 165,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 51,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 182,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 52,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 176,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 53,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nn-step Q, 1 threads\nn-step Q, 2 threads\nn-step Q, 4 threads\nn-step Q, 8 threads\nn-step Q, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 181,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 54,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n100\n200\n300\n400\n500",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 138,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 55,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n30\n20\n10\n0\n10\n20",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 133,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 56,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n2000\n4000\n6000\n8000\n10000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 144,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 57,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n200\n400\n600\n800\n1000\n1200\n1400",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 156,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 58,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nA3C, 1 threads\nA3C, 2 threads\nA3C, 4 threads\nA3C, 8 threads\nA3C, 16 threads\nFigure 4.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 85,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 59,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nTraining speed comparison of different numbers of actor-learners on \ufb01ve Atari games.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 84,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 59,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nThe x-axis shows training time in\nhours while the y-axis shows the average score.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 81,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 59,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nEach curve shows the average over the three best learning rates.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 64,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 59,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nAll asynchronous\nmethods show signi\ufb01cant speedups from using greater numbers of parallel actor-learners.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 104,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 59,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nResults for Sarsa are shown in Supplementary\nFigure S10.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 59,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nBellemare, Marc G, Naddaf, Yavar, Veness, Joel, and\nBowling, Michael.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 69,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nThe arcade learning environment:\nAn evaluation platform for general agents.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 75,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nJournal of\nArti\ufb01cial Intelligence Research, 2012.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nBellemare, Marc G., Ostrovski, Georg, Guez, Arthur,\nThomas, Philip S., and Munos, R\u00e9mi.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nIncreasing the ac-\ntion gap: New operators for reinforcement learning.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 70,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nIn\nProceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence, 2016.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 72,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nBertsekas, Dimitri P.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 21,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nDistributed dynamic programming.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 32,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nAutomatic Control, IEEE Transactions on, 27(3):610\u2013\n616, 1982.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 62,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nChavez, Kevin, Ong, Hao Yi, and Hong, Augustus.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nDis-\ntributed deep q-learning.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 30,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nTechnical report, Stanford Uni-\nversity, June 2015.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nDegris, Thomas, Pilarski, Patrick M, and Sutton, Richard S.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 59,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nModel-free reinforcement learning with continuous ac-\ntion in practice.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 71,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 13
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nIn American Control Conference (ACC),\n2012, pp.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 14
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nIEEE, 2012.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 15
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nGrounds, Matthew and Kudenko, Daniel.",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 37,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 16
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nParallel rein-\nforcement learning with linear function approximation.",
    "chunk_index": 17,
    "num_sentences": 1,
    "chunk_size": 69,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 17
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nIn Proceedings of the 5th, 6th and 7th European Confer-\nence on Adaptive and Learning Agents and Multi-agent\nSystems: Adaptation and Multi-agent Learning, pp.",
    "chunk_index": 18,
    "num_sentences": 1,
    "chunk_size": 158,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 18
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nSpringer-Verlag, 2008.",
    "chunk_index": 19,
    "num_sentences": 1,
    "chunk_size": 22,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 19
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: References\n\nKoutn\u00edk, Jan, Schmidhuber, J\u00fcrgen, and Gomez, Faustino.",
    "chunk_index": 20,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "References",
      "section_index": 60,
      "chunk_index": 20
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nvision-based reinforcement learning.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 36,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nIn Proceedings of\nthe 2014 conference on Genetic and evolutionary com-\nputation, pp.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 84,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nLevine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,\nPieter.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 67,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nEnd-to-end training of deep visuomotor policies.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 48,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\narXiv preprint arXiv:1504.00702, 2015.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nLi, Yuxi and Schuurmans, Dale.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 30,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nMapreduce for parallel re-\ninforcement learning.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 48,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nIn Recent Advances in Reinforce-\nment Learning - 9th European Workshop, EWRL 2011,\nAthens, Greece, September 9-11, 2011, Revised Selected\nPapers, pp.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 149,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\n309\u2013320, 2011.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nLillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,\nHeess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,\nand Wierstra, Daan.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 135,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nContinuous control with deep re-\ninforcement learning.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 54,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\narXiv preprint arXiv:1509.02971,\n2015.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nGraves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and\nRiedmiller, Martin.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 126,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nPlaying atari with deep reinforce-\nment learning.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 13
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nIn NIPS Deep Learning Workshop.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 31,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 14
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nRusu, Andrei A., Veness, Joel, Bellemare, Marc G.,\nGraves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,\nOstrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,\nAmir, Antonoglou, Ioannis, King, Helen, Kumaran,\nDharshan, Wierstra, Daan, Legg, Shane, and Hassabis,\nDemis.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 327,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 15
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nHuman-level control through deep reinforcement\nlearning.",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 16
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nNature, 518(7540):529\u2013533, 02 2015. URL\nhttp://dx.doi.org/10.1038/nature14236.",
    "chunk_index": 17,
    "num_sentences": 2,
    "chunk_size": 78,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 17
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nNair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alci-\ncek, Cagdas, Fearon, Rory, Maria, Alessandro De, Pan-\nneershelvam, Vedavyas, Suleyman, Mustafa, Beattie,\nCharles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr,\nKavukcuoglu, Koray, and Silver, David.",
    "chunk_index": 18,
    "num_sentences": 1,
    "chunk_size": 253,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 18
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nMassively par-\nallel methods for deep reinforcement learning.",
    "chunk_index": 19,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 19
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nIn ICML\nDeep Learning Workshop.",
    "chunk_index": 20,
    "num_sentences": 1,
    "chunk_size": 31,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 20
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nPeng, Jing and Williams, Ronald J.",
    "chunk_index": 21,
    "num_sentences": 1,
    "chunk_size": 34,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 21
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nIncremental multi-step\nq-learning.",
    "chunk_index": 22,
    "num_sentences": 1,
    "chunk_size": 34,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 22
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nMachine Learning, 22(1-3):283\u2013290, 1996.",
    "chunk_index": 23,
    "num_sentences": 1,
    "chunk_size": 40,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 23
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nRecht, Benjamin, Re, Christopher, Wright, Stephen, and\nNiu, Feng.",
    "chunk_index": 24,
    "num_sentences": 1,
    "chunk_size": 65,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 24
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nHogwild: A lock-free approach to paralleliz-\ning stochastic gradient descent.",
    "chunk_index": 25,
    "num_sentences": 1,
    "chunk_size": 77,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 25
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nIn Advances in Neural\nInformation Processing Systems, pp.",
    "chunk_index": 26,
    "num_sentences": 1,
    "chunk_size": 57,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 26
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\n693\u2013701, 2011.",
    "chunk_index": 27,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 27
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nRiedmiller, Martin.",
    "chunk_index": 28,
    "num_sentences": 1,
    "chunk_size": 19,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 28
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nNeural \ufb01tted q iteration\u2013\ufb01rst experi-\nences with a data ef\ufb01cient neural reinforcement learning\nmethod.",
    "chunk_index": 29,
    "num_sentences": 1,
    "chunk_size": 102,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 29
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nIn Machine Learning: ECML 2005, pp.",
    "chunk_index": 30,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 30
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nSpringer Berlin Heidelberg, 2005.",
    "chunk_index": 31,
    "num_sentences": 1,
    "chunk_size": 33,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 31
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nRummery, Gavin A and Niranjan, Mahesan.",
    "chunk_index": 32,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 32
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nOn-line q-\nlearning using connectionist systems.",
    "chunk_index": 33,
    "num_sentences": 1,
    "chunk_size": 48,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 33
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nSchaul, Tom, Quan, John, Antonoglou, Ioannis, and Sil-\nver, David.",
    "chunk_index": 34,
    "num_sentences": 1,
    "chunk_size": 66,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 34
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nPrioritized experience replay.",
    "chunk_index": 35,
    "num_sentences": 1,
    "chunk_size": 30,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 35
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\narXiv preprint\narXiv:1511.05952, 2015.",
    "chunk_index": 36,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 36
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nSchulman, John, Levine, Sergey, Moritz, Philipp, Jordan,\nMichael I, and Abbeel, Pieter.",
    "chunk_index": 37,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 37
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Evolving deep unsupervised convolutional networks for\n\nTrust region policy op-\ntimization.",
    "chunk_index": 38,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Evolving deep unsupervised convolutional networks for",
      "section_index": 61,
      "chunk_index": 38
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nLearning (ICML), 2015a.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 23,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nSchulman, John, Moritz, Philipp, Levine, Sergey, Jordan,\nMichael, and Abbeel, Pieter.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 85,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nHigh-dimensional con-\ntinuous control using generalized advantage estimation.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 77,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\narXiv preprint arXiv:1506.02438, 2015b.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nand Barto, A.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nReinforcement Learning: an In-\ntroduction.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 42,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nMIT Press, 1998.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nTieleman, Tijmen and Hinton, Geoffrey.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nLecture 6.5-\nrmsprop: Divide the gradient by a running average of\nits recent magnitude.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nCOURSERA: Neural Networks for\nMachine Learning, 4, 2012.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nTodorov, E.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nMuJoCo: Modeling, Simulation and Visual-\nization of Multi-Joint Dynamics with Contact (ed 1.0).",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 95,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: In International Conference on Machine\n\nRoboti Publishing, 2015.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 24,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "In International Conference on Machine",
      "section_index": 62,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nTomassini, Marco.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nParallel and distributed evolutionary al-\ngorithms: A review.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nTechnical report, 1999.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 23,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nTsitsiklis, John N.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 19,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAsynchronous stochastic approxima-\ntion and q-learning.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nMachine Learning, 16(3):185\u2013202,\n1994.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nVan Hasselt, Hado, Guez, Arthur, and Silver, David.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nDeep\nreinforcement learning with double q-learning.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\narXiv\npreprint arXiv:1509.06461, 2015.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nvan Seijen, H., Rupam Mahmood, A., Pilarski, P.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nM.,\nMachado, M.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 15,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nC., and Sutton, R.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 63,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nTemporal-Difference Learning.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 29,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nArXiv e-prints, Decem-\nber 2015.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 32,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nWang, Z., de Freitas, N., and Lanctot, M.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 41,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nDueling Network\nArchitectures for Deep Reinforcement Learning.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 62,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nArXiv\ne-prints, November 2015.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 30,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nWatkins, Christopher John Cornish Hellaby.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 42,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nLearning from\ndelayed rewards.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 30,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nPhD thesis, University of Cambridge\nEngland, 1989.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 50,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nWilliams, R.J.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nSimple statistical gradient-following algo-\nrithms for connectionist reinforcement learning.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 92,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nMa-\nchine Learning, 8(3):229\u2013256, 1992.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nWilliams, Ronald J and Peng, Jing.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 34,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nFunction optimization\nusing connectionist reinforcement learning algorithms.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 76,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nConnection Science, 3(3):241\u2013268, 1991.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 13
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nWymann, B., Espi\u00c3l\u2019, E., Guionneau, C., Dimitrakakis, C.,\nCoulom, R., and Sumner, A.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 84,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 14
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nTorcs: The open racing car\nsimulator, v1.3.5, 2013.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 15
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: True Online\n\nSupplementary Material for \"Asynchronous Methods for Deep\nReinforcement Learning\"\nJune 17, 2016",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 95,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "True Online",
      "section_index": 64,
      "chunk_index": 16
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nWe investigated two different optimization algorithms with our asynchronous framework \u2013 stochastic gradient\ndescent and RMSProp.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 128,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nOur implementations of these algorithms do not use any locking in order to maximize\nthroughput when using a large number of threads.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 132,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nMomentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and\nwell studied (Recht et al., 2011).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 135,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nLet \u03b8 be the parameter vector that is shared across all threads and let \u2206\u03b8i\nbe the accumulated gradients of the loss with respect to parameters \u03b8 computed by thread number i. Each\nthread i independently applies the standard momentum SGD update mi = \u03b1mi + (1 \u2212\u03b1)\u2206\u03b8i followed by\n\u03b8 \u2190\u03b8 \u2212\u03b7mi with learning rate \u03b7, momentum \u03b1 and without any locks. Note that in this setting, each thread\nmaintains its own separate gradient and momentum vector.",
    "chunk_index": 3,
    "num_sentences": 3,
    "chunk_size": 438,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nRMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature,\nit has not been extensively studied in the asynchronous optimization setting.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 180,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Optimization Details\n\nThe standard non-centered",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 25,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Optimization Details",
      "section_index": 65,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\ng = \u03b1g + (1 \u2212\u03b1)\u2206\u03b82\n(S2)\n\u03b8 \u2190\u03b8 \u2212\u03b7\n\u2206\u03b8\n\u221ag + \u03f5,\n(S3)\nwhere all operations are performed elementwise.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 95,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nIn order to apply RMSProp in the asynchronous optimiza-\ntion setting one must decide whether the moving average of elementwise squared gradients g is shared or\nper-thread.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 171,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nWe experimented with two versions of the algorithm.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nIn one version, which we refer to as RM-\nSProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared\nRMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing\nstatistics among threads also reduces memory requirements by using one fewer copy of the parameter vector\nper thread.",
    "chunk_index": 3,
    "num_sentences": 3,
    "chunk_size": 367,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nWe compared these three asynchronous optimization algorithms in terms of their sensitivity to different learn-\ning rates and random network initializations.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 156,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nFigure S5 shows a comparison of the methods for two different\nreinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games\n(Breakout, Beamrider, Seaquest and Space Invaders).",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 218,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nEach curve shows the scores for 50 experiments that\ncorrespond to 50 different random learning rates and initializations.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 121,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nThe x-axis shows the rank of the model\nafter sorting in descending order by \ufb01nal average score and the y-axis shows the \ufb01nal average score achieved\nby the corresponding model.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 175,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nIn this representation, the algorithm that performs better would achieve higher\nmaximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizon-\ntal, thus maximizing the area under the curve.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 235,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: RMSProp update is given by\n\nRMSProp with shared statistics tends to be more robust than\nRMSProp with per-thread statistics, which is in turn more robust than Momentum SGD.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 143,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "RMSProp update is given by",
      "section_index": 66,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS\nexperiments (Figure S6) used the following setup.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 153,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nEach experiment used 16 actor-learner threads running\non a single machine and no GPUs.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 86,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nAll methods performed updates after every 5 actions (tmax = 5 and\nIUpdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods\nused a shared target network that was updated every 40000 frames.",
    "chunk_index": 2,
    "num_sentences": 2,
    "chunk_size": 233,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe Atari experiments used the same\ninput preprocessing as (Mnih et al., 2015) and an action repeat of 4.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 105,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe agents used the network architecture\nfrom (Mnih et al., 2013).",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 66,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe network used a convolutional layer with 16 \ufb01lters of size 8 \u00d7 8 with stride\n4, followed by a convolutional layer with with 32 \ufb01lters of size 4 \u00d7 4 with stride 2, followed by a fully\nconnected layer with 256 hidden units.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 224,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nAll three hidden layers were followed by a recti\ufb01er nonlinearity.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 65,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe\nvalue-based methods had a single linear output unit for each action representing the action-value. The model\nused by actor-critic agents had two set of outputs \u2013 a softmax output with one entry per action representing the\nprobability of selecting the action, and a single linear output representing the value function.",
    "chunk_index": 7,
    "num_sentences": 2,
    "chunk_size": 322,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nAll experiments\nused a discount of \u03b3 = 0.99 and an RMSProp decay factor of \u03b1 = 0.99.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 84,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe value based methods sampled the exploration rate \u03f5 from a distribution taking three values \u03f51, \u03f52, \u03f53 with\nprobabilities 0.4, 0.3, 0.3.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 139,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe values of \u03f51, \u03f52, \u03f53 were annealed from 1 to 0.1, 0.01, 0.5 respectively over\nthe \ufb01rst four million frames.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nAdvantage actor-critic used entropy regularization with a weight \u03b2 = 0.01 for\nall Atari and TORCS experiments. We performed a set of 50 experiments for \ufb01ve Atari games and every\nTORCS level, each using a different random initialization and initial learning rate.",
    "chunk_index": 11,
    "num_sentences": 2,
    "chunk_size": 262,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nThe initial learning rate\nwas sampled from a LogUniform(10\u22124, 10\u22122) distribution and annealed to 0 over the course of training.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 127,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Experimental Setup\n\nNote that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used\n\ufb01xed hyperparameters.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 128,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Experimental Setup",
      "section_index": 67,
      "chunk_index": 13
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nTo apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly\nidentical to that used in the discrete action domains, so here we enumerate only the differences required for\nthe continuous action domains.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 249,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nThe essential elements for many of the tasks (i.e.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 50,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nthe physics models and\ntask objectives) are near identical to the tasks examined in (Lillicrap et al., 2015).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 109,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nHowever, the rewards and\nthus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco\nwhich altered the contact model.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 163,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nFor all the domains we attempted to learn the task using the physical state as input. The physical state\nconsisted of the joint positions and velocities as well as the target position if the task required a target.",
    "chunk_index": 4,
    "num_sentences": 2,
    "chunk_size": 214,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn\naddition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from\nRGB pixel inputs.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 130,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn the low dimensional physical state case, the inputs are mapped to a hidden state using\none hidden layer with 200 ReLU units.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 127,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn the cases where we used pixels, the input was passed through two\nlayers of spatial convolutions without any non-linearity or pooling.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 136,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn either case, the output of the encoder\nlayers were fed to a single layer of 128 LSTM cells.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 94,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nThe most important difference in the architecture is in the\nthe output layer of the policy network.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 99,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nUnlike the discrete action domain where the action output is a Softmax,\nhere the two outputs of the policy network are two real number vectors which we treat as the mean vector \u00b5\nand scalar variance \u03c32 of a multidimensional normal distribution with a spherical covariance.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 272,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nTo act, the input\nis passed through the model to the output layer where we sample from the normal distribution determined by\n\u00b5 and \u03c32. In practice, \u00b5 is modeled by a linear layer and \u03c32 by a SoftPlus operation, log(1 + exp(x)), as the\nactivation computed as a function of the output of a linear layer.",
    "chunk_index": 11,
    "num_sentences": 2,
    "chunk_size": 301,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 11
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn our experiments with continuous control\nproblems the networks for policy network and value network do not share any parameters, though this detail\nis unlikely to be crucial.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 176,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 12
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nFinally, since the episodes were typically at most several hundred time steps long,\nwe did not use any bootstrapping in the policy or value function updates and batched each episode into a\nsingle update.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 203,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 13
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nAs in the discrete action case, we included an entropy cost which encouraged exploration.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 89,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 14
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Continuous Action Control Using the MuJoCo Physics Simulator\n\nIn the continuous",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Continuous Action Control Using the MuJoCo Physics Simulator",
      "section_index": 68,
      "chunk_index": 15
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\ncase the we used a cost on the differential entropy of the normal distribution de\ufb01ned by the output of the\nactor network, \u22121\n2(log(2\u03c0\u03c32) + 1), we used a constant multiplier of 10\u22124 for this cost across all of the tasks\nexamined.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 228,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nThe asynchronous advantage actor-critic algorithm \ufb01nds solutions for all the domains.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 85,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nFigure S8\nshows learning curves against wall-clock time, and demonstrates that most of the domains from states can be\nsolved within a few hours.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 144,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAll of the experiments, including those done from pixel based observations, were\nrun on CPU.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 92,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nEven in the case of solving the domains directly from pixel inputs we found that it was possible\nto reliably discover solutions within 24 hours.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 144,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nFigure S7 shows scatter plots of the top scores against the\nsampled learning rates.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 83,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nIn most of the domains there is large range of learning rates that consistently achieve\ngood performance on the task.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 117,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAlgorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 87,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 7
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\n// Assume global shared parameter vector \u03b8. // Assume global shared target parameter vector \u03b8\u2212.",
    "chunk_index": 8,
    "num_sentences": 2,
    "chunk_size": 95,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 8
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\n// Assume global shared counter T = 0.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 9
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nInitialize thread step counter t \u21901\nInitialize target network parameters \u03b8\u2212\u2190\u03b8\nInitialize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\nInitialize network gradients d\u03b8 \u21900\nrepeat\nClear gradients d\u03b8 \u21900\nSynchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8\ntstart = t",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 241,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 69,
      "chunk_index": 10
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\nrepeat\nTake action at according to the \u03f5-greedy policy based on Q(st, a; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a 0\nfor terminal st\nmaxa Q(st, a; \u03b8\u2212)\nfor non-terminal st\nfor i \u2208{t \u22121, .",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 248,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 70,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\n, tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 +\n\u2202(R\u2212Q(si,ai;\u03b8\u2032))\n2\n\u2202\u03b8\u2032\nend for\nPerform asynchronous update of \u03b8 using d\u03b8.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 135,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 70,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\nif T\nmod Itarget == 0 then\n\u03b8\u2212\u2190\u03b8\nend if\nuntil T > Tmax",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 70,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nAlgorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 92,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 71,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\n// Assume global shared parameter vectors \u03b8 and \u03b8v and global shared counter T = 0\n// Assume thread-speci\ufb01c parameter vectors \u03b8\u2032 and \u03b8\u2032\nv\nInitialize thread step counter t \u21901\nrepeat\nReset gradients: d\u03b8 \u21900 and d\u03b8v \u21900. Synchronize thread-speci\ufb01c parameters \u03b8\u2032 = \u03b8 and \u03b8\u2032\nv = \u03b8v\ntstart = t",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 285,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 71,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\nrepeat\nPerform at according to policy \u03c0(at|st; \u03b8\u2032)\nReceive reward rt and new state st+1\nt \u2190t + 1\nT \u2190T + 1\nuntil terminal st or t \u2212tstart == tmax\nR =\n\u001a\n0\nfor terminal st\nV (st, \u03b8\u2032\nv)\nfor non-terminal st// Bootstrap from last state\nfor i \u2208{t \u22121, .",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 245,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 72,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\n, tstart} do\nR \u2190ri + \u03b3R\nAccumulate gradients wrt \u03b8\u2032: d\u03b8 \u2190d\u03b8 + \u2207\u03b8\u2032 log \u03c0(ai|si; \u03b8\u2032)(R \u2212V (si; \u03b8\u2032\nv))\nAccumulate gradients wrt \u03b8\u2032\nv: d\u03b8v \u2190d\u03b8v + \u2202(R \u2212V (si; \u03b8\u2032\nv))2/\u2202\u03b8\u2032\nv\nend for\nPerform asynchronous update of \u03b8 using d\u03b8 and of \u03b8v using d\u03b8v.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 238,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 72,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Get state st\n\nuntil T > Tmax",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Get state st",
      "section_index": 72,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nA3C, SGD\nA3C, RMSProp\nA3C, Shared RMSProp\nFigure S5.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 52,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nComparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested\nusing two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Break-\nout, Beamrider, Seaquest and Space Invaders).",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 262,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nEach curve shows the \ufb01nal scores for 50 experiments sorted in descending\norder that covers a search over 50 random initializations and learning rates.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 150,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nThe top row shows results using Async n-step\nQ algorithm and bottom row shows results with Async Advantage Actor-Critic.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 120,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nEach individual graph shows results for\none of the four games and three different optimization methods.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 103,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nShared RMSProp tends to be more robust to different\nlearning rates and random initializations than Momentum SGD and RMSProp without sharing.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 140,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n0\n10\n20\n30\n40\nTraining time (hours)\n1000\n0\n1000\n2000\n3000\n4000",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 62,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 73,
      "chunk_index": 6
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Human tester\n\nComparison of algorithms on the TORCS car racing simulator.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 59,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Human tester",
      "section_index": 74,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Human tester\n\nFour different con\ufb01gurations of car speed and\nopponent presence or absence are shown.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 85,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Human tester",
      "section_index": 74,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Human tester\n\nIn each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and\nAdvantage Actor-Critic) are compared on score vs training time in wall clock hours.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 159,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Human tester",
      "section_index": 74,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Human tester\n\nMulti-step algorithms achieve better\npolicies much faster than one-step algorithms on all four levels.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 102,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Human tester",
      "section_index": 74,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Human tester\n\nThe curves show averages over the 5 best runs from 50\nexperiments with learning rates sampled from LogUniform(10\u22124, 10\u22122) and all other hyperparameters \ufb01xed.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 157,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Human tester",
      "section_index": 74,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nPerformance for the Mujoco continuous action domains.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 75,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nScatter plot of the best score obtained against\nlearning rates sampled from LogUniform(10\u22125, 10\u22121).",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 99,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 75,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nFor nearly all of the tasks there is a wide range of learning\nrates that lead to good performance on the task.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 110,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 75,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nScore per episode vs wall-clock time plots for the Mujoco domains.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 66,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 76,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Asynchronous Methods for Deep Reinforcement Learning\n\nEach plot shows error bars for the top 5\nexperiments.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_index": 76,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 77,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 78,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 79,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n10\n20\n30",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 138,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 80,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S9.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 131,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 81,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nData ef\ufb01ciency comparison of different numbers of actor-learners one-step Sarsa on \ufb01ve Atari games.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 99,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 81,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nThe\nx-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 124,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 81,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nThe y-axis shows the average score.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 81,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nEach curve shows the average of the three best performing agents from a search over\n50 random learning rates.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 109,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 81,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nSarsa shows increased data ef\ufb01ciency with increased numbers of parallel workers.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 80,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 81,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Beamrider\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n50\n100\n150\n200\n250\n300",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 186,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Beamrider",
      "section_index": 82,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Breakout\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n25\n20\n15\n10\n5\n0\n5\n10\n15",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 185,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Breakout",
      "section_index": 83,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Pong\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n0\n500\n1000\n1500\n2000\n2500\n3000",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 192,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Pong",
      "section_index": 84,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nQ*bert\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0\n2\n4\n6\n8\n10\n12\n14\nTraining time (hours)\n100\n200\n300\n400\n500\n600\n700",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 196,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 85,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n1-step SARSA, 1 threads\n1-step SARSA, 2 threads\n1-step SARSA, 4 threads\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\nFigure S10.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 132,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nTraining speed comparison of different numbers of actor-learners for all one-step Sarsa on \ufb01ve Atari games.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 107,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nThe x-axis shows training time in hours while the y-axis shows the average score.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 81,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nEach curve shows the average of the\nthree best performing agents from a search over 50 random learning rates.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 109,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 3
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\nSarsa shows signi\ufb01cant speedups from using\ngreater numbers of parallel actor-learners.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 86,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 4
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Space Invaders\n\n10-4\n10-3\n10-2",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Space Invaders",
      "section_index": 86,
      "chunk_index": 5
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nn-step Q, Space Invaders\nFigure S11.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 36,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 87,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nScatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on \ufb01ve games (Beamrider,\nBreakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 201,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 87,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Score\n\nAll algorithms exhibit\nsome level of robustness to the choice of learning rate.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Score",
      "section_index": 87,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Gravitar\n\n216.5\n538.4\n200.5\n297.0\n218.0\n269.5\n303.5\n320.0\nH.E.R.O. 12952.5\n8963.4\n14892.5\n15207.9\n20506.4\n28765.8\n32464.1\n28889.5",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 119,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Gravitar",
      "section_index": 88,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Krull\n\n3864.0\n6363.1\n6796.1\n8051.6\n7406.5\n8066.6\n5560.0\n5911.4\nKung-Fu Master\n11875.0\n20620.0\n30207.0\n24288.0\n31244.0\n3046.0\n28819.0\n40835.0\nMontezuma\u2019s Revenge\n50.0\n84.0\n42.0\n22.0\n13.0\n53.0\n67.0\n41.0\nMs.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 197,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Krull",
      "section_index": 89,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Krull\n\nPacman\n763.5\n1263.0\n1241.3\n2250.6\n1824.6\n594.4\n653.7\n850.7",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 58,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Krull",
      "section_index": 89,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Private Eye\n\n298.2\n2598.6\n-575.5\n292.6\n179.0\n194.4\n206.9\n421.1\nQ*Bert\n4589.8\n7089.8\n11020.8\n14175.8\n11277.0\n13752.3\n15148.8\n21307.5",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 118,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Private Eye",
      "section_index": 90,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Zaxxon\n\n831.0\n6159.4\n8593.0\n10164.0\n9501.0\n2659.0\n24622.0\n23519.0\nTable S3.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 67,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Zaxxon",
      "section_index": 91,
      "chunk_index": 0
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Zaxxon\n\nRaw scores for the human start condition (30 minutes emulator time).",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 68,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Zaxxon",
      "section_index": 91,
      "chunk_index": 1
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Asynchronous Methods for Deep Reinforcement Learning\n\nSection: Zaxxon\n\nDQN scores taken from (Nair et al.,\n2015). Double DQN scores taken from (Van Hasselt et al., 2015), Dueling scores from (Wang et al., 2015) and Prioritized\nscores taken from (Schaul et al., 2015)",
    "chunk_index": 2,
    "num_sentences": 2,
    "chunk_size": 195,
    "metadata": {
      "title": "Asynchronous Methods for Deep Reinforcement Learning",
      "section_header": "Zaxxon",
      "section_index": 91,
      "chunk_index": 2
    },
    "paper_title": "Asynchronous Methods for Deep Reinforcement Learning",
    "pdf_path": "data/papers/1602.01783v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nModel-free deep reinforcement learning (RL) al-\ngorithms have been demonstrated on a range of\nchallenging decision making and control tasks.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 140,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nHowever, these methods typically suffer from two\nmajor challenges: very high sample complexity\nand brittle convergence properties, which necessi-\ntate meticulous hyperparameter tuning.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 184,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nBoth of\nthese challenges severely limit the applicability\nof such methods to complex, real-world domains.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 105,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nIn this paper, we propose soft actor-critic, an off-\npolicy actor-critic deep RL algorithm based on the\nmaximum entropy reinforcement learning frame-\nwork.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 155,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nIn this framework, the actor aims to maxi-\nmize expected reward while also maximizing en-\ntropy.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 96,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nThat is, to succeed at the task while acting\nas randomly as possible.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 69,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nPrior deep RL methods\nbased on this framework have been formulated\nas Q-learning methods.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 89,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nBy combining off-policy\nupdates with a stable stochastic actor-critic formu-\nlation, our method achieves state-of-the-art per-\nformance on a range of continuous control bench-\nmark tasks, outperforming prior on-policy and\noff-policy methods.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 241,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Abstract\n\nFurthermore, we demonstrate\nthat, in contrast to other off-policy algorithms, our\napproach is very stable, achieving very similar\nperformance across different random seeds.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 172,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Abstract",
      "section_index": 0,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nModel-free deep reinforcement learning (RL) algorithms\nhave been applied in a range of challenging domains, from\ngames (Mnih et al., 2013; Silver et al., 2016) to robotic\ncontrol (Schulman et al., 2015). The combination of RL\nand high-capacity function approximators such as neural\nnetworks holds the promise of automating a wide range of\ndecision making and control tasks, but widespread adoption\n1Berkeley Arti\ufb01cial Intelligence Research, University of Cal-\nifornia, Berkeley, USA.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 483,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nCorrespondence to: Tuomas Haarnoja\n<haarnoja@berkeley.edu>.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 59,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nof these methods in real-world domains has been hampered\nby two major challenges.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 81,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nFirst, model-free deep RL meth-\nods are notoriously expensive in terms of their sample com-\nplexity.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 100,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nEven relatively simple tasks can require millions of\nsteps of data collection, and complex behaviors with high-\ndimensional observations might need substantially more.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 167,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nSecond, these methods are often brittle with respect to their\nhyperparameters: learning rates, exploration constants, and\nother settings must be set carefully for different problem\nsettings to achieve good results.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 214,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nBoth of these challenges\nseverely limit the applicability of model-free deep RL to\nreal-world tasks. One cause for the poor sample ef\ufb01ciency of deep RL meth-\nods is on-policy learning: some of the most commonly used\ndeep RL algorithms, such as TRPO (Schulman et al., 2015),\nPPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016),\nrequire new samples to be collected for each gradient step. This quickly becomes extravagantly expensive, as the num-\nber of gradient steps and samples per step needed to learn\nan effective policy increases with task complexity.",
    "chunk_index": 6,
    "num_sentences": 3,
    "chunk_size": 559,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nOff-\npolicy algorithms aim to reuse past experience.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 52,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nThis is not\ndirectly feasible with conventional policy gradient formula-\ntions, but is relatively straightforward for Q-learning based\nmethods (Mnih et al., 2015). Unfortunately, the combina-\ntion of off-policy learning and high-dimensional, nonlinear\nfunction approximation with neural networks presents a ma-\njor challenge for stability and convergence (Bhatnagar et al.,\n2009). This challenge is further exacerbated in continuous\nstate and action spaces, where a separate actor network is\noften used to perform the maximization in Q-learning.",
    "chunk_index": 8,
    "num_sentences": 3,
    "chunk_size": 545,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nA\ncommonly used algorithm in such settings, deep determinis-\ntic policy gradient (DDPG) (Lillicrap et al., 2015), provides\nfor sample-ef\ufb01cient learning but is notoriously challenging\nto use due to its extreme brittleness and hyperparameter\nsensitivity (Duan et al., 2016; Henderson et al., 2017).",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 296,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nWe explore how to design an ef\ufb01cient and stable model-\nfree deep RL algorithm for continuous state and action\nspaces.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 117,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nTo that end, we draw on the maximum entropy\nframework, which augments the standard maximum reward\nreinforcement learning objective with an entropy maximiza-\ntion term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al.,\narXiv:1801.01290v2  [cs.LG]  8 Aug 2018\n\n\nSoft Actor-Critic\n2012; Fox et al., 2016; Haarnoja et al., 2017). Maximum en-\ntropy reinforcement learning alters the RL objective, though\nthe original objective can be recovered using a tempera-\nture parameter (Haarnoja et al., 2017).",
    "chunk_index": 11,
    "num_sentences": 2,
    "chunk_size": 499,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nMore importantly,\nthe maximum entropy formulation provides a substantial\nimprovement in exploration and robustness: as discussed\nby Ziebart (2010), maximum entropy policies are robust\nin the face of model and estimation errors, and as demon-\nstrated by (Haarnoja et al., 2017), they improve exploration\nby acquiring diverse behaviors. Prior work has proposed\nmodel-free deep RL algorithms that perform on-policy learn-\ning with entropy maximization (O\u2019Donoghue et al., 2016),\nas well as off-policy methods based on soft Q-learning and\nits variants (Schulman et al., 2017a; Nachum et al., 2017a;\nHaarnoja et al., 2017).",
    "chunk_index": 12,
    "num_sentences": 2,
    "chunk_size": 618,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nHowever, the on-policy variants suf-\nfer from poor sample complexity for the reasons discussed\nabove, while the off-policy variants require complex approx-\nimate inference procedures in continuous action spaces.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 211,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nIn this paper, we demonstrate that we can devise an off-\npolicy maximum entropy actor-critic algorithm, which we\ncall soft actor-critic (SAC), which provides for both sample-\nef\ufb01cient learning and stability.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 207,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 14
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nThis algorithm extends read-\nily to very complex, high-dimensional tasks, such as the\nHumanoid benchmark (Duan et al., 2016) with 21 action\ndimensions, where off-policy methods such as DDPG typi-\ncally struggle to obtain good results (Gu et al., 2016).",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 252,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 15
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nSAC\nalso avoids the complexity and potential instability associ-\nated with approximate inference in prior off-policy maxi-\nmum entropy algorithms based on soft Q-learning (Haarnoja\net al., 2017). We present a convergence proof for policy\niteration in the maximum entropy framework, and then in-\ntroduce a new algorithm based on an approximation to this\nprocedure that can be practically implemented with deep\nneural networks, which we call soft actor-critic. We present\nempirical results that show that soft actor-critic attains a\nsubstantial improvement in both performance and sample\nef\ufb01ciency over both off-policy and on-policy prior methods.",
    "chunk_index": 16,
    "num_sentences": 3,
    "chunk_size": 645,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 16
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Introduction\n\nWe also compare to twin delayed deep deterministic (TD3)\npolicy gradient algorithm (Fujimoto et al., 2018), which is\na concurrent work that proposes a deterministic algorithm\nthat substantially improves on DDPG.",
    "chunk_index": 17,
    "num_sentences": 1,
    "chunk_size": 211,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Introduction",
      "section_index": 1,
      "chunk_index": 17
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Related Work\n\nOur soft actor-critic algorithm incorporates three key in-\ngredients: an actor-critic architecture with separate policy\nand value function networks, an off-policy formulation that\nenables reuse of previously collected data for ef\ufb01ciency, and\nentropy maximization to enable stability and exploration.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 299,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Related Work",
      "section_index": 2,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nthis section.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nActor-critic algorithms are typically derived\nstarting from policy iteration, which alternates between pol-\nicy evaluation\u2014computing the value function for a policy\u2014\nand policy improvement\u2014using the value function to obtain\na better policy (Barto et al., 1983; Sutton & Barto, 1998). In\nlarge-scale reinforcement learning problems, it is typically\nimpractical to run either of these steps to convergence, and\ninstead the value function and policy are optimized jointly.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 469,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nIn this case, the policy is referred to as the actor, and the\nvalue function as the critic. Many actor-critic algorithms\nbuild on the standard, on-policy policy gradient formulation\nto update the actor (Peters & Schaal, 2008), and many of\nthem also consider the entropy of the policy, but instead of\nmaximizing the entropy, they use it as an regularizer (Schul-\nman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al.,\n2017).",
    "chunk_index": 2,
    "num_sentences": 2,
    "chunk_size": 428,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nOn-policy training tends to improve stability but\nresults in poor sample complexity.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 84,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nThere have been efforts to increase the sample ef\ufb01ciency\nwhile retaining robustness by incorporating off-policy sam-\nples and by using higher order variance reduction tech-\nniques (O\u2019Donoghue et al., 2016; Gu et al., 2016).",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 223,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nHow-\never, fully off-policy algorithms still attain better ef\ufb01-\nciency.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 71,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nA particularly popular off-policy actor-critic method,\nDDPG (Lillicrap et al., 2015), which is a deep variant of the\ndeterministic policy gradient (Silver et al., 2014) algorithm,\nuses a Q-function estimator to enable off-policy learning,\nand a deterministic actor that maximizes this Q-function. As such, this method can be viewed both as a determinis-\ntic actor-critic algorithm and an approximate Q-learning\nalgorithm.",
    "chunk_index": 6,
    "num_sentences": 2,
    "chunk_size": 421,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nUnfortunately, the interplay between the deter-\nministic actor network and the Q-function typically makes\nDDPG extremely dif\ufb01cult to stabilize and brittle to hyperpa-\nrameter settings (Duan et al., 2016; Henderson et al., 2017).",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 228,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nAs a consequence, it is dif\ufb01cult to extend DDPG to complex,\nhigh-dimensional tasks, and on-policy policy gradient meth-\nods still tend to produce the best results in such settings (Gu\net al., 2016).",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 198,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nOur method instead combines off-policy actor-\ncritic training with a stochastic actor, and further aims to\nmaximize the entropy of this actor with an entropy maxi-\nmization objective.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 183,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nWe \ufb01nd that this actually results in a\nconsiderably more stable and scalable algorithm that, in\npractice, exceeds both the ef\ufb01ciency and \ufb01nal performance\nof DDPG.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 162,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nA similar method can be derived as a zero-step\nspecial case of stochastic value gradients (SVG(0)) (Heess\net al., 2015).",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 120,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nHowever, SVG(0) differs from our method in\nthat it optimizes the standard maximum expected return ob-\njective, and it does not make use of a separate value network,\nwhich we found to make training more stable.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 209,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nMaximum entropy reinforcement learning optimizes poli-\ncies to maximize both the expected return and the ex-\npected entropy of the policy. This framework has been\nused in many contexts, from inverse reinforcement learn-\ning (Ziebart et al., 2008) to optimal control (Todorov, 2008;\nToussaint, 2009; Rawlik et al., 2012). In guided policy\nsearch (Levine & Koltun, 2013; Levine et al., 2016), the\nmaximum entropy distribution is used to guide policy learn-\n\n\nSoft Actor-Critic\ning towards high-reward regions. More recently, several\npapers have noted the connection between Q-learning and\npolicy gradient methods in the framework of maximum en-\ntropy learning (O\u2019Donoghue et al., 2016; Haarnoja et al.,\n2017; Nachum et al., 2017a; Schulman et al., 2017a).",
    "chunk_index": 13,
    "num_sentences": 4,
    "chunk_size": 753,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nWhile\nmost of the prior model-free works assume a discrete action\nspace, Nachum et al.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 86,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 14
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\n(2017b) approximate the maximum en-\ntropy distribution with a Gaussian and Haarnoja et al.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 90,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 15
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\n(2017)\nwith a sampling network trained to draw samples from the\noptimal policy.",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 16
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nAlthough the soft Q-learning algorithm pro-\nposed by Haarnoja et al.",
    "chunk_index": 17,
    "num_sentences": 1,
    "chunk_size": 68,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 17
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\n(2017) has a value function and\nactor network, it is not a true actor-critic algorithm: the\nQ-function is estimating the optimal Q-function, and the\nactor does not directly affect the Q-function except through\nthe data distribution.",
    "chunk_index": 18,
    "num_sentences": 1,
    "chunk_size": 232,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 18
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nHence, Haarnoja et al.",
    "chunk_index": 19,
    "num_sentences": 1,
    "chunk_size": 22,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 19
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\n(2017) moti-\nvates the actor network as an approximate sampler, rather\nthan the actor in an actor-critic algorithm.",
    "chunk_index": 20,
    "num_sentences": 1,
    "chunk_size": 115,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 20
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nCrucially, the\nconvergence of this method hinges on how well this sampler\napproximates the true posterior.",
    "chunk_index": 21,
    "num_sentences": 1,
    "chunk_size": 106,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 21
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nIn contrast, we prove that\nour method converges to the optimal policy from a given\npolicy class, regardless of the policy parameterization.",
    "chunk_index": 22,
    "num_sentences": 1,
    "chunk_size": 139,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 22
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nFur-\nthermore, these prior maximum entropy methods generally\ndo not exceed the performance of state-of-the-art off-policy\nalgorithms, such as DDPG, when learning from scratch,\nthough they may have other bene\ufb01ts, such as improved ex-\nploration and ease of \ufb01ne-tuning.",
    "chunk_index": 23,
    "num_sentences": 1,
    "chunk_size": 266,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 23
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: We review prior works that draw on some of these ideas in\n\nIn our experiments, we\ndemonstrate that our soft actor-critic algorithm does in fact\nexceed the performance of prior state-of-the-art off-policy\ndeep RL methods by a wide margin.",
    "chunk_index": 24,
    "num_sentences": 1,
    "chunk_size": 178,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "We review prior works that draw on some of these ideas in",
      "section_index": 3,
      "chunk_index": 24
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Preliminaries\n\nWe \ufb01rst introduce notation and summarize the standard and\nmaximum entropy reinforcement learning frameworks. Notation\nWe address policy learning in continuous action spaces.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 173,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Preliminaries",
      "section_index": 4,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Preliminaries\n\nWe consider an in\ufb01nite-horizon Markov decision process\n(MDP), de\ufb01ned by the tuple (S, A, p, r), where the state\nspace S and the action space A are continuous, and the\nunknown state transition probability p : S \u00d7 S \u00d7 A \u2192\n[0, \u221e) represents the probability density of the next state\nst+1 \u2208S given the current state st \u2208S and action at \u2208A.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 335,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Preliminaries",
      "section_index": 4,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Preliminaries\n\nThe environment emits a bounded reward r : S \u00d7 A \u2192\n[rmin, rmax] on each transition.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 83,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Preliminaries",
      "section_index": 4,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Preliminaries\n\nWe will use \u03c1\u03c0(st) and\n\u03c1\u03c0(st, at) to denote the state and state-action marginals of\nthe trajectory distribution induced by a policy \u03c0(at|st).",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 141,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Preliminaries",
      "section_index": 4,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Preliminaries\n\nMaximum Entropy Reinforcement Learning",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Preliminaries",
      "section_index": 4,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Standard RL maximizes the expected sum of rewards\n\nP\nt E(st,at)\u223c\u03c1\u03c0 [r(st, at)].",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 28,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Standard RL maximizes the expected sum of rewards",
      "section_index": 5,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Standard RL maximizes the expected sum of rewards\n\nWe will consider a more gen-\neral maximum entropy objective (see e.g.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 69,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Standard RL maximizes the expected sum of rewards",
      "section_index": 5,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Standard RL maximizes the expected sum of rewards\n\nZiebart (2010)),\nwhich favors stochastic policies by augmenting the objective\nwith the expected entropy of the policy over \u03c1\u03c0(st):\nJ(\u03c0) =",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 137,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Standard RL maximizes the expected sum of rewards",
      "section_index": 5,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: T\nX\n\nt=0\nE(st,at)\u223c\u03c1\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))] .",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 45,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "T\nX",
      "section_index": 6,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: T\nX\n\n(1)\nThe temperature parameter \u03b1 determines the relative im-\nportance of the entropy term against the reward, and thus\ncontrols the stochasticity of the optimal policy.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 167,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "T\nX",
      "section_index": 6,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: T\nX\n\nThe maxi-\nmum entropy objective differs from the standard maximum\nexpected reward objective used in conventional reinforce-\nment learning, though the conventional objective can be\nrecovered in the limit as \u03b1 \u21920.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 211,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "T\nX",
      "section_index": 6,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: T\nX\n\nFor the rest of this paper,\nwe will omit writing the temperature explicitly, as it can\nalways be subsumed into the reward by scaling it by \u03b1\u22121.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 143,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "T\nX",
      "section_index": 6,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective has a number of conceptual and practical\n\nadvantages.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective has a number of conceptual and practical",
      "section_index": 7,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective has a number of conceptual and practical\n\nFirst, the policy is incentivized to explore more\nwidely, while giving up on clearly unpromising avenues. Second, the policy can capture multiple modes of near-\noptimal behavior. In problem settings where multiple ac-\ntions seem equally attractive, the policy will commit equal\nprobability mass to those actions.",
    "chunk_index": 1,
    "num_sentences": 3,
    "chunk_size": 312,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective has a number of conceptual and practical",
      "section_index": 7,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective has a number of conceptual and practical\n\nLastly, prior work has ob-\nserved improved exploration with this objective (Haarnoja\net al., 2017; Schulman et al., 2017a), and in our experi-\nments, we observe that it considerably improves learning\nspeed over state-of-art methods that optimize the conven-\ntional RL objective function.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 287,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective has a number of conceptual and practical",
      "section_index": 7,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective has a number of conceptual and practical\n\nWe can extend the objective to\nin\ufb01nite horizon problems by introducing a discount factor \u03b3\nto ensure that the sum of expected rewards and entropies is\n\ufb01nite. Writing down the maximum entropy objective for the\nin\ufb01nite horizon discounted case is more involved (Thomas,\n2014) and is deferred to Appendix A.",
    "chunk_index": 3,
    "num_sentences": 2,
    "chunk_size": 303,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective has a number of conceptual and practical",
      "section_index": 7,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective has a number of conceptual and practical\n\nPrior methods have proposed directly solving for the op-\ntimal Q-function, from which the optimal policy can be\nrecovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja\net al., 2017). We will discuss how we can devise a soft\nactor-critic algorithm through a policy iteration formulation,\nwhere we instead evaluate the Q-function of the current\npolicy and update the policy through an off-policy gradient\nupdate. Though such algorithms have previously been pro-\nposed for conventional reinforcement learning, our method\nis, to our knowledge, the \ufb01rst off-policy actor-critic method\nin the maximum entropy reinforcement learning framework.",
    "chunk_index": 4,
    "num_sentences": 3,
    "chunk_size": 640,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective has a number of conceptual and practical",
      "section_index": 7,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nActor-Critic\nOur off-policy soft actor-critic algorithm can be derived\nstarting from a maximum entropy variant of the policy it-\neration method. We will \ufb01rst present this derivation, verify\nthat the corresponding algorithm converges to the optimal\npolicy from its density class, and then present a practical\ndeep reinforcement learning algorithm based on this theory.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 367,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nSoft Actor-Critic\n4.1.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 22,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nDerivation of Soft Policy Iteration\nWe will begin by deriving soft policy iteration, a general al-\ngorithm for learning optimal maximum entropy policies that\nalternates between policy evaluation and policy improve-\nment in the maximum entropy framework.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 253,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nOur derivation\nis based on a tabular setting, to enable theoretical analysis\nand convergence guarantees, and we extend this method\ninto the general continuous setting in the next section.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 187,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nWe\nwill show that soft policy iteration converges to the optimal\npolicy within a set of policies which might correspond, for\ninstance, to a set of parameterized densities. In the policy evaluation step of soft policy iteration, we\nwish to compute the value of a policy \u03c0 according to the\nmaximum entropy objective in Equation 1. For a \ufb01xed\npolicy, the soft Q-value can be computed iteratively, starting\nfrom any function Q : S \u00d7 A \u2192R and repeatedly applying\na modi\ufb01ed Bellman backup operator T \u03c0 given by\nT \u03c0Q(st, at) \u225cr(st, at) + \u03b3 Est+1\u223cp [V (st+1)] ,\n(2)\nwhere\nV (st) = Eat\u223c\u03c0 [Q(st, at) \u2212log \u03c0(at|st)]\n(3)\nis the soft state value function. We can obtain the soft value\nfunction for any policy \u03c0 by repeatedly applying T \u03c0 as\nformalized below. Lemma 1 (Soft Policy Evaluation).",
    "chunk_index": 4,
    "num_sentences": 5,
    "chunk_size": 779,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nConsider the soft Bell-\nman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S\u00d7A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nThen the sequence Qk will converge to the soft Q-value of\n\u03c0 as k \u2192\u221e.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 68,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nSee Appendix B.1.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nIn the policy improvement step, we update the policy to-\nwards the exponential of the new Q-function. This particular\nchoice of update can be guaranteed to result in an improved\npolicy in terms of its soft value.",
    "chunk_index": 8,
    "num_sentences": 2,
    "chunk_size": 212,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nSince in practice we prefer\npolicies that are tractable, we will additionally restrict the\npolicy to some set of policies \u03a0, which can correspond, for\nexample, to a parameterized family of distributions such as\nGaussians. To account for the constraint that \u03c0 \u2208\u03a0, we\nproject the improved policy into the desired set of policies.",
    "chunk_index": 9,
    "num_sentences": 2,
    "chunk_size": 327,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nWhile in principle we could choose any projection, it will\nturn out to be convenient to use the information projection\nde\ufb01ned in terms of the Kullback-Leibler divergence.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 170,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nIn the\nother words, in the policy improvement step, for each state,\nwe update the policy according to\n\u03c0new = arg min\n\u03c0\u2032\u2208\u03a0DKL\n\u0012\n\u03c0\u2032( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03c0old(st, \u00b7 ))\nZ\u03c0old(st)\n\u0013\n. (4)\nThe partition function Z\u03c0old(st) normalizes the distribution,\nand while it is intractable in general, it does not contribute to\nthe gradient with respect to the new policy and can thus be\nignored, as noted in the next section.",
    "chunk_index": 11,
    "num_sentences": 2,
    "chunk_size": 407,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nFor this projection, we\ncan show that the new, projected policy has a higher value\nthan the old policy with respect to the objective in Equa-\ntion 1.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 149,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nWe formalize this result in Lemma 2.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 36,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nLemma 2 (Soft Policy Improvement).",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 34,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 14
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nLet \u03c0old \u2208\u03a0 and let\n\u03c0new be the optimizer of the minimization problem de\ufb01ned\nin Equation 4.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 91,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 15
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nThen Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all\n(st, at) \u2208S \u00d7 A with |A| < \u221e.",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 71,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 16
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: From Soft Policy Iteration to Soft\n\nSee Appendix B.2.",
    "chunk_index": 17,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "From Soft Policy Iteration to Soft",
      "section_index": 8,
      "chunk_index": 17
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nthe soft policy evaluation and the soft policy improvement\nsteps, and it will provably converge to the optimal maxi-\nmum entropy policy among the policies in \u03a0 (Theorem 1).",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 172,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nAlthough this algorithm will provably \ufb01nd the optimal solu-\ntion, we can perform it in its exact form only in the tabular\ncase.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 127,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nTherefore, we will next approximate the algorithm for\ncontinuous domains, where we need to rely on a function\napproximator to represent the Q-values, and running the\ntwo steps until convergence would be computationally too\nexpensive.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 233,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThe approximation gives rise to a new practical\nalgorithm, called soft actor-critic. Theorem 1 (Soft Policy Iteration). Repeated application of\nsoft policy evaluation and soft policy improvement from any\n\u03c0 \u2208\u03a0 converges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265\nQ\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming\n|A| < \u221e.",
    "chunk_index": 3,
    "num_sentences": 3,
    "chunk_size": 319,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nSee Appendix B.3.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nSoft Actor-Critic\nAs discussed above, large continuous domains require us to\nderive a practical approximation to soft policy iteration.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 135,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nTo\nthat end, we will use function approximators for both the\nQ-function and the policy, and instead of running evaluation\nand improvement to convergence, alternate between opti-\nmizing both networks with stochastic gradient descent. We\nwill consider a parameterized state value function V\u03c8(st),\nsoft Q-function Q\u03b8(st, at), and a tractable policy \u03c0\u03c6(at|st).",
    "chunk_index": 6,
    "num_sentences": 2,
    "chunk_size": 356,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThe parameters of these networks are \u03c8, \u03b8, and \u03c6.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nFor\nexample, the value functions can be modeled as expressive\nneural networks, and the policy as a Gaussian with mean\nand covariance given by neural networks.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 158,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nWe will next\nderive update rules for these parameter vectors.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThe state value function approximates the soft value. There\nis no need in principle to include a separate function approx-\nimator for the state value, since it is related to the Q-function\nand policy according to Equation 3.",
    "chunk_index": 10,
    "num_sentences": 2,
    "chunk_size": 224,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThis quantity can be\n\n\nSoft Actor-Critic\nestimated from a single action sample from the current pol-\nicy without introducing a bias, but in practice, including a\nseparate function approximator for the soft value can stabi-\nlize training and is convenient to train simultaneously with\nthe other networks. The soft value function is trained to\nminimize the squared residual error\nJV (\u03c8) = Est\u223cD\nh\n1\n2\n\u0000V\u03c8(st) \u2212Eat\u223c\u03c0\u03c6 [Q\u03b8(st, at) \u2212log \u03c0\u03c6(at|st)]\n\u00012i\n(5)\nwhere D is the distribution of previously sampled states and\nactions, or a replay buffer.",
    "chunk_index": 11,
    "num_sentences": 2,
    "chunk_size": 540,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThe gradient of Equation 5 can\nbe estimated with an unbiased estimator\n\u02c6\u2207\u03c8JV (\u03c8) = \u2207\u03c8V\u03c8(st) (V\u03c8(st) \u2212Q\u03b8(st, at) + log \u03c0\u03c6(at|st)) ,\n(6)\nwhere the actions are sampled according to the current pol-\nicy, instead of the replay buffer.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 229,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nThe soft Q-function param-\neters can be trained to minimize the soft Bellman residual\nJQ(\u03b8) = E(st,at)\u223cD\n\u00141\n2\n\u0010\nQ\u03b8(st, at) \u2212\u02c6Q(st, at)\n\u00112\u0015\n,\n(7)\nwith\n\u02c6Q(st, at) = r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nV \u00af\n\u03c8(st+1)\n\u0003\n,\n(8)\nwhich again can be optimized with stochastic gradients\n\u02c6\u2207\u03b8JQ(\u03b8) = \u2207\u03b8Q\u03b8(at, st)\n\u0000Q\u03b8(st, at) \u2212r(st, at) \u2212\u03b3V \u00af\n\u03c8(st+1)\n\u0001.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 325,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\n(9)\nThe update makes use of a target value network V \u00af\n\u03c8, where\n\u00af\u03c8 can be an exponentially moving average of the value\nnetwork weights, which has been shown to stabilize train-\ning (Mnih et al., 2015).",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 201,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 14
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nAlternatively, we can update the\ntarget weights to match the current value function weights\nperiodically (see Appendix E).",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 122,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 15
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The full soft policy iteration algorithm alternates between\n\nFinally, the policy param-\neters can be learned by directly minimizing the expected\nKL-divergence in Equation 4:\nJ\u03c0(\u03c6) = Est\u223cD\n\u0014",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 128,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The full soft policy iteration algorithm alternates between",
      "section_index": 9,
      "chunk_index": 16
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\n\u0012\n\u03c0\u03c6( \u00b7 |st)\n\r\r\r\r\nexp (Q\u03b8(st, \u00b7 ))\nZ\u03b8(st)\n\u0013\u0015\n.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 46,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\n(10)\nThere are several options for minimizing J\u03c0.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nA typical\nsolution for policy gradient methods is to use the likelihood\nratio gradient estimator (Williams, 1992), which does not\nrequire backpropagating the gradient through the policy and\nthe target density networks.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 218,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nHowever, in our case, the target\ndensity is the Q-function, which is represented by a neural\nnetwork an can be differentiated, and it is thus convenient\nto apply the reparameterization trick instead, resulting in a\nlower variance estimator.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 240,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nTo that end, we reparameterize\nthe policy using a neural network transformation\nat = f\u03c6(\u03f5t; st),\n(11)\nAlgorithm 1 Soft Actor-Critic\nInitialize parameter vectors \u03c8, \u00af\u03c8, \u03b8, \u03c6.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 173,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nfor each iteration do\nfor each environment step do\nat \u223c\u03c0\u03c6(at|st)\nst+1 \u223cp(st+1|st, at)\nD \u2190D \u222a{(st, at, r(st, at), st+1)}\nend for\nfor each gradient step do\n\u03c8 \u2190\u03c8 \u2212\u03bbV \u02c6\u2207\u03c8JV (\u03c8)\n\u03b8i \u2190\u03b8i \u2212\u03bbQ \u02c6\u2207\u03b8iJQ(\u03b8i) for i \u2208{1, 2}\n\u03c6 \u2190\u03c6 \u2212\u03bb\u03c0 \u02c6\u2207\u03c6J\u03c0(\u03c6)\n\u00af\u03c8 \u2190\u03c4\u03c8 + (1 \u2212\u03c4) \u00af\u03c8\nend for\nend for\nwhere \u03f5t is an input noise vector, sampled from some \ufb01xed\ndistribution, such as a spherical Gaussian.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 363,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nWe can now\nrewrite the objective in Equation 10 as\nJ\u03c0(\u03c6) = Est\u223cD,\u03f5t\u223cN [log \u03c0\u03c6(f\u03c6(\u03f5t; st)|st) \u2212Q\u03b8(st, f\u03c6(\u03f5t; st))] ,\n(12)\nwhere \u03c0\u03c6 is de\ufb01ned implicitly in terms of f\u03c6, and we have\nnoted that the partition function is independent of \u03c6 and can\nthus be omitted.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 257,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nWe can approximate the gradient of Equa-\ntion 12 with\n\u02c6\u2207\u03c6J\u03c0(\u03c6) = \u2207\u03c6 log \u03c0\u03c6(at|st)\n+ (\u2207at log \u03c0\u03c6(at|st) \u2212\u2207atQ(st, at))\u2207\u03c6f\u03c6(\u03f5t; st),\n(13)\nwhere at is evaluated at f\u03c6(\u03f5t; st).",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 172,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nThis unbiased gradient\nestimator extends the DDPG style policy gradients (Lillicrap\net al., 2015) to any tractable stochastic policy.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 133,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nOur algorithm also makes use of two Q-functions to mitigate\npositive bias in the policy improvement step that is known\nto degrade performance of value based methods (Hasselt,\n2010; Fujimoto et al., 2018). In particular, we parameterize\ntwo Q-functions, with parameters \u03b8i, and train them inde-\npendently to optimize JQ(\u03b8i). We then use the minimum of\nthe Q-functions for the value gradient in Equation 6 and pol-\nicy gradient in Equation 13, as proposed by Fujimoto et al.",
    "chunk_index": 9,
    "num_sentences": 3,
    "chunk_size": 472,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nAlthough our algorithm can learn challenging tasks,\nincluding a 21-dimensional Humanoid, using just a single\nQ-function, we found two Q-functions signi\ufb01cantly speed\nup training, especially on harder tasks.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 205,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nThe complete algo-\nrithm is described in Algorithm 1.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 53,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nThe method alternates\nbetween collecting experience from the environment with\nthe current policy and updating the function approximators\nusing the stochastic gradients from batches sampled from a\nreplay buffer.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 210,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: DKL\n\nIn practice, we take a single environment step\nfollowed by one or several gradient steps (see Appendix D\n\n\nSoft Actor-Critic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 706,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "DKL",
      "section_index": 10,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SQL\n\nTD3 (concurrent)\n(f) Humanoid (rllab)\nFigure 1.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SQL",
      "section_index": 11,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SQL\n\nTraining curves on continuous control benchmarks.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SQL",
      "section_index": 11,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SQL\n\nSoft actor-critic (yellow) performs consistently across all tasks and\noutperforming both on-policy and off-policy methods in the most challenging tasks.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 152,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SQL",
      "section_index": 11,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SQL\n\nfor all hyperparameter).",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 24,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SQL",
      "section_index": 11,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SQL\n\nUsing off-policy data from a replay\nbuffer is feasible because both value estimators and the pol-\nicy can be trained entirely on off-policy data.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 145,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SQL",
      "section_index": 11,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SQL\n\nThe algorithm\nis agnostic to the parameterization of the policy, as long as\nit can be evaluated for any arbitrary state-action tuple.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 133,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SQL",
      "section_index": 11,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nhow the sample complexity and stability of our method\ncompares with prior off-policy and on-policy deep rein-\nforcement learning algorithms.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 140,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe compare our method\nto prior techniques on a range of challenging continuous\ncontrol tasks from the OpenAI gym benchmark suite (Brock-\nman et al., 2016) and also on the rllab implementation of\nthe Humanoid task (Duan et al., 2016). Although the easier\ntasks can be solved by a wide range of different algorithms,\nthe more complex benchmarks, such as the 21-dimensional\nHumanoid (rllab), are exceptionally dif\ufb01cult to solve with\noff-policy algorithms (Duan et al., 2016).",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 472,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nThe stability of\nthe algorithm also plays a large role in performance: eas-\nier tasks make it more practical to tune hyperparameters\nto achieve good results, while the already narrow basins of\neffective hyperparameters become prohibitively small for\nthe more sensitive algorithms on the hardest benchmarks,\nleading to poor performance (Gu et al., 2016).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 353,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe compare our method to deep deterministic policy gra-\ndient (DDPG) (Lillicrap et al., 2015), an algorithm that\nis regarded as one of the more ef\ufb01cient off-policy deep\nRL methods (Duan et al., 2016); proximal policy optimiza-\ntion (PPO) (Schulman et al., 2017b), a stable and effective\non-policy policy gradient algorithm; and soft Q-learning\n(SQL) (Haarnoja et al., 2017), a recent off-policy algorithm\nfor learning maximum entropy policies.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 443,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nOur SQL imple-\nmentation also includes two Q-functions, which we found\nto improve its performance in most environments.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 119,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe addi-\ntionally compare to twin delayed deep deterministic policy\ngradient algorithm (TD3) (Fujimoto et al., 2018), using\nthe author-provided implementation.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 159,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nThis is an extension\nto DDPG, proposed concurrently to our method, that \ufb01rst\napplied the double Q-learning trick to continuous control\nalong with other improvements.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 165,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe have included trust re-\ngion path consistency learning (Trust-PCL) (Nachum et al.,\n2017b) and two other variants of SAC in Appendix E.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 137,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe\nturned off the exploration noise for evaluation for DDPG\nand PPO. For maximum entropy algorithms, which do not\nexplicitly inject exploration noise, we either evaluated with\nthe exploration noise (SQL) or use the mean action (SAC).",
    "chunk_index": 8,
    "num_sentences": 2,
    "chunk_size": 233,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nThe source code of our SAC implementation1 and videos2\nare available online.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 76,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\n1github.com/haarnoja/sac\n2sites.google.com/view/soft-actor-critic\n\n\nSoft Actor-Critic\n5.1.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 90,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nComparative Evaluation\nFigure 1 shows the total average return of evaluation rollouts\nduring training for DDPG, PPO, and TD3.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nWe train \ufb01ve\ndifferent instances of each algorithm with different random\nseeds, with each performing one evaluation rollout every\n1000 environment steps.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 153,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nThe solid curves corresponds to the\nmean and the shaded region to the minimum and maximum\nreturns over the \ufb01ve trials.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 118,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nThe results show that, overall, SAC performs comparably\nto the baseline methods on the easier tasks and outperforms\nthem on the harder tasks with a large margin, both in terms\nof learning speed and the \ufb01nal performance.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 219,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 14
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nFor example,\nDDPG fails to make any progress on Ant-v1, Humanoid-\nv1, and Humanoid (rllab), a result that is corroborated by\nprior work (Gu et al., 2016; Duan et al., 2016).",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 173,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 15
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The goal of our experimental evaluation is to understand\n\nSAC also\nlearns considerably faster than PPO as a consequence of\nthe large batch sizes PPO needs to learn stably on more\nhigh-dimensional and complex tasks. Another maximum\nentropy RL algorithm, SQL, can also learn all tasks, but it\nis slower than SAC and has worse asymptotic performance.",
    "chunk_index": 16,
    "num_sentences": 2,
    "chunk_size": 289,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The goal of our experimental evaluation is to understand",
      "section_index": 12,
      "chunk_index": 16
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The quantitative results attained by SAC in our experiments\n\nalso compare very favorably to results reported by other\nmethods in prior work (Duan et al., 2016; Gu et al., 2016;\nHenderson et al., 2017), indicating that both the sample\nef\ufb01ciency and \ufb01nal performance of SAC on these benchmark\ntasks exceeds the state of the art. All hyperparameters used\nin this experiment for SAC are listed in Appendix D.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 343,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The quantitative results attained by SAC in our experiments",
      "section_index": 13,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The quantitative results attained by SAC in our experiments\n\nAblation Study",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The quantitative results attained by SAC in our experiments",
      "section_index": 13,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nbased on the maximum entropy principle can outperform\nconventional RL methods on challenging tasks such as the\nhumanoid tasks.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 126,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nIn this section, we further examine which\nparticular components of SAC are important for good perfor-\nmance.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 108,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nWe also examine how sensitive SAC is to some of\nthe most important hyperparameters, namely reward scaling\nand target value update smoothing constant.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 149,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nStochastic vs.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\ndeterministic policy.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 21,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nSoft actor-critic\nlearns stochastic policies via a maximum entropy objec-\ntive.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 79,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nThe entropy appears in both the policy and value\nfunction.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 58,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nIn the policy, it prevents premature convergence of\nthe policy variance (Equation 10).",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 86,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: The results in the previous section suggest that algorithms\n\nIn the value function, it\nencourages exploration by increasing the value of regions of\nstate space that lead to high-entropy behavior (Equation 5).",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 147,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "The results in the previous section suggest that algorithms",
      "section_index": 14,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nmaximization affects the performance, we compare to a\ndeterministic variant of SAC that does not maximize the en-\ntropy and that closely resembles DDPG, with the exception\nof having two Q-functions, using hard target updates, not\nhaving a separate target actor, and using \ufb01xed rather than\nlearned exploration noise.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 315,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nFigure 2 compares \ufb01ve individual\nruns with both variants, initialized with different random\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)\nstochastic policy\ndeterministic policy\nFigure 2. Comparison of SAC (blue) and a deterministic variant of\nSAC (red) in terms of the stability of individual random seeds on\nthe Humanoid (rllab) benchmark. The comparison indicates that\nstochasticity can stabilize training as the variability between the\nseeds becomes much higher with a deterministic policy. Soft actor-critic performs much more consistently,\nwhile the deterministic variant exhibits very high variability\nacross seeds, indicating substantially worse stability.",
    "chunk_index": 1,
    "num_sentences": 4,
    "chunk_size": 693,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nAs\nevident from the \ufb01gure, learning a stochastic policy with\nentropy maximization can drastically stabilize training.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 117,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nThis\nbecomes especially important with harder tasks, where tun-\ning hyperparameters is challenging.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 99,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nIn this comparison, we\nupdated the target value network weights with hard updates,\nby periodically overwriting the target network parameters\nto match the current value network (see Appendix E for\na comparison of average performance on all benchmark\ntasks).",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 256,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: To compare how the stochasticity of the policy and entropy\n\nPolicy evaluation.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "To compare how the stochasticity of the policy and entropy",
      "section_index": 15,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\npolicies, it is often bene\ufb01cial to make the \ufb01nal policy deter-\nministic at the end for best performance.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 104,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nFor evaluation, we\napproximate the maximum a posteriori action by choosing\nthe mean of the policy distribution.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 111,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nFigure 3(a) compares\ntraining returns to evaluation returns obtained with this strat-\negy indicating that deterministic evaluation can yield better\nperformance.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 160,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nIt should be noted that all of the training\ncurves depict the sum of rewards, which is different from\nthe objective optimized by SAC and other maximum en-\ntropy RL algorithms, including SQL and Trust-PCL, which\nmaximize also the entropy of the policy.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 251,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nReward scale.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nSoft actor-critic is particularly sensitive to\nthe scaling of the reward signal, because it serves the role\nof the temperature of the energy-based optimal policy and\nthus controls its stochasticity.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 198,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nLarger reward magnitudes\ncorrespond to lower entries. Figure 3(b) shows how learn-\ning performance changes when the reward scale is varied:\nFor small reward magnitudes, the policy becomes nearly\nuniform, and consequently fails to exploit the reward signal,\nresulting in substantial degradation of performance. For\nlarge reward magnitudes, the model learns quickly at \ufb01rst,\n\n\nSoft Actor-Critic\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\ndeterministic evaluation\nstochastic evaluation\n(a) Evaluation\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n1\n3\n10\n30\n100\n(b) Reward Scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n\u22122000\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n0.0001\n0.001\n0.01\n0.1\n(c) Target Smoothing Coef\ufb01cient (\u03c4)\nFigure 3. Sensitivity of soft actor-critic to selected hyperparameters on Ant-v1 task.",
    "chunk_index": 6,
    "num_sentences": 4,
    "chunk_size": 879,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\n(a) Evaluating the policy using the mean action\ngenerally results in a higher return. Note that the policy is trained to maximize also the entropy, and the mean action does not, in general,\ncorrespond the optimal action for the maximum return objective. (b) Soft actor-critic is sensitive to reward scaling since it is related to the\ntemperature of the optimal policy. The optimal reward scale varies between environments, and should be tuned for each task separately.",
    "chunk_index": 7,
    "num_sentences": 4,
    "chunk_size": 468,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\n(c) Target value smoothing coef\ufb01cient \u03c4 is used to stabilize training. Fast moving target (large \u03c4) can result in instabilities (red), whereas\nslow moving target (small \u03c4) makes training slower (blue).",
    "chunk_index": 8,
    "num_sentences": 2,
    "chunk_size": 201,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nbut the policy then becomes nearly deterministic, leading\nto poor local minima due to lack of adequate exploration.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 115,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nWith the right reward scaling, the model balances explo-\nration and exploitation, leading to faster learning and better\nasymptotic performance. In practice, we found reward scale\nto be the only hyperparameter that requires tuning, and its\nnatural interpretation as the inverse of the temperature in\nthe maximum entropy framework provides good intuition\nfor how to adjust this parameter.",
    "chunk_index": 10,
    "num_sentences": 2,
    "chunk_size": 386,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Since SAC converges to stochastic\n\nTarget network update.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 22,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Since SAC converges to stochastic",
      "section_index": 16,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\ntarget value network that slowly tracks the actual value func-\ntion to improve stability. We use an exponentially moving\naverage, with a smoothing constant \u03c4, to update the target\nvalue network weights as common in the prior work (Lill-\nicrap et al., 2015; Mnih et al., 2015).",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 276,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\nA value of one cor-\nresponds to a hard update where the weights are copied\ndirectly at every iteration and zero to not updating the target\nat all.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 146,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\nIn Figure 3(c), we compare the performance of SAC\nwhen \u03c4 varies.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 64,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\nLarge \u03c4 can lead to instabilities while small\n\u03c4 can make training slower.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 73,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\nHowever, we found the range\nof suitable values of \u03c4 to be relatively wide and we used\nthe same value (0.005) across all of the tasks.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 133,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: It is common to use a separate\n\nIn Figure 4\n(Appendix E) we also compare to another variant of SAC,\nwhere instead of using exponentially moving average, we\ncopy over the current network weights directly into the tar-\nget network every 1000 gradient steps. We found this variant\nto bene\ufb01t from taking more than one gradient step between\nthe environment steps, which can improve performance but\nalso increases the computational cost.",
    "chunk_index": 5,
    "num_sentences": 2,
    "chunk_size": 399,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "It is common to use a separate",
      "section_index": 17,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Conclusion\n\nWe present soft actor-critic (SAC), an off-policy maximum\nentropy deep reinforcement learning algorithm that provides\nsample-ef\ufb01cient learning while retaining the bene\ufb01ts of en-\ntropy maximization and stability. Our theoretical results\nderive soft policy iteration, which we show to converge to\nthe optimal policy. From this result, we can formulate a\nsoft actor-critic algorithm, and we empirically show that it\noutperforms state-of-the-art model-free deep RL methods,\nincluding the off-policy DDPG algorithm and the on-policy\nPPO algorithm.",
    "chunk_index": 0,
    "num_sentences": 3,
    "chunk_size": 542,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Conclusion",
      "section_index": 18,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Conclusion\n\nIn fact, the sample ef\ufb01ciency of this ap-\nproach actually exceeds that of DDPG by a substantial mar-\ngin.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 105,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Conclusion",
      "section_index": 18,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Conclusion\n\nOur results suggest that stochastic, entropy maximizing\nreinforcement learning algorithms can provide a promising\navenue for improved robustness and stability, and further\nexploration of maximum entropy methods, including meth-\nods that incorporate second order information (e.g., trust\nregions (Schulman et al., 2015)) or more expressive policy\nclasses is an exciting avenue for future work.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 392,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Conclusion",
      "section_index": 18,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Acknowledgments\n\nWe would like to thank Vitchyr Pong for insightful discus-\nsions and help in implementing our algorithm as well as\nproviding the DDPG baseline code; O\ufb01r Nachum for offer-\ning support in running Trust-PCL experiments; and George",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 227,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Acknowledgments",
      "section_index": 19,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nG., Sutton, R.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nS., and Anderson, C.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 20,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nNeuronlike\nadaptive elements that can solve dif\ufb01cult learning con-\ntrol problems.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 81,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nIEEE transactions on systems, man, and\ncybernetics, pp.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\n834\u2013846, 1983.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nBhatnagar, S., Precup, D., Silver, D., Sutton, R.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nS., Maei,\nH.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nR., and Szepesv\u00b4ari, C.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 23,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: References\n\nConvergent temporal-difference\nlearning with arbitrary smooth function approximation.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 85,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "References",
      "section_index": 20,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n(NIPS), pp.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n1204\u20131212, 2009.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 96,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nOpenAI gym.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint arXiv:1606.01540, 2016.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nDuan, Y., Chen, X.",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nHouthooft, R., Schulman, J., and Abbeel,\nP.",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nBenchmarking deep reinforcement learning for contin-\nuous control.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 66,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn International Conference on Machine\nLearning (ICML), 2016.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nFox, R., Pakman, A., and Tishby, N.",
    "chunk_index": 9,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nTaming the noise in\nreinforcement learning via soft updates.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 60,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn Conference\non Uncertainty in Arti\ufb01cial Intelligence (UAI), 2016.",
    "chunk_index": 11,
    "num_sentences": 1,
    "chunk_size": 67,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nFujimoto, S., van Hoof, H., and Meger, D.",
    "chunk_index": 12,
    "num_sentences": 1,
    "chunk_size": 41,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nAddressing func-\ntion approximation error in actor-critic methods.",
    "chunk_index": 13,
    "num_sentences": 1,
    "chunk_size": 66,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv\npreprint arXiv:1802.09477, 2018.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 14
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nGruslys, A., Azar, M.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 21,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 15
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nG., Bellemare, M.",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 16
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nG., and Munos, R.",
    "chunk_index": 17,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 17
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nThe reactor: A sample-ef\ufb01cient actor-critic architecture.",
    "chunk_index": 18,
    "num_sentences": 1,
    "chunk_size": 57,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 18
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint arXiv:1704.04651, 2017.",
    "chunk_index": 19,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 19
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R.",
    "chunk_index": 20,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 20
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nE., and\nLevine, S.",
    "chunk_index": 21,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 21
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nQ-prop: Sample-ef\ufb01cient policy gradient with\nan off-policy critic.",
    "chunk_index": 22,
    "num_sentences": 1,
    "chunk_size": 66,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 22
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint arXiv:1611.02247,\n2016.",
    "chunk_index": 23,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 23
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S.",
    "chunk_index": 24,
    "num_sentences": 1,
    "chunk_size": 50,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 24
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nRein-\nforcement learning with deep energy-based policies.",
    "chunk_index": 25,
    "num_sentences": 1,
    "chunk_size": 57,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 25
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn\nInternational Conference on Machine Learning (ICML),\npp.",
    "chunk_index": 26,
    "num_sentences": 1,
    "chunk_size": 59,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 26
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n1352\u20131361, 2017.",
    "chunk_index": 27,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 27
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nHasselt, H.",
    "chunk_index": 28,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 28
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nDouble Q-learning.",
    "chunk_index": 29,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 29
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn Advances in Neural\nInformation Processing Systems (NIPS), pp.",
    "chunk_index": 30,
    "num_sentences": 1,
    "chunk_size": 64,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 30
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n2613\u20132621,\n2010.",
    "chunk_index": 31,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 31
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and\nTassa, Y.",
    "chunk_index": 32,
    "num_sentences": 1,
    "chunk_size": 72,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 32
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nLearning continuous control policies by stochas-\ntic value gradients.",
    "chunk_index": 33,
    "num_sentences": 1,
    "chunk_size": 69,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 33
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn Advances in Neural Information\nProcessing Systems (NIPS), pp.",
    "chunk_index": 34,
    "num_sentences": 1,
    "chunk_size": 64,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 34
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n2944\u20132952, 2015.",
    "chunk_index": 35,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 35
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup,\nD., and Meger, D.",
    "chunk_index": 36,
    "num_sentences": 1,
    "chunk_size": 76,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 36
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nDeep reinforcement learning that\nmatters.",
    "chunk_index": 37,
    "num_sentences": 1,
    "chunk_size": 41,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 37
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint arXiv:1709.06560, 2017.",
    "chunk_index": 38,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 38
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nAdam: A method for stochastic\noptimization.",
    "chunk_index": 39,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 39
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn International Conference for Learning\nPresentations (ICLR), 2015.",
    "chunk_index": 40,
    "num_sentences": 1,
    "chunk_size": 68,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 40
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nand Koltun, V.",
    "chunk_index": 41,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 41
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nGuided policy search.",
    "chunk_index": 42,
    "num_sentences": 1,
    "chunk_size": 21,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 42
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn Interna-\ntional Conference on Machine Learning (ICML), pp.",
    "chunk_index": 43,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 43
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nLevine, S., Finn, C., Darrell, T., and Abbeel, P.",
    "chunk_index": 44,
    "num_sentences": 1,
    "chunk_size": 49,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 44
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nEnd-to-end\ntraining of deep visuomotor policies.",
    "chunk_index": 45,
    "num_sentences": 1,
    "chunk_size": 48,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 45
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nJournal of Machine\nLearning Research, 17(39):1\u201340, 2016.",
    "chunk_index": 46,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 46
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nLillicrap, T.",
    "chunk_index": 47,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 47
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nP., Hunt, J.",
    "chunk_index": 48,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 48
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nJ., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D.",
    "chunk_index": 49,
    "num_sentences": 1,
    "chunk_size": 77,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 49
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nContinuous\ncontrol with deep reinforcement learning.",
    "chunk_index": 50,
    "num_sentences": 1,
    "chunk_size": 52,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 50
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint\narXiv:1509.02971, 2015.",
    "chunk_index": 51,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 51
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M.",
    "chunk_index": 52,
    "num_sentences": 1,
    "chunk_size": 99,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 52
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nPlaying\natari with deep reinforcement learning.",
    "chunk_index": 53,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 53
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint\narXiv:1312.5602, 2013.",
    "chunk_index": 54,
    "num_sentences": 1,
    "chunk_size": 37,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 54
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.",
    "chunk_index": 55,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 55
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nA., Veness,\nJ., Bellemare, M.",
    "chunk_index": 56,
    "num_sentences": 1,
    "chunk_size": 29,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 56
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nG., Graves, A., Riedmiller, M., Fidje-\nland, A.",
    "chunk_index": 57,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 57
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nK., Ostrovski, G., et al.",
    "chunk_index": 58,
    "num_sentences": 1,
    "chunk_size": 25,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 58
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nHuman-level control\nthrough deep reinforcement learning.",
    "chunk_index": 59,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 59
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nNature, 518(7540):\n529\u2013533, 2015.",
    "chunk_index": 60,
    "num_sentences": 1,
    "chunk_size": 33,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 60
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMnih, V., Badia, A.",
    "chunk_index": 61,
    "num_sentences": 1,
    "chunk_size": 19,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 61
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nP., Mirza, M., Graves, A., Lillicrap,\nT.",
    "chunk_index": 62,
    "num_sentences": 1,
    "chunk_size": 40,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 62
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nP., Harley, T., Silver, D., and Kavukcuoglu, K.",
    "chunk_index": 63,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 63
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nAsyn-\nchronous methods for deep reinforcement learning.",
    "chunk_index": 64,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 64
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn\nInternational Conference on Machine Learning (ICML),\n2016.",
    "chunk_index": 65,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 65
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.",
    "chunk_index": 66,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 66
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nBridging the gap between value and policy based rein-\nforcement learning.",
    "chunk_index": 67,
    "num_sentences": 1,
    "chunk_size": 73,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 67
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn Advances in Neural Information\nProcessing Systems (NIPS), pp.",
    "chunk_index": 68,
    "num_sentences": 1,
    "chunk_size": 64,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 68
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n2772\u20132782, 2017a.",
    "chunk_index": 69,
    "num_sentences": 1,
    "chunk_size": 17,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 69
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.",
    "chunk_index": 70,
    "num_sentences": 1,
    "chunk_size": 51,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 70
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nTrust-PCL: An off-policy trust region method for contin-\nuous control.",
    "chunk_index": 71,
    "num_sentences": 1,
    "chunk_size": 70,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 71
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint arXiv:1707.01891, 2017b.",
    "chunk_index": 72,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 72
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nO\u2019Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.",
    "chunk_index": 73,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 73
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nPGQ: Combining policy gradient and Q-learning.",
    "chunk_index": 74,
    "num_sentences": 1,
    "chunk_size": 46,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 74
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv\npreprint arXiv:1611.01626, 2016.",
    "chunk_index": 75,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 75
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nand Schaal, S.",
    "chunk_index": 76,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 76
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nReinforcement learning of motor\nskills with policy gradients.",
    "chunk_index": 77,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 77
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nNeural networks, 21(4):682\u2013\n697, 2008.",
    "chunk_index": 78,
    "num_sentences": 1,
    "chunk_size": 38,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 78
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nRawlik, K., Toussaint, M., and Vijayakumar, S.",
    "chunk_index": 79,
    "num_sentences": 1,
    "chunk_size": 46,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 79
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nOn stochas-\ntic optimal control and reinforcement learning by approx-\nimate inference.",
    "chunk_index": 80,
    "num_sentences": 1,
    "chunk_size": 86,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 80
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nRobotics: Science and Systems (RSS),\n2012.",
    "chunk_index": 81,
    "num_sentences": 1,
    "chunk_size": 42,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 81
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M.",
    "chunk_index": 82,
    "num_sentences": 1,
    "chunk_size": 48,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 82
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nI., and\nMoritz, P.",
    "chunk_index": 83,
    "num_sentences": 1,
    "chunk_size": 18,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 83
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nTrust region policy optimization.",
    "chunk_index": 84,
    "num_sentences": 1,
    "chunk_size": 33,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 84
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn Inter-\nnational Conference on Machine Learning (ICML), pp.",
    "chunk_index": 85,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 85
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n1889\u20131897, 2015.",
    "chunk_index": 86,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 86
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSoft Actor-Critic\nSchulman, J., Abbeel, P., and Chen, X.",
    "chunk_index": 87,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 87
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nEquivalence be-\ntween policy gradients and soft Q-learning.",
    "chunk_index": 88,
    "num_sentences": 1,
    "chunk_size": 59,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 88
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint\narXiv:1704.06440, 2017a.",
    "chunk_index": 89,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 89
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O.",
    "chunk_index": 90,
    "num_sentences": 1,
    "chunk_size": 67,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 90
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nProximal policy optimization algorithms.",
    "chunk_index": 91,
    "num_sentences": 1,
    "chunk_size": 40,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 91
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\narXiv preprint arXiv:1707.06347, 2017b.",
    "chunk_index": 92,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 92
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,\nand Riedmiller, M.",
    "chunk_index": 93,
    "num_sentences": 1,
    "chunk_size": 78,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 93
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nDeterministic policy gradient algo-\nrithms.",
    "chunk_index": 94,
    "num_sentences": 1,
    "chunk_size": 43,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 94
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn International Conference on Machine Learning\n(ICML), 2014.",
    "chunk_index": 95,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 95
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSilver, D., Huang, A., Maddison, C.",
    "chunk_index": 96,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 96
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nJ., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD.",
    "chunk_index": 97,
    "num_sentences": 1,
    "chunk_size": 256,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 97
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMastering the game of go with deep neural networks\nand tree search.",
    "chunk_index": 98,
    "num_sentences": 1,
    "chunk_size": 67,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 98
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nNature, 529(7587):484\u2013489, Jan 2016.",
    "chunk_index": 99,
    "num_sentences": 1,
    "chunk_size": 36,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 99
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nISSN 0028-0836.",
    "chunk_index": 100,
    "num_sentences": 1,
    "chunk_size": 15,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 100
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nand Barto, A.",
    "chunk_index": 101,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 101
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nReinforcement learning: An\nintroduction, volume 1.",
    "chunk_index": 102,
    "num_sentences": 1,
    "chunk_size": 50,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 102
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMIT press Cambridge, 1998.",
    "chunk_index": 103,
    "num_sentences": 1,
    "chunk_size": 26,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 103
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nBias in natural actor-critic algorithms.",
    "chunk_index": 104,
    "num_sentences": 1,
    "chunk_size": 40,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 104
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn Inter-\nnational Conference on Machine Learning (ICML), pp.",
    "chunk_index": 105,
    "num_sentences": 1,
    "chunk_size": 61,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 105
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n441\u2013448, 2014.",
    "chunk_index": 106,
    "num_sentences": 1,
    "chunk_size": 14,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 106
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nTodorov, E.",
    "chunk_index": 107,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 107
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nGeneral duality between optimal control and\nestimation.",
    "chunk_index": 108,
    "num_sentences": 1,
    "chunk_size": 55,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 108
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn IEEE Conference on Decision and Control\n(CDC), pp. IEEE, 2008.",
    "chunk_index": 109,
    "num_sentences": 2,
    "chunk_size": 65,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 109
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nToussaint, M.",
    "chunk_index": 110,
    "num_sentences": 1,
    "chunk_size": 13,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 110
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nRobot trajectory optimization using approxi-\nmate inference.",
    "chunk_index": 111,
    "num_sentences": 1,
    "chunk_size": 60,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 111
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn International Conference on Machine\nLearning (ICML), pp.",
    "chunk_index": 112,
    "num_sentences": 1,
    "chunk_size": 59,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 112
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nWilliams, R.",
    "chunk_index": 113,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 113
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSimple statistical gradient-following algo-\nrithms for connectionist reinforcement learning.",
    "chunk_index": 114,
    "num_sentences": 1,
    "chunk_size": 92,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 114
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMachine\nlearning, 8(3-4):229\u2013256, 1992.",
    "chunk_index": 115,
    "num_sentences": 1,
    "chunk_size": 39,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 115
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nZiebart, B.",
    "chunk_index": 116,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 116
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nModeling purposeful adaptive behavior with\nthe principle of maximum causal entropy.",
    "chunk_index": 117,
    "num_sentences": 1,
    "chunk_size": 83,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 117
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nCarnegie Mel-\nlon University, 2010.",
    "chunk_index": 118,
    "num_sentences": 1,
    "chunk_size": 35,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 118
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nZiebart, B.",
    "chunk_index": 119,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 119
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nD., Maas, A.",
    "chunk_index": 120,
    "num_sentences": 1,
    "chunk_size": 12,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 120
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nL., Bagnell, J.",
    "chunk_index": 121,
    "num_sentences": 1,
    "chunk_size": 15,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 121
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nA., and Dey, A.",
    "chunk_index": 122,
    "num_sentences": 1,
    "chunk_size": 15,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 122
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMaximum entropy inverse reinforcement learning.",
    "chunk_index": 123,
    "num_sentences": 1,
    "chunk_size": 47,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 123
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nIn\nAAAI Conference on Arti\ufb01cial Intelligence (AAAI), pp.",
    "chunk_index": 124,
    "num_sentences": 1,
    "chunk_size": 56,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 124
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\n1433\u20131438, 2008.",
    "chunk_index": 125,
    "num_sentences": 1,
    "chunk_size": 16,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 125
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nSoft Actor-Critic\nA.",
    "chunk_index": 126,
    "num_sentences": 1,
    "chunk_size": 20,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 126
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nMaximum Entropy Objective\nThe exact de\ufb01nition of the discounted maximum entropy objective is complicated by the fact that, when using a discount\nfactor for policy gradient methods, we typically do not discount the state distribution, only the rewards. In that sense,\ndiscounted policy gradients typically do not optimize the true discounted objective. Instead, they optimize average reward,\nwith the discount serving to reduce variance, as discussed by Thomas (2014).",
    "chunk_index": 127,
    "num_sentences": 3,
    "chunk_size": 467,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 127
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: In Advances in Neural Information Processing Systems\n\nHowever, we can de\ufb01ne the objective that is\noptimized under a discount factor as\nJ(\u03c0) =\n\u221e\nX\nt=0\nE(st,at)\u223c\u03c1\u03c0\n\" \u221e\nX\nl=t\n\u03b3l\u2212t Esl\u223cp,al\u223c\u03c0 [r(st, at) + \u03b1H(\u03c0( \u00b7 |st))|st, at]\n#\n.",
    "chunk_index": 128,
    "num_sentences": 1,
    "chunk_size": 172,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "In Advances in Neural Information Processing Systems",
      "section_index": 21,
      "chunk_index": 128
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nevery state-action tuple (st, at) weighted by its probability \u03c1\u03c0 under the current policy.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 90,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nProofs\nB.1.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 11,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nLemma 1\nLemma 1 (Soft Policy Evaluation).",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 41,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nConsider the soft Bellman backup operator T \u03c0 in Equation 2 and a mapping\nQ0 : S \u00d7 A \u2192R with |A| < \u221e, and de\ufb01ne Qk+1 = T \u03c0Qk.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nThen the sequence Qk will converge to the soft Q-value of \u03c0\nas k \u2192\u221e.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 68,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nDe\ufb01ne the entropy augmented reward as r\u03c0(st, at) \u225cr(st, at) + Est+1\u223cp [H (\u03c0( \u00b7 |st+1))] and rewrite the update\nrule as\nQ(st, at) \u2190r\u03c0(st, at) + \u03b3 Est+1\u223cp,at+1\u223c\u03c0 [Q(st+1, at+1)]\n(15)\nand apply the standard convergence results for policy evaluation (Sutton & Barto, 1998). The assumption |A| < \u221eis\nrequired to guarantee that the entropy augmented reward is bounded.",
    "chunk_index": 5,
    "num_sentences": 2,
    "chunk_size": 362,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nLemma 2\nLemma 2 (Soft Policy Improvement).",
    "chunk_index": 6,
    "num_sentences": 1,
    "chunk_size": 42,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 6
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nLet \u03c0old \u2208\u03a0 and let \u03c0new be the optimizer of the minimization problem de\ufb01ned in\nEquation 4.",
    "chunk_index": 7,
    "num_sentences": 1,
    "chunk_size": 91,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 7
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nThen Q\u03c0new(st, at) \u2265Q\u03c0old(st, at) for all (st, at) \u2208S \u00d7 A with |A| < \u221e.",
    "chunk_index": 8,
    "num_sentences": 1,
    "chunk_size": 71,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 8
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nLet \u03c0old \u2208\u03a0 and let Q\u03c0old and V \u03c0old be the corresponding soft state-action value and soft state value, and let \u03c0new\nbe de\ufb01ned as\n\u03c0new( \u00b7 |st) = arg min\n\u03c0\u2032\u2208\u03a0 DKL (\u03c0\u2032( \u00b7 |st) \u2225exp (Q\u03c0old(st, \u00b7 ) \u2212log Z\u03c0old(st)))\n= arg min\n\u03c0\u2032\u2208\u03a0 J\u03c0old(\u03c0\u2032( \u00b7 |st)). (16)\nIt must be the case that J\u03c0old(\u03c0new( \u00b7 |st)) \u2264J\u03c0old(\u03c0old( \u00b7 |st)), since we can always choose \u03c0new = \u03c0old \u2208\u03a0. Hence\nEat\u223c\u03c0new [log \u03c0new(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)] \u2264Eat\u223c\u03c0old [log \u03c0old(at|st) \u2212Q\u03c0old(st, at) + log Z\u03c0old(st)],\n(17)\nand since partition function Z\u03c0old depends only on the state, the inequality reduces to\nEat\u223c\u03c0new [Q\u03c0old(st, at) \u2212log \u03c0new(at|st)] \u2265V \u03c0old(st). (18)\nNext, consider the soft Bellman equation:\nQ\u03c0old(st, at) = r(st, at) + \u03b3 Est+1\u223cp [V \u03c0old(st+1)]\n\u2264r(st, at) + \u03b3 Est+1\u223cp\n\u0002\nEat+1\u223c\u03c0new [Q\u03c0old(st+1, at+1) \u2212log \u03c0new(at+1|st+1)]\n\u0003\n... \u2264Q\u03c0new(st, at),\n(19)\nwhere we have repeatedly expanded Q\u03c0old on the RHS by applying the soft Bellman equation and the bound in Equation 18. Convergence to Q\u03c0new follows from Lemma 1.",
    "chunk_index": 9,
    "num_sentences": 6,
    "chunk_size": 997,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 9
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nSoft Actor-Critic\nB.3.",
    "chunk_index": 10,
    "num_sentences": 1,
    "chunk_size": 22,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 10
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nTheorem 1\nTheorem 1 (Soft Policy Iteration). Repeated application of soft policy evaluation and soft policy improvement to any \u03c0 \u2208\u03a0\nconverges to a policy \u03c0\u2217such that Q\u03c0\u2217(st, at) \u2265Q\u03c0(st, at) for all \u03c0 \u2208\u03a0 and (st, at) \u2208S \u00d7 A, assuming |A| < \u221e. Let \u03c0i be the policy at iteration i.",
    "chunk_index": 11,
    "num_sentences": 3,
    "chunk_size": 278,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 11
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nBy Lemma 2, the sequence Q\u03c0i is monotonically increasing. Since Q\u03c0 is bounded\nabove for \u03c0 \u2208\u03a0 (both the reward and entropy are bounded), the sequence converges to some \u03c0\u2217.",
    "chunk_index": 12,
    "num_sentences": 2,
    "chunk_size": 170,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 12
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nWe will still need to\nshow that \u03c0\u2217is indeed optimal. At convergence, it must be case that J\u03c0\u2217(\u03c0\u2217( \u00b7 |st)) < J\u03c0\u2217(\u03c0( \u00b7 |st)) for all \u03c0 \u2208\u03a0, \u03c0 \u0338= \u03c0\u2217. Using the same iterative argument as in the proof of Lemma 2, we get Q\u03c0\u2217(st, at) > Q\u03c0(st, at) for all (st, at) \u2208S \u00d7 A,\nthat is, the soft value of any other policy in \u03a0 is lower than that of the converged policy. Hence \u03c0\u2217is optimal in \u03a0.",
    "chunk_index": 13,
    "num_sentences": 4,
    "chunk_size": 382,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 13
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nEnforcing Action Bounds\nWe use an unbounded Gaussian as the action distribution.",
    "chunk_index": 14,
    "num_sentences": 1,
    "chunk_size": 80,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 14
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nHowever, in practice, the actions needs to be bounded to a \ufb01nite\ninterval.",
    "chunk_index": 15,
    "num_sentences": 1,
    "chunk_size": 74,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 15
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nTo that end, we apply an invertible squashing function (tanh) to the Gaussian samples, and employ the change of\nvariables formula to compute the likelihoods of the bounded actions.",
    "chunk_index": 16,
    "num_sentences": 1,
    "chunk_size": 180,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 16
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\nIn the other words, let u \u2208RD be a random variable\nand \u00b5(u|s) the corresponding density with in\ufb01nite support. Then a = tanh(u), where tanh is applied elementwise, is a\nrandom variable with support in (\u22121, 1) with a density given by\n\u03c0(a|s) = \u00b5(u|s)\n\f\f\f\fdet\n\u0012 da\ndu\n\u0013\f\f\f\f\n\u22121\n.",
    "chunk_index": 17,
    "num_sentences": 2,
    "chunk_size": 274,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 17
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from\n\n(20)\nSince the Jacobian da/du = diag(1 \u2212tanh2(u)) is diagonal, the log-likelihood has a simple form\nlog \u03c0(a|s) = log \u00b5(u|s) \u2212",
    "chunk_index": 18,
    "num_sentences": 1,
    "chunk_size": 125,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "This objective corresponds to maximizing the discounted expected reward and entropy for future states originating from",
      "section_index": 22,
      "chunk_index": 18
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: D\nX\n\ni=1\nlog\n\u00001 \u2212tanh2(ui)\n\u0001\n,\n(21)\nwhere ui is the ith element of u.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 64,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "D\nX",
      "section_index": 23,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: D\nX\n\nSoft Actor-Critic\nD.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 20,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "D\nX",
      "section_index": 23,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: D\nX\n\nHyperparameters\nTable 1 lists the common SAC parameters used in the comparative evaluation in Figure 1 and Figure 4.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 116,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "D\nX",
      "section_index": 23,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: D\nX\n\nTable 2 lists the\nreward scale parameter that was tuned for each environment.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 77,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "D\nX",
      "section_index": 23,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: D\nX\n\nSAC Hyperparameters",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 19,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "D\nX",
      "section_index": 23,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Shared\n\noptimizer\nAdam (Kingma & Ba, 2015)\nlearning rate\n3 \u00b7 10\u22124\ndiscount (\u03b3)\n0.99\nreplay buffer size\n106\nnumber of hidden layers (all networks)\n2\nnumber of hidden units per layer\n256\nnumber of samples per minibatch\n256\nnonlinearity",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 225,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Shared",
      "section_index": 24,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SAC\n\ntarget smoothing coef\ufb01cient (\u03c4)\n0.005\ntarget update interval\n1\ngradient steps\n1\nSAC (hard target update)\ntarget smoothing coef\ufb01cient (\u03c4)\n1\ntarget update interval\n1000\ngradient steps (except humanoids)\n4\ngradient steps (humanoids)\n1\nTable 2.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 240,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SAC",
      "section_index": 25,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SAC\n\nSAC Environment Speci\ufb01c Parameters",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 34,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SAC",
      "section_index": 25,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\nHopper-v1\n3\n5\nWalker2d-v1\n6\n5\nHalfCheetah-v1\n6\n5\nAnt-v1\n8\n5\nHumanoid-v1\n17\n20\nHumanoid (rllab)\n21\n10\n\n\nSoft Actor-Critic\nE.",
    "chunk_index": 0,
    "num_sentences": 1,
    "chunk_size": 123,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\nAdditional Baseline Results\nFigure 4 compares SAC to Trust-PCL (Figure 4.",
    "chunk_index": 1,
    "num_sentences": 1,
    "chunk_size": 73,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\nTrust-PC fails to solve most of the task within the given number of\nenvironment steps, although it can eventually solve the easier tasks (Nachum et al., 2017b) if ran longer.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 174,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\nThe \ufb01gure also\nincludes two variants of SAC: a variant that periodically copies the target value network weights directly instead of using\nexponentially moving average, and a deterministic ablation which assumes a deterministic policy in the value update\n(Equation 6) and the policy update (Equation 13), and thus strongly resembles DDPG with the exception of having two\nQ-functions, using hard target updates, not having a separate target actor, and using \ufb01xed exploration noise rather than\nlearned.",
    "chunk_index": 3,
    "num_sentences": 1,
    "chunk_size": 500,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 3
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\nBoth of these methods can learn all of the tasks and they perform comparably to SAC on all but Humanoid (rllab)\ntask, on which SAC is the fastest.",
    "chunk_index": 4,
    "num_sentences": 1,
    "chunk_size": 146,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 4
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: Reward Scale\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\naverage return\nHopper-v1\n(a) Hopper-v1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmillion steps\n0\n1000\n2000\n3000\n4000\n5000\n6000\naverage return\nWalker2d-v1\n(b) Walker2d-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n5000\n10000\n15000\naverage return\nHalfCheetah-v1\n(c) HalfCheetah-v1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nmillion steps\n0\n2000\n4000\n6000\naverage return\nAnt-v1\n(d) Ant-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\n8000\naverage return\nHumanoid-v1\n(e) Humanoid-v1\n0\n2\n4\n6\n8\n10\nmillion steps\n0\n2000\n4000\n6000\naverage return\nHumanoid (rllab)",
    "chunk_index": 5,
    "num_sentences": 1,
    "chunk_size": 581,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "Reward Scale",
      "section_index": 26,
      "chunk_index": 5
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SAC\n\nSAC (hard target update)\nSAC (hard target update, deterministic)\nTrust-PCL\n(f) Humanoid (rllab)\nFigure 4. Training curves for additional baseline (Trust-PCL) and for two SAC variants.",
    "chunk_index": 0,
    "num_sentences": 2,
    "chunk_size": 183,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SAC",
      "section_index": 27,
      "chunk_index": 0
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SAC\n\nSoft actor-critic with hard target update (blue)\ndiffers from standard SAC in that it copies the value function network weights directly every 1000 iterations, instead of using exponentially\nsmoothed average of the weights. The deterministic ablation (red) uses a deterministic policy with \ufb01xed Gaussian exploration noise,\ndoes not use a value function, drops the entropy terms in the actor and critic function updates, and uses hard target updates for the target\nQ-functions.",
    "chunk_index": 1,
    "num_sentences": 2,
    "chunk_size": 476,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SAC",
      "section_index": 27,
      "chunk_index": 1
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  },
  {
    "text": "Title: Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\nSection: SAC\n\nIt is equivalent to DDPG that uses two Q-functions, hard target updates, and removes the target actor.",
    "chunk_index": 2,
    "num_sentences": 1,
    "chunk_size": 102,
    "metadata": {
      "title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "section_header": "SAC",
      "section_index": 27,
      "chunk_index": 2
    },
    "paper_title": "Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "pdf_path": "data/papers/1801.01290v2.pdf"
  }
]